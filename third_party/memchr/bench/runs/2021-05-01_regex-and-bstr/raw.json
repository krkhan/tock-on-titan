{
  "name": "2021-05-01",
  "benchmarks": {
    "memchr1/fallback/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/fallback/empty/never",
        "directory_name": "memchr1/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.320866426256059,
            "upper_bound": 2.3624086410666916
          },
          "point_estimate": 2.343639241751144,
          "standard_error": 0.010718133925291296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.321166373731038,
            "upper_bound": 2.370110601315853
          },
          "point_estimate": 2.35526496468151,
          "standard_error": 0.011774711560830651
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001492102153984729,
            "upper_bound": 0.05571144162750319
          },
          "point_estimate": 0.022639287194686032,
          "standard_error": 0.013267254370471816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.325416394428523,
            "upper_bound": 2.366536608899155
          },
          "point_estimate": 2.3525590250433295,
          "standard_error": 0.010769066919835538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010264764356985373,
            "upper_bound": 0.0468067055153909
          },
          "point_estimate": 0.03577991270806395,
          "standard_error": 0.0094085052678817
        }
      }
    },
    "memchr1/fallback/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/common",
        "directory_name": "memchr1/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 643096.4067531851,
            "upper_bound": 644508.8814745195
          },
          "point_estimate": 643784.4131558063,
          "standard_error": 361.0628817798243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642943.1127819549,
            "upper_bound": 644519.5515350876
          },
          "point_estimate": 643798.5122807018,
          "standard_error": 383.52589047873784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.5938096508835,
            "upper_bound": 2050.223266890828
          },
          "point_estimate": 1035.195086007611,
          "standard_error": 474.3173651581878
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642973.1675741076,
            "upper_bound": 644118.962037863
          },
          "point_estimate": 643659.8949191159,
          "standard_error": 294.1283079531415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 620.4472363647824,
            "upper_bound": 1584.1110940851793
          },
          "point_estimate": 1207.6582619249557,
          "standard_error": 251.39141014929552
        }
      }
    },
    "memchr1/fallback/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/never",
        "directory_name": "memchr1/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31952.282105968592,
            "upper_bound": 31982.026120560993
          },
          "point_estimate": 31966.85324319329,
          "standard_error": 7.650242635295355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31949.0532262114,
            "upper_bound": 31998.727592267136
          },
          "point_estimate": 31960.527833919157,
          "standard_error": 11.298893224539402
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1287679848760988,
            "upper_bound": 48.92537571751937
          },
          "point_estimate": 20.36484183309102,
          "standard_error": 11.654575111403274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31954.09179059078,
            "upper_bound": 31990.297619595323
          },
          "point_estimate": 31970.80953598247,
          "standard_error": 9.273520213955631
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.335385946488316,
            "upper_bound": 31.321249320212218
          },
          "point_estimate": 25.58839908858384,
          "standard_error": 4.24225488322582
        }
      }
    },
    "memchr1/fallback/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/rare",
        "directory_name": "memchr1/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32645.903343058053,
            "upper_bound": 32726.264171342013
          },
          "point_estimate": 32680.341673399165,
          "standard_error": 20.91861034288507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32636.231597845603,
            "upper_bound": 32697.586288150807
          },
          "point_estimate": 32666.042130460803,
          "standard_error": 15.916890868041284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.327234128941202,
            "upper_bound": 82.88026646394633
          },
          "point_estimate": 43.88065604322382,
          "standard_error": 18.94505719385786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32626.84087746286,
            "upper_bound": 32661.370048512166
          },
          "point_estimate": 32640.164116673273,
          "standard_error": 8.778798826451808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.902067891380124,
            "upper_bound": 101.26724830205109
          },
          "point_estimate": 70.00418035725585,
          "standard_error": 24.076337878942905
        }
      }
    },
    "memchr1/fallback/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/uncommon",
        "directory_name": "memchr1/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152173.6615250299,
            "upper_bound": 152376.57083149036
          },
          "point_estimate": 152278.86685229462,
          "standard_error": 51.61201172742357
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152183.09170153417,
            "upper_bound": 152404.20621637776
          },
          "point_estimate": 152269.91527196654,
          "standard_error": 47.31109666163204
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.98286267551695,
            "upper_bound": 292.21642263636306
          },
          "point_estimate": 117.03412009793811,
          "standard_error": 72.38767558755208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152218.80759383002,
            "upper_bound": 152432.6654504366
          },
          "point_estimate": 152337.7580285823,
          "standard_error": 54.52502684190348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.61764968644167,
            "upper_bound": 232.18647553039665
          },
          "point_estimate": 171.8931951924668,
          "standard_error": 40.20202605814093
        }
      }
    },
    "memchr1/fallback/huge/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/verycommon",
        "directory_name": "memchr1/fallback_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1139349.3628472222,
            "upper_bound": 1144770.3455562063
          },
          "point_estimate": 1141867.916987847,
          "standard_error": 1390.3010806669397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1138624.175,
            "upper_bound": 1144066.2109375
          },
          "point_estimate": 1141562.306640625,
          "standard_error": 1286.0670260418
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 813.4454636835137,
            "upper_bound": 7339.185501344036
          },
          "point_estimate": 3456.1454257245286,
          "standard_error": 1817.9883866140872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1139051.3991964285,
            "upper_bound": 1142337.7118440345
          },
          "point_estimate": 1140710.204788961,
          "standard_error": 856.8818544831811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2096.679621445168,
            "upper_bound": 6353.6466664772715
          },
          "point_estimate": 4650.061530141169,
          "standard_error": 1185.1477669111728
        }
      }
    },
    "memchr1/fallback/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/common",
        "directory_name": "memchr1/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.40064426773654,
            "upper_bound": 268.70044115063826
          },
          "point_estimate": 267.02716698680916,
          "standard_error": 0.8450551900906075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264.6234248329865,
            "upper_bound": 270.15924578281687
          },
          "point_estimate": 265.4332426402453,
          "standard_error": 1.870269483029113
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17642293048453211,
            "upper_bound": 4.087416300001529
          },
          "point_estimate": 1.7383022579947829,
          "standard_error": 1.208845629341762
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.08469694548666,
            "upper_bound": 269.22206022505424
          },
          "point_estimate": 266.5497588196367,
          "standard_error": 1.03887208994909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7577255354254415,
            "upper_bound": 3.0470015690826533
          },
          "point_estimate": 2.814337770134274,
          "standard_error": 0.3295610289697554
        }
      }
    },
    "memchr1/fallback/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/never",
        "directory_name": "memchr1/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.571525916321505,
            "upper_bound": 39.97291105356544
          },
          "point_estimate": 39.80042099339923,
          "standard_error": 0.10426630849723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.71134530093751,
            "upper_bound": 40.03205346506592
          },
          "point_estimate": 39.83605529993372,
          "standard_error": 0.07964196686173942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.044363121912357334,
            "upper_bound": 0.4000676724225114
          },
          "point_estimate": 0.22125703538797284,
          "standard_error": 0.09449003209632556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.7794214801328,
            "upper_bound": 40.00689075681525
          },
          "point_estimate": 39.928986003523114,
          "standard_error": 0.057400230066622
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11586361149641897,
            "upper_bound": 0.5043072268778884
          },
          "point_estimate": 0.3472138940376189,
          "standard_error": 0.11966221173105566
        }
      }
    },
    "memchr1/fallback/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/rare",
        "directory_name": "memchr1/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.75172474777487,
            "upper_bound": 48.34716680692012
          },
          "point_estimate": 48.055636415005395,
          "standard_error": 0.1522969926490166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.63422619047619,
            "upper_bound": 48.475930705060726
          },
          "point_estimate": 48.05944629321501,
          "standard_error": 0.20318198259284315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1150646845905802,
            "upper_bound": 0.8649014131173093
          },
          "point_estimate": 0.6239555455840905,
          "standard_error": 0.19814217597392425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.6679601471443,
            "upper_bound": 48.246951368158186
          },
          "point_estimate": 47.98584827527542,
          "standard_error": 0.1455791631242992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2924979368999811,
            "upper_bound": 0.6373625319988394
          },
          "point_estimate": 0.507297161262892,
          "standard_error": 0.08935414204084903
        }
      }
    },
    "memchr1/fallback/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/uncommon",
        "directory_name": "memchr1/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.37003045763727,
            "upper_bound": 79.5265568959082
          },
          "point_estimate": 79.44784119351652,
          "standard_error": 0.040209147188693534
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.34455232277969,
            "upper_bound": 79.57821680734608
          },
          "point_estimate": 79.42928276999626,
          "standard_error": 0.06283028985852188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.032756659665095736,
            "upper_bound": 0.22080959985468615
          },
          "point_estimate": 0.14099113487412232,
          "standard_error": 0.049704077176558656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.36166476612003,
            "upper_bound": 79.51583178387101
          },
          "point_estimate": 79.4337095412354,
          "standard_error": 0.04043033024228787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08034735079368939,
            "upper_bound": 0.16262457931474888
          },
          "point_estimate": 0.13379328360441128,
          "standard_error": 0.020810530116327365
        }
      }
    },
    "memchr1/fallback/small/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/verycommon",
        "directory_name": "memchr1/fallback_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497.4243151636958,
            "upper_bound": 497.94974540313353
          },
          "point_estimate": 497.7135570997492,
          "standard_error": 0.13509467891438068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497.5184005418789,
            "upper_bound": 498.0407995002265
          },
          "point_estimate": 497.7251052309491,
          "standard_error": 0.12749379913718126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01981726397208374,
            "upper_bound": 0.651458088471402
          },
          "point_estimate": 0.3154187719461177,
          "standard_error": 0.14292942726094662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497.60974529330616,
            "upper_bound": 498.0290973919099
          },
          "point_estimate": 497.8660022278327,
          "standard_error": 0.10851788832348956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18193194310776525,
            "upper_bound": 0.6374760447674519
          },
          "point_estimate": 0.45051247422651847,
          "standard_error": 0.13529928744272826
        }
      }
    },
    "memchr1/fallback/tiny/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/common",
        "directory_name": "memchr1/fallback_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.92132774473461,
            "upper_bound": 49.98388349197136
          },
          "point_estimate": 49.95255233306265,
          "standard_error": 0.01597049090419991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.910143769418774,
            "upper_bound": 50.00326964365045
          },
          "point_estimate": 49.951521779580546,
          "standard_error": 0.022543747199145273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015539424178812327,
            "upper_bound": 0.09175265431610946
          },
          "point_estimate": 0.06903420934233793,
          "standard_error": 0.019752045263690517
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.92257372751096,
            "upper_bound": 49.97944134444354
          },
          "point_estimate": 49.95045051940496,
          "standard_error": 0.014801446626214562
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03144493365252852,
            "upper_bound": 0.06648923453956161
          },
          "point_estimate": 0.053336058547903936,
          "standard_error": 0.008982536493810001
        }
      }
    },
    "memchr1/fallback/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/never",
        "directory_name": "memchr1/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.110767658862093,
            "upper_bound": 6.114666808566579
          },
          "point_estimate": 6.112786686602687,
          "standard_error": 0.0009955074938757468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.110098531791394,
            "upper_bound": 6.115019589705294
          },
          "point_estimate": 6.1138761966872845,
          "standard_error": 0.0014254075608397725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003526090572162053,
            "upper_bound": 0.0054991109970488495
          },
          "point_estimate": 0.0023821941115434765,
          "standard_error": 0.0014497263286057582
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.110111242100388,
            "upper_bound": 6.11424741012776
          },
          "point_estimate": 6.112114172414693,
          "standard_error": 0.001067492497599848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017007256426026791,
            "upper_bound": 0.004132086750215049
          },
          "point_estimate": 0.003307327201826862,
          "standard_error": 0.00059732677939792
        }
      }
    },
    "memchr1/fallback/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/rare",
        "directory_name": "memchr1/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.294863046450995,
            "upper_bound": 9.304259793932593
          },
          "point_estimate": 9.29933499023682,
          "standard_error": 0.0024091716246741105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.29361991661964,
            "upper_bound": 9.30686489985242
          },
          "point_estimate": 9.29615645505623,
          "standard_error": 0.0034493675007537653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007339525372074138,
            "upper_bound": 0.012965154681851996
          },
          "point_estimate": 0.007496706805290993,
          "standard_error": 0.003360120499773982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.294363506680972,
            "upper_bound": 9.301459345285352
          },
          "point_estimate": 9.297103860284864,
          "standard_error": 0.0017894759257408165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004148810821646713,
            "upper_bound": 0.010163822568300312
          },
          "point_estimate": 0.008000066645878264,
          "standard_error": 0.0015618632955874743
        }
      }
    },
    "memchr1/fallback/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/uncommon",
        "directory_name": "memchr1/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.224627792558906,
            "upper_bound": 35.30185738353792
          },
          "point_estimate": 35.2599167302688,
          "standard_error": 0.01979666989180008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.20472565753578,
            "upper_bound": 35.29222860577556
          },
          "point_estimate": 35.256181025009,
          "standard_error": 0.02548495540937293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011995049801176324,
            "upper_bound": 0.10789167569400832
          },
          "point_estimate": 0.0678670770077304,
          "standard_error": 0.024013677054965973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.218259562547985,
            "upper_bound": 35.27279342830438
          },
          "point_estimate": 35.2444646253691,
          "standard_error": 0.014135390182785947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03148851726353635,
            "upper_bound": 0.08954126528460751
          },
          "point_estimate": 0.06583608083291295,
          "standard_error": 0.016734324010358587
        }
      }
    },
    "memchr1/krate/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/krate/empty/never",
        "directory_name": "memchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24445144091614335,
            "upper_bound": 0.2447213099568243
          },
          "point_estimate": 0.2445910297898828,
          "standard_error": 0.000069205636058474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24441602819255684,
            "upper_bound": 0.24479824979136589
          },
          "point_estimate": 0.2445952058140365,
          "standard_error": 0.00009771457844545991
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00005708380717974886,
            "upper_bound": 0.00040186080725253834
          },
          "point_estimate": 0.00024571022107317935,
          "standard_error": 0.00009031101888681697
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24435427911950297,
            "upper_bound": 0.24461197843636437
          },
          "point_estimate": 0.2444926327646488,
          "standard_error": 0.00006611573671905996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00012600598069727873,
            "upper_bound": 0.00028760685848817463
          },
          "point_estimate": 0.0002307401847554508,
          "standard_error": 0.000041460629858766935
        }
      }
    },
    "memchr1/krate/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/common",
        "directory_name": "memchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219883.70575707592,
            "upper_bound": 220326.5536696787
          },
          "point_estimate": 220099.4853927615,
          "standard_error": 113.58645709985385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219787.40227576977,
            "upper_bound": 220357.11777108433
          },
          "point_estimate": 220091.617383821,
          "standard_error": 138.6446002648443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.43919584908836,
            "upper_bound": 655.9991332331983
          },
          "point_estimate": 354.5715614008703,
          "standard_error": 145.2791190388776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219941.520093532,
            "upper_bound": 220184.1563800909
          },
          "point_estimate": 220044.2916914411,
          "standard_error": 60.49322226116964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.24473587068664,
            "upper_bound": 485.8317595231053
          },
          "point_estimate": 379.1763276640976,
          "standard_error": 71.91868334470459
        }
      }
    },
    "memchr1/krate/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/never",
        "directory_name": "memchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8905.870170122227,
            "upper_bound": 8916.494110480216
          },
          "point_estimate": 8910.355711824403,
          "standard_error": 2.77913198506668
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8904.162662417259,
            "upper_bound": 8912.818021165318
          },
          "point_estimate": 8908.503893587753,
          "standard_error": 2.3828804436157203
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1157588579805138,
            "upper_bound": 10.313498655462624
          },
          "point_estimate": 4.629310631223452,
          "standard_error": 2.2688350891803855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8904.265672607376,
            "upper_bound": 8909.776628828866
          },
          "point_estimate": 8906.666146846534,
          "standard_error": 1.4199930190903307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.989498458161136,
            "upper_bound": 13.594161690077772
          },
          "point_estimate": 9.222817417651182,
          "standard_error": 3.2972919934250813
        }
      }
    },
    "memchr1/krate/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/rare",
        "directory_name": "memchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9334.341236993514,
            "upper_bound": 9347.525280481086
          },
          "point_estimate": 9340.847174225732,
          "standard_error": 3.3577604423646727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9335.297579263068,
            "upper_bound": 9345.064942159384
          },
          "point_estimate": 9341.81800771208,
          "standard_error": 2.6404676266820655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2641499203608997,
            "upper_bound": 18.04951128624142
          },
          "point_estimate": 5.081416080096445,
          "standard_error": 4.140597138312936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9334.647733827062,
            "upper_bound": 9343.1446918641
          },
          "point_estimate": 9339.116835041565,
          "standard_error": 2.2036793159895045
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.79271824247038,
            "upper_bound": 15.64334775805664
          },
          "point_estimate": 11.191721208960017,
          "standard_error": 3.0361657011308165
        }
      }
    },
    "memchr1/krate/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/uncommon",
        "directory_name": "memchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76574.11594233474,
            "upper_bound": 76749.381520218
          },
          "point_estimate": 76654.88695733708,
          "standard_error": 44.883556162312466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76562.79602672292,
            "upper_bound": 76752.33597046413
          },
          "point_estimate": 76613.84733942803,
          "standard_error": 43.51856430626648
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.357823077052448,
            "upper_bound": 228.31101240236688
          },
          "point_estimate": 103.88634664369826,
          "standard_error": 54.53706946517742
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76581.12332981716,
            "upper_bound": 76708.54884646795
          },
          "point_estimate": 76635.30439476136,
          "standard_error": 32.65258828810596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.75644858046341,
            "upper_bound": 201.2164529859184
          },
          "point_estimate": 149.5620571548403,
          "standard_error": 38.35621927197694
        }
      }
    },
    "memchr1/krate/huge/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/verycommon",
        "directory_name": "memchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459262.7374843007,
            "upper_bound": 460605.1723732887
          },
          "point_estimate": 459818.7735565476,
          "standard_error": 356.311960399826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459139.675,
            "upper_bound": 460079.91041666665
          },
          "point_estimate": 459369.7511160714,
          "standard_error": 279.6009666755701
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.1002446823081,
            "upper_bound": 1134.80333222816
          },
          "point_estimate": 461.8098149262686,
          "standard_error": 298.7930660448518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459252.3153175996,
            "upper_bound": 459829.5208168514
          },
          "point_estimate": 459581.93386363634,
          "standard_error": 149.3842268801284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.99280640983994,
            "upper_bound": 1765.2485207549996
          },
          "point_estimate": 1190.6730955881842,
          "standard_error": 459.8566169532965
        }
      }
    },
    "memchr1/krate/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/common",
        "directory_name": "memchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.60074157864997,
            "upper_bound": 204.86139654059275
          },
          "point_estimate": 204.7300719658516,
          "standard_error": 0.0667884408591013
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.52470365529365,
            "upper_bound": 204.8993899862532
          },
          "point_estimate": 204.7299145708448,
          "standard_error": 0.08451897748423715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04067719544000319,
            "upper_bound": 0.4044989312454833
          },
          "point_estimate": 0.24036807436535992,
          "standard_error": 0.0968834482118068
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.51701591192207,
            "upper_bound": 204.7787805013539
          },
          "point_estimate": 204.64230001305316,
          "standard_error": 0.06661578565928043
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12776972619356092,
            "upper_bound": 0.2793064426872109
          },
          "point_estimate": 0.2223388410287059,
          "standard_error": 0.03926557765834259
        }
      }
    },
    "memchr1/krate/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/never",
        "directory_name": "memchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.550686406911427,
            "upper_bound": 7.580082153464664
          },
          "point_estimate": 7.564397180580104,
          "standard_error": 0.007527318285385697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.550607418463583,
            "upper_bound": 7.576701021580941
          },
          "point_estimate": 7.560909530811395,
          "standard_error": 0.006497251270040532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00290356377433464,
            "upper_bound": 0.03983988037616271
          },
          "point_estimate": 0.012443181897565495,
          "standard_error": 0.00910006051484352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.55008928413423,
            "upper_bound": 7.567646987325952
          },
          "point_estimate": 7.559926704698325,
          "standard_error": 0.004488698245040357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008646980739954749,
            "upper_bound": 0.034812961364107275
          },
          "point_estimate": 0.025156267015325556,
          "standard_error": 0.0068458245346562875
        }
      }
    },
    "memchr1/krate/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/rare",
        "directory_name": "memchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.14245840927627,
            "upper_bound": 12.189767951455316
          },
          "point_estimate": 12.166238219651934,
          "standard_error": 0.012080905394024417
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.136239446600356,
            "upper_bound": 12.191279502280514
          },
          "point_estimate": 12.170247187969665,
          "standard_error": 0.015052227349287682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007005273625201972,
            "upper_bound": 0.07230821792900495
          },
          "point_estimate": 0.0324982500954361,
          "standard_error": 0.016655301002197618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.149289305191706,
            "upper_bound": 12.184769610902316
          },
          "point_estimate": 12.17157710052324,
          "standard_error": 0.009088046659580072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02152028548534792,
            "upper_bound": 0.052210311803868134
          },
          "point_estimate": 0.040343257188317216,
          "standard_error": 0.007876549027876075
        }
      }
    },
    "memchr1/krate/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/uncommon",
        "directory_name": "memchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.96204635772636,
            "upper_bound": 46.05302650832057
          },
          "point_estimate": 46.00756703272834,
          "standard_error": 0.023339124185082757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.93768879790668,
            "upper_bound": 46.07180608455453
          },
          "point_estimate": 46.017680597721125,
          "standard_error": 0.03444900152010245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016338406413465813,
            "upper_bound": 0.13363438887107854
          },
          "point_estimate": 0.10225743388593664,
          "standard_error": 0.03193797237349594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.95478798520327,
            "upper_bound": 46.034905170175186
          },
          "point_estimate": 45.99524873170908,
          "standard_error": 0.020730816711714072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.046065519677229265,
            "upper_bound": 0.09656326144570078
          },
          "point_estimate": 0.07780120384490169,
          "standard_error": 0.012974692810857148
        }
      }
    },
    "memchr1/krate/small/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/verycommon",
        "directory_name": "memchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.2810233545523,
            "upper_bound": 495.7206232980787
          },
          "point_estimate": 495.4947183366166,
          "standard_error": 0.11293094094304976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.0943232810944,
            "upper_bound": 495.85382198762454
          },
          "point_estimate": 495.41600067417613,
          "standard_error": 0.18974207055381345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02024958125186352,
            "upper_bound": 0.6193859307361638
          },
          "point_estimate": 0.4820071989797986,
          "standard_error": 0.14786537349683268
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.4436612722059,
            "upper_bound": 495.8169945099163
          },
          "point_estimate": 495.6314256770413,
          "standard_error": 0.09482753622257506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22890359254924056,
            "upper_bound": 0.4494895797903922
          },
          "point_estimate": 0.3762615445625064,
          "standard_error": 0.056344556959291775
        }
      }
    },
    "memchr1/krate/tiny/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/common",
        "directory_name": "memchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.88321295327368,
            "upper_bound": 51.96839522192791
          },
          "point_estimate": 51.921740987584805,
          "standard_error": 0.02188502984527358
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.86823479490267,
            "upper_bound": 51.95884258017129
          },
          "point_estimate": 51.9113381572638,
          "standard_error": 0.026281349232550495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01596786586047164,
            "upper_bound": 0.11240299535141676
          },
          "point_estimate": 0.06604198307773651,
          "standard_error": 0.02357457554071384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.86391575388982,
            "upper_bound": 51.94237407046083
          },
          "point_estimate": 51.89717676644528,
          "standard_error": 0.020233485372404544
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03307814975255512,
            "upper_bound": 0.10082249316065071
          },
          "point_estimate": 0.07288839659702716,
          "standard_error": 0.01978262821946412
        }
      }
    },
    "memchr1/krate/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/never",
        "directory_name": "memchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.668202660857412,
            "upper_bound": 3.6734941586321663
          },
          "point_estimate": 3.67081840579054,
          "standard_error": 0.0013543864785231137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6669030269599014,
            "upper_bound": 3.675493468905064
          },
          "point_estimate": 3.6697510120158943,
          "standard_error": 0.0022931698006763623
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011046535700839364,
            "upper_bound": 0.00725950740911503
          },
          "point_estimate": 0.005840995213573878,
          "standard_error": 0.0017398846875209906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.668209756927566,
            "upper_bound": 3.673109259216539
          },
          "point_estimate": 3.670323034782729,
          "standard_error": 0.0012478417174331278
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027986347631445037,
            "upper_bound": 0.005418528968227905
          },
          "point_estimate": 0.004516458686352367,
          "standard_error": 0.0006674743304949186
        }
      }
    },
    "memchr1/krate/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/rare",
        "directory_name": "memchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.851575631834534,
            "upper_bound": 5.9076611346705965
          },
          "point_estimate": 5.876811363683293,
          "standard_error": 0.014503117679628112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.849655713644639,
            "upper_bound": 5.901318974672159
          },
          "point_estimate": 5.856234894202674,
          "standard_error": 0.0140653633236698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001965798554115797,
            "upper_bound": 0.07173283486946636
          },
          "point_estimate": 0.012679672720139552,
          "standard_error": 0.018835612716349877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.845948899833303,
            "upper_bound": 5.861546810111312
          },
          "point_estimate": 5.853137595161202,
          "standard_error": 0.003935017951305135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012686127320909586,
            "upper_bound": 0.06509110646227852
          },
          "point_estimate": 0.048387209799079525,
          "standard_error": 0.013405850780721956
        }
      }
    },
    "memchr1/krate/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/uncommon",
        "directory_name": "memchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.9314899317393,
            "upper_bound": 17.946127975425572
          },
          "point_estimate": 17.938787957656565,
          "standard_error": 0.0037478455883121617
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.92564692116483,
            "upper_bound": 17.950711555318993
          },
          "point_estimate": 17.939940855000962,
          "standard_error": 0.006970624128568268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017656886670842423,
            "upper_bound": 0.02059686709152368
          },
          "point_estimate": 0.01864230440630726,
          "standard_error": 0.005411168875512794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.92940274581003,
            "upper_bound": 17.950206819951735
          },
          "point_estimate": 17.940290492482283,
          "standard_error": 0.005476632087151317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008251871373657663,
            "upper_bound": 0.014454835926811294
          },
          "point_estimate": 0.01251600443780632,
          "standard_error": 0.0016007391073526167
        }
      }
    },
    "memchr1/libc/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/libc/empty/never",
        "directory_name": "memchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.595970465611674,
            "upper_bound": 2.6169224421908233
          },
          "point_estimate": 2.6059568394752306,
          "standard_error": 0.005371513895744575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5909932430152702,
            "upper_bound": 2.6177566813564703
          },
          "point_estimate": 2.604368988475052,
          "standard_error": 0.008006781942117236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003292704070458804,
            "upper_bound": 0.030727840295026787
          },
          "point_estimate": 0.018166915466971023,
          "standard_error": 0.00694969695604971
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.599702249013328,
            "upper_bound": 2.613874523415564
          },
          "point_estimate": 2.6081630986072852,
          "standard_error": 0.003616409940385276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009639532678239976,
            "upper_bound": 0.02290672683662808
          },
          "point_estimate": 0.0178649922025004,
          "standard_error": 0.0035525317894039505
        }
      }
    },
    "memchr1/libc/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/common",
        "directory_name": "memchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282978.9974790513,
            "upper_bound": 283847.44346253236
          },
          "point_estimate": 283364.3283277963,
          "standard_error": 222.9975162741966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282924.577002584,
            "upper_bound": 283558.6522702104
          },
          "point_estimate": 283310.33527131786,
          "standard_error": 161.9681054528421
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.40717893283124,
            "upper_bound": 932.7603423549212
          },
          "point_estimate": 398.2139568837582,
          "standard_error": 205.45570824538555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282694.34958485677,
            "upper_bound": 283410.94349633035
          },
          "point_estimate": 282994.0532568207,
          "standard_error": 183.44587514684488
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.90325857613593,
            "upper_bound": 1073.1488035035677
          },
          "point_estimate": 743.0401070126234,
          "standard_error": 243.10121418331653
        }
      }
    },
    "memchr1/libc/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/never",
        "directory_name": "memchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8319.506846877453,
            "upper_bound": 8326.523792841335
          },
          "point_estimate": 8323.151326875835,
          "standard_error": 1.7975620955277407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8318.972495675622,
            "upper_bound": 8328.894127747251
          },
          "point_estimate": 8323.335998822606,
          "standard_error": 2.959058301091104
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0478159910128542,
            "upper_bound": 11.117255838954748
          },
          "point_estimate": 7.394877100858049,
          "standard_error": 2.5827719888452907
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8317.824846214608,
            "upper_bound": 8327.043520569225
          },
          "point_estimate": 8322.395069216498,
          "standard_error": 2.43241734263711
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.52081764293809,
            "upper_bound": 7.610320163006785
          },
          "point_estimate": 5.9948059015254405,
          "standard_error": 1.1008196545635225
        }
      }
    },
    "memchr1/libc/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/rare",
        "directory_name": "memchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9950.209874736356,
            "upper_bound": 9971.327179049793
          },
          "point_estimate": 9959.986625625135,
          "standard_error": 5.406548600713422
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9945.725753424658,
            "upper_bound": 9974.067397260274
          },
          "point_estimate": 9956.254522450534,
          "standard_error": 6.128014823719345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.797874323616424,
            "upper_bound": 29.756858605272836
          },
          "point_estimate": 15.742533675537864,
          "standard_error": 7.082219333652049
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9947.126775538716,
            "upper_bound": 9956.914368178155
          },
          "point_estimate": 9951.7110663583,
          "standard_error": 2.5364941001668937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.979759800122792,
            "upper_bound": 23.95028821485304
          },
          "point_estimate": 17.919877739073158,
          "standard_error": 4.302016538759243
        }
      }
    },
    "memchr1/libc/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/uncommon",
        "directory_name": "memchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73604.61263136289,
            "upper_bound": 73687.74321501014
          },
          "point_estimate": 73643.77944299558,
          "standard_error": 21.420663226432307
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73583.45298625197,
            "upper_bound": 73703.55603448275
          },
          "point_estimate": 73637.51090263692,
          "standard_error": 25.001429816870317
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.072647520634897,
            "upper_bound": 127.35586401687657
          },
          "point_estimate": 57.933281354846265,
          "standard_error": 29.749770255529757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73595.61712016846,
            "upper_bound": 73684.81142329011
          },
          "point_estimate": 73636.34558099102,
          "standard_error": 22.768622030069523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.946480938684616,
            "upper_bound": 90.08305563547546
          },
          "point_estimate": 71.28290323353386,
          "standard_error": 14.898914653680423
        }
      }
    },
    "memchr1/libc/huge/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/verycommon",
        "directory_name": "memchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 591314.2548319892,
            "upper_bound": 591953.527047715
          },
          "point_estimate": 591654.4884242192,
          "standard_error": 164.44839653080248
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 591159.4225806452,
            "upper_bound": 592076.3366935484
          },
          "point_estimate": 591800.92281106,
          "standard_error": 222.9714274878446
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.56256484078192,
            "upper_bound": 902.0678937164676
          },
          "point_estimate": 402.7437659682044,
          "standard_error": 222.4626559444755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 591114.4516972891,
            "upper_bound": 591826.9802236991
          },
          "point_estimate": 591466.5979053205,
          "standard_error": 178.58777797102863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232.82239925264273,
            "upper_bound": 679.4407876664674
          },
          "point_estimate": 547.9668799358019,
          "standard_error": 115.97682297738388
        }
      }
    },
    "memchr1/libc/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/common",
        "directory_name": "memchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.4290384115436,
            "upper_bound": 219.64578727169564
          },
          "point_estimate": 219.5365435228613,
          "standard_error": 0.05532859241970024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.3648631160082,
            "upper_bound": 219.6807618575342
          },
          "point_estimate": 219.5700362544524,
          "standard_error": 0.08432589066326901
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05063269321913687,
            "upper_bound": 0.3166560166086607
          },
          "point_estimate": 0.18308302972636,
          "standard_error": 0.07363599995652444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.44747205725056,
            "upper_bound": 219.64445594206217
          },
          "point_estimate": 219.5771607513067,
          "standard_error": 0.050782891949573555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1114696043940019,
            "upper_bound": 0.2297050284054305
          },
          "point_estimate": 0.18463809593174085,
          "standard_error": 0.030471166293758244
        }
      }
    },
    "memchr1/libc/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/never",
        "directory_name": "memchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.381560361468803,
            "upper_bound": 7.421238928113668
          },
          "point_estimate": 7.402248719734554,
          "standard_error": 0.010157594483005338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.377216079317925,
            "upper_bound": 7.42730321868157
          },
          "point_estimate": 7.407538554096243,
          "standard_error": 0.01097178203143077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0039956216645662766,
            "upper_bound": 0.05574949829450315
          },
          "point_estimate": 0.02700843356486556,
          "standard_error": 0.01320577895083552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.402167267166519,
            "upper_bound": 7.436106444410345
          },
          "point_estimate": 7.420509372221421,
          "standard_error": 0.009021327728227328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016507699292934636,
            "upper_bound": 0.043921278262309425
          },
          "point_estimate": 0.03374269587953783,
          "standard_error": 0.007169299430327557
        }
      }
    },
    "memchr1/libc/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/rare",
        "directory_name": "memchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.19675886122835,
            "upper_bound": 11.220716869593366
          },
          "point_estimate": 11.207881408187212,
          "standard_error": 0.006129539051381021
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.194180955516275,
            "upper_bound": 11.21994780363159
          },
          "point_estimate": 11.201519574544552,
          "standard_error": 0.00722470437079989
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016852508256256966,
            "upper_bound": 0.03247839824869809
          },
          "point_estimate": 0.019008650907738636,
          "standard_error": 0.007885479410571475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.199392387400112,
            "upper_bound": 11.213953946640393
          },
          "point_estimate": 11.206759075608367,
          "standard_error": 0.003741473814836765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009379767114207727,
            "upper_bound": 0.027703708091841184
          },
          "point_estimate": 0.020468278797793383,
          "standard_error": 0.0050573332810101655
        }
      }
    },
    "memchr1/libc/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/uncommon",
        "directory_name": "memchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.21520537891519,
            "upper_bound": 47.2688452036557
          },
          "point_estimate": 47.24330150239956,
          "standard_error": 0.01375219819701198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.21578479370387,
            "upper_bound": 47.27957913326764
          },
          "point_estimate": 47.248260700162206,
          "standard_error": 0.017095264647449347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009740992502893746,
            "upper_bound": 0.07878876969363406
          },
          "point_estimate": 0.0415630034454919,
          "standard_error": 0.016210910752435616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.23196213271411,
            "upper_bound": 47.28370116858798
          },
          "point_estimate": 47.261919820456235,
          "standard_error": 0.013128824222991137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022895680105203924,
            "upper_bound": 0.05996608975901898
          },
          "point_estimate": 0.04591671611405726,
          "standard_error": 0.009783510056017415
        }
      }
    },
    "memchr1/libc/small/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/verycommon",
        "directory_name": "memchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 535.643806724581,
            "upper_bound": 536.3079616516121
          },
          "point_estimate": 535.9462951943784,
          "standard_error": 0.17058171233271358
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 535.4781389057257,
            "upper_bound": 536.238354301258
          },
          "point_estimate": 535.905603129666,
          "standard_error": 0.21050882063698784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0831660637483094,
            "upper_bound": 0.9069957614355852
          },
          "point_estimate": 0.5545220086937569,
          "standard_error": 0.19876808738568957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 535.6394027989505,
            "upper_bound": 536.0106200227544
          },
          "point_estimate": 535.8572308669407,
          "standard_error": 0.0939876550908062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2631300318968542,
            "upper_bound": 0.7838733192670538
          },
          "point_estimate": 0.5690158350094793,
          "standard_error": 0.15100152780258258
        }
      }
    },
    "memchr1/libc/tiny/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/common",
        "directory_name": "memchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.36705865594141,
            "upper_bound": 49.44065824036803
          },
          "point_estimate": 49.40615435169966,
          "standard_error": 0.01886962602652992
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.37157131412871,
            "upper_bound": 49.45420187333935
          },
          "point_estimate": 49.41099418410235,
          "standard_error": 0.01874759948812484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007852018506774697,
            "upper_bound": 0.0993125580474102
          },
          "point_estimate": 0.061254032455372615,
          "standard_error": 0.02373206419914506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.4017096524339,
            "upper_bound": 49.435473797943416
          },
          "point_estimate": 49.41720020113862,
          "standard_error": 0.008481112778440901
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028927879926963985,
            "upper_bound": 0.0852862430604543
          },
          "point_estimate": 0.06285597059035217,
          "standard_error": 0.01552787677549279
        }
      }
    },
    "memchr1/libc/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/never",
        "directory_name": "memchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4251237087312925,
            "upper_bound": 3.435366483818523
          },
          "point_estimate": 3.429620946410927,
          "standard_error": 0.002656460640651997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4234305382370147,
            "upper_bound": 3.4346203647074325
          },
          "point_estimate": 3.4258232533637987,
          "standard_error": 0.0026889473722626032
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000658498376915556,
            "upper_bound": 0.011508726197086776
          },
          "point_estimate": 0.004734545388090382,
          "standard_error": 0.0030253456089465816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4242538345617506,
            "upper_bound": 3.4304944486489264
          },
          "point_estimate": 3.426526049159784,
          "standard_error": 0.0016002698806693331
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026865234065087088,
            "upper_bound": 0.012297631552834216
          },
          "point_estimate": 0.008840382215716716,
          "standard_error": 0.002668229494788931
        }
      }
    },
    "memchr1/libc/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/rare",
        "directory_name": "memchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.127635712738937,
            "upper_bound": 6.143821666916043
          },
          "point_estimate": 6.134259930782346,
          "standard_error": 0.004287569700207953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.12667758286683,
            "upper_bound": 6.13792903236369
          },
          "point_estimate": 6.128564352934644,
          "standard_error": 0.0025775188085454576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00026748576808622687,
            "upper_bound": 0.0138221958472058
          },
          "point_estimate": 0.0029901994015073484,
          "standard_error": 0.0033794489558390925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.127601841455769,
            "upper_bound": 6.131018310275043
          },
          "point_estimate": 6.129131257548272,
          "standard_error": 0.0008897067935533161
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017333537492037225,
            "upper_bound": 0.02079508105922946
          },
          "point_estimate": 0.014279257026414995,
          "standard_error": 0.005531433648685809
        }
      }
    },
    "memchr1/libc/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/uncommon",
        "directory_name": "memchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.68648210749971,
            "upper_bound": 18.71324229819379
          },
          "point_estimate": 18.699529197786184,
          "standard_error": 0.0068463245991275895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.6809999218144,
            "upper_bound": 18.724425785919596
          },
          "point_estimate": 18.696848288729814,
          "standard_error": 0.010438932810077897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005639739654989624,
            "upper_bound": 0.03685393930328597
          },
          "point_estimate": 0.026231918604989496,
          "standard_error": 0.009049245330961865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.697925794830994,
            "upper_bound": 18.721795874739612
          },
          "point_estimate": 18.70920823291699,
          "standard_error": 0.00605964916387541
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012622705482179955,
            "upper_bound": 0.0275268837727545
          },
          "point_estimate": 0.02274183324738662,
          "standard_error": 0.003646535160663382
        }
      }
    },
    "memchr1/naive/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/naive/empty/never",
        "directory_name": "memchr1/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4885524080217292,
            "upper_bound": 0.4891288107331771
          },
          "point_estimate": 0.48885445030996494,
          "standard_error": 0.0001469540306519422
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.488679763964708,
            "upper_bound": 0.48912257308821205
          },
          "point_estimate": 0.4888529642788294,
          "standard_error": 0.00008821718325244416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00003007101632234566,
            "upper_bound": 0.0007503906730140968
          },
          "point_estimate": 0.00015367020608422538,
          "standard_error": 0.0002009405539220109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4887082415517312,
            "upper_bound": 0.489257737872827
          },
          "point_estimate": 0.48896556440142447,
          "standard_error": 0.00014109844176828627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00015533517499726165,
            "upper_bound": 0.0006950305064748323
          },
          "point_estimate": 0.0004885109090298064,
          "standard_error": 0.00013789220206994498
        }
      }
    },
    "memchr1/naive/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/common",
        "directory_name": "memchr1/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455453.0895816219,
            "upper_bound": 456454.1437462673
          },
          "point_estimate": 455952.247125496,
          "standard_error": 255.86720437129077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455154.3229166667,
            "upper_bound": 456578.5417361112
          },
          "point_estimate": 456035.82265625,
          "standard_error": 355.33120774478243
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235.70096394046277,
            "upper_bound": 1472.758113228279
          },
          "point_estimate": 829.3411747450389,
          "standard_error": 325.5526099205484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455590.02095808386,
            "upper_bound": 456545.141057998
          },
          "point_estimate": 456163.03665584413,
          "standard_error": 244.05980398841217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.4394751334165,
            "upper_bound": 1078.2475515822637
          },
          "point_estimate": 851.4798589572955,
          "standard_error": 151.30462691574655
        }
      }
    },
    "memchr1/naive/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/never",
        "directory_name": "memchr1/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145546.7918857143,
            "upper_bound": 145791.60526666668
          },
          "point_estimate": 145659.90689238097,
          "standard_error": 63.2748954097291
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145500.1580952381,
            "upper_bound": 145831.91506666667
          },
          "point_estimate": 145586.0318,
          "standard_error": 74.41169037443497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.3161768972903,
            "upper_bound": 348.14550957918584
          },
          "point_estimate": 174.47716570241406,
          "standard_error": 77.17843410106899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145524.9039760479,
            "upper_bound": 145707.98798987342
          },
          "point_estimate": 145613.07785974027,
          "standard_error": 46.059962277650506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.89430801740296,
            "upper_bound": 267.20962502242276
          },
          "point_estimate": 210.84399430869615,
          "standard_error": 48.04110414751604
        }
      }
    },
    "memchr1/naive/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/rare",
        "directory_name": "memchr1/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146740.07706797234,
            "upper_bound": 147084.4552771697
          },
          "point_estimate": 146899.17970014078,
          "standard_error": 87.86088453009913
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146686.13306451612,
            "upper_bound": 147057.40781810036
          },
          "point_estimate": 146879.4516705069,
          "standard_error": 98.12943514315366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.73044761886774,
            "upper_bound": 453.6173540636243
          },
          "point_estimate": 266.088669489701,
          "standard_error": 104.62199145646572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146759.59924095945,
            "upper_bound": 147049.31409571168
          },
          "point_estimate": 146925.6753560955,
          "standard_error": 73.70328645149615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.7971691613549,
            "upper_bound": 397.7905025287
          },
          "point_estimate": 293.4982591772231,
          "standard_error": 73.71302518410164
        }
      }
    },
    "memchr1/naive/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/uncommon",
        "directory_name": "memchr1/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198011.7611154244,
            "upper_bound": 198360.35637508624
          },
          "point_estimate": 198168.0965118185,
          "standard_error": 89.92029157967274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197952.15036231885,
            "upper_bound": 198377.953125
          },
          "point_estimate": 198035.63354037263,
          "standard_error": 100.07036686798318
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.72115193394388,
            "upper_bound": 433.4959380285765
          },
          "point_estimate": 148.26572082788655,
          "standard_error": 107.13401964350884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197975.59972894535,
            "upper_bound": 198181.34332473867
          },
          "point_estimate": 198067.73101355167,
          "standard_error": 53.08469638122075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.15532772829248,
            "upper_bound": 392.4823170484587
          },
          "point_estimate": 300.10429586254673,
          "standard_error": 79.63707178150581
        }
      }
    },
    "memchr1/naive/huge/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/verycommon",
        "directory_name": "memchr1/naive_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 913187.0971319446,
            "upper_bound": 914828.9495752977
          },
          "point_estimate": 913970.5723283732,
          "standard_error": 421.8379347960176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912843.3765625,
            "upper_bound": 915073.1504166666
          },
          "point_estimate": 913568.69875,
          "standard_error": 588.2299307459285
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 261.4106374214886,
            "upper_bound": 2290.498351335588
          },
          "point_estimate": 1405.8281671665816,
          "standard_error": 527.4264232817263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 913213.2871698114,
            "upper_bound": 915154.6335760518
          },
          "point_estimate": 913978.093181818,
          "standard_error": 495.0322967649758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721.8306043750491,
            "upper_bound": 1729.3984870904924
          },
          "point_estimate": 1405.2476071252038,
          "standard_error": 252.10136171559407
        }
      }
    },
    "memchr1/naive/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/common",
        "directory_name": "memchr1/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249.62269990034224,
            "upper_bound": 256.14003592873894
          },
          "point_estimate": 253.0066929635583,
          "standard_error": 1.666351126141886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249.2092764188032,
            "upper_bound": 256.95746807865197
          },
          "point_estimate": 253.44247164407253,
          "standard_error": 1.6237777958853412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0706384380122935,
            "upper_bound": 9.488591107984076
          },
          "point_estimate": 3.361786036328599,
          "standard_error": 2.334731205408023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249.58443574488996,
            "upper_bound": 255.80509623829929
          },
          "point_estimate": 252.97502131910173,
          "standard_error": 1.6791009109013493
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6063304851726503,
            "upper_bound": 7.302294842321394
          },
          "point_estimate": 5.566468625534374,
          "standard_error": 1.211082388048274
        }
      }
    },
    "memchr1/naive/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/never",
        "directory_name": "memchr1/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.05229178909067,
            "upper_bound": 172.30795752936987
          },
          "point_estimate": 172.18039802354863,
          "standard_error": 0.06560005694622636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.95097899608402,
            "upper_bound": 172.3509207177986
          },
          "point_estimate": 172.20020861516554,
          "standard_error": 0.11882044698832483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03993150003867518,
            "upper_bound": 0.3468388236355213
          },
          "point_estimate": 0.2526563978719674,
          "standard_error": 0.08663877783738588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.1422342322847,
            "upper_bound": 172.37999107441024
          },
          "point_estimate": 172.28595552637702,
          "standard_error": 0.0612844163854409
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14222214653510223,
            "upper_bound": 0.26061594337714933
          },
          "point_estimate": 0.21893270596345635,
          "standard_error": 0.03039934299839143
        }
      }
    },
    "memchr1/naive/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/rare",
        "directory_name": "memchr1/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.1004090268747,
            "upper_bound": 180.46101294629116
          },
          "point_estimate": 180.2649458714386,
          "standard_error": 0.0927546727208832
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.0371123232644,
            "upper_bound": 180.5134654900287
          },
          "point_estimate": 180.1658387019923,
          "standard_error": 0.10060548593534016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.048909591245968595,
            "upper_bound": 0.4520725068392411
          },
          "point_estimate": 0.195931220526124,
          "standard_error": 0.09809616973734768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.0399850866265,
            "upper_bound": 180.44081565042148
          },
          "point_estimate": 180.1966099614756,
          "standard_error": 0.10374630661481858
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09979580345570552,
            "upper_bound": 0.38845410885101234
          },
          "point_estimate": 0.3092004566816941,
          "standard_error": 0.0750493267357251
        }
      }
    },
    "memchr1/naive/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/uncommon",
        "directory_name": "memchr1/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.21849614352303,
            "upper_bound": 209.6321429088269
          },
          "point_estimate": 206.38980228257495,
          "standard_error": 1.6465358582082248
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.4189259291163,
            "upper_bound": 212.2712447239211
          },
          "point_estimate": 206.12775405507767,
          "standard_error": 3.1148692644234
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3448267064574348,
            "upper_bound": 8.899476091550493
          },
          "point_estimate": 8.57430806998822,
          "standard_error": 2.4154023998488103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.9427963310088,
            "upper_bound": 207.1053858804586
          },
          "point_estimate": 202.99519405566437,
          "standard_error": 1.5830494972924538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.569554467054502,
            "upper_bound": 6.292384545566137
          },
          "point_estimate": 5.484529081594122,
          "standard_error": 0.6948325580221375
        }
      }
    },
    "memchr1/naive/small/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/verycommon",
        "directory_name": "memchr1/naive_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328.02351765505637,
            "upper_bound": 333.9520771270652
          },
          "point_estimate": 331.04341010303506,
          "standard_error": 1.5139638103531574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327.65836702333144,
            "upper_bound": 334.41971160078185
          },
          "point_estimate": 331.90049636597223,
          "standard_error": 1.5812558843616933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9149719307683696,
            "upper_bound": 8.416615362348741
          },
          "point_estimate": 3.9882322010684934,
          "standard_error": 2.1788162562644984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327.26992608412695,
            "upper_bound": 335.68856343586896
          },
          "point_estimate": 331.9076058189715,
          "standard_error": 2.1270550461066207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.484298580380411,
            "upper_bound": 6.545884153247522
          },
          "point_estimate": 5.055084935753813,
          "standard_error": 1.0332179860014394
        }
      }
    },
    "memchr1/naive/tiny/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/common",
        "directory_name": "memchr1/naive_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.98103578662172,
            "upper_bound": 40.05603286179401
          },
          "point_estimate": 40.019593921338355,
          "standard_error": 0.01916867766240517
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.97858051916342,
            "upper_bound": 40.07006678183693
          },
          "point_estimate": 40.02131467768402,
          "standard_error": 0.02073004965655217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014703454979208774,
            "upper_bound": 0.11066391155170983
          },
          "point_estimate": 0.0422771158008558,
          "standard_error": 0.02471577986982331
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.98751291927457,
            "upper_bound": 40.03293698578132
          },
          "point_estimate": 40.00999427100009,
          "standard_error": 0.011619378787807026
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031418780298985524,
            "upper_bound": 0.08484438294534481
          },
          "point_estimate": 0.0639791255387721,
          "standard_error": 0.013947802427558003
        }
      }
    },
    "memchr1/naive/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/never",
        "directory_name": "memchr1/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.224669552415985,
            "upper_bound": 27.459206936562076
          },
          "point_estimate": 27.34095452594808,
          "standard_error": 0.05966361766171292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.240842879249605,
            "upper_bound": 27.461687188178477
          },
          "point_estimate": 27.331602012181193,
          "standard_error": 0.04645896005321976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02684050707536219,
            "upper_bound": 0.33204754286837557
          },
          "point_estimate": 0.09259104790132532,
          "standard_error": 0.08179525246242499
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.264738408171848,
            "upper_bound": 27.37679001748075
          },
          "point_estimate": 27.31468188218872,
          "standard_error": 0.028168007512992605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07325974423023102,
            "upper_bound": 0.2722647123632259
          },
          "point_estimate": 0.19900512623272015,
          "standard_error": 0.048806467940693814
        }
      }
    },
    "memchr1/naive/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/rare",
        "directory_name": "memchr1/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.38943053466168,
            "upper_bound": 27.564910748019315
          },
          "point_estimate": 27.473273183880373,
          "standard_error": 0.04504651958972782
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.338803092460203,
            "upper_bound": 27.570212944110317
          },
          "point_estimate": 27.460059908054095,
          "standard_error": 0.052776875794828135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.035298969547350255,
            "upper_bound": 0.2522800602560846
          },
          "point_estimate": 0.15388227729765788,
          "standard_error": 0.060707005464872024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.38355596437091,
            "upper_bound": 27.514335248739307
          },
          "point_estimate": 27.46146948651903,
          "standard_error": 0.033311064976594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07619027101092335,
            "upper_bound": 0.19437707048833344
          },
          "point_estimate": 0.1503803777121111,
          "standard_error": 0.031216309470427694
        }
      }
    },
    "memchr1/naive/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr1/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/uncommon",
        "directory_name": "memchr1/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.88199777328852,
            "upper_bound": 27.172686307515573
          },
          "point_estimate": 27.02693445156707,
          "standard_error": 0.07432570013669053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.87583815883316,
            "upper_bound": 27.2314407027061
          },
          "point_estimate": 27.017448208472153,
          "standard_error": 0.08182342082728632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03671734761557922,
            "upper_bound": 0.4325921726378295
          },
          "point_estimate": 0.26360816109303054,
          "standard_error": 0.10214071520346887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.87677782182635,
            "upper_bound": 27.271406536025143
          },
          "point_estimate": 27.049611538525593,
          "standard_error": 0.10465701149565391
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12741358165537597,
            "upper_bound": 0.3242863478513357
          },
          "point_estimate": 0.24760993231027592,
          "standard_error": 0.050690749527638974
        }
      }
    },
    "memchr2/fallback/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/fallback/empty/never",
        "directory_name": "memchr2/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.960179148635935,
            "upper_bound": 1.9638506053549565
          },
          "point_estimate": 1.9620338762094605,
          "standard_error": 0.00093712323735235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9582073391327144,
            "upper_bound": 1.9646164593307016
          },
          "point_estimate": 1.962093641426761,
          "standard_error": 0.0014928624472765638
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00025283926412683004,
            "upper_bound": 0.005477460933921298
          },
          "point_estimate": 0.003824985749679964,
          "standard_error": 0.0014058338977614895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9590384887508847,
            "upper_bound": 1.9636675808011257
          },
          "point_estimate": 1.96091501113816,
          "standard_error": 0.0011940584924830864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001866926842866959,
            "upper_bound": 0.003784631806604057
          },
          "point_estimate": 0.003121118653266123,
          "standard_error": 0.0004878834998225736
        }
      }
    },
    "memchr2/fallback/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/common",
        "directory_name": "memchr2/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1206445.8623229647,
            "upper_bound": 1207744.2122676652
          },
          "point_estimate": 1207113.695550435,
          "standard_error": 331.37651749681726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1206089.910394265,
            "upper_bound": 1207999.7225806452
          },
          "point_estimate": 1207302.147311828,
          "standard_error": 510.4394599219925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216.0017969180399,
            "upper_bound": 1859.7170788115695
          },
          "point_estimate": 1077.548878289008,
          "standard_error": 453.75142154190183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1206839.3299883062,
            "upper_bound": 1207930.682764441
          },
          "point_estimate": 1207304.1044826142,
          "standard_error": 278.01309470070277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 617.8689660629434,
            "upper_bound": 1356.6772891987173
          },
          "point_estimate": 1105.9932230759832,
          "standard_error": 186.12323037837268
        }
      }
    },
    "memchr2/fallback/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/never",
        "directory_name": "memchr2/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83592.63810408684,
            "upper_bound": 83748.62194139071
          },
          "point_estimate": 83669.38643833241,
          "standard_error": 40.02392451591477
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83585.31840676884,
            "upper_bound": 83786.96206896551
          },
          "point_estimate": 83647.77257799671,
          "standard_error": 48.42828701918992
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.862129599486913,
            "upper_bound": 218.1539352649124
          },
          "point_estimate": 102.87217786714729,
          "standard_error": 55.28041930215234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83592.7966386501,
            "upper_bound": 83708.27812001473
          },
          "point_estimate": 83630.25755485894,
          "standard_error": 29.742606179393267
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.3208043372282,
            "upper_bound": 171.73318139291646
          },
          "point_estimate": 133.30888085430263,
          "standard_error": 26.37100895908561
        }
      }
    },
    "memchr2/fallback/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/rare",
        "directory_name": "memchr2/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89431.36593236864,
            "upper_bound": 89534.88660283251
          },
          "point_estimate": 89486.460007526,
          "standard_error": 26.54195666680615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89428.59113300493,
            "upper_bound": 89554.11617405583
          },
          "point_estimate": 89508.00246305419,
          "standard_error": 37.94412489593904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.51637122299744,
            "upper_bound": 158.38947419952297
          },
          "point_estimate": 94.99757302609947,
          "standard_error": 32.567121521572474
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89461.56323340973,
            "upper_bound": 89547.98334448622
          },
          "point_estimate": 89511.9444437336,
          "standard_error": 22.003328596673025
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.27203788089552,
            "upper_bound": 116.74458897345916
          },
          "point_estimate": 88.67786447614641,
          "standard_error": 19.46513070887892
        }
      }
    },
    "memchr2/fallback/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/uncommon",
        "directory_name": "memchr2/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 319046.4411652569,
            "upper_bound": 319593.5608540448
          },
          "point_estimate": 319309.6454612225,
          "standard_error": 140.20322513439748
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318914.6246867168,
            "upper_bound": 319647.9381091618
          },
          "point_estimate": 319277.06743421056,
          "standard_error": 197.1947686268904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.13071819307774,
            "upper_bound": 792.6604021555454
          },
          "point_estimate": 504.8521685809524,
          "standard_error": 184.7305097081859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 319022.99152365525,
            "upper_bound": 319725.2678609174
          },
          "point_estimate": 319336.98999772157,
          "standard_error": 182.1919885008946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 244.95024611432945,
            "upper_bound": 585.2174384637042
          },
          "point_estimate": 467.150355737841,
          "standard_error": 86.79268775965373
        }
      }
    },
    "memchr2/fallback/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/common",
        "directory_name": "memchr2/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 434.8045311259965,
            "upper_bound": 435.37125133765903
          },
          "point_estimate": 435.0760314863336,
          "standard_error": 0.14515346095026965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 434.76885949694713,
            "upper_bound": 435.4501476077671
          },
          "point_estimate": 434.9525663113312,
          "standard_error": 0.15965592543308676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08534754559364524,
            "upper_bound": 0.8145871322067331
          },
          "point_estimate": 0.39057278717476845,
          "standard_error": 0.19472858017068695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 434.83325059246096,
            "upper_bound": 435.3383838059696
          },
          "point_estimate": 435.08117244372227,
          "standard_error": 0.12895985495903392
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21299312172003448,
            "upper_bound": 0.6351040570212405
          },
          "point_estimate": 0.4844639195176296,
          "standard_error": 0.10742535364803744
        }
      }
    },
    "memchr2/fallback/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/never",
        "directory_name": "memchr2/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.1970892699514,
            "upper_bound": 103.410319663229
          },
          "point_estimate": 103.30481775861335,
          "standard_error": 0.054509185143689035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.1373355857201,
            "upper_bound": 103.45664360877356
          },
          "point_estimate": 103.30310279074132,
          "standard_error": 0.08592090770628472
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027033183085317197,
            "upper_bound": 0.30237488734478435
          },
          "point_estimate": 0.18721892983075183,
          "standard_error": 0.06994389187619245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.22617116833332,
            "upper_bound": 103.4090954240336
          },
          "point_estimate": 103.31026406782364,
          "standard_error": 0.04734955408398746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10951429035397764,
            "upper_bound": 0.2214254508780322
          },
          "point_estimate": 0.18116288034294575,
          "standard_error": 0.028592080476444227
        }
      }
    },
    "memchr2/fallback/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/rare",
        "directory_name": "memchr2/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.13649533822785,
            "upper_bound": 117.35321844793872
          },
          "point_estimate": 116.7835974965116,
          "standard_error": 0.3122525127588513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.3200723677834,
            "upper_bound": 117.53418839315522
          },
          "point_estimate": 116.8681645828622,
          "standard_error": 0.29573773970175427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1303429525530099,
            "upper_bound": 1.652959380953599
          },
          "point_estimate": 0.900024193629498,
          "standard_error": 0.4069700841354567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.85722415669026,
            "upper_bound": 117.68791793479198
          },
          "point_estimate": 117.30799251986988,
          "standard_error": 0.20995170689813644
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.47082398604002984,
            "upper_bound": 1.4221990712332926
          },
          "point_estimate": 1.0440607464959475,
          "standard_error": 0.26602380354915234
        }
      }
    },
    "memchr2/fallback/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/uncommon",
        "directory_name": "memchr2/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.90983554499113,
            "upper_bound": 168.369271271939
          },
          "point_estimate": 168.13857792036677,
          "standard_error": 0.11761601393203044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.8489914924155,
            "upper_bound": 168.46981930026914
          },
          "point_estimate": 168.07925411758532,
          "standard_error": 0.125780853988596
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024878093823127848,
            "upper_bound": 0.7015837880289416
          },
          "point_estimate": 0.33542923316980827,
          "standard_error": 0.2070679730574581
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.87577033089966,
            "upper_bound": 168.15868274263912
          },
          "point_estimate": 168.0193380000025,
          "standard_error": 0.070120461528273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2079015176013531,
            "upper_bound": 0.5072939670805442
          },
          "point_estimate": 0.3937395880546416,
          "standard_error": 0.0777413508148781
        }
      }
    },
    "memchr2/fallback/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/never",
        "directory_name": "memchr2/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.217878845816042,
            "upper_bound": 13.233930154512416
          },
          "point_estimate": 13.225106774647324,
          "standard_error": 0.004134593329598284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.215427030027875,
            "upper_bound": 13.230391188647722
          },
          "point_estimate": 13.223813604543825,
          "standard_error": 0.003951829615472314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025816723696742753,
            "upper_bound": 0.019690633021401924
          },
          "point_estimate": 0.010262014554899588,
          "standard_error": 0.004143121771596156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.214828461919565,
            "upper_bound": 13.233403177464345
          },
          "point_estimate": 13.223503026971342,
          "standard_error": 0.004844515506204683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00549185546227415,
            "upper_bound": 0.019452285976399
          },
          "point_estimate": 0.013735612971380132,
          "standard_error": 0.004090478910218488
        }
      }
    },
    "memchr2/fallback/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/rare",
        "directory_name": "memchr2/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.51507364580969,
            "upper_bound": 21.550706143716223
          },
          "point_estimate": 21.530626224261425,
          "standard_error": 0.009238209230126066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.508948827680157,
            "upper_bound": 21.537460674183947
          },
          "point_estimate": 21.52809343490481,
          "standard_error": 0.0074562956076445475
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002818064297288655,
            "upper_bound": 0.0388446969276727
          },
          "point_estimate": 0.014992450031790082,
          "standard_error": 0.008863558883247328
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.51558810175514,
            "upper_bound": 21.533222569613883
          },
          "point_estimate": 21.52628541590767,
          "standard_error": 0.004458625001943167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01036971151246216,
            "upper_bound": 0.044850229827220874
          },
          "point_estimate": 0.030821361061779996,
          "standard_error": 0.010481994946175814
        }
      }
    },
    "memchr2/fallback/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/uncommon",
        "directory_name": "memchr2/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.28340634209252,
            "upper_bound": 55.47764296489449
          },
          "point_estimate": 55.38087289613888,
          "standard_error": 0.0498370451083134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.192937108059915,
            "upper_bound": 55.53110541786858
          },
          "point_estimate": 55.4213088034072,
          "standard_error": 0.08306324923527733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01902063819192583,
            "upper_bound": 0.2841429003600044
          },
          "point_estimate": 0.18695095661859948,
          "standard_error": 0.07001851956082208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.27820532965111,
            "upper_bound": 55.52106266263658
          },
          "point_estimate": 55.40526303350393,
          "standard_error": 0.06272987968509507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10438340845242612,
            "upper_bound": 0.19876892580247027
          },
          "point_estimate": 0.16634593579622814,
          "standard_error": 0.02413074889583645
        }
      }
    },
    "memchr2/krate/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/krate/empty/never",
        "directory_name": "memchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4888386645441158,
            "upper_bound": 0.48914536741874526
          },
          "point_estimate": 0.4889836881411626,
          "standard_error": 0.00007828045939715766
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48876046204251633,
            "upper_bound": 0.489164520825634
          },
          "point_estimate": 0.4889516397494007,
          "standard_error": 0.00012214663049573934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00004693844653115574,
            "upper_bound": 0.0004713494486804026
          },
          "point_estimate": 0.00027125100230301024,
          "standard_error": 0.00010386884814161456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.488760737959228,
            "upper_bound": 0.4889673500345653
          },
          "point_estimate": 0.48883027623353137,
          "standard_error": 0.00005292022628709766
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00014594675841646742,
            "upper_bound": 0.0003371162368431173
          },
          "point_estimate": 0.0002619421135581492,
          "standard_error": 0.00005214642550996894
        }
      }
    },
    "memchr2/krate/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/common",
        "directory_name": "memchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409420.51292312064,
            "upper_bound": 409944.6249928772
          },
          "point_estimate": 409657.5447899946,
          "standard_error": 134.7153222345125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409353.117676565,
            "upper_bound": 409732.7893258427
          },
          "point_estimate": 409666.9707865169,
          "standard_error": 90.36249935279818
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.52633070012948,
            "upper_bound": 672.4710862354283
          },
          "point_estimate": 107.07144248112978,
          "standard_error": 167.69700026814868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409383.2275550131,
            "upper_bound": 409900.642727097
          },
          "point_estimate": 409637.0260615789,
          "standard_error": 129.19853733017786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.17982917179008,
            "upper_bound": 642.327441587106
          },
          "point_estimate": 450.1472960983115,
          "standard_error": 143.04852772979166
        }
      }
    },
    "memchr2/krate/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/never",
        "directory_name": "memchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11466.564774279745,
            "upper_bound": 11490.860203218152
          },
          "point_estimate": 11476.895773715669,
          "standard_error": 6.336688378536937
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11464.941414380322,
            "upper_bound": 11481.317123935669
          },
          "point_estimate": 11472.429668611374,
          "standard_error": 3.756162454266546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.573022129909309,
            "upper_bound": 22.2659171070663
          },
          "point_estimate": 9.607711093225348,
          "standard_error": 5.670911359903313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11469.098947106071,
            "upper_bound": 11476.855435414433
          },
          "point_estimate": 11472.370945295635,
          "standard_error": 1.9814058488958224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.370059359817571,
            "upper_bound": 31.10189182056717
          },
          "point_estimate": 21.153590622519467,
          "standard_error": 7.776403526784273
        }
      }
    },
    "memchr2/krate/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/rare",
        "directory_name": "memchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15340.680348013697,
            "upper_bound": 15363.991687337097
          },
          "point_estimate": 15351.237592179406,
          "standard_error": 5.971379587831907
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15339.243224989446,
            "upper_bound": 15367.989070132062
          },
          "point_estimate": 15343.694111439425,
          "standard_error": 6.287946332898144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0980135169952265,
            "upper_bound": 28.83938921643287
          },
          "point_estimate": 9.929737835530297,
          "standard_error": 6.685573337808806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15339.640786862004,
            "upper_bound": 15362.434307907355
          },
          "point_estimate": 15349.635554483508,
          "standard_error": 5.801014923354505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.9666460434047615,
            "upper_bound": 25.02200704619902
          },
          "point_estimate": 19.810542550347115,
          "standard_error": 4.858527171883456
        }
      }
    },
    "memchr2/krate/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/uncommon",
        "directory_name": "memchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160092.67280235988,
            "upper_bound": 160948.80031875527
          },
          "point_estimate": 160532.46684313106,
          "standard_error": 218.34651554659345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159998.99194058153,
            "upper_bound": 160995.9486725664
          },
          "point_estimate": 160620.6666666667,
          "standard_error": 229.89432929800216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.85942911184196,
            "upper_bound": 1269.231860873714
          },
          "point_estimate": 585.7834498215035,
          "standard_error": 311.6025106226991
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160601.36963575112,
            "upper_bound": 161249.38637627626
          },
          "point_estimate": 160968.62949086312,
          "standard_error": 161.17035392694262
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 343.8446754475302,
            "upper_bound": 936.7484289873844
          },
          "point_estimate": 728.025476906906,
          "standard_error": 147.2196238703623
        }
      }
    },
    "memchr2/krate/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/common",
        "directory_name": "memchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.43536708663186,
            "upper_bound": 407.8478084741524
          },
          "point_estimate": 407.6345592365644,
          "standard_error": 0.1059734079667944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.3445023829549,
            "upper_bound": 407.90719960751335
          },
          "point_estimate": 407.5716208098042,
          "standard_error": 0.17756292383417546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05998103584757057,
            "upper_bound": 0.6111703541621706
          },
          "point_estimate": 0.379635873645083,
          "standard_error": 0.14437966934802982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.3842304717912,
            "upper_bound": 407.8934731367456
          },
          "point_estimate": 407.6328814420791,
          "standard_error": 0.1363633150296748
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.208566859624136,
            "upper_bound": 0.43237844420382543
          },
          "point_estimate": 0.3536286634505517,
          "standard_error": 0.057656573362379664
        }
      }
    },
    "memchr2/krate/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/never",
        "directory_name": "memchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.042918569263035,
            "upper_bound": 12.078824430821143
          },
          "point_estimate": 12.060876240907698,
          "standard_error": 0.00913577823388617
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.036775340984672,
            "upper_bound": 12.077546692891726
          },
          "point_estimate": 12.065577981053746,
          "standard_error": 0.009690926675383656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001332823472005869,
            "upper_bound": 0.05226636101313442
          },
          "point_estimate": 0.020367759261447405,
          "standard_error": 0.0131374777792008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.04672571615419,
            "upper_bound": 12.069245115136004
          },
          "point_estimate": 12.060211105523848,
          "standard_error": 0.005755176752748674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01295190043437018,
            "upper_bound": 0.04065944848927445
          },
          "point_estimate": 0.03066424126278967,
          "standard_error": 0.006702143815481594
        }
      }
    },
    "memchr2/krate/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/rare",
        "directory_name": "memchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.557070445210652,
            "upper_bound": 23.67492380246944
          },
          "point_estimate": 23.629969345608735,
          "standard_error": 0.03294740240034141
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.636616429646587,
            "upper_bound": 23.678086320732103
          },
          "point_estimate": 23.665774754749528,
          "standard_error": 0.012677115646667148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011269309783333964,
            "upper_bound": 0.05935210467005853
          },
          "point_estimate": 0.021693136575016075,
          "standard_error": 0.01756967991516774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.63433481644552,
            "upper_bound": 23.68221592010369
          },
          "point_estimate": 23.66324905856666,
          "standard_error": 0.01246370324043436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013202614168135174,
            "upper_bound": 0.1684289006881797
          },
          "point_estimate": 0.11011427817977504,
          "standard_error": 0.05199353599207207
        }
      }
    },
    "memchr2/krate/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/uncommon",
        "directory_name": "memchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.28707706968524,
            "upper_bound": 111.98630031343836
          },
          "point_estimate": 111.64443369533232,
          "standard_error": 0.179993677265598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.19929297089512,
            "upper_bound": 111.92725848956464
          },
          "point_estimate": 111.74032599630188,
          "standard_error": 0.13856217026166848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.052476110961784145,
            "upper_bound": 1.12041093465617
          },
          "point_estimate": 0.18897294975418005,
          "standard_error": 0.28519939745608003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.40745663420856,
            "upper_bound": 111.76501920476863
          },
          "point_estimate": 111.64804553739788,
          "standard_error": 0.09281231759595006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1728092543633009,
            "upper_bound": 0.8039620221988382
          },
          "point_estimate": 0.6007226723049722,
          "standard_error": 0.14381405935843528
        }
      }
    },
    "memchr2/krate/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/never",
        "directory_name": "memchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.660156688338259,
            "upper_bound": 4.670836047810083
          },
          "point_estimate": 4.6646770617612585,
          "standard_error": 0.002789160330834205
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.658721408315177,
            "upper_bound": 4.66726640341416
          },
          "point_estimate": 4.662082761103422,
          "standard_error": 0.0021655614020993207
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005005005712290945,
            "upper_bound": 0.010463534599245023
          },
          "point_estimate": 0.005484042126605267,
          "standard_error": 0.002619054983216422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.658952630133321,
            "upper_bound": 4.662649779500472
          },
          "point_estimate": 4.660268048508757,
          "standard_error": 0.0009446178018061464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002378497559912265,
            "upper_bound": 0.013481460192734789
          },
          "point_estimate": 0.009295408234629928,
          "standard_error": 0.0032470122523604295
        }
      }
    },
    "memchr2/krate/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/rare",
        "directory_name": "memchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.190299480122288,
            "upper_bound": 11.211899407947778
          },
          "point_estimate": 11.20057231206454,
          "standard_error": 0.005536850840141538
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.1869262541306,
            "upper_bound": 11.218123433173956
          },
          "point_estimate": 11.196162018875835,
          "standard_error": 0.00760852173855384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018985971350929448,
            "upper_bound": 0.029606088513371265
          },
          "point_estimate": 0.014647815927152103,
          "standard_error": 0.00736265186406676
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.18987622827414,
            "upper_bound": 11.206911020411788
          },
          "point_estimate": 11.196136963425223,
          "standard_error": 0.004352844792630111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00803769536926661,
            "upper_bound": 0.02254484620220547
          },
          "point_estimate": 0.01846059846896763,
          "standard_error": 0.003400901047304855
        }
      }
    },
    "memchr2/krate/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/uncommon",
        "directory_name": "memchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.56853501162193,
            "upper_bound": 55.668664466216185
          },
          "point_estimate": 55.6148623052154,
          "standard_error": 0.025706219503240233
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.53755277487664,
            "upper_bound": 55.653007097561456
          },
          "point_estimate": 55.604498919653736,
          "standard_error": 0.026114706621316357
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00487481502616799,
            "upper_bound": 0.13152225498467796
          },
          "point_estimate": 0.07552237969114627,
          "standard_error": 0.03219184360622961
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.56233744146523,
            "upper_bound": 55.63578715511735
          },
          "point_estimate": 55.60134026048696,
          "standard_error": 0.018419607557989225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03849114376352583,
            "upper_bound": 0.11903605352873808
          },
          "point_estimate": 0.08582767401263366,
          "standard_error": 0.023258611559739788
        }
      }
    },
    "memchr2/naive/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/naive/empty/never",
        "directory_name": "memchr2/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6514153040681204,
            "upper_bound": 0.6534001103103169
          },
          "point_estimate": 0.6522057723978512,
          "standard_error": 0.0005367592074555849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6512285935687365,
            "upper_bound": 0.6522931169097358
          },
          "point_estimate": 0.6518542464130217,
          "standard_error": 0.00026517250515647165
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00002468942281411376,
            "upper_bound": 0.0013415241323413516
          },
          "point_estimate": 0.0006416420939139611,
          "standard_error": 0.00033790451060094643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6516595594853434,
            "upper_bound": 0.6521948423403175
          },
          "point_estimate": 0.651968938931912,
          "standard_error": 0.00013649971474376685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003660293904860192,
            "upper_bound": 0.0027151111965315784
          },
          "point_estimate": 0.0017913133258594558,
          "standard_error": 0.0007743047540135995
        }
      }
    },
    "memchr2/naive/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/common",
        "directory_name": "memchr2/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 825860.4954202161,
            "upper_bound": 828092.231828461
          },
          "point_estimate": 826843.9145088183,
          "standard_error": 575.1082614886691
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 825609.9087962962,
            "upper_bound": 827481.4015873016
          },
          "point_estimate": 826519.0891975309,
          "standard_error": 442.9878992177281
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257.38557868979126,
            "upper_bound": 2507.6218228142325
          },
          "point_estimate": 941.867209500776,
          "standard_error": 591.4913532251361
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 825783.9152610442,
            "upper_bound": 826842.5731641676
          },
          "point_estimate": 826228.3100721501,
          "standard_error": 267.76596148839104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667.0962183121986,
            "upper_bound": 2735.093803792474
          },
          "point_estimate": 1921.394589990168,
          "standard_error": 602.8384441137895
        }
      }
    },
    "memchr2/naive/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/never",
        "directory_name": "memchr2/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240786.70739947964,
            "upper_bound": 241419.873995585
          },
          "point_estimate": 241077.3667399874,
          "standard_error": 163.21612528718936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240715.31125827815,
            "upper_bound": 241429.64403973508
          },
          "point_estimate": 240975.57723509936,
          "standard_error": 150.5422443772884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.07354947125852,
            "upper_bound": 873.2481116490463
          },
          "point_estimate": 314.09191362904033,
          "standard_error": 198.0354027153377
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240703.34834506133,
            "upper_bound": 241077.37533476317
          },
          "point_estimate": 240909.71414810355,
          "standard_error": 95.7966245059604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.7080333256669,
            "upper_bound": 717.7522048350262
          },
          "point_estimate": 542.4020852801535,
          "standard_error": 137.15650401110815
        }
      }
    },
    "memchr2/naive/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/rare",
        "directory_name": "memchr2/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246190.53080115825,
            "upper_bound": 246565.82075450453
          },
          "point_estimate": 246371.48199967825,
          "standard_error": 96.24441942641626
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246124.2471042471,
            "upper_bound": 246708.36599099095
          },
          "point_estimate": 246232.0810810811,
          "standard_error": 169.69194130053685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.6279279944094,
            "upper_bound": 515.4355644978483
          },
          "point_estimate": 295.681309683021,
          "standard_error": 136.37896298549984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246081.92302892063,
            "upper_bound": 246448.1022091992
          },
          "point_estimate": 246209.49196209197,
          "standard_error": 93.85430495175206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.38945321960105,
            "upper_bound": 375.6296383969992
          },
          "point_estimate": 320.90037724360303,
          "standard_error": 47.58949155028864
        }
      }
    },
    "memchr2/naive/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/uncommon",
        "directory_name": "memchr2/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356286.2133858544,
            "upper_bound": 356802.79162114847
          },
          "point_estimate": 356540.3526777933,
          "standard_error": 132.78703760646815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356130.6791841737,
            "upper_bound": 356938.72303921566
          },
          "point_estimate": 356568.8892156863,
          "standard_error": 231.59546055648144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.73705044758107,
            "upper_bound": 719.9667177327788
          },
          "point_estimate": 587.5997317984521,
          "standard_error": 185.9157931306023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356224.32344896643,
            "upper_bound": 356667.100429428
          },
          "point_estimate": 356435.18500127323,
          "standard_error": 112.69633825660542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.36753423108826,
            "upper_bound": 529.0160820499226
          },
          "point_estimate": 442.1846576981028,
          "standard_error": 65.23280756500154
        }
      }
    },
    "memchr2/naive/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/common",
        "directory_name": "memchr2/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338.449501329653,
            "upper_bound": 346.1798866433057
          },
          "point_estimate": 342.45438098496345,
          "standard_error": 1.9599444178471956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.3730528504445,
            "upper_bound": 347.4995250143212
          },
          "point_estimate": 347.03315845585894,
          "standard_error": 4.213993761387946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09783498967033875,
            "upper_bound": 9.763544435280558
          },
          "point_estimate": 1.5239041746829896,
          "standard_error": 3.0825280870590284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.1230618484227,
            "upper_bound": 343.15480810964533
          },
          "point_estimate": 337.64064044796146,
          "standard_error": 2.07866568864272
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.149548445970323,
            "upper_bound": 7.077085841641597
          },
          "point_estimate": 6.529596189595491,
          "standard_error": 0.8553031575965163
        }
      }
    },
    "memchr2/naive/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/never",
        "directory_name": "memchr2/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.44827066836586,
            "upper_bound": 278.69804438209286
          },
          "point_estimate": 278.58096233664793,
          "standard_error": 0.06435365685418337
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.4027953461154,
            "upper_bound": 278.7355059297931
          },
          "point_estimate": 278.6522703403019,
          "standard_error": 0.08354182114867348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026780295285969483,
            "upper_bound": 0.330889385315573
          },
          "point_estimate": 0.12736268350961497,
          "standard_error": 0.08379742151636008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.34143269093005,
            "upper_bound": 278.70116753084807
          },
          "point_estimate": 278.5324304958398,
          "standard_error": 0.0922584214649586
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08848502144323434,
            "upper_bound": 0.27224950724273883
          },
          "point_estimate": 0.21448940929520713,
          "standard_error": 0.04545671540157866
        }
      }
    },
    "memchr2/naive/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/rare",
        "directory_name": "memchr2/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.9767145552325,
            "upper_bound": 296.39996125609525
          },
          "point_estimate": 296.1877008633632,
          "standard_error": 0.10882788621378936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.88393118952206,
            "upper_bound": 296.5586461082331
          },
          "point_estimate": 296.1129035031513,
          "standard_error": 0.21778163375628085
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0553910849319693,
            "upper_bound": 0.5816446325890556
          },
          "point_estimate": 0.4470505901230735,
          "standard_error": 0.1491666793053681
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.90840903559155,
            "upper_bound": 296.4087563824812
          },
          "point_estimate": 296.1808949204121,
          "standard_error": 0.12989606039409957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24231890932324915,
            "upper_bound": 0.42073503055015854
          },
          "point_estimate": 0.3632614052081441,
          "standard_error": 0.04552998654471106
        }
      }
    },
    "memchr2/naive/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/uncommon",
        "directory_name": "memchr2/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 377.3953114516751,
            "upper_bound": 377.66038251115367
          },
          "point_estimate": 377.5209679204024,
          "standard_error": 0.06840868330472899
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 377.3295486561388,
            "upper_bound": 377.7023707770534
          },
          "point_estimate": 377.45646903259126,
          "standard_error": 0.11205836163377544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.035094815901715905,
            "upper_bound": 0.40530538389593807
          },
          "point_estimate": 0.25066849956862297,
          "standard_error": 0.09368089474426725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 377.4593170498642,
            "upper_bound": 377.7967551443741
          },
          "point_estimate": 377.64772292103606,
          "standard_error": 0.08536985273610148
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12567666528825688,
            "upper_bound": 0.2870239861500209
          },
          "point_estimate": 0.2280476556865593,
          "standard_error": 0.04261015331466946
        }
      }
    },
    "memchr2/naive/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/never",
        "directory_name": "memchr2/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.894316314407405,
            "upper_bound": 35.95295118478687
          },
          "point_estimate": 35.92436523581055,
          "standard_error": 0.014989092199323251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.89509470600776,
            "upper_bound": 35.96323886541185
          },
          "point_estimate": 35.9233167217471,
          "standard_error": 0.017370368514225545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009953229256242146,
            "upper_bound": 0.08517412114462979
          },
          "point_estimate": 0.04395067096176522,
          "standard_error": 0.0185304312073698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.91243557598929,
            "upper_bound": 35.95373426232628
          },
          "point_estimate": 35.9299426178205,
          "standard_error": 0.010430818128074455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024144389128578792,
            "upper_bound": 0.06758415103963125
          },
          "point_estimate": 0.05008092164538345,
          "standard_error": 0.011292168177639283
        }
      }
    },
    "memchr2/naive/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/rare",
        "directory_name": "memchr2/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.65754035315244,
            "upper_bound": 38.7245680114107
          },
          "point_estimate": 38.69230622510209,
          "standard_error": 0.017151090269674735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.659142325086464,
            "upper_bound": 38.74426097366321
          },
          "point_estimate": 38.688836658685815,
          "standard_error": 0.026688243302872557
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032241605173821573,
            "upper_bound": 0.1068477735220698
          },
          "point_estimate": 0.04919229111921573,
          "standard_error": 0.02559063921638055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.63524097462278,
            "upper_bound": 38.74807707928886
          },
          "point_estimate": 38.70160142618754,
          "standard_error": 0.02934354418830756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.032435844767042504,
            "upper_bound": 0.07424770393006527
          },
          "point_estimate": 0.05710447810997384,
          "standard_error": 0.011401756939407225
        }
      }
    },
    "memchr2/naive/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr2/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/uncommon",
        "directory_name": "memchr2/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.10282474093237,
            "upper_bound": 40.15916201426302
          },
          "point_estimate": 40.12989934754021,
          "standard_error": 0.014436589964091967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.0945606314729,
            "upper_bound": 40.18101817255905
          },
          "point_estimate": 40.1196666057778,
          "standard_error": 0.02138437860515319
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004515784323415914,
            "upper_bound": 0.08172407965308538
          },
          "point_estimate": 0.03790667972689513,
          "standard_error": 0.01981297091334399
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.09144443867331,
            "upper_bound": 40.15982978790969
          },
          "point_estimate": 40.12171055014277,
          "standard_error": 0.017348295180368796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024516345605247247,
            "upper_bound": 0.058646373884453686
          },
          "point_estimate": 0.0480361673736895,
          "standard_error": 0.008341129908644579
        }
      }
    },
    "memchr3/fallback/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/fallback/empty/never",
        "directory_name": "memchr3/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5593332845110965,
            "upper_bound": 2.566525708510057
          },
          "point_estimate": 2.5631413599841744,
          "standard_error": 0.001845642277190991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.558794677882066,
            "upper_bound": 2.567437548528892
          },
          "point_estimate": 2.5652578449236896,
          "standard_error": 0.0023645870826632387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007181263412702778,
            "upper_bound": 0.010237521627477764
          },
          "point_estimate": 0.0063919160881855145,
          "standard_error": 0.0025152613543572688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5593074630269625,
            "upper_bound": 2.566196979701785
          },
          "point_estimate": 2.562260621903911,
          "standard_error": 0.001779364691403977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030329183188117902,
            "upper_bound": 0.008103283000854096
          },
          "point_estimate": 0.0061567215411573286,
          "standard_error": 0.001368656902809407
        }
      }
    },
    "memchr3/fallback/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/common",
        "directory_name": "memchr3/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1710833.130858586,
            "upper_bound": 1714426.5170243056
          },
          "point_estimate": 1712554.1075631313,
          "standard_error": 922.0260079541636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1710170.4848484849,
            "upper_bound": 1714981.331818182
          },
          "point_estimate": 1712118.9698863635,
          "standard_error": 1108.9025673716023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 750.123047856748,
            "upper_bound": 5174.947817217283
          },
          "point_estimate": 2618.559836844507,
          "standard_error": 1185.2240610789763
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1711226.9143426295,
            "upper_bound": 1713119.002072002
          },
          "point_estimate": 1712168.0897284534,
          "standard_error": 485.7605533462039
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1496.0790709941198,
            "upper_bound": 3916.073359080762
          },
          "point_estimate": 3071.016069944369,
          "standard_error": 616.7232891856909
        }
      }
    },
    "memchr3/fallback/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/never",
        "directory_name": "memchr3/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124024.70191588452,
            "upper_bound": 124374.9532805508
          },
          "point_estimate": 124210.6689372393,
          "standard_error": 90.0443441792402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123974.92218430036,
            "upper_bound": 124417.17128839593
          },
          "point_estimate": 124325.22372013652,
          "standard_error": 108.99282457260124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.418532522407123,
            "upper_bound": 483.9784426875247
          },
          "point_estimate": 200.97034798496784,
          "standard_error": 120.88555483820492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124036.43161336408,
            "upper_bound": 124441.14332078722
          },
          "point_estimate": 124276.84007801072,
          "standard_error": 102.81363433852572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.7793034995388,
            "upper_bound": 374.61429622119334
          },
          "point_estimate": 300.49619884235454,
          "standard_error": 62.50794717538013
        }
      }
    },
    "memchr3/fallback/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/rare",
        "directory_name": "memchr3/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129011.16793815432,
            "upper_bound": 129202.45988576143
          },
          "point_estimate": 129104.3655081335,
          "standard_error": 48.98116304123318
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128961.22163120568,
            "upper_bound": 129237.5121749409
          },
          "point_estimate": 129087.66555851063,
          "standard_error": 70.41262898149792
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.59638933517838,
            "upper_bound": 288.2971227540495
          },
          "point_estimate": 179.33778718254666,
          "standard_error": 61.44910307414119
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129027.08003191324,
            "upper_bound": 129225.8508160674
          },
          "point_estimate": 129119.68655245464,
          "standard_error": 50.5366683430929
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.59014602630786,
            "upper_bound": 202.76112888092365
          },
          "point_estimate": 164.03229292196914,
          "standard_error": 28.000491804308204
        }
      }
    },
    "memchr3/fallback/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/uncommon",
        "directory_name": "memchr3/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 524960.7848828798,
            "upper_bound": 525531.3867868623
          },
          "point_estimate": 525251.9864109977,
          "standard_error": 146.31011832132833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 524724.6693877551,
            "upper_bound": 525651.0266071429
          },
          "point_estimate": 525303.5471428572,
          "standard_error": 198.85128648013907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.59109548443483,
            "upper_bound": 864.5494303655363
          },
          "point_estimate": 634.3571384877828,
          "standard_error": 234.8480187947736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 524990.1279265251,
            "upper_bound": 525589.9349404762
          },
          "point_estimate": 525306.4103896103,
          "standard_error": 150.72893416222874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.3069650335735,
            "upper_bound": 594.5584947081244
          },
          "point_estimate": 487.7505414534948,
          "standard_error": 79.95609393147433
        }
      }
    },
    "memchr3/fallback/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/common",
        "directory_name": "memchr3/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 740.274461454136,
            "upper_bound": 741.800028755018
          },
          "point_estimate": 740.942544434058,
          "standard_error": 0.3966045376464219
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 740.1512163854745,
            "upper_bound": 741.5610932180932
          },
          "point_estimate": 740.2646285127989,
          "standard_error": 0.4204662924648854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05163224850744647,
            "upper_bound": 1.733789507715175
          },
          "point_estimate": 0.4903540036338284,
          "standard_error": 0.4930340380141788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 740.1191895390724,
            "upper_bound": 740.8021000571093
          },
          "point_estimate": 740.3347223703993,
          "standard_error": 0.17858544870443666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.42129910705722307,
            "upper_bound": 1.8605886689197884
          },
          "point_estimate": 1.320239655796914,
          "standard_error": 0.41074623696471807
        }
      }
    },
    "memchr3/fallback/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/never",
        "directory_name": "memchr3/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.09806270990848,
            "upper_bound": 145.3302194478257
          },
          "point_estimate": 145.21332289683775,
          "standard_error": 0.05916136179771166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.05500966044727,
            "upper_bound": 145.32189727990547
          },
          "point_estimate": 145.2393206150751,
          "standard_error": 0.07095965589653232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03638408500470531,
            "upper_bound": 0.37067173657114594
          },
          "point_estimate": 0.13698133642823662,
          "standard_error": 0.08194011800409674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.1285666746675,
            "upper_bound": 145.2990687391848
          },
          "point_estimate": 145.24189095297092,
          "standard_error": 0.04371833243429306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10332594510664336,
            "upper_bound": 0.26003177680014
          },
          "point_estimate": 0.1972322516666501,
          "standard_error": 0.040900277119902975
        }
      }
    },
    "memchr3/fallback/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/rare",
        "directory_name": "memchr3/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.20192760297954,
            "upper_bound": 171.15990658914598
          },
          "point_estimate": 170.68024406855267,
          "standard_error": 0.24557303430230829
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.94422069224805,
            "upper_bound": 171.45649552365063
          },
          "point_estimate": 170.6470358242223,
          "standard_error": 0.3848176557593942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24271598403596945,
            "upper_bound": 1.3264572360319913
          },
          "point_estimate": 1.1210493126161276,
          "standard_error": 0.296078260904495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.98539498423554,
            "upper_bound": 171.2557641941946
          },
          "point_estimate": 170.64527662670295,
          "standard_error": 0.335087529071619
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4992311810760547,
            "upper_bound": 0.9975460761098136
          },
          "point_estimate": 0.8171172744668889,
          "standard_error": 0.12672748852414326
        }
      }
    },
    "memchr3/fallback/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/uncommon",
        "directory_name": "memchr3/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.3201004581603,
            "upper_bound": 278.7790927127045
          },
          "point_estimate": 278.554291785147,
          "standard_error": 0.11782895106801132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.2757067774015,
            "upper_bound": 278.9193660838786
          },
          "point_estimate": 278.5315831813794,
          "standard_error": 0.1979260675330234
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06885033343388378,
            "upper_bound": 0.6961365853028262
          },
          "point_estimate": 0.4577871880541015,
          "standard_error": 0.16114631816126576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.32351607265514,
            "upper_bound": 278.8404459355294
          },
          "point_estimate": 278.54368094785036,
          "standard_error": 0.13329024683710997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.23894140902512065,
            "upper_bound": 0.48975602063511625
          },
          "point_estimate": 0.39155688684552503,
          "standard_error": 0.06625799410346066
        }
      }
    },
    "memchr3/fallback/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/never",
        "directory_name": "memchr3/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.55778656411062,
            "upper_bound": 18.57316141073003
          },
          "point_estimate": 18.56605506726588,
          "standard_error": 0.003914810867754032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.560788819637583,
            "upper_bound": 18.575341932020095
          },
          "point_estimate": 18.564909475173316,
          "standard_error": 0.003957880194447958
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006047915243703771,
            "upper_bound": 0.019299398463436784
          },
          "point_estimate": 0.008762821774913545,
          "standard_error": 0.004773423558316341
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.562941554374248,
            "upper_bound": 18.57027135210941
          },
          "point_estimate": 18.56671825391753,
          "standard_error": 0.001921659308509375
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0053436847962827815,
            "upper_bound": 0.01838102459165859
          },
          "point_estimate": 0.013083069014518434,
          "standard_error": 0.003650799383087576
        }
      }
    },
    "memchr3/fallback/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/rare",
        "directory_name": "memchr3/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.649933630801755,
            "upper_bound": 32.71937137380615
          },
          "point_estimate": 32.683195415510184,
          "standard_error": 0.017789204426397583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.63983358712448,
            "upper_bound": 32.7366068148787
          },
          "point_estimate": 32.667115867368935,
          "standard_error": 0.022854717758098175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00986601874463775,
            "upper_bound": 0.10503177940477718
          },
          "point_estimate": 0.05287939610135049,
          "standard_error": 0.023721807556869288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.65341177829056,
            "upper_bound": 32.698629136003596
          },
          "point_estimate": 32.673180749464706,
          "standard_error": 0.01149120149180193
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02880518363215606,
            "upper_bound": 0.07357310452139806
          },
          "point_estimate": 0.059222226088680635,
          "standard_error": 0.011177893175574029
        }
      }
    },
    "memchr3/fallback/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/uncommon",
        "directory_name": "memchr3/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.12892242120516,
            "upper_bound": 93.28804628397415
          },
          "point_estimate": 93.20625227368744,
          "standard_error": 0.04087869144852335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.12449894995665,
            "upper_bound": 93.32711269323924
          },
          "point_estimate": 93.18128598217284,
          "standard_error": 0.04876458945116471
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00434956766276048,
            "upper_bound": 0.24519414434128983
          },
          "point_estimate": 0.11340059104935749,
          "standard_error": 0.06700843429017019
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.12883936380732,
            "upper_bound": 93.26821059464358
          },
          "point_estimate": 93.17993544535696,
          "standard_error": 0.0356599545665498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06947578999425795,
            "upper_bound": 0.17028463504194494
          },
          "point_estimate": 0.13562417832025603,
          "standard_error": 0.02488444589849424
        }
      }
    },
    "memchr3/krate/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/krate/empty/never",
        "directory_name": "memchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48898499497383374,
            "upper_bound": 0.4898652941997453
          },
          "point_estimate": 0.48941946875991144,
          "standard_error": 0.0002253927875240657
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4886700431370522,
            "upper_bound": 0.49005114795021887
          },
          "point_estimate": 0.48936670022264206,
          "standard_error": 0.00029689012079144093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00012178778054562257,
            "upper_bound": 0.0013335507719850483
          },
          "point_estimate": 0.0008776446732933702,
          "standard_error": 0.00032215968432355293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4891511359546484,
            "upper_bound": 0.4897756804345765
          },
          "point_estimate": 0.48950115899737096,
          "standard_error": 0.0001595115830233067
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00043534980886631824,
            "upper_bound": 0.0009286269215869192
          },
          "point_estimate": 0.000749887697299912,
          "standard_error": 0.00012768071872242947
        }
      }
    },
    "memchr3/krate/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/common",
        "directory_name": "memchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 628810.1573029557,
            "upper_bound": 629594.7865968801
          },
          "point_estimate": 629229.0618842364,
          "standard_error": 201.18812692306128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629012.4162561577,
            "upper_bound": 629560.6048850575
          },
          "point_estimate": 629271.6683908046,
          "standard_error": 135.53779335752503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.935754410201376,
            "upper_bound": 988.8681943407288
          },
          "point_estimate": 398.36128413457527,
          "standard_error": 242.17769382812776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 629055.2044675673,
            "upper_bound": 629550.7162933813
          },
          "point_estimate": 629349.1795342588,
          "standard_error": 125.64813584292624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 223.93147977709444,
            "upper_bound": 953.432644492348
          },
          "point_estimate": 671.7945643249254,
          "standard_error": 195.06682653635997
        }
      }
    },
    "memchr3/krate/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/never",
        "directory_name": "memchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15040.642974601698,
            "upper_bound": 15104.86460652112
          },
          "point_estimate": 15071.10569443624,
          "standard_error": 16.459697371490368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15030.102881946495,
            "upper_bound": 15120.579164944193
          },
          "point_estimate": 15052.408875338751,
          "standard_error": 21.664257477869327
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.679532427440277,
            "upper_bound": 84.76242854064138
          },
          "point_estimate": 36.82707238463339,
          "standard_error": 24.077328803999592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15024.733443674244,
            "upper_bound": 15060.08988616328
          },
          "point_estimate": 15040.738153041666,
          "standard_error": 9.319894480900253
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.893033276686637,
            "upper_bound": 66.4937767802834
          },
          "point_estimate": 54.623369843764145,
          "standard_error": 10.113194618331224
        }
      }
    },
    "memchr3/krate/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/rare",
        "directory_name": "memchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20411.847721319988,
            "upper_bound": 20446.543490346074
          },
          "point_estimate": 20429.701866842537,
          "standard_error": 8.901221165550364
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20413.81643700787,
            "upper_bound": 20453.072553430826
          },
          "point_estimate": 20426.000149981253,
          "standard_error": 11.59119461462296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7649831783790466,
            "upper_bound": 53.01787512173579
          },
          "point_estimate": 24.399120990714827,
          "standard_error": 12.429130190231152
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20414.832959082534,
            "upper_bound": 20430.909018411185
          },
          "point_estimate": 20421.19023125356,
          "standard_error": 4.0603820817793945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.765794039961056,
            "upper_bound": 40.118281693515975
          },
          "point_estimate": 29.68776174875885,
          "standard_error": 6.680310150993344
        }
      }
    },
    "memchr3/krate/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/uncommon",
        "directory_name": "memchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218564.38431470393,
            "upper_bound": 218843.8048731109
          },
          "point_estimate": 218699.72643332384,
          "standard_error": 71.3585151249472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218448.13073852297,
            "upper_bound": 218854.56886227545
          },
          "point_estimate": 218703.36197604792,
          "standard_error": 95.08125303331607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.688825043896344,
            "upper_bound": 453.8300875716525
          },
          "point_estimate": 226.40174586079084,
          "standard_error": 101.03350855539612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218558.29329065653,
            "upper_bound": 218726.0197791916
          },
          "point_estimate": 218656.47098530212,
          "standard_error": 42.20589428162584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.39850277700532,
            "upper_bound": 310.4985701698995
          },
          "point_estimate": 237.79572472778364,
          "standard_error": 48.24922233392854
        }
      }
    },
    "memchr3/krate/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/common",
        "directory_name": "memchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630.2388593362609,
            "upper_bound": 631.0149348681737
          },
          "point_estimate": 630.6052068994727,
          "standard_error": 0.19925471962275948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630.0108903158625,
            "upper_bound": 631.1171711211384
          },
          "point_estimate": 630.542291449728,
          "standard_error": 0.2466260169233544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06475718107876592,
            "upper_bound": 1.119376488717894
          },
          "point_estimate": 0.6497101348988508,
          "standard_error": 0.267458620419516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630.2721037533319,
            "upper_bound": 630.8498721251439
          },
          "point_estimate": 630.5681653195815,
          "standard_error": 0.143188525682359
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.298732069456638,
            "upper_bound": 0.8280417987223021
          },
          "point_estimate": 0.6637305730631576,
          "standard_error": 0.13400136149643718
        }
      }
    },
    "memchr3/krate/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/never",
        "directory_name": "memchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.35100760485092,
            "upper_bound": 15.36793895762746
          },
          "point_estimate": 15.359307292506898,
          "standard_error": 0.004343598690021296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.347885271437663,
            "upper_bound": 15.369173000679124
          },
          "point_estimate": 15.35844897097493,
          "standard_error": 0.004922213982566585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035549109500372517,
            "upper_bound": 0.025396160569139035
          },
          "point_estimate": 0.015780593406530905,
          "standard_error": 0.0055437440760622006
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.343138943753074,
            "upper_bound": 15.367514480851446
          },
          "point_estimate": 15.354930365063172,
          "standard_error": 0.006531417824707314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0074017888811574685,
            "upper_bound": 0.019148153018169356
          },
          "point_estimate": 0.01456161841610992,
          "standard_error": 0.003032788170322861
        }
      }
    },
    "memchr3/krate/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/rare",
        "directory_name": "memchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.54994766357417,
            "upper_bound": 33.64757271610752
          },
          "point_estimate": 33.59720110679147,
          "standard_error": 0.024985221390189837
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.53167483909097,
            "upper_bound": 33.66921904867441
          },
          "point_estimate": 33.56848189138525,
          "standard_error": 0.04104378282141247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014015381261802733,
            "upper_bound": 0.1311187315281718
          },
          "point_estimate": 0.09873328918082087,
          "standard_error": 0.03170294772606518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.54451838159369,
            "upper_bound": 33.612572338157634
          },
          "point_estimate": 33.571417905122644,
          "standard_error": 0.017150936248961034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04938006629293215,
            "upper_bound": 0.10256472888989632
          },
          "point_estimate": 0.08345141744217217,
          "standard_error": 0.013737041565353854
        }
      }
    },
    "memchr3/krate/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/uncommon",
        "directory_name": "memchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.14300018090705,
            "upper_bound": 172.77484047079116
          },
          "point_estimate": 172.46780667862637,
          "standard_error": 0.16151250385451835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.15516300552696,
            "upper_bound": 172.80154409285808
          },
          "point_estimate": 172.52976149211068,
          "standard_error": 0.1985667891521491
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04975439211575988,
            "upper_bound": 0.9370891962825614
          },
          "point_estimate": 0.46915657084185686,
          "standard_error": 0.2109465895210624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.18159914452133,
            "upper_bound": 172.71245665264198
          },
          "point_estimate": 172.46664279353394,
          "standard_error": 0.1364514938724678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.25712237006641503,
            "upper_bound": 0.7232124717410543
          },
          "point_estimate": 0.5387382367641059,
          "standard_error": 0.12185931794198492
        }
      }
    },
    "memchr3/krate/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/never",
        "directory_name": "memchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.38018665099546,
            "upper_bound": 5.389501398380745
          },
          "point_estimate": 5.384602811660928,
          "standard_error": 0.002387145429715515
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.378848326237765,
            "upper_bound": 5.391620259658097
          },
          "point_estimate": 5.381851024426586,
          "standard_error": 0.00320218586247981
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000939486461954168,
            "upper_bound": 0.01266916930040741
          },
          "point_estimate": 0.006392283393450814,
          "standard_error": 0.00305798598196844
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.377560186224054,
            "upper_bound": 5.391136499819759
          },
          "point_estimate": 5.382849252145663,
          "standard_error": 0.0035823960736212023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00392908880659285,
            "upper_bound": 0.010072554448200818
          },
          "point_estimate": 0.007945556979256198,
          "standard_error": 0.0015591599560635775
        }
      }
    },
    "memchr3/krate/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/rare",
        "directory_name": "memchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.10441871181013,
            "upper_bound": 20.136375292147637
          },
          "point_estimate": 20.120042713646747,
          "standard_error": 0.008213168375825624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.09320184194416,
            "upper_bound": 20.15367446121359
          },
          "point_estimate": 20.11357190732016,
          "standard_error": 0.014723430726467987
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002215869161329289,
            "upper_bound": 0.04446553339989356
          },
          "point_estimate": 0.03158493063807172,
          "standard_error": 0.011215175286695078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.104174087999592,
            "upper_bound": 20.140713022775856
          },
          "point_estimate": 20.122338301565524,
          "standard_error": 0.009816282010369643
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016679264384082293,
            "upper_bound": 0.0316274999743789
          },
          "point_estimate": 0.0272658515043347,
          "standard_error": 0.003767533506324021
        }
      }
    },
    "memchr3/krate/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/uncommon",
        "directory_name": "memchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.55809945972688,
            "upper_bound": 82.73373925766215
          },
          "point_estimate": 82.63972947288937,
          "standard_error": 0.045329023829352645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.54198370984814,
            "upper_bound": 82.74313028941594
          },
          "point_estimate": 82.60322153728276,
          "standard_error": 0.04333196865669495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02329770345935271,
            "upper_bound": 0.24637098591373985
          },
          "point_estimate": 0.08845257818825694,
          "standard_error": 0.058220403964734255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.55456893399074,
            "upper_bound": 82.71741477932297
          },
          "point_estimate": 82.61763661649412,
          "standard_error": 0.041509529994952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0585736123193534,
            "upper_bound": 0.1977366753438452
          },
          "point_estimate": 0.15069563378011855,
          "standard_error": 0.03569801671396832
        }
      }
    },
    "memchr3/naive/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/naive/empty/never",
        "directory_name": "memchr3/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6108991493627023,
            "upper_bound": 0.6116202233952115
          },
          "point_estimate": 0.6112544288032653,
          "standard_error": 0.00018485448098687937
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6107238625657576,
            "upper_bound": 0.6118026690394244
          },
          "point_estimate": 0.6111851167586608,
          "standard_error": 0.00031489155034377346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000027741625464754463,
            "upper_bound": 0.0010176015245227646
          },
          "point_estimate": 0.0006900192435212133,
          "standard_error": 0.0002424173114646187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6109115527045947,
            "upper_bound": 0.6117514152490137
          },
          "point_estimate": 0.6112806413057219,
          "standard_error": 0.000215581929617059
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000378276447966028,
            "upper_bound": 0.0007461822347076244
          },
          "point_estimate": 0.0006159082304727075,
          "standard_error": 0.00009400513256363777
        }
      }
    },
    "memchr3/naive/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/common",
        "directory_name": "memchr3/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1150581.154216828,
            "upper_bound": 1152849.207032366
          },
          "point_estimate": 1151665.8216542655,
          "standard_error": 579.4175321050095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1150159.8020833333,
            "upper_bound": 1152653.93125
          },
          "point_estimate": 1151659.2672991073,
          "standard_error": 572.2500016487188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292.6248604298949,
            "upper_bound": 3296.5018404336306
          },
          "point_estimate": 1616.574502550123,
          "standard_error": 828.3445104579255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1150187.677528769,
            "upper_bound": 1152055.3906932315
          },
          "point_estimate": 1151209.834090909,
          "standard_error": 494.3649140096988
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 943.8806985849768,
            "upper_bound": 2590.6586160721386
          },
          "point_estimate": 1929.2937826198436,
          "standard_error": 445.55177448622953
        }
      }
    },
    "memchr3/naive/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/never",
        "directory_name": "memchr3/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290886.4453275555,
            "upper_bound": 291334.2368
          },
          "point_estimate": 291098.8354755555,
          "standard_error": 115.11745073232808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290798.4596,
            "upper_bound": 291597.712
          },
          "point_estimate": 290965.90266666666,
          "standard_error": 182.8189735626566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.9903477787952,
            "upper_bound": 545.3147668520322
          },
          "point_estimate": 280.50297302006067,
          "standard_error": 145.95112195606768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290782.0714815266,
            "upper_bound": 291059.3119355507
          },
          "point_estimate": 290893.8951272727,
          "standard_error": 70.21469996511111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.87166957173125,
            "upper_bound": 450.5312034092381
          },
          "point_estimate": 384.7144486978305,
          "standard_error": 63.81572007745759
        }
      }
    },
    "memchr3/naive/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/rare",
        "directory_name": "memchr3/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294952.1818996416,
            "upper_bound": 295478.8265642601
          },
          "point_estimate": 295205.0269828469,
          "standard_error": 134.45456605842315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294851.97614247317,
            "upper_bound": 295605.4735023042
          },
          "point_estimate": 295119.96673387097,
          "standard_error": 174.3121062163216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.08919609545723,
            "upper_bound": 770.1494095126126
          },
          "point_estimate": 463.9146319520411,
          "standard_error": 187.4393942046501
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295007.1479663777,
            "upper_bound": 295428.32336968667
          },
          "point_estimate": 295198.9280687055,
          "standard_error": 106.52023246692532
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233.58622059549768,
            "upper_bound": 561.3374812849695
          },
          "point_estimate": 448.76684537353145,
          "standard_error": 82.17683822187533
        }
      }
    },
    "memchr3/naive/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/uncommon",
        "directory_name": "memchr3/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462761.8550512357,
            "upper_bound": 463761.2212926462
          },
          "point_estimate": 463205.2879063693,
          "standard_error": 258.90847215644726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462540.7025316456,
            "upper_bound": 463560.7199819168
          },
          "point_estimate": 462950.6522503516,
          "standard_error": 238.5301411571458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.74895179045151,
            "upper_bound": 1173.9973402544908
          },
          "point_estimate": 614.8619426704907,
          "standard_error": 303.19116375584764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462654.2107296542,
            "upper_bound": 463439.1171479791
          },
          "point_estimate": 463020.97455203027,
          "standard_error": 195.74211726679687
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284.79125443249603,
            "upper_bound": 1187.9042736787983
          },
          "point_estimate": 859.447442988877,
          "standard_error": 248.59940985426624
        }
      }
    },
    "memchr3/naive/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/common",
        "directory_name": "memchr3/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361.89483585877485,
            "upper_bound": 363.319841610284
          },
          "point_estimate": 362.5319672359271,
          "standard_error": 0.36735513745682674
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361.7228444834744,
            "upper_bound": 363.0760837458087
          },
          "point_estimate": 362.2165233813667,
          "standard_error": 0.3651900311120987
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.23903314540508844,
            "upper_bound": 1.7433177415830603
          },
          "point_estimate": 0.8762209786260693,
          "standard_error": 0.3734475902242846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361.78275318267794,
            "upper_bound": 362.54259057439594
          },
          "point_estimate": 362.04639588616715,
          "standard_error": 0.1953166638556277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4770306419333284,
            "upper_bound": 1.7076513650201428
          },
          "point_estimate": 1.223989969688546,
          "standard_error": 0.3542782016399296
        }
      }
    },
    "memchr3/naive/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/never",
        "directory_name": "memchr3/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.9636026961232,
            "upper_bound": 334.35374583628277
          },
          "point_estimate": 334.1628999970232,
          "standard_error": 0.09955821673974824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.98844080090464,
            "upper_bound": 334.40301718194104
          },
          "point_estimate": 334.13840012134915,
          "standard_error": 0.10062907749324718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05952950265255541,
            "upper_bound": 0.5641772416087457
          },
          "point_estimate": 0.2663542457102463,
          "standard_error": 0.12605708332827148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.0058978229408,
            "upper_bound": 334.3551792720233
          },
          "point_estimate": 334.1576956024293,
          "standard_error": 0.08820314348820828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14899360824868052,
            "upper_bound": 0.4498186632521189
          },
          "point_estimate": 0.3311149942049179,
          "standard_error": 0.07789409584190282
        }
      }
    },
    "memchr3/naive/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/rare",
        "directory_name": "memchr3/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355.55362644370604,
            "upper_bound": 359.72018245829173
          },
          "point_estimate": 357.7754302811853,
          "standard_error": 1.0674503026599715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 354.9896606968028,
            "upper_bound": 360.26208872714324
          },
          "point_estimate": 359.3728273476936,
          "standard_error": 1.420311815947342
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2981613981025685,
            "upper_bound": 5.329233529038504
          },
          "point_estimate": 1.616580295868026,
          "standard_error": 1.2960490358839232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 353.0487546262089,
            "upper_bound": 359.573076368731
          },
          "point_estimate": 355.215257142703,
          "standard_error": 1.538501531823038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7155291445301557,
            "upper_bound": 4.281966970204809
          },
          "point_estimate": 3.5611370984685284,
          "standard_error": 0.7739422676662248
        }
      }
    },
    "memchr3/naive/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/uncommon",
        "directory_name": "memchr3/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408.5086532352206,
            "upper_bound": 408.9237949711565
          },
          "point_estimate": 408.7120101382512,
          "standard_error": 0.10649446756779028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408.4455799685743,
            "upper_bound": 409.014945317588
          },
          "point_estimate": 408.6680131327791,
          "standard_error": 0.12470950771546868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06263330353327269,
            "upper_bound": 0.6059241496936091
          },
          "point_estimate": 0.35537101270005106,
          "standard_error": 0.1543399707888003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408.4193733850115,
            "upper_bound": 408.8405355337147
          },
          "point_estimate": 408.6510153632972,
          "standard_error": 0.10691892510591172
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1880963640334764,
            "upper_bound": 0.45485540452814627
          },
          "point_estimate": 0.35393102255572295,
          "standard_error": 0.06865951388419977
        }
      }
    },
    "memchr3/naive/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/never",
        "directory_name": "memchr3/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.6531624783711,
            "upper_bound": 42.26519652896755
          },
          "point_estimate": 41.960690748977306,
          "standard_error": 0.15624709422953126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.5881398414138,
            "upper_bound": 42.37682491648475
          },
          "point_estimate": 41.9551206166836,
          "standard_error": 0.1687740145570449
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12711002561214327,
            "upper_bound": 0.9512111714446468
          },
          "point_estimate": 0.3854649081298218,
          "standard_error": 0.22661930978223285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.66423456677135,
            "upper_bound": 42.37599546008188
          },
          "point_estimate": 42.01793044232054,
          "standard_error": 0.18011305701286368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.275899866933208,
            "upper_bound": 0.6679273654694837
          },
          "point_estimate": 0.5195972666686058,
          "standard_error": 0.10130974729692732
        }
      }
    },
    "memchr3/naive/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/rare",
        "directory_name": "memchr3/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.428826429761145,
            "upper_bound": 39.32589422058565
          },
          "point_estimate": 38.848990010307226,
          "standard_error": 0.2300489554357675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.21463434077724,
            "upper_bound": 39.314287494428235
          },
          "point_estimate": 38.854010519936494,
          "standard_error": 0.2867875773235303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10410188558126024,
            "upper_bound": 1.2335545796994332
          },
          "point_estimate": 0.8438372208109758,
          "standard_error": 0.3108265960059274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.26181184963907,
            "upper_bound": 39.22898454761311
          },
          "point_estimate": 38.681036960891646,
          "standard_error": 0.24680418663887627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.374359662553057,
            "upper_bound": 1.017079673356226
          },
          "point_estimate": 0.7683818656628829,
          "standard_error": 0.17728764488524007
        }
      }
    },
    "memchr3/naive/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memchr3/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/uncommon",
        "directory_name": "memchr3/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.00834868330948,
            "upper_bound": 45.100743044965256
          },
          "point_estimate": 45.05279317333805,
          "standard_error": 0.02368706572006707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.970116635454595,
            "upper_bound": 45.10757608873832
          },
          "point_estimate": 45.049831169185985,
          "standard_error": 0.03678216905369316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007281502719557895,
            "upper_bound": 0.1418267168832731
          },
          "point_estimate": 0.08830212139132625,
          "standard_error": 0.03306235799201048
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.03403858470438,
            "upper_bound": 45.13651641711983
          },
          "point_estimate": 45.090428178355815,
          "standard_error": 0.025583636975122157
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04575541458050617,
            "upper_bound": 0.1010163723582411
          },
          "point_estimate": 0.07889599623257296,
          "standard_error": 0.015095267603890138
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40942.799448694546,
            "upper_bound": 41023.67910341172
          },
          "point_estimate": 40982.18806951379,
          "standard_error": 20.718445057664436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40920.51014656144,
            "upper_bound": 41030.18313917074
          },
          "point_estimate": 40979.156275836154,
          "standard_error": 31.217552022230976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.609142360985231,
            "upper_bound": 122.45635049789244
          },
          "point_estimate": 81.30058797789698,
          "standard_error": 25.670657891577477
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40947.50892983656,
            "upper_bound": 41015.045695156594
          },
          "point_estimate": 40980.32919076414,
          "standard_error": 17.20352647231439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.49393594279106,
            "upper_bound": 88.18100883780846
          },
          "point_estimate": 68.9344315033298,
          "standard_error": 12.87262631465141
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48787.4749078341,
            "upper_bound": 48835.97379873913
          },
          "point_estimate": 48811.48684779827,
          "standard_error": 12.36893386399614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48786.94727662571,
            "upper_bound": 48836.37785618279
          },
          "point_estimate": 48809.7376344086,
          "standard_error": 14.486664539119213
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2806509901435277,
            "upper_bound": 70.32311437651695
          },
          "point_estimate": 29.834335357434227,
          "standard_error": 16.005892935639828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48792.53693196011,
            "upper_bound": 48833.39392003583
          },
          "point_estimate": 48817.05303379416,
          "standard_error": 10.3235276726342
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.63358998644711,
            "upper_bound": 55.08595224117067
          },
          "point_estimate": 41.306418310403785,
          "standard_error": 9.089748704121522
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48668.24912772884,
            "upper_bound": 48815.09845144474
          },
          "point_estimate": 48732.59567077322,
          "standard_error": 37.98221805866204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48639.95107238606,
            "upper_bound": 48805.176943699735
          },
          "point_estimate": 48686.12191210264,
          "standard_error": 38.27641576199233
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.055432440135151,
            "upper_bound": 178.15225135012028
          },
          "point_estimate": 71.8470327392065,
          "standard_error": 42.08181894490859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48651.602329048605,
            "upper_bound": 48734.00182023423
          },
          "point_estimate": 48685.04489049824,
          "standard_error": 21.23234903148183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.78519085563923,
            "upper_bound": 177.71902814328936
          },
          "point_estimate": 126.99206717693416,
          "standard_error": 38.46191709832593
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14216.131608351934,
            "upper_bound": 14240.505278490824
          },
          "point_estimate": 14227.979543464782,
          "standard_error": 6.215044182266009
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14211.14095703125,
            "upper_bound": 14240.75693359375
          },
          "point_estimate": 14229.679779730905,
          "standard_error": 9.285843876735786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8228209539680225,
            "upper_bound": 40.077901171288346
          },
          "point_estimate": 20.835693633998655,
          "standard_error": 8.82954259046035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14215.621760177692,
            "upper_bound": 14241.226261393227
          },
          "point_estimate": 14227.099517045455,
          "standard_error": 6.5940202294047685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.498366214148664,
            "upper_bound": 27.13900465295223
          },
          "point_estimate": 20.683077156275186,
          "standard_error": 4.1857676476639725
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27317.71329143859,
            "upper_bound": 27346.72958296066
          },
          "point_estimate": 27331.833067809144,
          "standard_error": 7.35253666093573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27317.912325736703,
            "upper_bound": 27341.37577814747
          },
          "point_estimate": 27330.966892061108,
          "standard_error": 5.980576898513198
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.091528063071414,
            "upper_bound": 39.42572326699669
          },
          "point_estimate": 13.874107804023604,
          "standard_error": 8.804418531609082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27325.305927209574,
            "upper_bound": 27337.582277304267
          },
          "point_estimate": 27332.03607872218,
          "standard_error": 3.108636813604035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.085923267872886,
            "upper_bound": 34.62929509445316
          },
          "point_estimate": 24.65436025840319,
          "standard_error": 6.697714079519879
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17356.385074953792,
            "upper_bound": 17386.83102352859
          },
          "point_estimate": 17370.900953274617,
          "standard_error": 7.818129620891245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17352.361595591756,
            "upper_bound": 17389.45325560347
          },
          "point_estimate": 17363.890013005683,
          "standard_error": 8.8832555055743
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.846715761632164,
            "upper_bound": 42.62325346145926
          },
          "point_estimate": 25.450727283344943,
          "standard_error": 10.654358747361382
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17359.29744839162,
            "upper_bound": 17377.318151718453
          },
          "point_estimate": 17368.531458192025,
          "standard_error": 4.621598916732743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.917916808226993,
            "upper_bound": 33.90724738021411
          },
          "point_estimate": 26.034782305094733,
          "standard_error": 5.4663137246571445
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17367.37476577068,
            "upper_bound": 17390.15604975099
          },
          "point_estimate": 17377.528550507115,
          "standard_error": 5.86575102916118
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17366.03354823305,
            "upper_bound": 17384.767478510028
          },
          "point_estimate": 17374.74542343203,
          "standard_error": 4.320581138243107
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4013420599580835,
            "upper_bound": 25.885376593925717
          },
          "point_estimate": 9.15045268854865,
          "standard_error": 5.913558970272746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17362.598269656002,
            "upper_bound": 17374.475821276177
          },
          "point_estimate": 17367.987782036493,
          "standard_error": 2.9998601889282366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.540502307541214,
            "upper_bound": 27.89058535133772
          },
          "point_estimate": 19.5428626853885,
          "standard_error": 6.144196198970295
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17582.638030679645,
            "upper_bound": 17609.7370974706
          },
          "point_estimate": 17593.735042923887,
          "standard_error": 7.225041386493189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17583.484127943106,
            "upper_bound": 17596.82307878202
          },
          "point_estimate": 17587.21419365233,
          "standard_error": 3.4119609342622828
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8280656450359235,
            "upper_bound": 23.5878682013373
          },
          "point_estimate": 5.646963420771844,
          "standard_error": 5.695501834542901
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17576.334710482537,
            "upper_bound": 17593.100686947437
          },
          "point_estimate": 17583.52027643695,
          "standard_error": 4.331948524353665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.227689501430507,
            "upper_bound": 36.04365954364095
          },
          "point_estimate": 24.113816595789416,
          "standard_error": 9.837143574955403
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38579.97419533192,
            "upper_bound": 38739.361057862974
          },
          "point_estimate": 38659.09408489274,
          "standard_error": 41.01222352137981
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38515.426695065675,
            "upper_bound": 38810.691870784525
          },
          "point_estimate": 38667.35658501953,
          "standard_error": 70.72065355047704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.286424772995009,
            "upper_bound": 240.64210696310468
          },
          "point_estimate": 218.88007087448585,
          "standard_error": 68.17874780193698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38531.25266356944,
            "upper_bound": 38664.45470250144
          },
          "point_estimate": 38574.89495318313,
          "standard_error": 34.20648499737458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.97137143860088,
            "upper_bound": 159.76203504865953
          },
          "point_estimate": 136.7902129735377,
          "standard_error": 18.73876855767878
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18783.81440070765,
            "upper_bound": 19003.939801977365
          },
          "point_estimate": 18880.59249639622,
          "standard_error": 55.80915193718256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18769.813180008845,
            "upper_bound": 19026.66544117647
          },
          "point_estimate": 18778.042985322783,
          "standard_error": 59.46185393903809
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.44194208832156,
            "upper_bound": 240.9476456062272
          },
          "point_estimate": 20.53512315795992,
          "standard_error": 58.56614068469231
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18769.819164247623,
            "upper_bound": 18883.30726520514
          },
          "point_estimate": 18801.06392853792,
          "standard_error": 30.2635983918241
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.36980040560985,
            "upper_bound": 232.79419146100383
          },
          "point_estimate": 186.0440314811104,
          "standard_error": 53.118401016685866
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18542.294259934824,
            "upper_bound": 18568.953175318595
          },
          "point_estimate": 18555.64774989056,
          "standard_error": 6.851758074249803
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18536.91700715015,
            "upper_bound": 18574.807186633592
          },
          "point_estimate": 18554.65360061287,
          "standard_error": 10.485127458115942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.450281627060047,
            "upper_bound": 37.42971792588832
          },
          "point_estimate": 24.351654087590617,
          "standard_error": 8.217433105197248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18543.202409364425,
            "upper_bound": 18570.31390799402
          },
          "point_estimate": 18555.98570897948,
          "standard_error": 6.889105659150946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.507569057062469,
            "upper_bound": 28.590219491116027
          },
          "point_estimate": 22.825142699221814,
          "standard_error": 3.874068210160276
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17724.508398922706,
            "upper_bound": 17756.108491749088
          },
          "point_estimate": 17739.908148773728,
          "standard_error": 8.063417817063558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17728.277895799958,
            "upper_bound": 17758.10324232082
          },
          "point_estimate": 17733.43538720949,
          "standard_error": 7.373024481330168
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0137575502614693,
            "upper_bound": 46.78221771325613
          },
          "point_estimate": 11.110128349806992,
          "standard_error": 12.134826820370396
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17730.255136331245,
            "upper_bound": 17743.63172708319
          },
          "point_estimate": 17737.911901068215,
          "standard_error": 3.3703886780720813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.5757446921545,
            "upper_bound": 36.355219857468754
          },
          "point_estimate": 26.97271346868864,
          "standard_error": 6.499657357765153
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17610.127182155535,
            "upper_bound": 17627.860317252398
          },
          "point_estimate": 17618.90586819481,
          "standard_error": 4.541212519317296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17605.738201839304,
            "upper_bound": 17632.67927442032
          },
          "point_estimate": 17618.171236689253,
          "standard_error": 5.837918352814886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.442105330952304,
            "upper_bound": 27.280663068429742
          },
          "point_estimate": 14.982682782164694,
          "standard_error": 6.2898131045949155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17608.230862185384,
            "upper_bound": 17627.429647182544
          },
          "point_estimate": 17616.949082862928,
          "standard_error": 4.899080113685951
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.598874292388503,
            "upper_bound": 18.992941332575093
          },
          "point_estimate": 15.175999841050054,
          "standard_error": 2.6743426278005815
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17563.73302580708,
            "upper_bound": 17579.438389651164
          },
          "point_estimate": 17571.359239680238,
          "standard_error": 4.031969726454055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17560.89001450677,
            "upper_bound": 17583.67148210832
          },
          "point_estimate": 17568.398996615088,
          "standard_error": 5.6824459599176524
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4456190555255226,
            "upper_bound": 22.910937141603895
          },
          "point_estimate": 12.02812997987968,
          "standard_error": 5.184105410276294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17562.036197305973,
            "upper_bound": 17580.112388763624
          },
          "point_estimate": 17571.19876535457,
          "standard_error": 4.535818318188966
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.27924781717083,
            "upper_bound": 16.49864346411233
          },
          "point_estimate": 13.429418784799672,
          "standard_error": 2.2881872731422237
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17595.84905693916,
            "upper_bound": 17608.78769207118
          },
          "point_estimate": 17602.35915938353,
          "standard_error": 3.3167883098358284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17592.49539951574,
            "upper_bound": 17611.852542372882
          },
          "point_estimate": 17602.587155540183,
          "standard_error": 6.084367793277939
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5560838028818158,
            "upper_bound": 17.614400534739975
          },
          "point_estimate": 14.118424212625902,
          "standard_error": 4.186774904537754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17596.541584347808,
            "upper_bound": 17611.58793885571
          },
          "point_estimate": 17605.245794786326,
          "standard_error": 3.930039144907753
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.143108092079751,
            "upper_bound": 13.003409447193947
          },
          "point_estimate": 11.022719422748873,
          "standard_error": 1.501514841365708
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15283.533812972886,
            "upper_bound": 15455.674881252626
          },
          "point_estimate": 15362.473504390266,
          "standard_error": 44.259450508004235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15263.28981598244,
            "upper_bound": 15459.521358764188
          },
          "point_estimate": 15275.483837746951,
          "standard_error": 57.64961451630526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.304075028450971,
            "upper_bound": 233.3100708894449
          },
          "point_estimate": 30.756709912136447,
          "standard_error": 62.1493131585943
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15271.449420268393,
            "upper_bound": 15320.517751153233
          },
          "point_estimate": 15288.890018178545,
          "standard_error": 12.849959402651915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.4925664966306,
            "upper_bound": 182.07140013631212
          },
          "point_estimate": 147.60035863971305,
          "standard_error": 34.592240592522224
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16278.206218637995,
            "upper_bound": 16294.32936705816
          },
          "point_estimate": 16286.294164408744,
          "standard_error": 4.106965995398635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16275.29211469534,
            "upper_bound": 16291.336021505376
          },
          "point_estimate": 16288.64083781362,
          "standard_error": 3.9353687126391654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6001059201249125,
            "upper_bound": 22.6383229403827
          },
          "point_estimate": 4.248293244469376,
          "standard_error": 6.355222366012962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16280.698550916764,
            "upper_bound": 16292.477164258846
          },
          "point_estimate": 16286.79136526556,
          "standard_error": 3.0585906449390396
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.065085774565383,
            "upper_bound": 18.467309985021437
          },
          "point_estimate": 13.62224370100252,
          "standard_error": 3.406130609962299
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17969.715799417805,
            "upper_bound": 18035.65452200488
          },
          "point_estimate": 17994.00628794655,
          "standard_error": 18.97153148323047
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17965.96721041358,
            "upper_bound": 17985.41306846268
          },
          "point_estimate": 17978.96608643457,
          "standard_error": 7.264296985001014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1845488078151383,
            "upper_bound": 25.990815455524455
          },
          "point_estimate": 13.596011566582828,
          "standard_error": 7.647669336709425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17971.483418135056,
            "upper_bound": 17984.16402603981
          },
          "point_estimate": 17979.57426093432,
          "standard_error": 3.232716045500168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.509922834427443,
            "upper_bound": 97.13971181358856
          },
          "point_estimate": 63.126171456845874,
          "standard_error": 31.769231962182214
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18193.21872275,
            "upper_bound": 18235.091764999997
          },
          "point_estimate": 18212.15628833333,
          "standard_error": 10.73892313377503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18184.19004166667,
            "upper_bound": 18238.691
          },
          "point_estimate": 18203.630125,
          "standard_error": 12.575845356445877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.213549125195369,
            "upper_bound": 58.35524615898804
          },
          "point_estimate": 29.29313614994177,
          "standard_error": 13.315105586809596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18195.411248777964,
            "upper_bound": 18243.14544057377
          },
          "point_estimate": 18218.63908051948,
          "standard_error": 12.249896560907557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.433876047030749,
            "upper_bound": 48.29972355499181
          },
          "point_estimate": 35.79351560264768,
          "standard_error": 9.162879745211171
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68616.40441899587,
            "upper_bound": 68823.32308031776
          },
          "point_estimate": 68702.43756368708,
          "standard_error": 54.52402321203151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68596.93383742911,
            "upper_bound": 68725.52819785758
          },
          "point_estimate": 68663.11487082546,
          "standard_error": 32.084947943997335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.449308307949394,
            "upper_bound": 180.8868425636528
          },
          "point_estimate": 88.37479961250733,
          "standard_error": 43.5056230228551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68624.43733848757,
            "upper_bound": 68705.76974405606
          },
          "point_estimate": 68665.65560110967,
          "standard_error": 20.32219084971227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.23082447831841,
            "upper_bound": 270.3681752335061
          },
          "point_estimate": 181.58114469463072,
          "standard_error": 70.3008268627034
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1219878.2809756943,
            "upper_bound": 1221924.2436766203
          },
          "point_estimate": 1220873.0665092592,
          "standard_error": 522.4253465967348
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1219742.53125,
            "upper_bound": 1222105.4779629628
          },
          "point_estimate": 1220631.3444444444,
          "standard_error": 511.3604726630136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 283.2671983042854,
            "upper_bound": 2914.559274256301
          },
          "point_estimate": 1056.1852769157233,
          "standard_error": 730.649353999691
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1219909.965412494,
            "upper_bound": 1221984.362438337
          },
          "point_estimate": 1220990.04987013,
          "standard_error": 534.312666468056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.8014392147264,
            "upper_bound": 2289.2452421405615
          },
          "point_estimate": 1745.9934778561858,
          "standard_error": 374.9501357977159
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205081.84466482827,
            "upper_bound": 205447.00973006905
          },
          "point_estimate": 205252.9070471258,
          "standard_error": 93.62651356124852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205071.030535378,
            "upper_bound": 205481.9745762712
          },
          "point_estimate": 205153.09145480223,
          "standard_error": 98.55715579256338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.723231469620387,
            "upper_bound": 512.7633783966158
          },
          "point_estimate": 196.64492871222637,
          "standard_error": 129.73554707746214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205110.3321992228,
            "upper_bound": 205449.30319286787
          },
          "point_estimate": 205242.0134565999,
          "standard_error": 87.72765396417081
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.38016589434096,
            "upper_bound": 411.5793427141326
          },
          "point_estimate": 312.3831484670524,
          "standard_error": 74.0214849201097
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7439.690669289419,
            "upper_bound": 7453.273017703579
          },
          "point_estimate": 7446.651164443664,
          "standard_error": 3.4812430811680506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7437.111742424242,
            "upper_bound": 7458.587701337701
          },
          "point_estimate": 7447.397046478296,
          "standard_error": 5.8349145776196085
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8292078053632577,
            "upper_bound": 20.272255152994603
          },
          "point_estimate": 15.92012805990852,
          "standard_error": 4.6268888337286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7441.422121395712,
            "upper_bound": 7451.793670158941
          },
          "point_estimate": 7447.23927587564,
          "standard_error": 2.626367038979167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.072609776972804,
            "upper_bound": 14.263791763795668
          },
          "point_estimate": 11.603408192871578,
          "standard_error": 1.8754421510463253
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7575.418265762167,
            "upper_bound": 7588.126859418586
          },
          "point_estimate": 7581.761425130125,
          "standard_error": 3.2531627784946577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7571.515301229935,
            "upper_bound": 7592.426850114655
          },
          "point_estimate": 7581.887059041531,
          "standard_error": 6.546507187333226
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6905200878044576,
            "upper_bound": 16.688440066447665
          },
          "point_estimate": 15.432630140164576,
          "standard_error": 4.345321017229417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7575.258107818029,
            "upper_bound": 7589.352245655206
          },
          "point_estimate": 7581.7377197328415,
          "standard_error": 3.642010408496968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.268328989990985,
            "upper_bound": 12.494696100303877
          },
          "point_estimate": 10.825304651200286,
          "standard_error": 1.3374567584296249
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14122.750872650986,
            "upper_bound": 14144.815735055408
          },
          "point_estimate": 14133.12839716174,
          "standard_error": 5.651208903860263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14116.842345451012,
            "upper_bound": 14145.87240798341
          },
          "point_estimate": 14129.090785381026,
          "standard_error": 8.10033728269199
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.911735271011508,
            "upper_bound": 32.79551108183723
          },
          "point_estimate": 20.73130053591247,
          "standard_error": 7.047561680792238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14117.430358052365,
            "upper_bound": 14138.002581795705
          },
          "point_estimate": 14126.500328209891,
          "standard_error": 5.38386812930264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.959494265715326,
            "upper_bound": 24.556547690294902
          },
          "point_estimate": 18.831473386419233,
          "standard_error": 3.982236375982217
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.87757430382684,
            "upper_bound": 60.11271242036313
          },
          "point_estimate": 59.99446916480258,
          "standard_error": 0.06000847963882448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.87508264614448,
            "upper_bound": 60.09819500820451
          },
          "point_estimate": 59.98880256947258,
          "standard_error": 0.05614503247492663
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026940559354827708,
            "upper_bound": 0.3396875975221936
          },
          "point_estimate": 0.1439137771365428,
          "standard_error": 0.07648109708886776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.91021758744152,
            "upper_bound": 60.07365644969786
          },
          "point_estimate": 59.993880422194614,
          "standard_error": 0.04152504460428373
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07798250907695198,
            "upper_bound": 0.2718330210374861
          },
          "point_estimate": 0.1995053892639272,
          "standard_error": 0.048379336233894
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.25223396622345,
            "upper_bound": 33.313793503187625
          },
          "point_estimate": 33.28106206049268,
          "standard_error": 0.015799342414389402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.249350342398714,
            "upper_bound": 33.31866201873355
          },
          "point_estimate": 33.26275030454512,
          "standard_error": 0.017703590393143395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003220156297647312,
            "upper_bound": 0.08570436240360196
          },
          "point_estimate": 0.026114796657823663,
          "standard_error": 0.02278588942067712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.260139491730804,
            "upper_bound": 33.31886603968468
          },
          "point_estimate": 33.28238591217001,
          "standard_error": 0.01505920402167915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022302485259209484,
            "upper_bound": 0.06883384287289122
          },
          "point_estimate": 0.05267214353356503,
          "standard_error": 0.011885083488325949
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.906183057962465,
            "upper_bound": 36.94647694724253
          },
          "point_estimate": 36.92680142619469,
          "standard_error": 0.01031979535054326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.89678295508172,
            "upper_bound": 36.9604516351062
          },
          "point_estimate": 36.931700133330864,
          "standard_error": 0.0152152567457298
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007210323070528374,
            "upper_bound": 0.05674792484243004
          },
          "point_estimate": 0.046619380760312114,
          "standard_error": 0.013208908503778264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.9147589746316,
            "upper_bound": 36.951104655794694
          },
          "point_estimate": 36.9333083500221,
          "standard_error": 0.009114997812277353
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020241319884187304,
            "upper_bound": 0.0421830725457231
          },
          "point_estimate": 0.034370017615331594,
          "standard_error": 0.005597206786161763
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.33941703980552,
            "upper_bound": 29.393003938242536
          },
          "point_estimate": 29.365450181254698,
          "standard_error": 0.01369890903138537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.339110530140665,
            "upper_bound": 29.393795427235673
          },
          "point_estimate": 29.353378467460637,
          "standard_error": 0.017175716935555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0044845223093001796,
            "upper_bound": 0.0777712263213779
          },
          "point_estimate": 0.04258338633659667,
          "standard_error": 0.020188347040748875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.34437942995098,
            "upper_bound": 29.3735387177482
          },
          "point_estimate": 29.354316666672,
          "standard_error": 0.007471980085325123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021804423758248183,
            "upper_bound": 0.06098454150296265
          },
          "point_estimate": 0.04551411800552109,
          "standard_error": 0.010511892520455804
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.987243789242964,
            "upper_bound": 23.021347520756596
          },
          "point_estimate": 23.004612378402996,
          "standard_error": 0.008691046654355774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.984044871222448,
            "upper_bound": 23.023680843273596
          },
          "point_estimate": 23.00816798155964,
          "standard_error": 0.010061431769703089
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008475301479957348,
            "upper_bound": 0.049845591217267914
          },
          "point_estimate": 0.02379361972239083,
          "standard_error": 0.011400860669747786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.99772457185512,
            "upper_bound": 23.02653544748325
          },
          "point_estimate": 23.01233923226789,
          "standard_error": 0.007258439428005512
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013651481286945369,
            "upper_bound": 0.03877426784679515
          },
          "point_estimate": 0.028868656232059826,
          "standard_error": 0.006540145305444795
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.620017523375864,
            "upper_bound": 32.6837108434726
          },
          "point_estimate": 32.649847602237045,
          "standard_error": 0.016417480387107997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.60943448892881,
            "upper_bound": 32.6927646681151
          },
          "point_estimate": 32.635753525295115,
          "standard_error": 0.017538905507727895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006519239280646505,
            "upper_bound": 0.08904983844079435
          },
          "point_estimate": 0.03995019577441088,
          "standard_error": 0.020781799678697973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.603096691741705,
            "upper_bound": 32.66551881152821
          },
          "point_estimate": 32.62822940858686,
          "standard_error": 0.015827874847514203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022082354514397173,
            "upper_bound": 0.07063165233316272
          },
          "point_estimate": 0.05464731154893306,
          "standard_error": 0.012345631320133863
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.73397951080208,
            "upper_bound": 43.76689885822985
          },
          "point_estimate": 43.74936858910896,
          "standard_error": 0.008464517099288091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.72538874095507,
            "upper_bound": 43.77499751196079
          },
          "point_estimate": 43.73802713055045,
          "standard_error": 0.01139358887407553
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003656056368678428,
            "upper_bound": 0.04943410297919944
          },
          "point_estimate": 0.02239373245012971,
          "standard_error": 0.01133403520104936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.72562447988705,
            "upper_bound": 43.779647921992726
          },
          "point_estimate": 43.74866606894881,
          "standard_error": 0.014473193533541642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011964283040126302,
            "upper_bound": 0.03495457579307415
          },
          "point_estimate": 0.028306347184572585,
          "standard_error": 0.005886661244332754
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.05889532948264,
            "upper_bound": 51.221401576522695
          },
          "point_estimate": 51.12473830618469,
          "standard_error": 0.043480102811235835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.05562840171139,
            "upper_bound": 51.139614712913925
          },
          "point_estimate": 51.08972565219289,
          "standard_error": 0.022019734549696596
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013106888939334049,
            "upper_bound": 0.1175394874116264
          },
          "point_estimate": 0.0545369458421456,
          "standard_error": 0.028013621014117973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.043301166666375,
            "upper_bound": 51.10259416900604
          },
          "point_estimate": 51.07027454665685,
          "standard_error": 0.015129917128004702
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02909125587613948,
            "upper_bound": 0.21835687230605144
          },
          "point_estimate": 0.14462266035927135,
          "standard_error": 0.06078792561162701
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.41123904236083,
            "upper_bound": 40.44023923023096
          },
          "point_estimate": 40.42517010778099,
          "standard_error": 0.0074180625963085585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.4042998407258,
            "upper_bound": 40.44109161925023
          },
          "point_estimate": 40.424576847771505,
          "standard_error": 0.00957571746180519
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00609661941401543,
            "upper_bound": 0.042424646676754565
          },
          "point_estimate": 0.025249867179663637,
          "standard_error": 0.008709681843809625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.41133588228138,
            "upper_bound": 40.440995573013446
          },
          "point_estimate": 40.42791410872995,
          "standard_error": 0.007602695876009778
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013015870654127392,
            "upper_bound": 0.03225199043185227
          },
          "point_estimate": 0.024683134876332863,
          "standard_error": 0.005133124386087606
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.42702626323493,
            "upper_bound": 59.5204264736651
          },
          "point_estimate": 59.47585930391182,
          "standard_error": 0.023971198248485048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.4138290760844,
            "upper_bound": 59.52897257189306
          },
          "point_estimate": 59.49609780622744,
          "standard_error": 0.027289361807111952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0048828699511013806,
            "upper_bound": 0.13262018177192847
          },
          "point_estimate": 0.058770797881228576,
          "standard_error": 0.03754472349299194
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.46578862788664,
            "upper_bound": 59.527587419752386
          },
          "point_estimate": 59.49992810195143,
          "standard_error": 0.015594924682437023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03635947813771143,
            "upper_bound": 0.10080365662545386
          },
          "point_estimate": 0.08034332515917482,
          "standard_error": 0.01576605155966439
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.58755002341112,
            "upper_bound": 42.63389476503777
          },
          "point_estimate": 42.6093146491485,
          "standard_error": 0.011928218758173726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.578945958535776,
            "upper_bound": 42.632100402172895
          },
          "point_estimate": 42.60410072367189,
          "standard_error": 0.01399636184788458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050296203059574775,
            "upper_bound": 0.06414255958128617
          },
          "point_estimate": 0.038940015041577895,
          "standard_error": 0.014577609349798282
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.57921693925932,
            "upper_bound": 42.623916492311714
          },
          "point_estimate": 42.598538898502746,
          "standard_error": 0.011553197329782468
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017642383156927892,
            "upper_bound": 0.05147386769193177
          },
          "point_estimate": 0.03956896552206971,
          "standard_error": 0.00874979051441014
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.68355036539884,
            "upper_bound": 35.72178498042169
          },
          "point_estimate": 35.702494255049814,
          "standard_error": 0.009779427665328211
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.679241837893315,
            "upper_bound": 35.72866958658575
          },
          "point_estimate": 35.69948004484327,
          "standard_error": 0.012477237156272605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008466909632934009,
            "upper_bound": 0.055520126561004655
          },
          "point_estimate": 0.03276421515013352,
          "standard_error": 0.012684176162151587
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.68985535244358,
            "upper_bound": 35.723039353832505
          },
          "point_estimate": 35.70404953807293,
          "standard_error": 0.008373527199427955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017474555356463764,
            "upper_bound": 0.04133853240516676
          },
          "point_estimate": 0.03256450080467894,
          "standard_error": 0.006031039422877387
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.47486989223648,
            "upper_bound": 64.55613125518379
          },
          "point_estimate": 64.50982289704571,
          "standard_error": 0.020508853568883937
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.47086143372331,
            "upper_bound": 64.56286687277563
          },
          "point_estimate": 64.48339750082033,
          "standard_error": 0.018701547800950458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050526795339060245,
            "upper_bound": 0.1052397629712809
          },
          "point_estimate": 0.019303254581664177,
          "standard_error": 0.017874842483542132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.47707381047034,
            "upper_bound": 64.55025668979498
          },
          "point_estimate": 64.5013178226249,
          "standard_error": 0.019066748618065885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010336709693869515,
            "upper_bound": 0.08590506545222694
          },
          "point_estimate": 0.06844544125438522,
          "standard_error": 0.02031164184999172
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97996.8321555106,
            "upper_bound": 98150.16465798946
          },
          "point_estimate": 98071.60755176912,
          "standard_error": 39.18911410142086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97992.14824797845,
            "upper_bound": 98162.31590296495
          },
          "point_estimate": 98065.97927897576,
          "standard_error": 33.041804907376594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.734225009772272,
            "upper_bound": 227.6712034265915
          },
          "point_estimate": 63.763343522068354,
          "standard_error": 65.51361361253235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98013.13036646212,
            "upper_bound": 98173.41331434676
          },
          "point_estimate": 98087.468638639,
          "standard_error": 40.02526272765825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.01205059368301,
            "upper_bound": 172.61256916175802
          },
          "point_estimate": 130.94736378528802,
          "standard_error": 28.638717189415264
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53871.94962417989,
            "upper_bound": 53990.630845502645
          },
          "point_estimate": 53922.07863068783,
          "standard_error": 31.130411061934204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53857.86851851852,
            "upper_bound": 53947.18037037037
          },
          "point_estimate": 53899.52604232804,
          "standard_error": 25.43652304555077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.58699903885442,
            "upper_bound": 110.39070715128412
          },
          "point_estimate": 65.83045894238842,
          "standard_error": 24.195527526414377
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53854.27304789596,
            "upper_bound": 53907.632043533784
          },
          "point_estimate": 53881.14044829245,
          "standard_error": 13.73453512308856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.157986851411216,
            "upper_bound": 152.78813679321968
          },
          "point_estimate": 103.95909076939432,
          "standard_error": 37.630388914518925
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144847.6856103413,
            "upper_bound": 145872.7705339286
          },
          "point_estimate": 145376.93168031744,
          "standard_error": 262.72135797717476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144671.30244444445,
            "upper_bound": 145953.50971428573
          },
          "point_estimate": 145491.398,
          "standard_error": 314.3428732622328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.97353622406241,
            "upper_bound": 1467.7923581814653
          },
          "point_estimate": 691.2264435283157,
          "standard_error": 367.0311158550297
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144283.0278946902,
            "upper_bound": 145697.49563398692
          },
          "point_estimate": 144802.44493506494,
          "standard_error": 356.0230962288887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409.14172730089206,
            "upper_bound": 1125.7596249552569
          },
          "point_estimate": 877.2684129133551,
          "standard_error": 176.1859495471897
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391793.74340456986,
            "upper_bound": 392838.8786488202
          },
          "point_estimate": 392240.4945549582,
          "standard_error": 272.42534415484175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391699.5440860215,
            "upper_bound": 392450.9428763441
          },
          "point_estimate": 392018.1345579451,
          "standard_error": 204.4210548475275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.32343229028697,
            "upper_bound": 1020.1267583675764
          },
          "point_estimate": 575.4508316743135,
          "standard_error": 245.0101048916287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391948.4878065922,
            "upper_bound": 392364.74632195954
          },
          "point_estimate": 392197.1628264209,
          "standard_error": 106.22713384944976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272.255511464893,
            "upper_bound": 1335.537084648434
          },
          "point_estimate": 912.3635724994504,
          "standard_error": 327.3351150962862
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570422.5091432291,
            "upper_bound": 572112.8593774182
          },
          "point_estimate": 571156.1902883184,
          "standard_error": 438.2509153392041
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570278.7789713542,
            "upper_bound": 571709.61328125
          },
          "point_estimate": 570458.04375,
          "standard_error": 388.5208343809381
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.22254279440981,
            "upper_bound": 1793.136825977859
          },
          "point_estimate": 502.747624082258,
          "standard_error": 483.0114205878287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570369.1855096337,
            "upper_bound": 571486.5022515075
          },
          "point_estimate": 570738.4949675325,
          "standard_error": 290.4749415014177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300.80176777692833,
            "upper_bound": 2057.752228093773
          },
          "point_estimate": 1462.9304282558317,
          "standard_error": 474.0391112213584
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42402.027099744104,
            "upper_bound": 42476.57656875834
          },
          "point_estimate": 42439.08218986426,
          "standard_error": 19.00107400216906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42393.13065337116,
            "upper_bound": 42481.321651090344
          },
          "point_estimate": 42439.493107476635,
          "standard_error": 21.09675962872843
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.0355595293169,
            "upper_bound": 109.3731407276295
          },
          "point_estimate": 62.33140689573164,
          "standard_error": 26.95356785047095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42372.433402725736,
            "upper_bound": 42448.18731328034
          },
          "point_estimate": 42407.665526762954,
          "standard_error": 19.652042916833608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.42288414352033,
            "upper_bound": 81.15042037690509
          },
          "point_estimate": 63.506330488399946,
          "standard_error": 12.05015407500952
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96998.99714258732,
            "upper_bound": 97120.642562
          },
          "point_estimate": 97058.67775386244,
          "standard_error": 30.978429833557968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96977.57733333332,
            "upper_bound": 97107.93037037038
          },
          "point_estimate": 97076.54686666669,
          "standard_error": 33.038027354817125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.53444511073108,
            "upper_bound": 209.02781668901065
          },
          "point_estimate": 50.33869494186322,
          "standard_error": 50.14738168086766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96992.14566459628,
            "upper_bound": 97088.18963143633
          },
          "point_estimate": 97046.40474458874,
          "standard_error": 24.36102135302594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.98810342060058,
            "upper_bound": 137.86390550979402
          },
          "point_estimate": 103.28162098735632,
          "standard_error": 22.70856919351725
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72633.46153933334,
            "upper_bound": 72728.53927523809
          },
          "point_estimate": 72681.10447857142,
          "standard_error": 24.32820083316276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72627.2191,
            "upper_bound": 72730.54233333333
          },
          "point_estimate": 72692.88274999999,
          "standard_error": 25.34915864545246
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.873594260223296,
            "upper_bound": 136.93555282891097
          },
          "point_estimate": 75.85279044333002,
          "standard_error": 33.37814259414021
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72656.34996353352,
            "upper_bound": 72727.62552688172
          },
          "point_estimate": 72698.74799480519,
          "standard_error": 17.992463761680977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.07811202155591,
            "upper_bound": 108.79003420443831
          },
          "point_estimate": 81.11674976279213,
          "standard_error": 17.90895032172989
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306312.31569407764,
            "upper_bound": 306670.2482352941
          },
          "point_estimate": 306488.96257503005,
          "standard_error": 91.79716509480258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306246.4943977591,
            "upper_bound": 306868.34453781514
          },
          "point_estimate": 306395.6307322929,
          "standard_error": 161.03282707761832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.163407030469713,
            "upper_bound": 499.8362738992742
          },
          "point_estimate": 322.4696472162072,
          "standard_error": 128.541735802045
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306368.7865021008,
            "upper_bound": 306708.0164771816
          },
          "point_estimate": 306527.045945651,
          "standard_error": 86.78584580390424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.12824896315803,
            "upper_bound": 362.2886975076085
          },
          "point_estimate": 306.4646436353727,
          "standard_error": 43.89583523167295
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36588.11775298617,
            "upper_bound": 36640.06889211933
          },
          "point_estimate": 36610.77782846774,
          "standard_error": 13.521633987305778
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36577.45610911702,
            "upper_bound": 36626.90454355109
          },
          "point_estimate": 36604.36370742602,
          "standard_error": 12.84094830947388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4174301905848,
            "upper_bound": 55.23488287366181
          },
          "point_estimate": 31.063576676315332,
          "standard_error": 13.325506462894666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36584.91700126461,
            "upper_bound": 36620.73807449948
          },
          "point_estimate": 36604.281122495595,
          "standard_error": 8.982197554583422
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.039881471885106,
            "upper_bound": 64.93774484634692
          },
          "point_estimate": 45.13907875014325,
          "standard_error": 14.812003585753956
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63936.96653206685,
            "upper_bound": 64042.831938307245
          },
          "point_estimate": 63987.57006897368,
          "standard_error": 27.136661566312423
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63918.112819761765,
            "upper_bound": 64085.22741652021
          },
          "point_estimate": 63966.16177504393,
          "standard_error": 40.960565325318456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.942495581581404,
            "upper_bound": 147.72095893278888
          },
          "point_estimate": 71.87431011236659,
          "standard_error": 36.225214694748146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63925.52696353437,
            "upper_bound": 64043.867726374985
          },
          "point_estimate": 63974.59193846575,
          "standard_error": 29.549219490124543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.7123613356127,
            "upper_bound": 108.54020878590002
          },
          "point_estimate": 90.2645306242228,
          "standard_error": 14.984787041423134
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168198.54698138396,
            "upper_bound": 168493.60073584592
          },
          "point_estimate": 168351.44043760514,
          "standard_error": 75.57737977740224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168186.64372119814,
            "upper_bound": 168553.04928315413
          },
          "point_estimate": 168386.26889400923,
          "standard_error": 99.89305561842728
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.57612669862725,
            "upper_bound": 426.51266944400584
          },
          "point_estimate": 244.27127815180364,
          "standard_error": 91.36521623033724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168246.69981604748,
            "upper_bound": 168557.42928221956
          },
          "point_estimate": 168400.64713627385,
          "standard_error": 79.503956150575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.24597776659414,
            "upper_bound": 324.9856310645823
          },
          "point_estimate": 252.3756087689209,
          "standard_error": 49.66724517000057
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34354.429365015545,
            "upper_bound": 34398.34968644411
          },
          "point_estimate": 34377.338078118664,
          "standard_error": 11.239912076081357
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34351.25949452629,
            "upper_bound": 34407.34317250078
          },
          "point_estimate": 34382.32426679281,
          "standard_error": 13.874627471915606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.458146574086022,
            "upper_bound": 63.93828438994458
          },
          "point_estimate": 40.993268099093235,
          "standard_error": 14.830849018908856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34352.84549689441,
            "upper_bound": 34407.95810867908
          },
          "point_estimate": 34383.92656010026,
          "standard_error": 14.2270423600418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.689087347068053,
            "upper_bound": 47.228643725124506
          },
          "point_estimate": 37.43178085087357,
          "standard_error": 7.203915835975923
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11528.852662194004,
            "upper_bound": 11580.676345797496
          },
          "point_estimate": 11553.271463601794,
          "standard_error": 13.380150730812748
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11517.613007061696,
            "upper_bound": 11591.941170224063
          },
          "point_estimate": 11544.08338465895,
          "standard_error": 18.54270390114235
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.52936797138398,
            "upper_bound": 76.80188251352553
          },
          "point_estimate": 38.46300751720906,
          "standard_error": 18.384226063708176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11517.408397199766,
            "upper_bound": 11543.753775445555
          },
          "point_estimate": 11526.714045750365,
          "standard_error": 6.795524535802169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.69780036670299,
            "upper_bound": 55.22777928139799
          },
          "point_estimate": 44.543543141998,
          "standard_error": 8.893619413928954
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498540.03460768645,
            "upper_bound": 499301.0128033675
          },
          "point_estimate": 498910.1555452271,
          "standard_error": 194.85805208188825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498351.1726027397,
            "upper_bound": 499600.4375
          },
          "point_estimate": 498568.7020547945,
          "standard_error": 401.1461165064855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.68804439695852,
            "upper_bound": 1040.8021908029525
          },
          "point_estimate": 396.8219448728233,
          "standard_error": 288.1519375792922
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498429.06805529946,
            "upper_bound": 499448.55525858654
          },
          "point_estimate": 498985.3528553638,
          "standard_error": 253.9949118512368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395.52937168472215,
            "upper_bound": 722.9726632487202
          },
          "point_estimate": 648.564961945773,
          "standard_error": 83.42007805867019
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 997.6110647193624,
            "upper_bound": 998.4494345676125
          },
          "point_estimate": 998.026219980303,
          "standard_error": 0.214483922772113
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 997.4754000587244,
            "upper_bound": 998.560669456067
          },
          "point_estimate": 997.9047050484476,
          "standard_error": 0.28151167535292193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11003459119777464,
            "upper_bound": 1.1761906736240653
          },
          "point_estimate": 0.8798181670950561,
          "standard_error": 0.31399657336154463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 997.4592543400868,
            "upper_bound": 998.4052109086996
          },
          "point_estimate": 998.0332498705876,
          "standard_error": 0.24131912205036513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3852286034738081,
            "upper_bound": 0.9178203474347092
          },
          "point_estimate": 0.715439187180733,
          "standard_error": 0.13471513880547917
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40915.106674174174,
            "upper_bound": 40968.71161036035
          },
          "point_estimate": 40941.13327014515,
          "standard_error": 13.780279040085915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40897.55292792793,
            "upper_bound": 40978.829673423425
          },
          "point_estimate": 40935.137137137135,
          "standard_error": 19.219377646600325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.312888895518151,
            "upper_bound": 80.14471310417676
          },
          "point_estimate": 53.797676071926574,
          "standard_error": 17.743863275917175
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40919.961817286036,
            "upper_bound": 40970.75061188356
          },
          "point_estimate": 40944.46014976015,
          "standard_error": 12.94333999808164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.71182725400627,
            "upper_bound": 56.35754108347794
          },
          "point_estimate": 45.76631728193828,
          "standard_error": 7.795091856367667
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49467.54820256992,
            "upper_bound": 49557.03744007126
          },
          "point_estimate": 49511.50165036174,
          "standard_error": 22.923114260419897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49432.38843537415,
            "upper_bound": 49582.92380952381
          },
          "point_estimate": 49505.82071050642,
          "standard_error": 37.2848514532736
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.662608275611465,
            "upper_bound": 124.65013309313947
          },
          "point_estimate": 111.59187087598995,
          "standard_error": 28.514300590353752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49452.4089640293,
            "upper_bound": 49530.660269635126
          },
          "point_estimate": 49486.032105309656,
          "standard_error": 19.78632618957499
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.54696762631703,
            "upper_bound": 92.23534338785116
          },
          "point_estimate": 76.39900927468318,
          "standard_error": 11.59297102477697
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48570.453329884986,
            "upper_bound": 48685.968727983614
          },
          "point_estimate": 48622.9763527608,
          "standard_error": 29.706490556379844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48571.40520090612,
            "upper_bound": 48660.466755793226
          },
          "point_estimate": 48601.79249427044,
          "standard_error": 23.91476075680823
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.397472845993374,
            "upper_bound": 154.7849733683274
          },
          "point_estimate": 57.48637342794939,
          "standard_error": 35.614473916600694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48580.864010018886,
            "upper_bound": 48650.44537422563
          },
          "point_estimate": 48607.27606778248,
          "standard_error": 17.690792070748735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.64216576963039,
            "upper_bound": 137.61478229345727
          },
          "point_estimate": 99.147948524317,
          "standard_error": 27.957864805693063
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14116.560606957928,
            "upper_bound": 14132.887208044383
          },
          "point_estimate": 14123.94128509786,
          "standard_error": 4.199588329354848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14113.312923408845,
            "upper_bound": 14131.580055478502
          },
          "point_estimate": 14120.87572815534,
          "standard_error": 4.055232954597868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7874318922612957,
            "upper_bound": 20.996960559269063
          },
          "point_estimate": 8.8402542411118,
          "standard_error": 4.804383364341251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14118.153747123855,
            "upper_bound": 14129.069721450467
          },
          "point_estimate": 14122.498043626276,
          "standard_error": 2.7976414636677793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.266263773035839,
            "upper_bound": 19.02772423044443
          },
          "point_estimate": 14.04289779458367,
          "standard_error": 3.7780905599826977
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27104.53492781092,
            "upper_bound": 27149.51400326249
          },
          "point_estimate": 27127.768720689368,
          "standard_error": 11.523957949662137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27102.107289336316,
            "upper_bound": 27160.10130499627
          },
          "point_estimate": 27129.88349283288,
          "standard_error": 14.19428722606795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.24911726415987,
            "upper_bound": 65.99131532730219
          },
          "point_estimate": 35.44033069071983,
          "standard_error": 14.5013108989379
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27116.799763168103,
            "upper_bound": 27142.34734871068
          },
          "point_estimate": 27130.73232032695,
          "standard_error": 6.481038480821488
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.396253419689128,
            "upper_bound": 49.58707604684811
          },
          "point_estimate": 38.39651548901441,
          "standard_error": 7.63044012343381
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17325.06018737144,
            "upper_bound": 17359.87289797863
          },
          "point_estimate": 17340.33903674841,
          "standard_error": 9.041542596305405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17321.57224670478,
            "upper_bound": 17350.40722717545
          },
          "point_estimate": 17333.88183658885,
          "standard_error": 7.259795286429034
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.579301683543004,
            "upper_bound": 37.95517540841942
          },
          "point_estimate": 17.115588976747244,
          "standard_error": 8.56111254009831
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17324.302554787995,
            "upper_bound": 17344.73792078224
          },
          "point_estimate": 17334.76757763437,
          "standard_error": 5.067720146334099
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.894205291563711,
            "upper_bound": 43.00436638494861
          },
          "point_estimate": 30.127991621115527,
          "standard_error": 9.76264513683176
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.754605801485,
            "upper_bound": 17347.105512597693
          },
          "point_estimate": 17337.653078861473,
          "standard_error": 4.9460895419745015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.15195610687,
            "upper_bound": 17349.356393129772
          },
          "point_estimate": 17338.420233551435,
          "standard_error": 6.435024887831454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.9849123160781135,
            "upper_bound": 28.447740668614223
          },
          "point_estimate": 16.460148872850752,
          "standard_error": 5.848826641781246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17328.409477544257,
            "upper_bound": 17354.313245921072
          },
          "point_estimate": 17341.07470010905,
          "standard_error": 6.996222653323088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.762777385280932,
            "upper_bound": 21.56979040508853
          },
          "point_estimate": 16.487617662464675,
          "standard_error": 3.341588837149004
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17332.62780677874,
            "upper_bound": 17376.73070588602
          },
          "point_estimate": 17352.492838615515,
          "standard_error": 11.38282162672866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17326.611419269946,
            "upper_bound": 17375.22405907575
          },
          "point_estimate": 17332.924952358266,
          "standard_error": 15.62991595124804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9568871663078615,
            "upper_bound": 63.5694191818864
          },
          "point_estimate": 16.26284677416313,
          "standard_error": 15.962760954661908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17328.393569892945,
            "upper_bound": 17364.418406061526
          },
          "point_estimate": 17349.045891983194,
          "standard_error": 9.588057601212162
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.832322462035778,
            "upper_bound": 52.884126974997265
          },
          "point_estimate": 38.07906272937908,
          "standard_error": 10.73532393603739
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36332.165936611214,
            "upper_bound": 36416.093683246734
          },
          "point_estimate": 36368.6659788757,
          "standard_error": 21.60629907215572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36320.06721006721,
            "upper_bound": 36395.14983733734
          },
          "point_estimate": 36352.50880880881,
          "standard_error": 21.332814504603004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.298201763644284,
            "upper_bound": 92.68778667611627
          },
          "point_estimate": 48.48838302304578,
          "standard_error": 20.149457071033833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36325.999302307646,
            "upper_bound": 36372.63474521033
          },
          "point_estimate": 36344.27358787359,
          "standard_error": 11.88920136291958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.63784131087149,
            "upper_bound": 102.859169406987
          },
          "point_estimate": 71.99561661875988,
          "standard_error": 22.900039909657323
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17887.892780025413,
            "upper_bound": 17919.056695223255
          },
          "point_estimate": 17903.868281705043,
          "standard_error": 7.997521867222778
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17880.018059166607,
            "upper_bound": 17925.70272995573
          },
          "point_estimate": 17909.04832759469,
          "standard_error": 12.314874126859722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.09708322780488,
            "upper_bound": 44.061893739389006
          },
          "point_estimate": 25.58804016426512,
          "standard_error": 9.977742092179971
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17891.19652843297,
            "upper_bound": 17928.940368557025
          },
          "point_estimate": 17914.356444637506,
          "standard_error": 9.622437482517942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.094248546859482,
            "upper_bound": 32.76221838003529
          },
          "point_estimate": 26.594165935852946,
          "standard_error": 4.4681438814761085
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18472.424041089263,
            "upper_bound": 18486.01179712629
          },
          "point_estimate": 18479.199869404336,
          "standard_error": 3.4795534769791385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18465.95022854241,
            "upper_bound": 18487.04272473337
          },
          "point_estimate": 18480.946673438295,
          "standard_error": 4.804968487267122
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6607403717349496,
            "upper_bound": 22.841627425867568
          },
          "point_estimate": 9.448876685983285,
          "standard_error": 5.503171612824601
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18473.900503255,
            "upper_bound": 18485.849714281383
          },
          "point_estimate": 18480.994908088356,
          "standard_error": 3.026335958109557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.6661212779234535,
            "upper_bound": 14.73284619323588
          },
          "point_estimate": 11.6283450288896,
          "standard_error": 2.1077361329499427
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17712.801254302158,
            "upper_bound": 17734.62719948582
          },
          "point_estimate": 17722.775918681673,
          "standard_error": 5.6167695510755555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17710.360684176394,
            "upper_bound": 17736.900228136
          },
          "point_estimate": 17714.202375162127,
          "standard_error": 6.408738754053228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.156129921109333,
            "upper_bound": 29.78525439581826
          },
          "point_estimate": 11.57965315045154,
          "standard_error": 7.065715385005375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17713.15740887795,
            "upper_bound": 17730.4276057429
          },
          "point_estimate": 17720.295563191674,
          "standard_error": 4.385539738198167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.312840829739251,
            "upper_bound": 23.761719272264457
          },
          "point_estimate": 18.670118845100344,
          "standard_error": 4.498711642995013
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17588.337681921505,
            "upper_bound": 17601.2357474644
          },
          "point_estimate": 17595.012194250543,
          "standard_error": 3.31483267915255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17585.009180546916,
            "upper_bound": 17604.076439283985
          },
          "point_estimate": 17597.24496855346,
          "standard_error": 5.741654585651425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5466164456924616,
            "upper_bound": 18.807617084255003
          },
          "point_estimate": 11.420472003343049,
          "standard_error": 5.010853181876483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17582.428589291903,
            "upper_bound": 17598.5399948293
          },
          "point_estimate": 17590.42162365936,
          "standard_error": 4.2361453861415574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.46763349990404,
            "upper_bound": 13.380096650292876
          },
          "point_estimate": 11.046410379155304,
          "standard_error": 1.775713872344354
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17294.621213327715,
            "upper_bound": 17325.581132093466
          },
          "point_estimate": 17309.337470933533,
          "standard_error": 7.933557400929011
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17286.79176601494,
            "upper_bound": 17328.913678270546
          },
          "point_estimate": 17302.57087505961,
          "standard_error": 10.848665760671029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.311235885678339,
            "upper_bound": 44.97750177659703
          },
          "point_estimate": 31.35224363866685,
          "standard_error": 10.189150269193837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17293.18903637147,
            "upper_bound": 17319.650710802274
          },
          "point_estimate": 17308.03186370139,
          "standard_error": 6.745407745535803
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.096736100040722,
            "upper_bound": 34.26039652845219
          },
          "point_estimate": 26.504203918296945,
          "standard_error": 5.371646425535787
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17315.83855327934,
            "upper_bound": 17346.07767601825
          },
          "point_estimate": 17331.556420026165,
          "standard_error": 7.706032849907408
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17321.725393044308,
            "upper_bound": 17346.629049547402
          },
          "point_estimate": 17331.198454290403,
          "standard_error": 6.237217861200592
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.338800451794422,
            "upper_bound": 40.1880990959516
          },
          "point_estimate": 14.24847560220818,
          "standard_error": 9.088553108506114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17325.314230910255,
            "upper_bound": 17349.83898096838
          },
          "point_estimate": 17334.344084690918,
          "standard_error": 6.483331740566128
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.367595779233154,
            "upper_bound": 36.21262140348533
          },
          "point_estimate": 25.632177437244792,
          "standard_error": 7.04499632410304
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15349.46226040138,
            "upper_bound": 15388.481303175273
          },
          "point_estimate": 15368.170948695788,
          "standard_error": 10.028669110705009
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15342.50887203763,
            "upper_bound": 15406.477838750528
          },
          "point_estimate": 15357.417874396137,
          "standard_error": 14.827023129626872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.668610920516704,
            "upper_bound": 52.82379721612254
          },
          "point_estimate": 29.820772162990657,
          "standard_error": 13.794358530258243
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15338.04432506762,
            "upper_bound": 15370.783756539897
          },
          "point_estimate": 15350.806631106336,
          "standard_error": 8.375566280341983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.16385921654154,
            "upper_bound": 39.77728067772153
          },
          "point_estimate": 33.4232305667617,
          "standard_error": 5.472245273996119
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16401.187126504643,
            "upper_bound": 16467.41622767186
          },
          "point_estimate": 16431.137615809323,
          "standard_error": 17.063410039594807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16393.220847735698,
            "upper_bound": 16472.135047661985
          },
          "point_estimate": 16405.801713255183,
          "standard_error": 19.629477959503618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.352491836524558,
            "upper_bound": 78.9310134544207
          },
          "point_estimate": 23.59106875948839,
          "standard_error": 21.086751333161505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16395.130908939118,
            "upper_bound": 16415.245631035203
          },
          "point_estimate": 16401.716808169287,
          "standard_error": 5.262467662719609
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.423990920073283,
            "upper_bound": 72.59296561767651
          },
          "point_estimate": 56.97147188738185,
          "standard_error": 14.879234284768982
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17691.84790759361,
            "upper_bound": 17712.35771453699
          },
          "point_estimate": 17701.79596707103,
          "standard_error": 5.247356240818215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17689.401412566975,
            "upper_bound": 17713.27877902257
          },
          "point_estimate": 17698.92710532013,
          "standard_error": 6.114212195901111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.139471843113812,
            "upper_bound": 29.081689554323127
          },
          "point_estimate": 17.488071900920847,
          "standard_error": 6.682390553130523
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17688.902341859226,
            "upper_bound": 17708.096258874342
          },
          "point_estimate": 17699.877371727154,
          "standard_error": 4.915091298474981
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.082614032114211,
            "upper_bound": 22.66986359649956
          },
          "point_estimate": 17.536442940797148,
          "standard_error": 3.523803529436052
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18122.95243092152,
            "upper_bound": 18133.552906109053
          },
          "point_estimate": 18128.432155082373,
          "standard_error": 2.698199410996997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18123.447220837486,
            "upper_bound": 18136.215312396147
          },
          "point_estimate": 18128.231455633104,
          "standard_error": 3.204822182013024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2429523956820328,
            "upper_bound": 16.028819939755433
          },
          "point_estimate": 6.411708171310037,
          "standard_error": 3.640417613056442
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18123.83745653363,
            "upper_bound": 18132.59648530447
          },
          "point_estimate": 18128.375082544575,
          "standard_error": 2.192628803091501
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.407197790418235,
            "upper_bound": 11.873738802437884
          },
          "point_estimate": 8.950694637504093,
          "standard_error": 1.9629894009774544
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68250.43302242474,
            "upper_bound": 68309.7510184751
          },
          "point_estimate": 68279.23320483041,
          "standard_error": 15.106777555997713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68250.24409005628,
            "upper_bound": 68298.29164433127
          },
          "point_estimate": 68278.34803001877,
          "standard_error": 10.5214017813615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.999496551890835,
            "upper_bound": 83.54436943612403
          },
          "point_estimate": 19.841712405709657,
          "standard_error": 19.776847912285746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68225.88926108796,
            "upper_bound": 68289.8928287942
          },
          "point_estimate": 68260.65725981336,
          "standard_error": 16.985503415814627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.77237334719296,
            "upper_bound": 70.93302053030132
          },
          "point_estimate": 50.39667008479225,
          "standard_error": 13.568151914338248
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217335.7766403442,
            "upper_bound": 1219512.4285888888
          },
          "point_estimate": 1218425.8592156086,
          "standard_error": 556.8819031345269
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217207.9566666668,
            "upper_bound": 1219756.1185185183
          },
          "point_estimate": 1218348.0654761903,
          "standard_error": 664.7564850486737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.2249017826017,
            "upper_bound": 2966.373672336282
          },
          "point_estimate": 1425.1442826985117,
          "standard_error": 740.2611361399815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217703.311052794,
            "upper_bound": 1219485.9706539074
          },
          "point_estimate": 1218456.4486580086,
          "standard_error": 463.23313458323986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 890.0829606516107,
            "upper_bound": 2478.519719648461
          },
          "point_estimate": 1863.98775426496,
          "standard_error": 398.6944827708884
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205111.6948533764,
            "upper_bound": 205588.40867198008
          },
          "point_estimate": 205373.38990516544,
          "standard_error": 121.97745209142872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205298.78531073447,
            "upper_bound": 205548.24802259883
          },
          "point_estimate": 205401.77676553672,
          "standard_error": 64.96210709698813
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.00704307298924,
            "upper_bound": 538.5161888010048
          },
          "point_estimate": 171.66260062469874,
          "standard_error": 121.414842897851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205356.29958494427,
            "upper_bound": 205496.4767247553
          },
          "point_estimate": 205412.19687431213,
          "standard_error": 35.591692130716275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.14992569567497,
            "upper_bound": 592.8234040410975
          },
          "point_estimate": 406.07825367469945,
          "standard_error": 137.29021458716355
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7411.731901071866,
            "upper_bound": 7425.006234492002
          },
          "point_estimate": 7418.124683227633,
          "standard_error": 3.396670117269585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7409.015628186824,
            "upper_bound": 7425.09695169624
          },
          "point_estimate": 7418.998928093911,
          "standard_error": 4.320333688578809
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9216211869049056,
            "upper_bound": 19.73291653984028
          },
          "point_estimate": 12.106501248456723,
          "standard_error": 4.339335300314475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7409.445697890425,
            "upper_bound": 7421.260345585858
          },
          "point_estimate": 7415.019014067719,
          "standard_error": 3.0117887513366246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.011010465020592,
            "upper_bound": 14.943082073068648
          },
          "point_estimate": 11.360879352051946,
          "standard_error": 2.4006092601124
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7492.534282649171,
            "upper_bound": 7504.05356190697
          },
          "point_estimate": 7498.131681753063,
          "standard_error": 2.9514182622512424
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7488.045538841953,
            "upper_bound": 7505.4033845044305
          },
          "point_estimate": 7499.407844803902,
          "standard_error": 4.650783749995221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7532963422497778,
            "upper_bound": 17.95961261335977
          },
          "point_estimate": 13.11233860916264,
          "standard_error": 4.384901475172633
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7491.480853950983,
            "upper_bound": 7509.299484672092
          },
          "point_estimate": 7500.299505728148,
          "standard_error": 4.759393802346184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.9274093726252675,
            "upper_bound": 12.175489363871227
          },
          "point_estimate": 9.803248821959476,
          "standard_error": 1.6549043270931063
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14083.515957149011,
            "upper_bound": 14105.961465326229
          },
          "point_estimate": 14094.323607127477,
          "standard_error": 5.756394303687236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14076.86093453919,
            "upper_bound": 14116.583720930232
          },
          "point_estimate": 14090.785193798449,
          "standard_error": 9.684600687558971
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6765814059687045,
            "upper_bound": 30.287269139619244
          },
          "point_estimate": 21.96428762168352,
          "standard_error": 8.141167901019339
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14080.994331124395,
            "upper_bound": 14099.590025077863
          },
          "point_estimate": 14089.380277861674,
          "standard_error": 4.692006154620126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.113589989153414,
            "upper_bound": 22.469041968377564
          },
          "point_estimate": 19.23027790925685,
          "standard_error": 2.899952618245938
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.19083205043732,
            "upper_bound": 36.076862029523575
          },
          "point_estimate": 35.651074189909664,
          "standard_error": 0.22749810612208052
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.006225439802186,
            "upper_bound": 36.32156444228137
          },
          "point_estimate": 35.71885896667839,
          "standard_error": 0.3440008493197645
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16370245083984372,
            "upper_bound": 1.2748281283162763
          },
          "point_estimate": 0.8758659856092617,
          "standard_error": 0.27901510344751235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.463991453352556,
            "upper_bound": 36.3449629764659
          },
          "point_estimate": 36.05630800442346,
          "standard_error": 0.22633325195859097
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4263518370221563,
            "upper_bound": 0.9638612416006714
          },
          "point_estimate": 0.7599223356568648,
          "standard_error": 0.14173194956621665
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.043311006303732,
            "upper_bound": 9.054260233286746
          },
          "point_estimate": 9.048814680164323,
          "standard_error": 0.002806892979253639
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.040902631176117,
            "upper_bound": 9.058404713797342
          },
          "point_estimate": 9.048857784044577,
          "standard_error": 0.00427436376671738
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002910780305627198,
            "upper_bound": 0.015838033150549085
          },
          "point_estimate": 0.012974293616776222,
          "standard_error": 0.003383401159312259
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.042683464916188,
            "upper_bound": 9.053694942138115
          },
          "point_estimate": 9.047575442617603,
          "standard_error": 0.0027816435973970605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00565033097280019,
            "upper_bound": 0.01139608875266324
          },
          "point_estimate": 0.009339911390196394,
          "standard_error": 0.0014629921917356527
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045141165463454,
            "upper_bound": 9.067249889777116
          },
          "point_estimate": 9.055396293082143,
          "standard_error": 0.005658048285995301
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04208891133171,
            "upper_bound": 9.070096325446782
          },
          "point_estimate": 9.047024666470726,
          "standard_error": 0.007482691454419843
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007225379347379727,
            "upper_bound": 0.03009727096874838
          },
          "point_estimate": 0.012125344098398144,
          "standard_error": 0.007692616365346214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045334096311151,
            "upper_bound": 9.068369339020228
          },
          "point_estimate": 9.055025435700406,
          "standard_error": 0.006023326882398692
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007643357756555187,
            "upper_bound": 0.023508327737394735
          },
          "point_estimate": 0.018886283435992215,
          "standard_error": 0.003977829117161875
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04255082617319,
            "upper_bound": 9.049853746076092
          },
          "point_estimate": 9.046178864283087,
          "standard_error": 0.0018685750415494595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.040757073541116,
            "upper_bound": 9.053417731786794
          },
          "point_estimate": 9.046218734386477,
          "standard_error": 0.0029943533424277007
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010510144796474427,
            "upper_bound": 0.01051992070305869
          },
          "point_estimate": 0.008936160157523845,
          "standard_error": 0.0026540805424446856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04233199847196,
            "upper_bound": 9.050763208916155
          },
          "point_estimate": 9.046461303357678,
          "standard_error": 0.002245970346228036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037605879808793054,
            "upper_bound": 0.007423821896845471
          },
          "point_estimate": 0.006223479773302744,
          "standard_error": 0.00091388782179295
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.795775956120057,
            "upper_bound": 8.810230933431452
          },
          "point_estimate": 8.802777000500129,
          "standard_error": 0.0037170036231048537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.792002306312007,
            "upper_bound": 8.813239028779996
          },
          "point_estimate": 8.800284982677724,
          "standard_error": 0.005908068054240097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030703561250448475,
            "upper_bound": 0.020269668036824905
          },
          "point_estimate": 0.015415581971840698,
          "standard_error": 0.004570311703924513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.79564547079386,
            "upper_bound": 8.810188075138951
          },
          "point_estimate": 8.803312775170381,
          "standard_error": 0.003704967463147525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007298564784203866,
            "upper_bound": 0.015447175649686454
          },
          "point_estimate": 0.012420366414945958,
          "standard_error": 0.002107261100095543
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.532945443428604,
            "upper_bound": 9.547077429656513
          },
          "point_estimate": 9.53972818065925,
          "standard_error": 0.0036170970420273674
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.53334455313961,
            "upper_bound": 9.546757179688358
          },
          "point_estimate": 9.53643714064027,
          "standard_error": 0.0036658879866200487
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008060501081988774,
            "upper_bound": 0.020603214772425417
          },
          "point_estimate": 0.005938608583281873,
          "standard_error": 0.0051865338581436205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.53525723217864,
            "upper_bound": 9.5429401735914
          },
          "point_estimate": 9.538666536661667,
          "standard_error": 0.0020230912135856477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005121097592187039,
            "upper_bound": 0.016281309324869066
          },
          "point_estimate": 0.012095442433373625,
          "standard_error": 0.0029039183127982124
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.269267249341558,
            "upper_bound": 10.28130986564256
          },
          "point_estimate": 10.27496141573758,
          "standard_error": 0.0030843062938293265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.267384021510754,
            "upper_bound": 10.2810942028064
          },
          "point_estimate": 10.273735936014614,
          "standard_error": 0.003470551080722127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002351702308975785,
            "upper_bound": 0.016620169355044564
          },
          "point_estimate": 0.009710683236315677,
          "standard_error": 0.0036846386140210303
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.268421890274496,
            "upper_bound": 10.282737216627426
          },
          "point_estimate": 10.276149350897075,
          "standard_error": 0.0037321919445226417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004979614678865419,
            "upper_bound": 0.01344892289293004
          },
          "point_estimate": 0.010243627119177994,
          "standard_error": 0.00223504471168833
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.273077448353604,
            "upper_bound": 8.330714410034297
          },
          "point_estimate": 8.302959924765716,
          "standard_error": 0.014788038308142425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.268730439191595,
            "upper_bound": 8.354238067443907
          },
          "point_estimate": 8.302863773653684,
          "standard_error": 0.024853331669021435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001661963398896356,
            "upper_bound": 0.09476790860754516
          },
          "point_estimate": 0.05571919729938973,
          "standard_error": 0.0220396188965181
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.273673955762924,
            "upper_bound": 8.337305827399753
          },
          "point_estimate": 8.30287870700137,
          "standard_error": 0.016214652663748337
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02920268492883735,
            "upper_bound": 0.06264606776395042
          },
          "point_estimate": 0.049351818365649266,
          "standard_error": 0.009103400823680014
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.818567940507757,
            "upper_bound": 9.85891035387762
          },
          "point_estimate": 9.83923415914727,
          "standard_error": 0.010325904562743304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.800420554670614,
            "upper_bound": 9.86891838307182
          },
          "point_estimate": 9.844860056292967,
          "standard_error": 0.016159734205761108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009301829030476437,
            "upper_bound": 0.05720152611286955
          },
          "point_estimate": 0.04099781908306361,
          "standard_error": 0.013318952774952346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.80375623682401,
            "upper_bound": 9.866033840707283
          },
          "point_estimate": 9.840028551724668,
          "standard_error": 0.015965517795246967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02011669876328375,
            "upper_bound": 0.041342452216044906
          },
          "point_estimate": 0.034466856645791986,
          "standard_error": 0.005309941853872579
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.547723310173884,
            "upper_bound": 12.564230787768922
          },
          "point_estimate": 12.55561858416265,
          "standard_error": 0.004237396799636018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.545324647105076,
            "upper_bound": 12.570128792849031
          },
          "point_estimate": 12.551968268969894,
          "standard_error": 0.005647840584231323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024901160502290797,
            "upper_bound": 0.023179910269658727
          },
          "point_estimate": 0.012029025560356264,
          "standard_error": 0.0056948811988157795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.548214028813549,
            "upper_bound": 12.56601620519714
          },
          "point_estimate": 12.556810762462328,
          "standard_error": 0.004548627869818414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006510592827431824,
            "upper_bound": 0.01749144711724817
          },
          "point_estimate": 0.01411203853528742,
          "standard_error": 0.0026212594844654898
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047449339881316,
            "upper_bound": 9.060331050645807
          },
          "point_estimate": 9.053921830786678,
          "standard_error": 0.00329217274008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045247808817798,
            "upper_bound": 9.063657519465655
          },
          "point_estimate": 9.05330181454407,
          "standard_error": 0.004712291682298928
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003063009721982353,
            "upper_bound": 0.019070568686158276
          },
          "point_estimate": 0.013106009850491335,
          "standard_error": 0.004148816805649938
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.053459540391597,
            "upper_bound": 9.066044766868163
          },
          "point_estimate": 9.061583542008378,
          "standard_error": 0.0031609073045140773
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006569959502811069,
            "upper_bound": 0.013420240267479872
          },
          "point_estimate": 0.010978944974730436,
          "standard_error": 0.001742032181777128
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.772889273266946,
            "upper_bound": 9.787901253147965
          },
          "point_estimate": 9.780014510810032,
          "standard_error": 0.003859886122798185
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.768797587518884,
            "upper_bound": 9.790276737051876
          },
          "point_estimate": 9.778248605486011,
          "standard_error": 0.005548836209755643
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014512878361039878,
            "upper_bound": 0.02261776273805705
          },
          "point_estimate": 0.013908262817794264,
          "standard_error": 0.0055752374748740926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.77278467722584,
            "upper_bound": 9.786280827288945
          },
          "point_estimate": 9.77963235685935,
          "standard_error": 0.0035230541886634953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005928326460711139,
            "upper_bound": 0.015735437473578768
          },
          "point_estimate": 0.012872893181279935,
          "standard_error": 0.0024229979359088527
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.541024974854743,
            "upper_bound": 19.57154424165993
          },
          "point_estimate": 19.556470757825547,
          "standard_error": 0.007812945140907078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.531631405953178,
            "upper_bound": 19.57723837212205
          },
          "point_estimate": 19.558715356508067,
          "standard_error": 0.009733407963747163
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004095297062005064,
            "upper_bound": 0.045573434541665896
          },
          "point_estimate": 0.030185307597227977,
          "standard_error": 0.011340745319465186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.54693964311801,
            "upper_bound": 19.578863363518614
          },
          "point_estimate": 19.56290791028488,
          "standard_error": 0.008071669255203206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014423316395379674,
            "upper_bound": 0.03298503039889065
          },
          "point_estimate": 0.025991251819578925,
          "standard_error": 0.004717745589852985
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98390.06265226018,
            "upper_bound": 98777.79629205487
          },
          "point_estimate": 98585.44405851018,
          "standard_error": 99.5138231383426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98240.53994565216,
            "upper_bound": 98816.66334541063
          },
          "point_estimate": 98691.9965304736,
          "standard_error": 172.70834826237513
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.73125991918852,
            "upper_bound": 552.5918017384503
          },
          "point_estimate": 386.4623787367494,
          "standard_error": 141.90977499177228
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98388.47510445684,
            "upper_bound": 98763.79844546242
          },
          "point_estimate": 98590.31347402596,
          "standard_error": 104.32296639681938
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.80021164919097,
            "upper_bound": 399.73375792008966
          },
          "point_estimate": 330.08437831961055,
          "standard_error": 49.35534360792585
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53829.0530989187,
            "upper_bound": 53926.19712841645
          },
          "point_estimate": 53875.59564507607,
          "standard_error": 24.697363054310856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53821.627588757394,
            "upper_bound": 53924.22698647506
          },
          "point_estimate": 53859.50330374754,
          "standard_error": 28.35680288905851
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.87156043717771,
            "upper_bound": 137.83331227190257
          },
          "point_estimate": 86.76630577837578,
          "standard_error": 32.300295660273434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53862.66000268962,
            "upper_bound": 53922.88981273663
          },
          "point_estimate": 53886.873830016135,
          "standard_error": 15.401290264744784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.5317712053351,
            "upper_bound": 110.3729198650878
          },
          "point_estimate": 82.28315877448206,
          "standard_error": 18.549898889108498
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143658.18500548968,
            "upper_bound": 144168.28451051665
          },
          "point_estimate": 143874.9100086266,
          "standard_error": 134.3346941177085
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143604.61377752683,
            "upper_bound": 144067.84453227933
          },
          "point_estimate": 143692.7489020641,
          "standard_error": 140.48403566538818
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.596243647521418,
            "upper_bound": 558.6498092124289
          },
          "point_estimate": 204.85318076693713,
          "standard_error": 148.24206238311018
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143614.75824935,
            "upper_bound": 143917.9841205664
          },
          "point_estimate": 143725.48818849135,
          "standard_error": 77.85959359832066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.5972722413708,
            "upper_bound": 650.4684261115063
          },
          "point_estimate": 446.538607974587,
          "standard_error": 156.2065841024782
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365112.76716916665,
            "upper_bound": 365627.6254526191
          },
          "point_estimate": 365369.21320119046,
          "standard_error": 130.88831162211258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365090.55333333334,
            "upper_bound": 365576.6433333333
          },
          "point_estimate": 365361.930625,
          "standard_error": 101.77572665284336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.215422283868417,
            "upper_bound": 714.6895412117264
          },
          "point_estimate": 196.97971510291757,
          "standard_error": 196.60283243278863
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365223.4068779343,
            "upper_bound": 365753.6230063291
          },
          "point_estimate": 365483.79742857144,
          "standard_error": 133.2142823909886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.14699533507718,
            "upper_bound": 591.0774448729469
          },
          "point_estimate": 434.8499138840692,
          "standard_error": 103.21840125334978
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486497.1156780951,
            "upper_bound": 487358.8872211639
          },
          "point_estimate": 486924.0667068783,
          "standard_error": 220.84920965118224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486244.5033333333,
            "upper_bound": 487614.8965185185
          },
          "point_estimate": 486826.0406666667,
          "standard_error": 438.18757538186816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.46882778013051,
            "upper_bound": 1163.3329417467592
          },
          "point_estimate": 988.6071856042174,
          "standard_error": 287.2067465602148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486712.76587072015,
            "upper_bound": 487510.75264031853
          },
          "point_estimate": 487240.02905627707,
          "standard_error": 203.61787325587315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492.0775675049585,
            "upper_bound": 857.0584593744011
          },
          "point_estimate": 738.0161427665387,
          "standard_error": 93.69846141372554
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41834.044113676515,
            "upper_bound": 41945.26559020569
          },
          "point_estimate": 41887.38013273283,
          "standard_error": 28.42056987814143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41819.71314878893,
            "upper_bound": 41931.81351403306
          },
          "point_estimate": 41882.689013840834,
          "standard_error": 23.99443514793127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.022740346404108,
            "upper_bound": 155.0416280433105
          },
          "point_estimate": 73.3464513429767,
          "standard_error": 38.20468820703344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41841.76302790023,
            "upper_bound": 41911.847106271656
          },
          "point_estimate": 41876.57434353421,
          "standard_error": 18.54418777588604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.17493064480063,
            "upper_bound": 129.8937779658517
          },
          "point_estimate": 94.59928812587847,
          "standard_error": 23.677888325772315
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96103.02837263267,
            "upper_bound": 96264.01637356175
          },
          "point_estimate": 96176.688106366,
          "standard_error": 41.3289701911949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96071.73275699168,
            "upper_bound": 96250.740446796
          },
          "point_estimate": 96142.38977072312,
          "standard_error": 46.363301210579365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.72749580403408,
            "upper_bound": 212.9120162597427
          },
          "point_estimate": 128.15059007055143,
          "standard_error": 47.39248528863228
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96098.18869392226,
            "upper_bound": 96216.8232524894
          },
          "point_estimate": 96168.08757644473,
          "standard_error": 30.448200711004436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.3281410283184,
            "upper_bound": 188.5642069720107
          },
          "point_estimate": 137.3467024583643,
          "standard_error": 36.193019759477394
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73735.11025745257,
            "upper_bound": 74077.98806857335
          },
          "point_estimate": 73927.6164251839,
          "standard_error": 88.83806884281707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73796.62601626017,
            "upper_bound": 74114.93069105691
          },
          "point_estimate": 74035.27076074333,
          "standard_error": 80.46638835748176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.620546486086205,
            "upper_bound": 379.9178755213699
          },
          "point_estimate": 132.7627595332343,
          "standard_error": 95.33203699385172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73903.66613023878,
            "upper_bound": 74111.80809193851
          },
          "point_estimate": 74028.02361946995,
          "standard_error": 54.08956481686746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.97083616141946,
            "upper_bound": 422.9816674895139
          },
          "point_estimate": 297.53492259200186,
          "standard_error": 94.65403461609534
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265231.5317986618,
            "upper_bound": 265457.0113067721
          },
          "point_estimate": 265335.4388402271,
          "standard_error": 58.14112042213495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265197.56934306567,
            "upper_bound": 265475.9517437145
          },
          "point_estimate": 265272.5620437956,
          "standard_error": 64.44380851457508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.64733075399846,
            "upper_bound": 312.02969791538357
          },
          "point_estimate": 133.61887171538564,
          "standard_error": 68.48361908374956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265242.0904418434,
            "upper_bound": 265479.5729088011
          },
          "point_estimate": 265367.1870698644,
          "standard_error": 60.6547221846162
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.62710516444449,
            "upper_bound": 251.83799894919017
          },
          "point_estimate": 193.71456415298132,
          "standard_error": 46.90791588544085
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36990.70611887605,
            "upper_bound": 37034.20898674288
          },
          "point_estimate": 37013.25378328408,
          "standard_error": 11.115771559179128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36980.75963603481,
            "upper_bound": 37039.91522550017
          },
          "point_estimate": 37021.074887371025,
          "standard_error": 13.43650597568181
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.073322661808634,
            "upper_bound": 66.56278126995178
          },
          "point_estimate": 32.49800177298827,
          "standard_error": 14.806651059261776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36975.00784268657,
            "upper_bound": 37025.94730165907
          },
          "point_estimate": 36998.0315678218,
          "standard_error": 13.464205806501123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.06981347359024,
            "upper_bound": 46.78274653372547
          },
          "point_estimate": 37.019314829215645,
          "standard_error": 7.128065673115805
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64330.87253560894,
            "upper_bound": 64450.61743657817
          },
          "point_estimate": 64389.78774947324,
          "standard_error": 30.676537601203645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64299.95289190898,
            "upper_bound": 64480.373967551626
          },
          "point_estimate": 64396.43460176991,
          "standard_error": 48.44371384025753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.382456930332953,
            "upper_bound": 175.4943529905461
          },
          "point_estimate": 132.2393566213125,
          "standard_error": 43.685626681625315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64337.982122517016,
            "upper_bound": 64451.82761846085
          },
          "point_estimate": 64395.633963912194,
          "standard_error": 28.60626428218642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.81814639969306,
            "upper_bound": 125.11355768769036
          },
          "point_estimate": 102.60556083097744,
          "standard_error": 16.36831050745674
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154230.5387981763,
            "upper_bound": 154646.24174873353
          },
          "point_estimate": 154418.56124518745,
          "standard_error": 107.40865441547744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154125.8170212766,
            "upper_bound": 154619.46312056738
          },
          "point_estimate": 154327.70402735562,
          "standard_error": 125.89684318197644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.766510585563312,
            "upper_bound": 521.9307394147667
          },
          "point_estimate": 303.262647616007,
          "standard_error": 140.30006795804033
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154232.1251643088,
            "upper_bound": 154645.77118188626
          },
          "point_estimate": 154462.9771096988,
          "standard_error": 105.2542621375043
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.6584962899484,
            "upper_bound": 478.991402777812
          },
          "point_estimate": 359.53001633967375,
          "standard_error": 92.53903028768651
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34318.41576050613,
            "upper_bound": 34367.24022426818
          },
          "point_estimate": 34342.161510896774,
          "standard_error": 12.53907884526664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34308.33684068528,
            "upper_bound": 34384.4910292729
          },
          "point_estimate": 34331.3328611898,
          "standard_error": 20.838965897649892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.414157681658635,
            "upper_bound": 69.12482377278639
          },
          "point_estimate": 42.85585423915531,
          "standard_error": 16.22454033375058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34302.99250105825,
            "upper_bound": 34364.03447572207
          },
          "point_estimate": 34331.76595906454,
          "standard_error": 16.596055790978586
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.021574782172777,
            "upper_bound": 50.563191538314626
          },
          "point_estimate": 41.696212381982015,
          "standard_error": 6.539291076219388
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11105.964435697544,
            "upper_bound": 11144.790810203483
          },
          "point_estimate": 11126.301183840193,
          "standard_error": 9.97561755065028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11104.699047182308,
            "upper_bound": 11155.518211992838
          },
          "point_estimate": 11129.29069601549,
          "standard_error": 14.643964808029946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.940971643816171,
            "upper_bound": 61.971135941218634
          },
          "point_estimate": 36.17596626771506,
          "standard_error": 12.707617222664602
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11129.283071626924,
            "upper_bound": 11156.424575697254
          },
          "point_estimate": 11146.760421968738,
          "standard_error": 6.907204842148773
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.451499276963823,
            "upper_bound": 43.244909868265815
          },
          "point_estimate": 33.30198473508429,
          "standard_error": 6.830418820428536
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494843.75604429434,
            "upper_bound": 495501.8655225224
          },
          "point_estimate": 495129.9534984984,
          "standard_error": 170.1276932720903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 494771.3040540541,
            "upper_bound": 495240.6648648649
          },
          "point_estimate": 495069.3573573574,
          "standard_error": 111.6567198803703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.56917373231379,
            "upper_bound": 698.7814238103051
          },
          "point_estimate": 272.32957300301746,
          "standard_error": 169.9193896653804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495000.16811772675,
            "upper_bound": 495334.3095636995
          },
          "point_estimate": 495130.6253071254,
          "standard_error": 85.08526424399497
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.45261923906185,
            "upper_bound": 824.716288629705
          },
          "point_estimate": 567.3162758671109,
          "standard_error": 196.5154317717856
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963.4245914716714,
            "upper_bound": 964.8335540227644
          },
          "point_estimate": 964.1289833561584,
          "standard_error": 0.3596026862553289
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963.252890595928,
            "upper_bound": 964.9733688621906
          },
          "point_estimate": 964.1599861783,
          "standard_error": 0.3696399005136078
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13835200257636693,
            "upper_bound": 2.088851425623606
          },
          "point_estimate": 0.9984919897422418,
          "standard_error": 0.5298862253382093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963.367506679581,
            "upper_bound": 964.6342841733888
          },
          "point_estimate": 964.0202812391478,
          "standard_error": 0.31982443671450206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6139807940513465,
            "upper_bound": 1.56392103809426
          },
          "point_estimate": 1.197050038195557,
          "standard_error": 0.245287841737687
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40966.52658327063,
            "upper_bound": 41108.68321673788
          },
          "point_estimate": 41031.08398625891,
          "standard_error": 36.564620607495904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40941.07957110609,
            "upper_bound": 41103.49390519188
          },
          "point_estimate": 40996.14915976925,
          "standard_error": 49.28506994742249
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.7470968787789545,
            "upper_bound": 183.15916245934264
          },
          "point_estimate": 92.31279329340214,
          "standard_error": 51.21990203355069
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40972.27091579022,
            "upper_bound": 41072.73003764399
          },
          "point_estimate": 41025.84616985723,
          "standard_error": 25.045018873122867
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.96541944250377,
            "upper_bound": 166.1741329070069
          },
          "point_estimate": 121.89291692210082,
          "standard_error": 32.14239356452813
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48609.568809523815,
            "upper_bound": 48720.44584770829
          },
          "point_estimate": 48655.01061824016,
          "standard_error": 29.52982988977031
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48599.0702811245,
            "upper_bound": 48670.86520154693
          },
          "point_estimate": 48625.5172021419,
          "standard_error": 20.47068712131299
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.6882582748308765,
            "upper_bound": 91.31292550738958
          },
          "point_estimate": 58.10868993221857,
          "standard_error": 20.94945363823895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48616.61855793047,
            "upper_bound": 48665.06410200682
          },
          "point_estimate": 48642.3546932318,
          "standard_error": 12.430974933084409
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.456290159480783,
            "upper_bound": 147.03805099256408
          },
          "point_estimate": 98.33517365386332,
          "standard_error": 39.099588362133055
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48815.113254110736,
            "upper_bound": 48941.37171498882
          },
          "point_estimate": 48872.33588926174,
          "standard_error": 32.37448927567951
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48788.75693512304,
            "upper_bound": 48927.01704697987
          },
          "point_estimate": 48856.355659955254,
          "standard_error": 36.82848486395796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.427148419340226,
            "upper_bound": 158.4504675225184
          },
          "point_estimate": 92.03052767820984,
          "standard_error": 39.283695018965176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48789.13469336285,
            "upper_bound": 48890.319049638936
          },
          "point_estimate": 48836.54501525321,
          "standard_error": 25.625452839753866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.12293104084419,
            "upper_bound": 144.1873770662864
          },
          "point_estimate": 107.50920863927205,
          "standard_error": 27.375638001206276
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14036.03290001472,
            "upper_bound": 14051.911250628666
          },
          "point_estimate": 14044.537809747677,
          "standard_error": 4.089518561444949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14035.480765928216,
            "upper_bound": 14051.589661073083
          },
          "point_estimate": 14050.235815945389,
          "standard_error": 4.768827094521198
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4414890309961136,
            "upper_bound": 21.016198351772147
          },
          "point_estimate": 2.569138864744176,
          "standard_error": 6.53158010118692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14040.952374973598,
            "upper_bound": 14052.783606694096
          },
          "point_estimate": 14047.681225034625,
          "standard_error": 3.1552114934499733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.987337476003979,
            "upper_bound": 17.113761740413917
          },
          "point_estimate": 13.611102347993446,
          "standard_error": 2.9474846139584745
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25625.675639286277,
            "upper_bound": 25688.15825575928
          },
          "point_estimate": 25654.472860277157,
          "standard_error": 16.049901057174168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25621.88834038552,
            "upper_bound": 25680.68267512929
          },
          "point_estimate": 25645.846654129447,
          "standard_error": 12.40990011478312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.580376846524938,
            "upper_bound": 89.29719855570232
          },
          "point_estimate": 32.53038332938123,
          "standard_error": 21.7681948697749
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25632.717775748188,
            "upper_bound": 25667.858191619376
          },
          "point_estimate": 25648.843135566833,
          "standard_error": 8.683731454736858
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.2857667592024,
            "upper_bound": 73.00922737393363
          },
          "point_estimate": 53.51771110687549,
          "standard_error": 14.132906656760056
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17615.459001894855,
            "upper_bound": 17629.00719559201
          },
          "point_estimate": 17621.80880693206,
          "standard_error": 3.4718460096726615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17613.534470246734,
            "upper_bound": 17630.35510401548
          },
          "point_estimate": 17619.178164812125,
          "standard_error": 4.346778794618019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1937488056073045,
            "upper_bound": 18.742212749020624
          },
          "point_estimate": 8.551590503714857,
          "standard_error": 4.413534536583146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17611.50354249041,
            "upper_bound": 17633.039417696156
          },
          "point_estimate": 17620.705527177226,
          "standard_error": 5.601301568244077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.3868301599165855,
            "upper_bound": 15.284987969881126
          },
          "point_estimate": 11.591606685961455,
          "standard_error": 2.613935882995576
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17602.37274480608,
            "upper_bound": 17639.6521197561
          },
          "point_estimate": 17619.019482242835,
          "standard_error": 9.635713576945294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17597.519460594314,
            "upper_bound": 17633.074709302324
          },
          "point_estimate": 17612.681204242035,
          "standard_error": 9.47623645058118
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.134238680653479,
            "upper_bound": 44.42892041036174
          },
          "point_estimate": 21.563468100699193,
          "standard_error": 9.946148717916158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17602.847615710933,
            "upper_bound": 17619.930249770863
          },
          "point_estimate": 17610.868167975434,
          "standard_error": 4.317973709616931
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.087572438318205,
            "upper_bound": 45.1916838911097
          },
          "point_estimate": 32.188405312309555,
          "standard_error": 9.570202030567556
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17590.492815303973,
            "upper_bound": 17621.459487632306
          },
          "point_estimate": 17605.782965272017,
          "standard_error": 7.949477230206796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17583.030754352032,
            "upper_bound": 17627.42307382334
          },
          "point_estimate": 17608.326590371806,
          "standard_error": 9.176390074581455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.732303799439508,
            "upper_bound": 52.569349107008016
          },
          "point_estimate": 24.00704024432655,
          "standard_error": 13.627594258069326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17587.770098376306,
            "upper_bound": 17608.038992964313
          },
          "point_estimate": 17598.960478786204,
          "standard_error": 5.125751377926977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.146065509576973,
            "upper_bound": 33.646162970067884
          },
          "point_estimate": 26.60132232730572,
          "standard_error": 5.004424933740686
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39482.00113733225,
            "upper_bound": 39539.62322542619
          },
          "point_estimate": 39510.03647321968,
          "standard_error": 14.646230153440134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39473.21994015234,
            "upper_bound": 39530.31964091404
          },
          "point_estimate": 39515.08271067585,
          "standard_error": 13.374628217385707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.139319042252927,
            "upper_bound": 89.03128460001103
          },
          "point_estimate": 24.368303377680796,
          "standard_error": 21.42704722878295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39507.189505375856,
            "upper_bound": 39575.57270528332
          },
          "point_estimate": 39540.422797224535,
          "standard_error": 19.36667139800127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.490977335688864,
            "upper_bound": 66.68110618530794
          },
          "point_estimate": 48.72813375474891,
          "standard_error": 11.914535909380003
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18768.17186213547,
            "upper_bound": 18794.166030641685
          },
          "point_estimate": 18781.268120954843,
          "standard_error": 6.634636863858391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18764.754056847545,
            "upper_bound": 18796.063759689925
          },
          "point_estimate": 18782.672572905132,
          "standard_error": 6.400693339186839
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.128581205772678,
            "upper_bound": 37.229988072883565
          },
          "point_estimate": 13.258105113462165,
          "standard_error": 9.63517755858383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18772.659530908368,
            "upper_bound": 18792.648952096355
          },
          "point_estimate": 18780.527174737406,
          "standard_error": 5.098208552391967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.57775796094632,
            "upper_bound": 29.16433739602622
          },
          "point_estimate": 22.143818153857435,
          "standard_error": 4.678568119580328
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18806.341229112833,
            "upper_bound": 18827.27460907777
          },
          "point_estimate": 18816.8651060662,
          "standard_error": 5.3729915443509935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18801.04217054263,
            "upper_bound": 18833.86754521964
          },
          "point_estimate": 18817.39904023625,
          "standard_error": 9.753174468761074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0625513254744723,
            "upper_bound": 28.400217712849138
          },
          "point_estimate": 21.543416310032757,
          "standard_error": 6.857672785846479
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18809.87431221228,
            "upper_bound": 18829.932183188343
          },
          "point_estimate": 18818.6117560992,
          "standard_error": 5.161200778536833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.566105090354649,
            "upper_bound": 21.028983334697564
          },
          "point_estimate": 17.957277227742853,
          "standard_error": 2.4172016815795536
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17484.873916320066,
            "upper_bound": 17511.63809865828
          },
          "point_estimate": 17498.33842763188,
          "standard_error": 6.865366194550255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17471.369465832533,
            "upper_bound": 17518.607635547
          },
          "point_estimate": 17498.688322104586,
          "standard_error": 10.967696826931371
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.517378233997584,
            "upper_bound": 38.49261081806503
          },
          "point_estimate": 32.78308223800522,
          "standard_error": 9.80415272483655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17485.835985146274,
            "upper_bound": 17509.83325780216
          },
          "point_estimate": 17498.341484694323,
          "standard_error": 6.11242018621378
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.78876016859518,
            "upper_bound": 27.316471811714305
          },
          "point_estimate": 22.866432869946557,
          "standard_error": 3.3827565861850446
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17384.44507102814,
            "upper_bound": 17407.244672618097
          },
          "point_estimate": 17395.59415879089,
          "standard_error": 5.842765522530241
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17377.755380200862,
            "upper_bound": 17412.75638450502
          },
          "point_estimate": 17394.171940591958,
          "standard_error": 9.926608092667058
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.690053531070736,
            "upper_bound": 32.10818052035301
          },
          "point_estimate": 25.35099026410373,
          "standard_error": 7.748893037293448
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17380.086450777,
            "upper_bound": 17413.629059925002
          },
          "point_estimate": 17397.43854863453,
          "standard_error": 8.835907779592675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.971139574226855,
            "upper_bound": 23.17513098197269
          },
          "point_estimate": 19.51591106394188,
          "standard_error": 2.8322114133222955
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17362.023705018237,
            "upper_bound": 17381.31569932087
          },
          "point_estimate": 17371.75832495317,
          "standard_error": 4.910793420770113
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17360.177854753943,
            "upper_bound": 17380.13794924882
          },
          "point_estimate": 17374.31301617637,
          "standard_error": 4.553649110174839
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.352720502906898,
            "upper_bound": 27.290778636369865
          },
          "point_estimate": 10.929228926846209,
          "standard_error": 6.786220758321651
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17368.0776104115,
            "upper_bound": 17379.1938840238
          },
          "point_estimate": 17374.986603458652,
          "standard_error": 2.8265797002312905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.8681660943422305,
            "upper_bound": 22.053724141002007
          },
          "point_estimate": 16.36171173021416,
          "standard_error": 3.7533608786725594
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17607.06741165646,
            "upper_bound": 17635.157186848537
          },
          "point_estimate": 17620.093508892114,
          "standard_error": 7.215702554990315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17602.60133583172,
            "upper_bound": 17630.64446056308
          },
          "point_estimate": 17619.09121131528,
          "standard_error": 7.995304658166773
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.544465248137035,
            "upper_bound": 36.533515291118846
          },
          "point_estimate": 19.021836523998193,
          "standard_error": 7.989565200187501
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17605.460461920567,
            "upper_bound": 17628.949644987097
          },
          "point_estimate": 17619.926624632622,
          "standard_error": 6.071973618372069
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.595820941800277,
            "upper_bound": 33.35312962845797
          },
          "point_estimate": 24.09754044370549,
          "standard_error": 6.4794297012965565
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15274.106373106288,
            "upper_bound": 15287.437663435066
          },
          "point_estimate": 15280.835095881415,
          "standard_error": 3.4338998201107653
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15269.204289164216,
            "upper_bound": 15291.342573148537
          },
          "point_estimate": 15282.526821130245,
          "standard_error": 6.844055036165469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3515259314866412,
            "upper_bound": 17.10247658283215
          },
          "point_estimate": 14.139070051375604,
          "standard_error": 4.569665676767046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15269.765039146912,
            "upper_bound": 15281.54501887978
          },
          "point_estimate": 15274.451969869691,
          "standard_error": 2.990239504648725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.640182960373269,
            "upper_bound": 13.21567532962033
          },
          "point_estimate": 11.458706337966968,
          "standard_error": 1.4343727888090851
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16364.95273428972,
            "upper_bound": 16389.006204127232
          },
          "point_estimate": 16375.64371513695,
          "standard_error": 6.215292673290927
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16362.180712032448,
            "upper_bound": 16383.577579990986
          },
          "point_estimate": 16371.934903109508,
          "standard_error": 5.68308426260718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.997530055212166,
            "upper_bound": 26.622769558897293
          },
          "point_estimate": 12.756077928107736,
          "standard_error": 5.831338414614097
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16360.251843310432,
            "upper_bound": 16376.912620099143
          },
          "point_estimate": 16367.74408268613,
          "standard_error": 4.2297345001010545
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.824281420800044,
            "upper_bound": 29.611150187724046
          },
          "point_estimate": 20.75026392407935,
          "standard_error": 6.480250145034192
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17965.585447531954,
            "upper_bound": 18000.517153626155
          },
          "point_estimate": 17982.41331609114,
          "standard_error": 8.96816861169088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17959.524380340372,
            "upper_bound": 18009.690163124073
          },
          "point_estimate": 17974.754829873127,
          "standard_error": 13.144341010275737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.211589916125486,
            "upper_bound": 51.0115003795853
          },
          "point_estimate": 30.7579974865606,
          "standard_error": 11.70249003760781
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17963.8625694145,
            "upper_bound": 17991.53432336639
          },
          "point_estimate": 17977.13129786674,
          "standard_error": 7.349839859328525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.890293298122888,
            "upper_bound": 37.06051128694209
          },
          "point_estimate": 29.94388669792607,
          "standard_error": 5.311306787749013
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18442.478936115334,
            "upper_bound": 18462.833945990675
          },
          "point_estimate": 18451.832795494996,
          "standard_error": 5.224937335695319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18439.392097919837,
            "upper_bound": 18460.62567224759
          },
          "point_estimate": 18448.64434699975,
          "standard_error": 4.604781015910485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.088394736306111,
            "upper_bound": 26.993668635431916
          },
          "point_estimate": 10.487520564697371,
          "standard_error": 6.505770754791766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18438.612954592216,
            "upper_bound": 18452.217381996757
          },
          "point_estimate": 18444.553572252204,
          "standard_error": 3.518533678037727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.126712982153045,
            "upper_bound": 23.737690694232853
          },
          "point_estimate": 17.411732418360007,
          "standard_error": 4.581477755180676
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68648.60737840671,
            "upper_bound": 68712.20744115005
          },
          "point_estimate": 68679.75547993412,
          "standard_error": 16.435303572294718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68624.33396226415,
            "upper_bound": 68747.53396226415
          },
          "point_estimate": 68663.1183962264,
          "standard_error": 29.642692201352386
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.265412150687353,
            "upper_bound": 94.17470370542576
          },
          "point_estimate": 63.540231985153085,
          "standard_error": 23.833983327345695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68651.97207840158,
            "upper_bound": 68724.71357881924
          },
          "point_estimate": 68685.59583925508,
          "standard_error": 18.9969461025668
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.54790182173909,
            "upper_bound": 63.470259741333486
          },
          "point_estimate": 55.07800233291592,
          "standard_error": 7.523186191926258
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228431.5525550265,
            "upper_bound": 1230870.8175849863
          },
          "point_estimate": 1229673.499732804,
          "standard_error": 621.6745799720333
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228520.361904762,
            "upper_bound": 1230837.788888889
          },
          "point_estimate": 1229891.856481481,
          "standard_error": 574.755832388655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387.1748056263307,
            "upper_bound": 3498.1032108962836
          },
          "point_estimate": 1554.1225182979615,
          "standard_error": 756.8449903497897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1229051.0469463747,
            "upper_bound": 1230241.2276255707
          },
          "point_estimate": 1229620.0772294372,
          "standard_error": 311.6958260479469
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 857.0610034843809,
            "upper_bound": 2809.176048366797
          },
          "point_estimate": 2067.6961365774036,
          "standard_error": 497.66139271252274
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208236.48196643993,
            "upper_bound": 209063.69544595235
          },
          "point_estimate": 208584.2822426304,
          "standard_error": 216.5814687202312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208138.80514285713,
            "upper_bound": 208721.19015873017
          },
          "point_estimate": 208459.525,
          "standard_error": 139.6514491345527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.21695325631408,
            "upper_bound": 728.0848101215392
          },
          "point_estimate": 305.678791373124,
          "standard_error": 171.72064654672542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208179.73290141908,
            "upper_bound": 208692.7516203074
          },
          "point_estimate": 208436.3849944341,
          "standard_error": 132.02507121475156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.93796361060353,
            "upper_bound": 1068.986234629784
          },
          "point_estimate": 723.1252843255222,
          "standard_error": 270.66268649236133
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5645.95714235019,
            "upper_bound": 5667.207847822075
          },
          "point_estimate": 5655.613639725607,
          "standard_error": 5.469427487604525
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5644.271524547804,
            "upper_bound": 5666.646976744186
          },
          "point_estimate": 5651.43494324474,
          "standard_error": 5.4018962343870225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7083260933908764,
            "upper_bound": 26.736932092766377
          },
          "point_estimate": 13.140546766062608,
          "standard_error": 6.369214197628281
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5638.322142404254,
            "upper_bound": 5654.873371457618
          },
          "point_estimate": 5645.025541528239,
          "standard_error": 4.279823051647258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.434553300425298,
            "upper_bound": 25.057824935918603
          },
          "point_estimate": 18.15240692200142,
          "standard_error": 4.968922283766875
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5972.0111109683385,
            "upper_bound": 5999.41374023666
          },
          "point_estimate": 5984.1410320788955,
          "standard_error": 7.099345243097638
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5968.267105263158,
            "upper_bound": 5999.064506578948
          },
          "point_estimate": 5974.9443359375,
          "standard_error": 6.618422877535349
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.713304261039704,
            "upper_bound": 31.39169723873857
          },
          "point_estimate": 11.86182395388467,
          "standard_error": 6.9962805001055575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5972.285039487564,
            "upper_bound": 5994.208484254878
          },
          "point_estimate": 5981.256339712919,
          "standard_error": 5.5043597114147635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.926956024972277,
            "upper_bound": 31.707455122036187
          },
          "point_estimate": 23.63446853828758,
          "standard_error": 6.91877712345117
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14307.93158556811,
            "upper_bound": 14329.28977530406
          },
          "point_estimate": 14318.62825588913,
          "standard_error": 5.483740801126845
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14304.954807818443,
            "upper_bound": 14333.543486816214
          },
          "point_estimate": 14317.019058863214,
          "standard_error": 5.299762406790019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5766639016084897,
            "upper_bound": 32.8062440456709
          },
          "point_estimate": 13.717482858592325,
          "standard_error": 10.156726003356196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14314.797282227424,
            "upper_bound": 14329.068204648802
          },
          "point_estimate": 14319.859806702543,
          "standard_error": 3.747929918670539
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.62018300589946,
            "upper_bound": 23.894033534572237
          },
          "point_estimate": 18.23025410477792,
          "standard_error": 3.816779100114462
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.380368491695215,
            "upper_bound": 62.57373399329643
          },
          "point_estimate": 62.476220453105384,
          "standard_error": 0.04941865481824046
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.31818967709128,
            "upper_bound": 62.60248198999915
          },
          "point_estimate": 62.49682091702687,
          "standard_error": 0.07149093590755413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031168651874823067,
            "upper_bound": 0.29600732350150355
          },
          "point_estimate": 0.1899621941866076,
          "standard_error": 0.0660195162053439
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.329070320137845,
            "upper_bound": 62.502563957464474
          },
          "point_estimate": 62.40053371020877,
          "standard_error": 0.044520453731464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09730869042054172,
            "upper_bound": 0.20592021793751977
          },
          "point_estimate": 0.16486457285664324,
          "standard_error": 0.028289204036417277
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.029896358066985,
            "upper_bound": 24.07265372373517
          },
          "point_estimate": 24.050256980269747,
          "standard_error": 0.010934492678963502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.02480780104556,
            "upper_bound": 24.074535829191756
          },
          "point_estimate": 24.045483684759233,
          "standard_error": 0.01191607872919416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004203559797203521,
            "upper_bound": 0.06001569801790918
          },
          "point_estimate": 0.0368633866103182,
          "standard_error": 0.014694781899843369
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.041274539621693,
            "upper_bound": 24.06909056763609
          },
          "point_estimate": 24.05534926906101,
          "standard_error": 0.007211795504735566
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01770189194004584,
            "upper_bound": 0.047534869722714305
          },
          "point_estimate": 0.03646789281766727,
          "standard_error": 0.007743768457675175
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.485958619357937,
            "upper_bound": 24.540042472978552
          },
          "point_estimate": 24.510797928604863,
          "standard_error": 0.01391856426269206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.476747830701463,
            "upper_bound": 24.540727248887237
          },
          "point_estimate": 24.494527300442556,
          "standard_error": 0.016293324737090508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008636499739555508,
            "upper_bound": 0.06994663698847443
          },
          "point_estimate": 0.03936172265125159,
          "standard_error": 0.016277780422619518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.484735560263005,
            "upper_bound": 24.52968111239602
          },
          "point_estimate": 24.5042011530117,
          "standard_error": 0.011506621916425404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01895503731870967,
            "upper_bound": 0.06217595110894847
          },
          "point_estimate": 0.04642515272756484,
          "standard_error": 0.01175684645379504
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.25619665041068,
            "upper_bound": 24.29355097506336
          },
          "point_estimate": 24.27368616080473,
          "standard_error": 0.009629103044114976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.24977029657923,
            "upper_bound": 24.295202893970803
          },
          "point_estimate": 24.26941755765848,
          "standard_error": 0.009894081690537631
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004441648180362778,
            "upper_bound": 0.05389354645795139
          },
          "point_estimate": 0.023197161088030775,
          "standard_error": 0.012452449337547055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.26133783165793,
            "upper_bound": 24.28628395767096
          },
          "point_estimate": 24.273545474319675,
          "standard_error": 0.006245752381664828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01339967199539755,
            "upper_bound": 0.041566266620400874
          },
          "point_estimate": 0.03214814664868852,
          "standard_error": 0.007290484477925668
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.784059807306555,
            "upper_bound": 23.826707078711912
          },
          "point_estimate": 23.803583140821747,
          "standard_error": 0.011048804559694154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.77714193507111,
            "upper_bound": 23.825916900686043
          },
          "point_estimate": 23.78567964251815,
          "standard_error": 0.01615704423389722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00185354037946183,
            "upper_bound": 0.06567235034232176
          },
          "point_estimate": 0.019133620502851483,
          "standard_error": 0.01628846325965435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.78165051321331,
            "upper_bound": 23.81230175578105
          },
          "point_estimate": 23.79338855674201,
          "standard_error": 0.00794751576025185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01653352646408941,
            "upper_bound": 0.04989314427516118
          },
          "point_estimate": 0.03684082005761228,
          "standard_error": 0.00934162493113361
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.90384640173602,
            "upper_bound": 19.92399007339504
          },
          "point_estimate": 19.91371193108833,
          "standard_error": 0.005180380312176973
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.89858859099449,
            "upper_bound": 19.93050986159519
          },
          "point_estimate": 19.908670862551546,
          "standard_error": 0.009167620161554252
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004321104289531608,
            "upper_bound": 0.0271754158291324
          },
          "point_estimate": 0.02220024070403636,
          "standard_error": 0.006439951296610565
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.897771092881953,
            "upper_bound": 19.92340882019577
          },
          "point_estimate": 19.908638264550255,
          "standard_error": 0.006862006303650452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010732102238911776,
            "upper_bound": 0.02025257972245108
          },
          "point_estimate": 0.017239942702117702,
          "standard_error": 0.002427894879628159
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.044436910743052,
            "upper_bound": 26.088595018564412
          },
          "point_estimate": 26.067149389347797,
          "standard_error": 0.0112955649572748
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.043144775891456,
            "upper_bound": 26.09240176443199
          },
          "point_estimate": 26.071303585679367,
          "standard_error": 0.01120564635558643
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001596747384460394,
            "upper_bound": 0.06293016271970966
          },
          "point_estimate": 0.02653829562847241,
          "standard_error": 0.015688137533550705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.058086937734355,
            "upper_bound": 26.10414027027595
          },
          "point_estimate": 26.07938647722137,
          "standard_error": 0.012138263269592496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01813727721043721,
            "upper_bound": 0.04991803457949101
          },
          "point_estimate": 0.03765941156724161,
          "standard_error": 0.008300146945278992
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.02859874724357,
            "upper_bound": 33.069415543513976
          },
          "point_estimate": 33.050790386774864,
          "standard_error": 0.010509135974296672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.03455863310431,
            "upper_bound": 33.079458429768614
          },
          "point_estimate": 33.05197071327252,
          "standard_error": 0.013548140908556707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002056526430596985,
            "upper_bound": 0.05949550565945811
          },
          "point_estimate": 0.03044015684558965,
          "standard_error": 0.013154588307706378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.037075390310186,
            "upper_bound": 33.07365463987896
          },
          "point_estimate": 33.05590517413769,
          "standard_error": 0.00948466437832903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01686090641498958,
            "upper_bound": 0.04791368714699627
          },
          "point_estimate": 0.03487427002256846,
          "standard_error": 0.009074675213177162
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.258646995517623,
            "upper_bound": 27.3173230453258
          },
          "point_estimate": 27.2916589481848,
          "standard_error": 0.015089250526429256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.27835870556416,
            "upper_bound": 27.31968931529498
          },
          "point_estimate": 27.301707472492023,
          "standard_error": 0.010755320120334696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0043048580477089695,
            "upper_bound": 0.06081417210568682
          },
          "point_estimate": 0.030312023184969415,
          "standard_error": 0.014743872843723142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.292598706211336,
            "upper_bound": 27.32886357504609
          },
          "point_estimate": 27.31208461300225,
          "standard_error": 0.009156643337955694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014718780796109289,
            "upper_bound": 0.07238313973926086
          },
          "point_estimate": 0.04997412657519593,
          "standard_error": 0.01667108140607368
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.950064805798505,
            "upper_bound": 35.02313725767568
          },
          "point_estimate": 34.98383064802192,
          "standard_error": 0.018850375394933776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.94460890312287,
            "upper_bound": 35.043390081524485
          },
          "point_estimate": 34.95607734766742,
          "standard_error": 0.021907160368646875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014984716852989373,
            "upper_bound": 0.09833342046969493
          },
          "point_estimate": 0.04307522733824896,
          "standard_error": 0.02541012735797053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.94063852358705,
            "upper_bound": 34.9852016157836
          },
          "point_estimate": 34.96081686031273,
          "standard_error": 0.011225025170947771
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02174645921043539,
            "upper_bound": 0.07905585202226946
          },
          "point_estimate": 0.06280676393609995,
          "standard_error": 0.014407673585844546
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.816665937230457,
            "upper_bound": 26.88728325748584
          },
          "point_estimate": 26.853990249846703,
          "standard_error": 0.01807286227451002
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.808596479448447,
            "upper_bound": 26.902981515108063
          },
          "point_estimate": 26.869776659984257,
          "standard_error": 0.02367044243797442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007997725224640668,
            "upper_bound": 0.10274631907875333
          },
          "point_estimate": 0.056980265446205665,
          "standard_error": 0.02266343490085941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.85793860652814,
            "upper_bound": 26.899970689711164
          },
          "point_estimate": 26.878747473014123,
          "standard_error": 0.0107027125965523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02936670138478012,
            "upper_bound": 0.07640530859125846
          },
          "point_estimate": 0.06043790133106245,
          "standard_error": 0.012006450364106127
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.169540342931835,
            "upper_bound": 19.20977308621624
          },
          "point_estimate": 19.192061644401814,
          "standard_error": 0.01041353662624762
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.178487093538287,
            "upper_bound": 19.219954222537215
          },
          "point_estimate": 19.200815528057085,
          "standard_error": 0.011247285997579484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024829187793648766,
            "upper_bound": 0.04755261722745406
          },
          "point_estimate": 0.02921932677508338,
          "standard_error": 0.01087152536144212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.18876576339693,
            "upper_bound": 19.214413142417904
          },
          "point_estimate": 19.202235369753357,
          "standard_error": 0.006573170036272689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013362414153398734,
            "upper_bound": 0.04900967753960974
          },
          "point_estimate": 0.03465641582158547,
          "standard_error": 0.010590851613471033
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.822361934565546,
            "upper_bound": 28.857379041397955
          },
          "point_estimate": 28.84127704768881,
          "standard_error": 0.00898580821156918
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.82326669856756,
            "upper_bound": 28.86700493246984
          },
          "point_estimate": 28.844657811392743,
          "standard_error": 0.010793018858918568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00542783364287553,
            "upper_bound": 0.046147716719773024
          },
          "point_estimate": 0.02770400747782289,
          "standard_error": 0.011812508310279291
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.838134225094915,
            "upper_bound": 28.864741877789037
          },
          "point_estimate": 28.85286919756734,
          "standard_error": 0.006906525893208217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013252502638953004,
            "upper_bound": 0.03994739488245973
          },
          "point_estimate": 0.02997621915442328,
          "standard_error": 0.007333906406300943
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99616.96466402478,
            "upper_bound": 99738.6781539139
          },
          "point_estimate": 99677.09984148727,
          "standard_error": 31.204637275142066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99615.3691780822,
            "upper_bound": 99746.31196347032
          },
          "point_estimate": 99669.34794520547,
          "standard_error": 29.99453492517169
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.527852017589378,
            "upper_bound": 180.4211449749673
          },
          "point_estimate": 79.59581673757538,
          "standard_error": 41.650552843965755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99586.92894389611,
            "upper_bound": 99751.59656710978
          },
          "point_estimate": 99681.59215086284,
          "standard_error": 42.58997490312418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.24657811001221,
            "upper_bound": 139.08357470766194
          },
          "point_estimate": 104.02436121970904,
          "standard_error": 23.35081948533538
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52462.52127410157,
            "upper_bound": 52587.32921645022
          },
          "point_estimate": 52523.78964062392,
          "standard_error": 31.98238185805137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52428.246464646465,
            "upper_bound": 52651.97258297258
          },
          "point_estimate": 52501.55687830688,
          "standard_error": 55.021435204024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.367836144071202,
            "upper_bound": 175.99089243108824
          },
          "point_estimate": 125.81870788171432,
          "standard_error": 42.98612546788625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52420.4959449153,
            "upper_bound": 52517.95546669795
          },
          "point_estimate": 52454.36063042297,
          "standard_error": 24.901688170083418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.54219373405799,
            "upper_bound": 124.9609675907615
          },
          "point_estimate": 106.71099576912192,
          "standard_error": 14.83531462049756
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145132.52898248276,
            "upper_bound": 145283.41468570163
          },
          "point_estimate": 145208.90304844116,
          "standard_error": 38.704139286481016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145106.3224245874,
            "upper_bound": 145310.4314741036
          },
          "point_estimate": 145220.00265604252,
          "standard_error": 52.31853077891055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.626008989445815,
            "upper_bound": 227.20443779896956
          },
          "point_estimate": 121.6965017239244,
          "standard_error": 49.09279260700807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145064.66130219708,
            "upper_bound": 145236.88342642327
          },
          "point_estimate": 145137.92560666424,
          "standard_error": 43.6294493908882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.65140594780111,
            "upper_bound": 161.99991692298806
          },
          "point_estimate": 129.38319888123328,
          "standard_error": 22.662509481489625
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401729.8338920068,
            "upper_bound": 402264.89390676783
          },
          "point_estimate": 401980.1097309437,
          "standard_error": 137.22729872766396
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401641.64203296707,
            "upper_bound": 402230.89033882786
          },
          "point_estimate": 401965.11237571953,
          "standard_error": 150.99510987697892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.59264058102069,
            "upper_bound": 731.3851867955566
          },
          "point_estimate": 407.2624933190802,
          "standard_error": 167.58141179443308
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401770.3309943069,
            "upper_bound": 402210.92766531714
          },
          "point_estimate": 401985.0045383189,
          "standard_error": 114.0025571724785
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.95894858233095,
            "upper_bound": 613.4098647827717
          },
          "point_estimate": 456.194232039139,
          "standard_error": 109.45459326505537
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570271.3979735243,
            "upper_bound": 571492.1372217726
          },
          "point_estimate": 570825.9403156001,
          "standard_error": 314.41194857360483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570213.4880208333,
            "upper_bound": 571320.482421875
          },
          "point_estimate": 570561.1884920634,
          "standard_error": 275.6368562934545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.92108134608213,
            "upper_bound": 1499.3424140584652
          },
          "point_estimate": 531.4656857728494,
          "standard_error": 389.25193694598624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570309.1717686567,
            "upper_bound": 570922.7498030121
          },
          "point_estimate": 570631.8212256493,
          "standard_error": 154.85150027548065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.78200503416906,
            "upper_bound": 1459.5206562571643
          },
          "point_estimate": 1049.7639201658392,
          "standard_error": 295.9741397625488
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39689.92015657779,
            "upper_bound": 39747.37088741274
          },
          "point_estimate": 39719.131900029526,
          "standard_error": 14.70258221811945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39678.71395575006,
            "upper_bound": 39765.30381108679
          },
          "point_estimate": 39718.590413019694,
          "standard_error": 24.5566319348068
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.61017165796274,
            "upper_bound": 84.17162420587688
          },
          "point_estimate": 61.73340779259447,
          "standard_error": 18.414107888108745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39695.68910894353,
            "upper_bound": 39744.59486129603
          },
          "point_estimate": 39715.394074852935,
          "standard_error": 12.313948802221333
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.426330983216943,
            "upper_bound": 60.21976011921516
          },
          "point_estimate": 49.02170702149915,
          "standard_error": 7.751600594513516
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97583.9742227329,
            "upper_bound": 97656.56198633985
          },
          "point_estimate": 97620.9213742074,
          "standard_error": 18.514569650900977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97584.16845397676,
            "upper_bound": 97668.98126303247
          },
          "point_estimate": 97614.92068173114,
          "standard_error": 23.8451426667238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.701517727206083,
            "upper_bound": 113.02891343569132
          },
          "point_estimate": 46.79376118890933,
          "standard_error": 24.845846863803715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97605.70179418437,
            "upper_bound": 97651.70238849426
          },
          "point_estimate": 97630.12254447964,
          "standard_error": 11.596870682707756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.65242662028293,
            "upper_bound": 81.10317856769998
          },
          "point_estimate": 61.548991788066786,
          "standard_error": 12.797994938685292
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71057.12478326034,
            "upper_bound": 71227.9055823471
          },
          "point_estimate": 71134.5670467959,
          "standard_error": 43.88836300524729
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71035.13586804585,
            "upper_bound": 71196.36643835617
          },
          "point_estimate": 71103.98431996087,
          "standard_error": 39.53791063383592
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.567455715510437,
            "upper_bound": 212.9715520996218
          },
          "point_estimate": 119.52021964913165,
          "standard_error": 54.543956767467314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71005.43139485535,
            "upper_bound": 71109.61560904734
          },
          "point_estimate": 71042.62762091137,
          "standard_error": 26.363150082186557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.62742329502541,
            "upper_bound": 200.98190736739016
          },
          "point_estimate": 146.59300991569015,
          "standard_error": 40.011704728138255
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307572.13121337775,
            "upper_bound": 308298.0919741222
          },
          "point_estimate": 307885.1105444579,
          "standard_error": 189.2751209361006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307465.7378531074,
            "upper_bound": 308076.3399246704
          },
          "point_estimate": 307752.79533898307,
          "standard_error": 189.8447735588376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.87576004144914,
            "upper_bound": 787.4510065848997
          },
          "point_estimate": 412.16402668688386,
          "standard_error": 168.25829035798128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307634.5508223105,
            "upper_bound": 308036.93849388714
          },
          "point_estimate": 307851.3480959718,
          "standard_error": 101.24482694681262
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229.82360063159743,
            "upper_bound": 915.553956010146
          },
          "point_estimate": 630.8259146835486,
          "standard_error": 214.04700951444457
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35337.84809955729,
            "upper_bound": 35376.76052316165
          },
          "point_estimate": 35357.95694984343,
          "standard_error": 9.92932037923796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35337.069582118565,
            "upper_bound": 35383.29244277076
          },
          "point_estimate": 35361.73922902494,
          "standard_error": 11.310463035376443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.717708502435514,
            "upper_bound": 58.07420220027165
          },
          "point_estimate": 27.761168214622963,
          "standard_error": 12.361877712697153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35352.254138367236,
            "upper_bound": 35378.85505865988
          },
          "point_estimate": 35368.59688008784,
          "standard_error": 6.690575282128588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.43189588843191,
            "upper_bound": 43.76523099470579
          },
          "point_estimate": 33.01709282231024,
          "standard_error": 7.116648723194778
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64516.39518826516,
            "upper_bound": 64586.29658707293
          },
          "point_estimate": 64552.85736104074,
          "standard_error": 17.911864445914116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64499.83488475177,
            "upper_bound": 64612.6670212766
          },
          "point_estimate": 64557.84141759541,
          "standard_error": 27.440727835629417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.517953199905738,
            "upper_bound": 105.705500756334
          },
          "point_estimate": 63.99785519802584,
          "standard_error": 26.521703980201867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64534.55990536206,
            "upper_bound": 64596.0586649677
          },
          "point_estimate": 64568.96313898867,
          "standard_error": 16.054430978772924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.72613853855209,
            "upper_bound": 73.55308613575214
          },
          "point_estimate": 59.67270998933745,
          "standard_error": 10.48532288172312
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171911.19426755767,
            "upper_bound": 172168.19508795204
          },
          "point_estimate": 172027.96251591045,
          "standard_error": 66.42091914041852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171854.6179245283,
            "upper_bound": 172178.47506738544
          },
          "point_estimate": 171987.95990566036,
          "standard_error": 80.9786655495963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.229485940697558,
            "upper_bound": 359.217122867903
          },
          "point_estimate": 200.48449402320009,
          "standard_error": 89.0537049110597
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171868.85216586787,
            "upper_bound": 172051.4681003742
          },
          "point_estimate": 171950.25651801028,
          "standard_error": 46.26163305958882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.78951874931016,
            "upper_bound": 303.13779677073467
          },
          "point_estimate": 220.85319563921465,
          "standard_error": 59.73698842835696
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34438.74848823052,
            "upper_bound": 34502.96402222936
          },
          "point_estimate": 34469.39691325456,
          "standard_error": 16.569061751695482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34421.75189393939,
            "upper_bound": 34531.101746632994
          },
          "point_estimate": 34443.36684027778,
          "standard_error": 28.324368071677476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.147874592545793,
            "upper_bound": 87.46286861767724
          },
          "point_estimate": 34.19419346489872,
          "standard_error": 24.118967706308094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34433.12679440692,
            "upper_bound": 34501.20702387973
          },
          "point_estimate": 34463.16822855175,
          "standard_error": 18.089164608307783
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.256698632984147,
            "upper_bound": 65.31208633919964
          },
          "point_estimate": 55.44370199882216,
          "standard_error": 9.35690574757326
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11281.786060103,
            "upper_bound": 11305.4502925207
          },
          "point_estimate": 11293.774962248095,
          "standard_error": 6.0689725956844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11278.318701580414,
            "upper_bound": 11313.445659391738
          },
          "point_estimate": 11289.979508831731,
          "standard_error": 7.821893891446594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6721747533935425,
            "upper_bound": 38.743973748166475
          },
          "point_estimate": 22.931213960207696,
          "standard_error": 10.649158592758395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11276.8606780952,
            "upper_bound": 11310.133467437858
          },
          "point_estimate": 11293.27009767425,
          "standard_error": 8.661273249322957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.685717154163864,
            "upper_bound": 25.34375691722648
          },
          "point_estimate": 20.24199501485342,
          "standard_error": 3.5625752809517905
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500460.3089275114,
            "upper_bound": 501478.24121656886
          },
          "point_estimate": 500929.7926320938,
          "standard_error": 262.25320840432204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500344.9915851272,
            "upper_bound": 501581.41735159815
          },
          "point_estimate": 500594.02910958906,
          "standard_error": 311.58406444048427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.81458075438106,
            "upper_bound": 1369.3469373331409
          },
          "point_estimate": 378.9685833404831,
          "standard_error": 323.1920442194522
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500403.4871823536,
            "upper_bound": 501029.4729629748
          },
          "point_estimate": 500701.6531933819,
          "standard_error": 157.93217109203317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.2464746705683,
            "upper_bound": 1079.943286474591
          },
          "point_estimate": 874.1976287343368,
          "standard_error": 202.39953826878065
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1010.1691272681852,
            "upper_bound": 1010.957265009946
          },
          "point_estimate": 1010.6107364656002,
          "standard_error": 0.2039077472103354
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1010.2633009273528,
            "upper_bound": 1011.078273984628
          },
          "point_estimate": 1010.860574902642,
          "standard_error": 0.2169961540482245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11014167484397898,
            "upper_bound": 0.9511601792304608
          },
          "point_estimate": 0.4107371960811894,
          "standard_error": 0.21878776718832685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1009.611993678465,
            "upper_bound": 1011.0492766474614
          },
          "point_estimate": 1010.3816211189142,
          "standard_error": 0.4061198382469397
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.234359817690408,
            "upper_bound": 0.958589346755888
          },
          "point_estimate": 0.6815296421599241,
          "standard_error": 0.20640547014848007
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41256.6921919536,
            "upper_bound": 41304.31448210006
          },
          "point_estimate": 41278.77553501613,
          "standard_error": 12.221096584879234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41241.84083743221,
            "upper_bound": 41304.84307604995
          },
          "point_estimate": 41270.81712069618,
          "standard_error": 16.63257560440571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6959914061982047,
            "upper_bound": 70.55744041024039
          },
          "point_estimate": 45.810534786396595,
          "standard_error": 15.459982749549516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41254.01883870514,
            "upper_bound": 41316.24409479693
          },
          "point_estimate": 41280.304612527085,
          "standard_error": 15.93322278339464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.297677953388025,
            "upper_bound": 54.90030374056105
          },
          "point_estimate": 40.840093105973345,
          "standard_error": 9.832502491490697
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48538.1584168765,
            "upper_bound": 48593.15120333184
          },
          "point_estimate": 48567.299249484706,
          "standard_error": 14.100210001495938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48535.458500669345,
            "upper_bound": 48599.31169120928
          },
          "point_estimate": 48582.28774356686,
          "standard_error": 15.872047822646422
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.42755590523744,
            "upper_bound": 75.6932074111974
          },
          "point_estimate": 25.24111018328863,
          "standard_error": 20.446959032409023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48538.79562815608,
            "upper_bound": 48604.29634848064
          },
          "point_estimate": 48577.262428067246,
          "standard_error": 16.437709831317793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.487852461032997,
            "upper_bound": 59.09101254664345
          },
          "point_estimate": 47.0686056268093,
          "standard_error": 9.924803448240109
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48097.830332993464,
            "upper_bound": 48174.84282717887
          },
          "point_estimate": 48134.34023772781,
          "standard_error": 19.675483895520888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48104.62861552028,
            "upper_bound": 48157.70987654321
          },
          "point_estimate": 48128.847236919464,
          "standard_error": 15.629680223727371
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.153015464066375,
            "upper_bound": 92.5909178006254
          },
          "point_estimate": 30.940081742368186,
          "standard_error": 20.85815086419942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48104.79855437649,
            "upper_bound": 48143.914183607165
          },
          "point_estimate": 48120.15046382189,
          "standard_error": 9.964586598807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.583149946753625,
            "upper_bound": 94.33232961488888
          },
          "point_estimate": 65.88133151614537,
          "standard_error": 19.59875242301631
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13992.77332135061,
            "upper_bound": 14013.640445129258
          },
          "point_estimate": 14002.787428889376,
          "standard_error": 5.355022994607412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13986.974730354392,
            "upper_bound": 14013.562692604006
          },
          "point_estimate": 14003.6572932717,
          "standard_error": 6.005451836661058
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7907610814581152,
            "upper_bound": 29.52053213191303
          },
          "point_estimate": 15.873565065517766,
          "standard_error": 6.831494182763196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13995.579991316858,
            "upper_bound": 14011.379302042416
          },
          "point_estimate": 14005.004211274088,
          "standard_error": 4.016368664696224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.74933343222655,
            "upper_bound": 23.98550072380855
          },
          "point_estimate": 17.884654958735844,
          "standard_error": 4.085321797451293
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25584.311478440457,
            "upper_bound": 25637.964948116056
          },
          "point_estimate": 25608.39891849687,
          "standard_error": 13.819420609592536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25578.2452045134,
            "upper_bound": 25638.268234938543
          },
          "point_estimate": 25594.60776915844,
          "standard_error": 12.430356503022388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.72665746897455,
            "upper_bound": 60.12617277599034
          },
          "point_estimate": 23.368854750844942,
          "standard_error": 14.085206414058376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25580.256911740096,
            "upper_bound": 25639.40012341326
          },
          "point_estimate": 25604.4076328467,
          "standard_error": 15.344564163331704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.35955113320814,
            "upper_bound": 60.314366093541906
          },
          "point_estimate": 46.1128402262688,
          "standard_error": 12.714070569702422
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17562.919734427815,
            "upper_bound": 17573.78324294107
          },
          "point_estimate": 17568.065925248902,
          "standard_error": 2.78408115165883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17560.563978754224,
            "upper_bound": 17578.296595847416
          },
          "point_estimate": 17564.09673265733,
          "standard_error": 4.6926488059975435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.016796795083385,
            "upper_bound": 14.630899486749028
          },
          "point_estimate": 7.018964264570087,
          "standard_error": 3.95597222963591
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17566.09381421924,
            "upper_bound": 17577.112646153324
          },
          "point_estimate": 17571.203096565434,
          "standard_error": 2.802570041402738
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.329035641276996,
            "upper_bound": 11.009214103594015
          },
          "point_estimate": 9.290496599226374,
          "standard_error": 1.564764277642562
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17323.889650691464,
            "upper_bound": 17372.970939261304
          },
          "point_estimate": 17345.442738662947,
          "standard_error": 12.715268996638263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17319.998807820695,
            "upper_bound": 17368.475798760133
          },
          "point_estimate": 17328.808351216023,
          "standard_error": 10.907407808844846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8049161576201062,
            "upper_bound": 56.46888884010806
          },
          "point_estimate": 22.40991927310308,
          "standard_error": 13.863350620876686
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17312.16994454775,
            "upper_bound": 17339.363402209805
          },
          "point_estimate": 17322.573288990458,
          "standard_error": 7.011361424654073
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.43270051045122,
            "upper_bound": 58.91839552907533
          },
          "point_estimate": 42.50539055269361,
          "standard_error": 13.017004978756775
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17570.68519013688,
            "upper_bound": 17589.700566150987
          },
          "point_estimate": 17580.17312113718,
          "standard_error": 4.864044776072941
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17565.415780998388,
            "upper_bound": 17595.544122383253
          },
          "point_estimate": 17578.30804347826,
          "standard_error": 7.029199921824098
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.022748137919751,
            "upper_bound": 27.89047834542856
          },
          "point_estimate": 19.502728253999027,
          "standard_error": 6.080380414470311
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17570.651492596095,
            "upper_bound": 17589.483015159087
          },
          "point_estimate": 17578.157184265012,
          "standard_error": 4.7994880845839605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.594795926339469,
            "upper_bound": 20.005000185243485
          },
          "point_estimate": 16.188934785360793,
          "standard_error": 2.658923521950195
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37378.02273029754,
            "upper_bound": 37436.89789269078
          },
          "point_estimate": 37406.01438683944,
          "standard_error": 15.133001112993613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37360.70936213992,
            "upper_bound": 37442.9196959305
          },
          "point_estimate": 37392.62131344307,
          "standard_error": 21.19509467701854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.260642101791614,
            "upper_bound": 82.03302301121369
          },
          "point_estimate": 49.84657709858624,
          "standard_error": 18.837529358391244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37371.375896633515,
            "upper_bound": 37429.684029772325
          },
          "point_estimate": 37404.677256693925,
          "standard_error": 14.92293636234811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.5480244048848,
            "upper_bound": 63.524202376912
          },
          "point_estimate": 50.37724360556347,
          "standard_error": 9.823215021484334
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17872.184742950743,
            "upper_bound": 17890.4723530771
          },
          "point_estimate": 17881.654257634254,
          "standard_error": 4.6746002761601835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17870.182467532468,
            "upper_bound": 17896.20515970516
          },
          "point_estimate": 17881.082612612612,
          "standard_error": 5.269127916825527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3764137593464032,
            "upper_bound": 27.081257504470926
          },
          "point_estimate": 14.856550281699374,
          "standard_error": 7.905824576172358
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17871.898796799298,
            "upper_bound": 17890.308457596522
          },
          "point_estimate": 17882.481688630778,
          "standard_error": 4.669380655880133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.093514773108193,
            "upper_bound": 19.979182073216865
          },
          "point_estimate": 15.55796003277552,
          "standard_error": 3.144886442498657
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18729.43308162214,
            "upper_bound": 18752.028280709503
          },
          "point_estimate": 18741.047649714183,
          "standard_error": 5.8234489843369674
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18722.57488776036,
            "upper_bound": 18758.04495105616
          },
          "point_estimate": 18747.62290056672,
          "standard_error": 11.078701920638563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.146419341358375,
            "upper_bound": 31.243483850261917
          },
          "point_estimate": 21.17067722332124,
          "standard_error": 7.669340517801388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18727.476779274308,
            "upper_bound": 18751.556251019996
          },
          "point_estimate": 18740.469654817105,
          "standard_error": 6.0749682500699835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.93070471745921,
            "upper_bound": 22.74997734302008
          },
          "point_estimate": 19.435263263077992,
          "standard_error": 2.704499082749397
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17705.669250730283,
            "upper_bound": 17734.845241619143
          },
          "point_estimate": 17719.237789755956,
          "standard_error": 7.5264300695761905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17701.82147030185,
            "upper_bound": 17738.920445588166
          },
          "point_estimate": 17710.702388293845,
          "standard_error": 9.33551213908955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.650902776696782,
            "upper_bound": 39.89233337161511
          },
          "point_estimate": 17.295263909110773,
          "standard_error": 9.886003372984678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17700.772090081446,
            "upper_bound": 17718.67045463313
          },
          "point_estimate": 17709.144377141845,
          "standard_error": 4.507857219680162
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.974162976052694,
            "upper_bound": 32.14605966947256
          },
          "point_estimate": 25.024557992629855,
          "standard_error": 5.69858295004042
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17577.87167144238,
            "upper_bound": 17596.58665381827
          },
          "point_estimate": 17586.802141878987,
          "standard_error": 4.79534016430505
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17574.42171338811,
            "upper_bound": 17599.988980183665
          },
          "point_estimate": 17583.07616976225,
          "standard_error": 6.147977690512202
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.052810143965241,
            "upper_bound": 25.55225003935009
          },
          "point_estimate": 14.798438353756303,
          "standard_error": 6.230184887855969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17574.759327613472,
            "upper_bound": 17589.406943310507
          },
          "point_estimate": 17580.744063572965,
          "standard_error": 3.7871806576769766
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.227309150205738,
            "upper_bound": 20.336904370781763
          },
          "point_estimate": 15.960730187340634,
          "standard_error": 3.1074988337521616
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17299.645195625002,
            "upper_bound": 17313.962733333334
          },
          "point_estimate": 17306.939258711263,
          "standard_error": 3.655658377785457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17297.91738095238,
            "upper_bound": 17316.34980952381
          },
          "point_estimate": 17307.798938492062,
          "standard_error": 4.502803216690338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4023366001001991,
            "upper_bound": 21.04956612629474
          },
          "point_estimate": 13.457893209885995,
          "standard_error": 5.119017046595087
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17302.467924379052,
            "upper_bound": 17314.008223328627
          },
          "point_estimate": 17309.94389115646,
          "standard_error": 2.958193219101889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.491276952193472,
            "upper_bound": 15.457154451501191
          },
          "point_estimate": 12.230878283730588,
          "standard_error": 2.242968376054036
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17548.119387385213,
            "upper_bound": 17578.12010703497
          },
          "point_estimate": 17563.021382367835,
          "standard_error": 7.672370202932607
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17544.36946189786,
            "upper_bound": 17587.515768487192
          },
          "point_estimate": 17560.13951989689,
          "standard_error": 9.485915393398331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6032239444679544,
            "upper_bound": 44.94250898461219
          },
          "point_estimate": 30.40283890562492,
          "standard_error": 12.29413368852732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17554.471467657593,
            "upper_bound": 17585.69860742126
          },
          "point_estimate": 17569.620909781373,
          "standard_error": 7.854899753326832
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.696627781747202,
            "upper_bound": 31.98730255029503
          },
          "point_estimate": 25.638976812993548,
          "standard_error": 4.463463928264306
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15330.824173388512,
            "upper_bound": 15354.371249520078
          },
          "point_estimate": 15341.769998577389,
          "standard_error": 6.036547751793387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15329.530198228596,
            "upper_bound": 15357.4072824406
          },
          "point_estimate": 15334.077551665963,
          "standard_error": 7.288152838972073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0451015927516747,
            "upper_bound": 31.66632373979315
          },
          "point_estimate": 7.663777009306384,
          "standard_error": 8.619009286454782
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15330.534558551662,
            "upper_bound": 15357.585068963112
          },
          "point_estimate": 15340.03221611792,
          "standard_error": 7.0942727981104765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.696678572925739,
            "upper_bound": 25.282493693414335
          },
          "point_estimate": 20.119460338743195,
          "standard_error": 4.286023536137891
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16326.537158753314,
            "upper_bound": 16346.0045431138
          },
          "point_estimate": 16336.091493509732,
          "standard_error": 4.9838723834298175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16325.027210502692,
            "upper_bound": 16347.597818885184
          },
          "point_estimate": 16335.887544883302,
          "standard_error": 5.731833401657476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7595280960327666,
            "upper_bound": 28.51770444583336
          },
          "point_estimate": 12.939610865426946,
          "standard_error": 6.134245083544999
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16331.2917630908,
            "upper_bound": 16345.222358236197
          },
          "point_estimate": 16337.8736750682,
          "standard_error": 3.525714440268343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.174366800267345,
            "upper_bound": 22.047012368271783
          },
          "point_estimate": 16.645133230682056,
          "standard_error": 3.5465232535468267
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17935.607883126406,
            "upper_bound": 17959.017865457205
          },
          "point_estimate": 17947.071888043167,
          "standard_error": 6.005766333969173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17929.0527873705,
            "upper_bound": 17967.77257030094
          },
          "point_estimate": 17946.09139121855,
          "standard_error": 9.36348662526382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.252002521463108,
            "upper_bound": 32.55056131160349
          },
          "point_estimate": 26.84078496173005,
          "standard_error": 7.576708640653526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17929.66816502161,
            "upper_bound": 17960.18064590189
          },
          "point_estimate": 17943.94038403629,
          "standard_error": 7.70347359521119
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.748272251374145,
            "upper_bound": 24.390213201927192
          },
          "point_estimate": 20.045782211087616,
          "standard_error": 3.1788970195270085
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18376.984054038872,
            "upper_bound": 18399.704231661373
          },
          "point_estimate": 18386.953668488295,
          "standard_error": 5.895572025603723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18374.45020222447,
            "upper_bound": 18398.72121009678
          },
          "point_estimate": 18379.988540613413,
          "standard_error": 4.8674001158579365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6896409541613413,
            "upper_bound": 24.77817692359946
          },
          "point_estimate": 7.687770566244609,
          "standard_error": 5.432550997225152
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18376.628391407237,
            "upper_bound": 18403.39339423781
          },
          "point_estimate": 18387.214879256233,
          "standard_error": 6.924656207156394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.359394031472438,
            "upper_bound": 26.484099783129913
          },
          "point_estimate": 19.614844345202783,
          "standard_error": 5.986541893517999
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68236.89830197259,
            "upper_bound": 68323.64516011122
          },
          "point_estimate": 68279.7102960928,
          "standard_error": 22.229222988658066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68214.98540754638,
            "upper_bound": 68336.79956781023
          },
          "point_estimate": 68272.49959349593,
          "standard_error": 31.70645821747575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.340786924786123,
            "upper_bound": 126.28020936690022
          },
          "point_estimate": 82.87369152432129,
          "standard_error": 28.71391592188634
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68252.96952073937,
            "upper_bound": 68338.52015698161
          },
          "point_estimate": 68293.09817499574,
          "standard_error": 21.724637324379117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.18432795955724,
            "upper_bound": 93.15088941250278
          },
          "point_estimate": 73.98402096490334,
          "standard_error": 12.975093031536757
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226352.5939365078,
            "upper_bound": 1228707.670023942
          },
          "point_estimate": 1227553.6283994708,
          "standard_error": 603.6274913116911
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225539.9,
            "upper_bound": 1229540.0111111111
          },
          "point_estimate": 1227561.1875925926,
          "standard_error": 997.9725786903168
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.0630034123789,
            "upper_bound": 3243.1616342003294
          },
          "point_estimate": 2951.6784394862552,
          "standard_error": 846.8866930602147
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226860.8605009634,
            "upper_bound": 1228638.876484787
          },
          "point_estimate": 1227721.301038961,
          "standard_error": 444.2510215458909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1203.3213503278696,
            "upper_bound": 2402.6953742721157
          },
          "point_estimate": 2004.3974575641637,
          "standard_error": 299.8589814228559
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207718.392810805,
            "upper_bound": 208165.7746190476
          },
          "point_estimate": 207906.8773473923,
          "standard_error": 116.8631511340938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207597.23936507935,
            "upper_bound": 207952.628
          },
          "point_estimate": 207863.1695238095,
          "standard_error": 85.28975794102587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.19069908176152,
            "upper_bound": 450.53365666809685
          },
          "point_estimate": 161.8218335271003,
          "standard_error": 110.0127459522164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207677.1086894241,
            "upper_bound": 208110.76884418735
          },
          "point_estimate": 207857.24946938775,
          "standard_error": 109.71488773832088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.15435125057893,
            "upper_bound": 581.2471589886131
          },
          "point_estimate": 392.0454314793569,
          "standard_error": 147.025416507536
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5544.374373318814,
            "upper_bound": 5560.192322329207
          },
          "point_estimate": 5552.373358041673,
          "standard_error": 4.038518029191229
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5543.577861837078,
            "upper_bound": 5560.208670844159
          },
          "point_estimate": 5552.120841611901,
          "standard_error": 3.411239926864697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7489150812840542,
            "upper_bound": 23.716444581240516
          },
          "point_estimate": 7.861465778206153,
          "standard_error": 6.1237253182456755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5538.084201783726,
            "upper_bound": 5556.023419935714
          },
          "point_estimate": 5548.280993864762,
          "standard_error": 4.633591071279194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.7831610685768915,
            "upper_bound": 18.215739091409542
          },
          "point_estimate": 13.534231430879888,
          "standard_error": 3.1103470053533755
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5900.755282074244,
            "upper_bound": 5915.292117375551
          },
          "point_estimate": 5908.221076009314,
          "standard_error": 3.706953676611101
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5903.070131878867,
            "upper_bound": 5915.291957017258
          },
          "point_estimate": 5908.623526764717,
          "standard_error": 3.1130812013994458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3053651117091167,
            "upper_bound": 20.334006574851113
          },
          "point_estimate": 8.935573556764156,
          "standard_error": 4.156720919699984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5907.786886568966,
            "upper_bound": 5919.241761508069
          },
          "point_estimate": 5913.046196720896,
          "standard_error": 2.91211967350108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.424428806335625,
            "upper_bound": 17.109456487388236
          },
          "point_estimate": 12.303189466712398,
          "standard_error": 3.317230356716684
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14082.704437398404,
            "upper_bound": 14102.112691723005
          },
          "point_estimate": 14091.55494437056,
          "standard_error": 5.008475439516154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14078.60826114128,
            "upper_bound": 14100.182525213342
          },
          "point_estimate": 14087.897027042003,
          "standard_error": 6.614613949527725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9052880266862104,
            "upper_bound": 26.474365097611788
          },
          "point_estimate": 14.65019122555534,
          "standard_error": 5.956990294420517
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14079.937639238558,
            "upper_bound": 14093.417570071144
          },
          "point_estimate": 14085.23719383797,
          "standard_error": 3.4178480624420224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.827386026017777,
            "upper_bound": 22.866143871781127
          },
          "point_estimate": 16.656001798802954,
          "standard_error": 4.361703687389227
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.98794897425793,
            "upper_bound": 36.035440937196576
          },
          "point_estimate": 36.0110076032544,
          "standard_error": 0.012145396223857413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.97835026024836,
            "upper_bound": 36.046588329338114
          },
          "point_estimate": 35.999437066158606,
          "standard_error": 0.017033044761662886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007075915331156078,
            "upper_bound": 0.06878218473944496
          },
          "point_estimate": 0.03922376390221722,
          "standard_error": 0.016844624059549573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.98394647445292,
            "upper_bound": 36.050758294122105
          },
          "point_estimate": 36.01344623249057,
          "standard_error": 0.017048092321400542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02138448174641739,
            "upper_bound": 0.04916593126940517
          },
          "point_estimate": 0.04043237599850184,
          "standard_error": 0.0068323939516301635
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23174799.389166668,
            "upper_bound": 23216975.43344742
          },
          "point_estimate": 23193691.150753967,
          "standard_error": 10897.470416004478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23167521.61904762,
            "upper_bound": 23212698.222222224
          },
          "point_estimate": 23180378.7,
          "standard_error": 11610.760432394243
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 515.1876058517769,
            "upper_bound": 50996.085261303946
          },
          "point_estimate": 25749.302142857585,
          "standard_error": 12596.009015407924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23170273.770760972,
            "upper_bound": 23215509.81462581
          },
          "point_estimate": 23189497.231168833,
          "standard_error": 11876.354727822722
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13189.09519460804,
            "upper_bound": 49807.1911742377
          },
          "point_estimate": 36344.58441856036,
          "standard_error": 10119.316782918631
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81062157.09083334,
            "upper_bound": 81137890.23333332
          },
          "point_estimate": 81102035.96666665,
          "standard_error": 19401.262600433107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81064552.0,
            "upper_bound": 81149646.5
          },
          "point_estimate": 81117974.83333334,
          "standard_error": 21496.094028186908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6101.88729167721,
            "upper_bound": 103593.46086086315
          },
          "point_estimate": 67395.7825035004,
          "standard_error": 26594.503502499352
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30667.876809741352,
            "upper_bound": 86998.71971057092
          },
          "point_estimate": 64887.30368677091,
          "standard_error": 15109.859865026478
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.050892304926752,
            "upper_bound": 9.065380601372937
          },
          "point_estimate": 9.057835695498586,
          "standard_error": 0.0037114060197855968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04802914523213,
            "upper_bound": 9.065795484895474
          },
          "point_estimate": 9.057601518822183,
          "standard_error": 0.0053489819063232144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003631666927467579,
            "upper_bound": 0.02133662756262327
          },
          "point_estimate": 0.01401041748628152,
          "standard_error": 0.004897625060456869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047761864429043,
            "upper_bound": 9.063433244272384
          },
          "point_estimate": 9.056869025449776,
          "standard_error": 0.004051823203856695
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006643139890225788,
            "upper_bound": 0.016101470273346314
          },
          "point_estimate": 0.012348363060823203,
          "standard_error": 0.002523398443765007
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.052866611642632,
            "upper_bound": 9.071771136983324
          },
          "point_estimate": 9.062054205665694,
          "standard_error": 0.004855103004944316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.048133735074943,
            "upper_bound": 9.07698650171742
          },
          "point_estimate": 9.058537038239509,
          "standard_error": 0.007924026256155908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004392897417444905,
            "upper_bound": 0.026031405461821935
          },
          "point_estimate": 0.01734664356232894,
          "standard_error": 0.005917006856492669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.046619739304244,
            "upper_bound": 9.065196225397916
          },
          "point_estimate": 9.054312118623118,
          "standard_error": 0.004785300499570572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009513089457246052,
            "upper_bound": 0.019331108089558523
          },
          "point_estimate": 0.01619098447827724,
          "standard_error": 0.002464750330543548
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.041807688379045,
            "upper_bound": 9.054071786019138
          },
          "point_estimate": 9.047700802617658,
          "standard_error": 0.003144031860580004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0391186567197,
            "upper_bound": 9.056563935167311
          },
          "point_estimate": 9.045159143088506,
          "standard_error": 0.00423980289543224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002057541203313437,
            "upper_bound": 0.017009868433998456
          },
          "point_estimate": 0.012103816749839754,
          "standard_error": 0.004205663362664431
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04127236757534,
            "upper_bound": 9.05209169787205
          },
          "point_estimate": 9.046339697988731,
          "standard_error": 0.0027596617338436083
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005705013825260918,
            "upper_bound": 0.013311369928411547
          },
          "point_estimate": 0.010479138709706489,
          "standard_error": 0.0019834237318323467
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.79549462836369,
            "upper_bound": 8.810721550749497
          },
          "point_estimate": 8.803860528531349,
          "standard_error": 0.0038998632501926456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.79957217374631,
            "upper_bound": 8.8133506710539
          },
          "point_estimate": 8.804434499583303,
          "standard_error": 0.003417787561613682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018848605779543243,
            "upper_bound": 0.018752243344091167
          },
          "point_estimate": 0.00802854212176419,
          "standard_error": 0.004263799690217804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.802720026271171,
            "upper_bound": 8.809299267962551
          },
          "point_estimate": 8.806272283807358,
          "standard_error": 0.001656936713006138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005044027314198442,
            "upper_bound": 0.018455866774676616
          },
          "point_estimate": 0.013025936323641478,
          "standard_error": 0.003904264607602507
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.531022712147411,
            "upper_bound": 9.545682595188772
          },
          "point_estimate": 9.537984039198394,
          "standard_error": 0.0037508463386460894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.530038470481443,
            "upper_bound": 9.546975288774922
          },
          "point_estimate": 9.534964196244776,
          "standard_error": 0.003840285379304798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008043710521334859,
            "upper_bound": 0.02170470960490167
          },
          "point_estimate": 0.00737822169465314,
          "standard_error": 0.00600307893909879
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.531116723232673,
            "upper_bound": 9.543051094194263
          },
          "point_estimate": 9.536480844363556,
          "standard_error": 0.0030572770268413184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005221579014892492,
            "upper_bound": 0.01596173973065985
          },
          "point_estimate": 0.012494921126304195,
          "standard_error": 0.0026255193138151047
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.462881688253589,
            "upper_bound": 10.477298399547184
          },
          "point_estimate": 10.469906568098706,
          "standard_error": 0.003684410345292373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.459137360938229,
            "upper_bound": 10.480233902083643
          },
          "point_estimate": 10.468354225888188,
          "standard_error": 0.0057770389878604314
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034839838091286143,
            "upper_bound": 0.01948602499117239
          },
          "point_estimate": 0.014725695137485038,
          "standard_error": 0.004459528660427095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.458713168823298,
            "upper_bound": 10.483181620504617
          },
          "point_estimate": 10.471734101154256,
          "standard_error": 0.006480956259402663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007369762046110506,
            "upper_bound": 0.015270765263611694
          },
          "point_estimate": 0.01231690249495885,
          "standard_error": 0.0020298206927818275
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.068535881470254,
            "upper_bound": 8.077515334810919
          },
          "point_estimate": 8.073654666232672,
          "standard_error": 0.0023398656227178005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.070287873393342,
            "upper_bound": 8.078497804416578
          },
          "point_estimate": 8.076532573535903,
          "standard_error": 0.002075275225591014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008035165058226988,
            "upper_bound": 0.009739483013238018
          },
          "point_estimate": 0.004125377756101262,
          "standard_error": 0.0022394472833421443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.070652558844845,
            "upper_bound": 8.076407592246886
          },
          "point_estimate": 8.072906009323379,
          "standard_error": 0.0014697369674183183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023051551185580927,
            "upper_bound": 0.011307435939005689
          },
          "point_estimate": 0.007823480351099548,
          "standard_error": 0.0026530326283056792
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.538709787912715,
            "upper_bound": 9.557213382511678
          },
          "point_estimate": 9.547445700948307,
          "standard_error": 0.004732344149896708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.535818382735991,
            "upper_bound": 9.55747628103548
          },
          "point_estimate": 9.542313043793351,
          "standard_error": 0.0059691384539049186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011503361546740909,
            "upper_bound": 0.026205090832183805
          },
          "point_estimate": 0.01831352593055843,
          "standard_error": 0.006841699689415061
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.541828816660594,
            "upper_bound": 9.556289482199489
          },
          "point_estimate": 9.548574553967168,
          "standard_error": 0.003728507488568712
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007750658547742108,
            "upper_bound": 0.02102749519833516
          },
          "point_estimate": 0.01578389511033071,
          "standard_error": 0.003628403965631887
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.54689598757789,
            "upper_bound": 12.571382138376778
          },
          "point_estimate": 12.558081110935758,
          "standard_error": 0.006317873083836235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.542878402520838,
            "upper_bound": 12.570772756698348
          },
          "point_estimate": 12.553723373589945,
          "standard_error": 0.007283007274318044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018588081459596987,
            "upper_bound": 0.0316842784543404
          },
          "point_estimate": 0.016207100250770833,
          "standard_error": 0.0071981998448281265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.542066210875022,
            "upper_bound": 12.558457152143731
          },
          "point_estimate": 12.548678451337134,
          "standard_error": 0.004145875893107365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009028737164960804,
            "upper_bound": 0.029058714729049903
          },
          "point_estimate": 0.021158698513681084,
          "standard_error": 0.005708763024391613
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.054257700742244,
            "upper_bound": 9.066160669406642
          },
          "point_estimate": 9.059865125973806,
          "standard_error": 0.0030540271598109843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.05039113948814,
            "upper_bound": 9.065665744994664
          },
          "point_estimate": 9.06026819035372,
          "standard_error": 0.004357551188360063
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009154730332512344,
            "upper_bound": 0.01820077331439381
          },
          "point_estimate": 0.011541857232909764,
          "standard_error": 0.004317512940043155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.052743229723308,
            "upper_bound": 9.065440602862964
          },
          "point_estimate": 9.060372480246205,
          "standard_error": 0.003235430129526442
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00539823038140341,
            "upper_bound": 0.013486943419976223
          },
          "point_estimate": 0.01017383675204868,
          "standard_error": 0.0022915564766686613
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.21035942636733,
            "upper_bound": 10.24258614813634
          },
          "point_estimate": 10.225756224200428,
          "standard_error": 0.008264466173978583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.206757946438476,
            "upper_bound": 10.249558012769745
          },
          "point_estimate": 10.216317054908314,
          "standard_error": 0.010767986892004384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003452379583515659,
            "upper_bound": 0.04469325370898969
          },
          "point_estimate": 0.01798783066493842,
          "standard_error": 0.01137420718375675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.215248044407664,
            "upper_bound": 10.237224548465363
          },
          "point_estimate": 10.22460131269807,
          "standard_error": 0.005562685560408074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012680674511442338,
            "upper_bound": 0.034336884017679925
          },
          "point_estimate": 0.027573903896286237,
          "standard_error": 0.005278157205262093
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.481814985360824,
            "upper_bound": 19.52350191473027
          },
          "point_estimate": 19.501852423676247,
          "standard_error": 0.01066640313383538
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.472710024543204,
            "upper_bound": 19.531315157708057
          },
          "point_estimate": 19.49479052343981,
          "standard_error": 0.01511472372031312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008176382353571392,
            "upper_bound": 0.0580074592630291
          },
          "point_estimate": 0.035067082987314,
          "standard_error": 0.013536843841039314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.47207334950907,
            "upper_bound": 19.533561945222058
          },
          "point_estimate": 19.49776382268274,
          "standard_error": 0.01671728333238983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017439629004905866,
            "upper_bound": 0.04342142103350714
          },
          "point_estimate": 0.035492910185970006,
          "standard_error": 0.006302974949724299
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99075.23058128975,
            "upper_bound": 99270.65717421392
          },
          "point_estimate": 99155.51987078848,
          "standard_error": 52.07072375047474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99057.92098092644,
            "upper_bound": 99213.77353920678
          },
          "point_estimate": 99089.25483651226,
          "standard_error": 38.489325772940994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.393901991176602,
            "upper_bound": 180.2458349789248
          },
          "point_estimate": 55.0586257663809,
          "standard_error": 42.081391621065805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99070.00266965096,
            "upper_bound": 99184.32470606726
          },
          "point_estimate": 99125.98603630702,
          "standard_error": 29.383329670079984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.80363433365126,
            "upper_bound": 258.16089467740824
          },
          "point_estimate": 174.05536721170984,
          "standard_error": 68.33024337925653
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52363.67436319245,
            "upper_bound": 52484.93752258445
          },
          "point_estimate": 52419.576292427155,
          "standard_error": 31.242359197438255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52335.10714056996,
            "upper_bound": 52480.0288184438
          },
          "point_estimate": 52399.26792146974,
          "standard_error": 38.1798955707504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.5527483092935,
            "upper_bound": 162.16954630318858
          },
          "point_estimate": 103.67995072535366,
          "standard_error": 40.41512082188578
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52336.95707791531,
            "upper_bound": 52398.07430582822
          },
          "point_estimate": 52358.396148807966,
          "standard_error": 15.649815618072536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.60305572288411,
            "upper_bound": 137.3105815133142
          },
          "point_estimate": 104.04158296384112,
          "standard_error": 24.991225405969093
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143519.06336545578,
            "upper_bound": 143740.1536580636
          },
          "point_estimate": 143632.30132207164,
          "standard_error": 56.609319832260134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143467.63010540183,
            "upper_bound": 143796.4540513834
          },
          "point_estimate": 143642.44212309428,
          "standard_error": 77.76906577453315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.22112155095832,
            "upper_bound": 323.4892175111934
          },
          "point_estimate": 188.58342035750175,
          "standard_error": 74.3757386132952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143539.21956585697,
            "upper_bound": 143757.2523375877
          },
          "point_estimate": 143680.14084492583,
          "standard_error": 55.69161810452217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.98277022912502,
            "upper_bound": 234.9123289810634
          },
          "point_estimate": 188.04336757525255,
          "standard_error": 33.28758920020407
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397536.7132161836,
            "upper_bound": 398040.2121980676
          },
          "point_estimate": 397772.9765791063,
          "standard_error": 129.6099489371419
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397459.8695652174,
            "upper_bound": 398093.5833333334
          },
          "point_estimate": 397596.269701087,
          "standard_error": 170.45443562296384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.65783218548968,
            "upper_bound": 705.4620941331405
          },
          "point_estimate": 305.4854271852514,
          "standard_error": 173.64217108428704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397540.0930347169,
            "upper_bound": 397969.6502816406
          },
          "point_estimate": 397746.1659796725,
          "standard_error": 111.97456308386718
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.7722522324726,
            "upper_bound": 550.27431779257
          },
          "point_estimate": 433.0441389815592,
          "standard_error": 91.67542497568772
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573252.1918012153,
            "upper_bound": 574155.3408574219
          },
          "point_estimate": 573684.3414961558,
          "standard_error": 230.56601420542157
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573265.3482142857,
            "upper_bound": 574068.63125
          },
          "point_estimate": 573631.5080295139,
          "standard_error": 194.2715060210681
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146.63550794357434,
            "upper_bound": 1241.0508478106758
          },
          "point_estimate": 530.4337307391515,
          "standard_error": 261.43022868812955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573425.9093055512,
            "upper_bound": 573989.4106700241
          },
          "point_estimate": 573742.576663961,
          "standard_error": 143.6605528416179
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 281.9627510632356,
            "upper_bound": 1080.7535091268626
          },
          "point_estimate": 767.7666076692753,
          "standard_error": 206.28049248272868
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39929.35587554629,
            "upper_bound": 40031.36202704356
          },
          "point_estimate": 39983.59580011886,
          "standard_error": 26.09473650808883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39940.53636109363,
            "upper_bound": 40036.90560756241
          },
          "point_estimate": 40000.95126651983,
          "standard_error": 27.37092097796826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.251370178974009,
            "upper_bound": 136.26973543008256
          },
          "point_estimate": 75.75399303835897,
          "standard_error": 31.16009028612247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39949.1294785525,
            "upper_bound": 40009.31758341789
          },
          "point_estimate": 39976.268582298755,
          "standard_error": 15.481299508128766
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.43508028493231,
            "upper_bound": 120.94652496320842
          },
          "point_estimate": 87.059903932085,
          "standard_error": 22.80061524183389
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98435.46598668756,
            "upper_bound": 98583.6723904038
          },
          "point_estimate": 98510.33114575288,
          "standard_error": 37.89997333014282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98397.81274131274,
            "upper_bound": 98617.3172972973
          },
          "point_estimate": 98545.22882882884,
          "standard_error": 63.513959032936555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.112248060266577,
            "upper_bound": 202.23905819332367
          },
          "point_estimate": 181.2574636468972,
          "standard_error": 51.865920276269826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98420.5732770126,
            "upper_bound": 98570.71190209076
          },
          "point_estimate": 98489.97673569672,
          "standard_error": 38.74537322869959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.43456093176897,
            "upper_bound": 153.40950543032145
          },
          "point_estimate": 126.44124269955078,
          "standard_error": 19.435788068643134
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67906.7413221364,
            "upper_bound": 67999.03479672545
          },
          "point_estimate": 67952.51327663337,
          "standard_error": 23.770276382158222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67882.56760299625,
            "upper_bound": 68017.48131242197
          },
          "point_estimate": 67958.4900124844,
          "standard_error": 38.69474609708616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.52119435186423,
            "upper_bound": 134.26694981352944
          },
          "point_estimate": 99.11560265982273,
          "standard_error": 30.23003095947045
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67913.60016150147,
            "upper_bound": 68030.11294095765
          },
          "point_estimate": 67968.83407753296,
          "standard_error": 30.624009849648676
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.46862687986242,
            "upper_bound": 96.69422238370372
          },
          "point_estimate": 79.39757887528543,
          "standard_error": 12.25951516094753
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307950.41374816594,
            "upper_bound": 308499.1341619981
          },
          "point_estimate": 308200.76066459913,
          "standard_error": 140.49087672938612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307883.5708883554,
            "upper_bound": 308377.8081232493
          },
          "point_estimate": 308172.75017507,
          "standard_error": 116.8769578079512
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.26432823535065,
            "upper_bound": 669.2754304149197
          },
          "point_estimate": 273.7315437186814,
          "standard_error": 151.5693915816839
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307951.6204135471,
            "upper_bound": 308184.92443061527
          },
          "point_estimate": 308051.4214558551,
          "standard_error": 58.692572970161166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.92984461509025,
            "upper_bound": 658.8257674206283
          },
          "point_estimate": 468.34039588360446,
          "standard_error": 134.182433747473
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35261.170909932596,
            "upper_bound": 35333.2252282547
          },
          "point_estimate": 35297.60306678865,
          "standard_error": 18.53380837719919
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35232.57307879377,
            "upper_bound": 35354.431663424126
          },
          "point_estimate": 35309.424794638995,
          "standard_error": 38.381014681031985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.499469558499618,
            "upper_bound": 94.398087380525
          },
          "point_estimate": 76.23371382167933,
          "standard_error": 25.953871748021548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35246.3772337194,
            "upper_bound": 35342.294354396865
          },
          "point_estimate": 35295.5028627015,
          "standard_error": 25.174857857032745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.30659273411548,
            "upper_bound": 69.78697949387494
          },
          "point_estimate": 61.84687866538796,
          "standard_error": 7.291108766518484
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65079.542255845314,
            "upper_bound": 65222.76195218824
          },
          "point_estimate": 65149.89005995203,
          "standard_error": 36.687723889895366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65056.663039568346,
            "upper_bound": 65246.361510791365
          },
          "point_estimate": 65143.573441247005,
          "standard_error": 52.71258393787378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.37098881355518,
            "upper_bound": 210.7545194598047
          },
          "point_estimate": 132.66247678146172,
          "standard_error": 44.68616575137893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65075.53044383601,
            "upper_bound": 65248.64011313238
          },
          "point_estimate": 65145.30580211156,
          "standard_error": 44.41600005014215
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.70721459836419,
            "upper_bound": 156.9157372344628
          },
          "point_estimate": 122.23266305039952,
          "standard_error": 22.895030293126837
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169998.14959671692,
            "upper_bound": 170224.79209056517
          },
          "point_estimate": 170104.1262373906,
          "standard_error": 58.31017726724379
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169959.3773364486,
            "upper_bound": 170256.05941255006
          },
          "point_estimate": 170039.95449766354,
          "standard_error": 78.13017984341748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.440137710342093,
            "upper_bound": 314.4316103055392
          },
          "point_estimate": 132.13613569536017,
          "standard_error": 74.31874828685106
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169992.71695334377,
            "upper_bound": 170167.00111990323
          },
          "point_estimate": 170064.78101711374,
          "standard_error": 43.94931438334329
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.2266960403955,
            "upper_bound": 237.52052448596683
          },
          "point_estimate": 193.875426619685,
          "standard_error": 40.32869797728214
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33885.49791514809,
            "upper_bound": 33915.7082227432
          },
          "point_estimate": 33900.232661427384,
          "standard_error": 7.735154395606468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33879.61057462091,
            "upper_bound": 33921.767225325886
          },
          "point_estimate": 33895.190321746326,
          "standard_error": 9.048301357812107
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.714173347218901,
            "upper_bound": 44.84489132053971
          },
          "point_estimate": 25.243801123065246,
          "standard_error": 12.55924322128598
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33883.96023002807,
            "upper_bound": 33906.79691661553
          },
          "point_estimate": 33894.41740308109,
          "standard_error": 5.664442904203029
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.61324512221939,
            "upper_bound": 32.71156401477703
          },
          "point_estimate": 25.75368621639575,
          "standard_error": 4.8779752441458495
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11230.26711534686,
            "upper_bound": 11252.24517010177
          },
          "point_estimate": 11241.2385766435,
          "standard_error": 5.638375579093981
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11223.637329931971,
            "upper_bound": 11252.979359925788
          },
          "point_estimate": 11244.18439325225,
          "standard_error": 9.137966128445957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5364339190490702,
            "upper_bound": 31.11794164606378
          },
          "point_estimate": 14.139003243106837,
          "standard_error": 8.87364090485669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11230.409859453266,
            "upper_bound": 11249.69924681722
          },
          "point_estimate": 11240.728012432835,
          "standard_error": 4.9562624612170305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.89066688114766,
            "upper_bound": 23.9597304221145
          },
          "point_estimate": 18.796699027971343,
          "standard_error": 3.376383973459729
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500391.28789954336,
            "upper_bound": 501191.1599105784
          },
          "point_estimate": 500805.069414003,
          "standard_error": 204.59693018208273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500404.427739726,
            "upper_bound": 501392.2602739726
          },
          "point_estimate": 500805.79375951295,
          "standard_error": 220.2098406346963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.2825567839466,
            "upper_bound": 1136.6695704592985
          },
          "point_estimate": 624.482617680347,
          "standard_error": 269.3929361896618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500654.66339012154,
            "upper_bound": 501106.6409710995
          },
          "point_estimate": 500892.59270592424,
          "standard_error": 113.37788286934476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.1372691104051,
            "upper_bound": 910.7087193363292
          },
          "point_estimate": 681.7832194433654,
          "standard_error": 152.6897387324964
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 973.079594297834,
            "upper_bound": 974.924200054085
          },
          "point_estimate": 973.9458134276764,
          "standard_error": 0.47455754808760664
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 972.7036381461515,
            "upper_bound": 974.762312155264
          },
          "point_estimate": 973.806066122241,
          "standard_error": 0.5080591942476248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.385913075255056,
            "upper_bound": 2.5735086474213547
          },
          "point_estimate": 1.341640914041448,
          "standard_error": 0.5508841478450616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 973.103124316732,
            "upper_bound": 974.3948413329172
          },
          "point_estimate": 973.7014587097136,
          "standard_error": 0.3241320060596657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7407527841789594,
            "upper_bound": 2.1433292789580154
          },
          "point_estimate": 1.584215835071003,
          "standard_error": 0.3867425870006413
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40965.53230292794,
            "upper_bound": 41022.34325107586
          },
          "point_estimate": 40991.91512722544,
          "standard_error": 14.52255572133354
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40955.15259009009,
            "upper_bound": 41013.81152670528
          },
          "point_estimate": 40990.157995495494,
          "standard_error": 17.786632049387126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.922903924430362,
            "upper_bound": 81.09039233569055
          },
          "point_estimate": 36.49574809362638,
          "standard_error": 18.7684457047562
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40962.66608356207,
            "upper_bound": 41007.91367980885
          },
          "point_estimate": 40988.37976775477,
          "standard_error": 11.625073021319231
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.17175463081683,
            "upper_bound": 66.1687756956789
          },
          "point_estimate": 48.36285850958482,
          "standard_error": 12.2392237850464
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49333.87420936314,
            "upper_bound": 49506.056450785014
          },
          "point_estimate": 49404.93573865597,
          "standard_error": 45.37705932112576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49315.182631340576,
            "upper_bound": 49416.70754076087
          },
          "point_estimate": 49388.79932280883,
          "standard_error": 28.19515896730619
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.405997728120065,
            "upper_bound": 153.2346114099836
          },
          "point_estimate": 47.93387394519654,
          "standard_error": 36.187521189400655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49326.01708315352,
            "upper_bound": 49397.6559045689
          },
          "point_estimate": 49361.95650056465,
          "standard_error": 18.31906244400226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.562260103646146,
            "upper_bound": 227.12489955433617
          },
          "point_estimate": 151.70814901058378,
          "standard_error": 60.147943829234485
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48835.61352225209,
            "upper_bound": 48875.74305739035
          },
          "point_estimate": 48855.40895315967,
          "standard_error": 10.294494164416449
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48828.77096774193,
            "upper_bound": 48887.83758960573
          },
          "point_estimate": 48849.30864508662,
          "standard_error": 16.276812940105998
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.022694660772276,
            "upper_bound": 55.615613985738754
          },
          "point_estimate": 37.629443485172544,
          "standard_error": 12.41323795620383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48842.12208728653,
            "upper_bound": 48877.42254428873
          },
          "point_estimate": 48855.76586719732,
          "standard_error": 9.027420666287911
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.96530745866316,
            "upper_bound": 41.72183418579146
          },
          "point_estimate": 34.31778128380333,
          "standard_error": 5.300909650535356
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14057.604971666133,
            "upper_bound": 14069.536332151303
          },
          "point_estimate": 14063.48644820546,
          "standard_error": 3.0716097756353102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14054.607761121855,
            "upper_bound": 14072.27257253385
          },
          "point_estimate": 14062.131517300666,
          "standard_error": 4.990978062558354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.351665296397363,
            "upper_bound": 16.996620254344084
          },
          "point_estimate": 13.05400599841922,
          "standard_error": 3.604371190567274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14054.602201508756,
            "upper_bound": 14068.115654284727
          },
          "point_estimate": 14060.396121480067,
          "standard_error": 3.488819516223343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.320372395220552,
            "upper_bound": 12.390639782616082
          },
          "point_estimate": 10.252825257833129,
          "standard_error": 1.551002790114253
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25764.994727777776,
            "upper_bound": 25827.985141562534
          },
          "point_estimate": 25794.333704210287,
          "standard_error": 16.174609730168434
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25755.39815602837,
            "upper_bound": 25821.333303782507
          },
          "point_estimate": 25787.709720815037,
          "standard_error": 19.492643914154492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.40990646405722,
            "upper_bound": 85.69442144153454
          },
          "point_estimate": 47.14357726941885,
          "standard_error": 17.95463805447257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25754.359048805774,
            "upper_bound": 25806.492033833605
          },
          "point_estimate": 25784.102752141476,
          "standard_error": 13.418947479838245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.4377446842485,
            "upper_bound": 73.35286568557271
          },
          "point_estimate": 53.85277803019815,
          "standard_error": 13.503005795250298
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17368.66522933477,
            "upper_bound": 17396.83462694814
          },
          "point_estimate": 17382.2008297155,
          "standard_error": 7.219293123108493
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17365.206722354815,
            "upper_bound": 17399.53627684964
          },
          "point_estimate": 17378.574362238134,
          "standard_error": 6.672220181342774
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.535708196830252,
            "upper_bound": 45.22207096325687
          },
          "point_estimate": 12.338072630598708,
          "standard_error": 11.63266561420225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17369.345727431537,
            "upper_bound": 17380.70147819426
          },
          "point_estimate": 17375.90937358584,
          "standard_error": 2.861396645394029
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.298046079679889,
            "upper_bound": 31.21304785462521
          },
          "point_estimate": 24.091272204703163,
          "standard_error": 5.122249939682866
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17354.6585500093,
            "upper_bound": 17380.055891469394
          },
          "point_estimate": 17366.66951445348,
          "standard_error": 6.497085603259465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17352.49864498645,
            "upper_bound": 17382.71568627451
          },
          "point_estimate": 17359.803843854614,
          "standard_error": 8.592596055066391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8407439556231484,
            "upper_bound": 37.09862550349362
          },
          "point_estimate": 17.236888965463105,
          "standard_error": 9.261919901831083
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17352.6444880308,
            "upper_bound": 17366.93449185859
          },
          "point_estimate": 17359.076643872628,
          "standard_error": 3.576866890994826
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.85544585021225,
            "upper_bound": 28.700477672089917
          },
          "point_estimate": 21.68848795035784,
          "standard_error": 4.767024925609948
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17592.05348479092,
            "upper_bound": 17619.60777127228
          },
          "point_estimate": 17605.025673701297,
          "standard_error": 7.033707836843165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17586.90773089942,
            "upper_bound": 17618.61823017408
          },
          "point_estimate": 17601.35594547297,
          "standard_error": 8.318268110531925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.017515965081669,
            "upper_bound": 38.3651689018125
          },
          "point_estimate": 20.787768494576913,
          "standard_error": 8.415229800765884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17590.76048123249,
            "upper_bound": 17610.51006892987
          },
          "point_estimate": 17601.495097842195,
          "standard_error": 5.083016019419355
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.17537488115979,
            "upper_bound": 31.580532160996192
          },
          "point_estimate": 23.46574158072104,
          "standard_error": 5.564257408779429
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96539.55590685132,
            "upper_bound": 96737.90352575156
          },
          "point_estimate": 96629.19487726832,
          "standard_error": 51.02644290675515
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96504.43633952257,
            "upper_bound": 96740.12413793104
          },
          "point_estimate": 96583.64909372236,
          "standard_error": 58.69279444652637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.929812458700307,
            "upper_bound": 248.55375632999855
          },
          "point_estimate": 116.63567675334588,
          "standard_error": 63.5872709680228
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96517.02199639691,
            "upper_bound": 96665.14020851944
          },
          "point_estimate": 96576.58593131008,
          "standard_error": 38.655586586653946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.14426843112876,
            "upper_bound": 225.9484012610262
          },
          "point_estimate": 169.85860165286488,
          "standard_error": 44.2044093380421
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105147.12879298277,
            "upper_bound": 105332.66382757016
          },
          "point_estimate": 105237.41988886594,
          "standard_error": 47.56699064880349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105109.9551300578,
            "upper_bound": 105336.9953034682
          },
          "point_estimate": 105226.88150289016,
          "standard_error": 60.31203248147293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.086342244150793,
            "upper_bound": 274.9158135874728
          },
          "point_estimate": 138.15112583490605,
          "standard_error": 68.43064443552645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105174.90375072022,
            "upper_bound": 105297.19391013672
          },
          "point_estimate": 105229.94492155244,
          "standard_error": 31.22413054589169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.76687090819628,
            "upper_bound": 211.1108408089676
          },
          "point_estimate": 158.7319295134605,
          "standard_error": 34.21630683739228
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18546.621121590822,
            "upper_bound": 18576.176628381825
          },
          "point_estimate": 18560.36766308531,
          "standard_error": 7.574857265443433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18540.34605240769,
            "upper_bound": 18571.648187850944
          },
          "point_estimate": 18562.42029096478,
          "standard_error": 8.217845554882395
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.685957821237595,
            "upper_bound": 42.06126917413905
          },
          "point_estimate": 18.005273127679516,
          "standard_error": 9.64105403529681
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18548.88652151719,
            "upper_bound": 18567.719327290735
          },
          "point_estimate": 18560.92400840609,
          "standard_error": 4.826963710023139
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.318362816607898,
            "upper_bound": 34.70718417658922
          },
          "point_estimate": 25.17473405968892,
          "standard_error": 6.477812128538461
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17749.110578278513,
            "upper_bound": 17762.425071091264
          },
          "point_estimate": 17755.92318006994,
          "standard_error": 3.4173642284559302
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17744.097581827064,
            "upper_bound": 17766.348168050805
          },
          "point_estimate": 17758.607927978228,
          "standard_error": 5.799677486428579
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8290584596298043,
            "upper_bound": 18.866422662122705
          },
          "point_estimate": 13.67291068544322,
          "standard_error": 4.2951938256878615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17749.424946765368,
            "upper_bound": 17763.00093300989
          },
          "point_estimate": 17755.922344387414,
          "standard_error": 3.464715272537795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.891782673149937,
            "upper_bound": 13.575137678453824
          },
          "point_estimate": 11.37793406733212,
          "standard_error": 1.7019862149526763
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17364.315898218367,
            "upper_bound": 17381.123353089206
          },
          "point_estimate": 17372.505644921926,
          "standard_error": 4.317595190896328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17362.73534725543,
            "upper_bound": 17383.958213773316
          },
          "point_estimate": 17370.1512605042,
          "standard_error": 5.589655836247259
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5782492976639018,
            "upper_bound": 24.27748593914578
          },
          "point_estimate": 12.128160285237836,
          "standard_error": 5.706951000194201
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17364.580006234202,
            "upper_bound": 17382.276190363064
          },
          "point_estimate": 17371.686213642886,
          "standard_error": 4.50057603140124
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.639424171123524,
            "upper_bound": 18.437684225949702
          },
          "point_estimate": 14.405151026340327,
          "standard_error": 2.7429328579533263
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17583.358751880507,
            "upper_bound": 17715.25589981118
          },
          "point_estimate": 17633.973340778604,
          "standard_error": 36.636709318515905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17575.9706237911,
            "upper_bound": 17630.164579303673
          },
          "point_estimate": 17592.728381842746,
          "standard_error": 15.772297651426952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.199487981540089,
            "upper_bound": 68.21649829407367
          },
          "point_estimate": 32.09803850655024,
          "standard_error": 19.618020297196544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17590.8859398723,
            "upper_bound": 17611.590608042403
          },
          "point_estimate": 17600.00177346831,
          "standard_error": 5.247316416890496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.51082843567302,
            "upper_bound": 187.13655986313904
          },
          "point_estimate": 122.51962759814984,
          "standard_error": 57.0117474519171
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17383.979589753544,
            "upper_bound": 17441.302526181847
          },
          "point_estimate": 17409.693263391197,
          "standard_error": 14.782977509784956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17375.079909221215,
            "upper_bound": 17436.007973005253
          },
          "point_estimate": 17394.237298932952,
          "standard_error": 14.265354858226862
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.550158113199217,
            "upper_bound": 69.18848478615023
          },
          "point_estimate": 32.16943545730634,
          "standard_error": 16.255443362257573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17387.47269980212,
            "upper_bound": 17478.89700952139
          },
          "point_estimate": 17433.90828426232,
          "standard_error": 25.28836586465061
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.863428946062427,
            "upper_bound": 67.95038458472519
          },
          "point_estimate": 49.130354436642584,
          "standard_error": 14.05919671087788
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15382.645196981632,
            "upper_bound": 15400.50546186301
          },
          "point_estimate": 15391.731351815935,
          "standard_error": 4.569264260735181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15378.695729034083,
            "upper_bound": 15406.05817776835
          },
          "point_estimate": 15392.671192193466,
          "standard_error": 7.612698474755003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.821682377980983,
            "upper_bound": 25.510890528078363
          },
          "point_estimate": 18.87400507433796,
          "standard_error": 5.434672559902665
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15382.141768481935,
            "upper_bound": 15402.019636267234
          },
          "point_estimate": 15392.729537327332,
          "standard_error": 5.058908886079024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.3696409662813,
            "upper_bound": 18.54768819378918
          },
          "point_estimate": 15.28237526900254,
          "standard_error": 2.3508077168188586
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16373.989994040914,
            "upper_bound": 16387.755365293866
          },
          "point_estimate": 16380.698552141426,
          "standard_error": 3.5339335577016064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16370.937762762764,
            "upper_bound": 16388.46655405405
          },
          "point_estimate": 16381.17731981982,
          "standard_error": 5.253697674223072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6276645525605573,
            "upper_bound": 21.43272853841287
          },
          "point_estimate": 13.015867049326609,
          "standard_error": 4.778635246529262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16372.691058375887,
            "upper_bound": 16383.835892014464
          },
          "point_estimate": 16377.828103428104,
          "standard_error": 2.838516654477241
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.6583873221860665,
            "upper_bound": 15.316190094137148
          },
          "point_estimate": 11.793996883338473,
          "standard_error": 2.3013374130407276
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17966.472871931124,
            "upper_bound": 17987.484293129015
          },
          "point_estimate": 17977.143115520717,
          "standard_error": 5.385099363765801
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17963.83305871368,
            "upper_bound": 17992.861930654613
          },
          "point_estimate": 17979.138358872962,
          "standard_error": 6.671980898401506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.322269699255682,
            "upper_bound": 33.050163888443294
          },
          "point_estimate": 15.220468068882347,
          "standard_error": 7.864810810127012
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17972.458586375596,
            "upper_bound": 17989.16222908969
          },
          "point_estimate": 17980.245593852513,
          "standard_error": 4.188206058939749
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.721276919473874,
            "upper_bound": 22.45158144393284
          },
          "point_estimate": 17.9297837016651,
          "standard_error": 3.2328670045866374
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18181.37170993911,
            "upper_bound": 18203.81768101435
          },
          "point_estimate": 18191.276562812815,
          "standard_error": 5.821727295655684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18176.85985985986,
            "upper_bound": 18199.21563229897
          },
          "point_estimate": 18188.206840173505,
          "standard_error": 5.210701022204306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.327291149875605,
            "upper_bound": 24.820641119655807
          },
          "point_estimate": 14.2306625088644,
          "standard_error": 6.275864879416784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18180.912584880945,
            "upper_bound": 18196.06765099131
          },
          "point_estimate": 18186.73716053716,
          "standard_error": 3.857763228764291
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.509554981275356,
            "upper_bound": 27.581313617161378
          },
          "point_estimate": 19.48309545090628,
          "standard_error": 6.112791692635952
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58123.70411,
            "upper_bound": 58180.195663257145
          },
          "point_estimate": 58152.10970552382,
          "standard_error": 14.46148735091053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58115.591733333335,
            "upper_bound": 58188.37131428571
          },
          "point_estimate": 58162.6078,
          "standard_error": 17.206339287854494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7908461904558894,
            "upper_bound": 87.18218757220924
          },
          "point_estimate": 52.5992667381694,
          "standard_error": 23.62430667179458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58120.41029132327,
            "upper_bound": 58172.73712144989
          },
          "point_estimate": 58143.04424727273,
          "standard_error": 13.371045270061636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.730936742075045,
            "upper_bound": 60.90038185910512
          },
          "point_estimate": 48.132280299535445,
          "standard_error": 8.830249065939592
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881749.4472828091,
            "upper_bound": 882410.2430069445
          },
          "point_estimate": 882093.2745719955,
          "standard_error": 168.95428090029924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881557.1592261905,
            "upper_bound": 882575.2103174604
          },
          "point_estimate": 882323.2291666667,
          "standard_error": 290.60005643045423
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.10276870212203,
            "upper_bound": 875.7979852847349
          },
          "point_estimate": 509.4163501225506,
          "standard_error": 228.63076775175017
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881755.6889215178,
            "upper_bound": 882596.3411536292
          },
          "point_estimate": 882257.0154607297,
          "standard_error": 217.5631105948203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305.9244634343451,
            "upper_bound": 678.8581488334736
          },
          "point_estimate": 562.885734769218,
          "standard_error": 91.62821634904252
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207929.54866031744,
            "upper_bound": 208500.58185827665
          },
          "point_estimate": 208226.19453900223,
          "standard_error": 145.9680063740898
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207787.67642857143,
            "upper_bound": 208647.5607142857
          },
          "point_estimate": 208328.22012698412,
          "standard_error": 210.30640005394525
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.21858268733637,
            "upper_bound": 815.6591833762963
          },
          "point_estimate": 487.7981716065185,
          "standard_error": 186.47170743086372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208170.53025817557,
            "upper_bound": 208521.48006596637
          },
          "point_estimate": 208375.3308497217,
          "standard_error": 89.06420269938667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256.13056858622605,
            "upper_bound": 605.1387487851772
          },
          "point_estimate": 486.6899716656,
          "standard_error": 88.33933037462951
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5643.049665145781,
            "upper_bound": 5669.114420767011
          },
          "point_estimate": 5654.4964501132035,
          "standard_error": 6.761230404289351
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5638.474432260072,
            "upper_bound": 5665.039359413076
          },
          "point_estimate": 5646.314590138436,
          "standard_error": 7.680040104965014
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3796724914584075,
            "upper_bound": 31.158964170465943
          },
          "point_estimate": 17.789700710769054,
          "standard_error": 7.156040031138656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5639.562355384279,
            "upper_bound": 5651.199062863265
          },
          "point_estimate": 5644.515794300582,
          "standard_error": 2.970110450142113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.822488869807671,
            "upper_bound": 32.01300646060911
          },
          "point_estimate": 22.564565123455186,
          "standard_error": 6.916872358639542
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5938.189711620832,
            "upper_bound": 5955.010168719648
          },
          "point_estimate": 5946.403375075025,
          "standard_error": 4.305226764408624
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5935.99750368309,
            "upper_bound": 5961.746876193594
          },
          "point_estimate": 5944.492633818956,
          "standard_error": 5.573842954560642
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9458400519588556,
            "upper_bound": 25.82794603434214
          },
          "point_estimate": 14.090433327354674,
          "standard_error": 6.451423542407411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5937.498520724692,
            "upper_bound": 5949.173686230206
          },
          "point_estimate": 5943.0417540227,
          "standard_error": 2.9077797377499985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.620535719106483,
            "upper_bound": 18.119801995785743
          },
          "point_estimate": 14.367164366084635,
          "standard_error": 2.628417064903058
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14112.612963578917,
            "upper_bound": 14127.885058611431
          },
          "point_estimate": 14119.734471792595,
          "standard_error": 3.9261908778791774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14109.870003880482,
            "upper_bound": 14130.290390265534
          },
          "point_estimate": 14117.325685551674,
          "standard_error": 3.81467648336079
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8974418699810478,
            "upper_bound": 19.913742620849384
          },
          "point_estimate": 8.227173738299436,
          "standard_error": 5.111998297890202
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14111.379484555106,
            "upper_bound": 14132.31616352076
          },
          "point_estimate": 14121.93147372612,
          "standard_error": 5.769565857703197
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.760913125258085,
            "upper_bound": 16.68637151737221
          },
          "point_estimate": 13.087796683062663,
          "standard_error": 3.020671897971703
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.06226584440399,
            "upper_bound": 62.16036642938803
          },
          "point_estimate": 62.113294114706285,
          "standard_error": 0.025303589790004893
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.04369105741591,
            "upper_bound": 62.18087646281487
          },
          "point_estimate": 62.13258498176732,
          "standard_error": 0.035285893154192456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010940208141944827,
            "upper_bound": 0.1426167788836536
          },
          "point_estimate": 0.07539040121662201,
          "standard_error": 0.032902228519456476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.042984527131225,
            "upper_bound": 62.17270996055652
          },
          "point_estimate": 62.11026569917964,
          "standard_error": 0.03298415702887082
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04078347762345615,
            "upper_bound": 0.10332111858579436
          },
          "point_estimate": 0.08443223586352269,
          "standard_error": 0.01548271396394531
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.734752328598866,
            "upper_bound": 34.77757451233443
          },
          "point_estimate": 34.755468528172386,
          "standard_error": 0.010999290034225508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.72969482486525,
            "upper_bound": 34.776739629646514
          },
          "point_estimate": 34.754115544674534,
          "standard_error": 0.012844066218552452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010580046157816634,
            "upper_bound": 0.06293925629494548
          },
          "point_estimate": 0.032467659172026535,
          "standard_error": 0.012998371849617694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.72741604982716,
            "upper_bound": 34.76330235969855
          },
          "point_estimate": 34.745027600531415,
          "standard_error": 0.009329515776480266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018461398858432596,
            "upper_bound": 0.04827944099131161
          },
          "point_estimate": 0.03659286733420362,
          "standard_error": 0.007769873019855013
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.390650963659304,
            "upper_bound": 38.4345967856942
          },
          "point_estimate": 38.4103409417663,
          "standard_error": 0.0112879935484365
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.392770896408095,
            "upper_bound": 38.4237666776897
          },
          "point_estimate": 38.40230267699193,
          "standard_error": 0.007620680855508994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004046813896799506,
            "upper_bound": 0.05063808490478504
          },
          "point_estimate": 0.017728888587514875,
          "standard_error": 0.011376307804335867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.39519517802584,
            "upper_bound": 38.448451648134736
          },
          "point_estimate": 38.4198561601796,
          "standard_error": 0.013684919224152168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010717186348695763,
            "upper_bound": 0.05377688265084089
          },
          "point_estimate": 0.03758465761551387,
          "standard_error": 0.012064811847284689
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.87862140750787,
            "upper_bound": 30.90456350926075
          },
          "point_estimate": 30.891737867398383,
          "standard_error": 0.0066356486726479045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.871759178126105,
            "upper_bound": 30.90931278612969
          },
          "point_estimate": 30.897510091845287,
          "standard_error": 0.011015941908383044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003155196035704746,
            "upper_bound": 0.03695430829856255
          },
          "point_estimate": 0.03208336756866335,
          "standard_error": 0.009368763629914867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.88788678623417,
            "upper_bound": 30.907943205960837
          },
          "point_estimate": 30.900088249410437,
          "standard_error": 0.005067764400152309
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013785802843180848,
            "upper_bound": 0.026902588618860687
          },
          "point_estimate": 0.022111737412996223,
          "standard_error": 0.0033897303031889853
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.46527326576717,
            "upper_bound": 24.5013478332702
          },
          "point_estimate": 24.47987381031873,
          "standard_error": 0.009687455065461675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.462260332734573,
            "upper_bound": 24.485070169836565
          },
          "point_estimate": 24.470410325894548,
          "standard_error": 0.005748580429959975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029658665029211324,
            "upper_bound": 0.026958490733849108
          },
          "point_estimate": 0.012790721128624744,
          "standard_error": 0.006461958111037557
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.46667172995428,
            "upper_bound": 24.485941183981087
          },
          "point_estimate": 24.47463704454987,
          "standard_error": 0.004938237475599845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007316042172490722,
            "upper_bound": 0.04869493383320303
          },
          "point_estimate": 0.032437668753085167,
          "standard_error": 0.013294855155542974
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.756907922607084,
            "upper_bound": 34.802624395264665
          },
          "point_estimate": 34.78024214995978,
          "standard_error": 0.011658497050858238
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.74999614883964,
            "upper_bound": 34.81610833094011
          },
          "point_estimate": 34.782049452750115,
          "standard_error": 0.016680630383227413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010438279024778962,
            "upper_bound": 0.06506400744067191
          },
          "point_estimate": 0.04900895972099428,
          "standard_error": 0.01402319372245072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.77016493613643,
            "upper_bound": 34.8144086004568
          },
          "point_estimate": 34.79530084664509,
          "standard_error": 0.011327153197951693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022287223986094264,
            "upper_bound": 0.04954344069749196
          },
          "point_estimate": 0.038884360851522456,
          "standard_error": 0.0071262668378220045
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.47534661935718,
            "upper_bound": 45.58174603631509
          },
          "point_estimate": 45.52592514822668,
          "standard_error": 0.027236051635802387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.464577429111486,
            "upper_bound": 45.5878538116773
          },
          "point_estimate": 45.504937937848254,
          "standard_error": 0.029602387673035125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011308745912851557,
            "upper_bound": 0.14551197745606645
          },
          "point_estimate": 0.07577970525529092,
          "standard_error": 0.037684811220445646
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.438902023927106,
            "upper_bound": 45.602314474564395
          },
          "point_estimate": 45.51820553366524,
          "standard_error": 0.04213635178256947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04319081350268638,
            "upper_bound": 0.11834528805102952
          },
          "point_estimate": 0.09092835546553626,
          "standard_error": 0.01950742030435233
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.908871190137745,
            "upper_bound": 53.01934263006749
          },
          "point_estimate": 52.958794445933506,
          "standard_error": 0.02840533937250999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.89187606628223,
            "upper_bound": 53.00595149904184
          },
          "point_estimate": 52.936763871020744,
          "standard_error": 0.033301409820040914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010600858705340471,
            "upper_bound": 0.14086071015349047
          },
          "point_estimate": 0.06884494887978568,
          "standard_error": 0.03362050238878837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.89337211561877,
            "upper_bound": 52.96494836248862
          },
          "point_estimate": 52.92367741281722,
          "standard_error": 0.01840850983771101
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03895611648837782,
            "upper_bound": 0.12841942258334926
          },
          "point_estimate": 0.09415121341558684,
          "standard_error": 0.02516963020772085
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.40640387184931,
            "upper_bound": 42.48187897665483
          },
          "point_estimate": 42.44529042675083,
          "standard_error": 0.01936234468578435
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.395261257329494,
            "upper_bound": 42.49590644764617
          },
          "point_estimate": 42.45170877826504,
          "standard_error": 0.02742993153207821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014694581012530076,
            "upper_bound": 0.10532330407539472
          },
          "point_estimate": 0.06892130806836623,
          "standard_error": 0.02412415266918942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.44689259785279,
            "upper_bound": 42.50495289352679
          },
          "point_estimate": 42.48393654872893,
          "standard_error": 0.01479477161278739
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03491598111580294,
            "upper_bound": 0.08053155086899193
          },
          "point_estimate": 0.06454907180176936,
          "standard_error": 0.011533184878378946
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.07004358653725,
            "upper_bound": 62.3830248046635
          },
          "point_estimate": 62.22658999685533,
          "standard_error": 0.0800400196518572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.99531252933136,
            "upper_bound": 62.42696785244104
          },
          "point_estimate": 62.25733546993597,
          "standard_error": 0.10360687017905344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020451700727569404,
            "upper_bound": 0.48837088016150393
          },
          "point_estimate": 0.25432235173776363,
          "standard_error": 0.115733443867242
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.20471489613015,
            "upper_bound": 62.53061235174101
          },
          "point_estimate": 62.393358686189295,
          "standard_error": 0.08543380550843618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15304586264786538,
            "upper_bound": 0.3366729842874597
          },
          "point_estimate": 0.26691531634478094,
          "standard_error": 0.047521816529249715
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.238418597978125,
            "upper_bound": 44.28963619315947
          },
          "point_estimate": 44.265533972341075,
          "standard_error": 0.01309838127816686
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.232304122152485,
            "upper_bound": 44.29558009193255
          },
          "point_estimate": 44.28435884142766,
          "standard_error": 0.0180914467206168
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036870660202417225,
            "upper_bound": 0.07127448384718693
          },
          "point_estimate": 0.0323938490980988,
          "standard_error": 0.018967135497084064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.265412397119526,
            "upper_bound": 44.29246317631536
          },
          "point_estimate": 44.28193123175959,
          "standard_error": 0.006884026951900137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0200960831334597,
            "upper_bound": 0.05569374006308546
          },
          "point_estimate": 0.04367331987475334,
          "standard_error": 0.00916928664309241
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.18075675446883,
            "upper_bound": 38.32214214954546
          },
          "point_estimate": 38.25689831273103,
          "standard_error": 0.03614984377804985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.21589113299916,
            "upper_bound": 38.3295580938434
          },
          "point_estimate": 38.27241433632916,
          "standard_error": 0.03024266483789551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006237094247874427,
            "upper_bound": 0.17201981880515874
          },
          "point_estimate": 0.07938773862836868,
          "standard_error": 0.04132955554157186
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.225481849135015,
            "upper_bound": 38.32655289072146
          },
          "point_estimate": 38.27102025051307,
          "standard_error": 0.02674627700789666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03890305958683197,
            "upper_bound": 0.1712152348690585
          },
          "point_estimate": 0.12051910398702168,
          "standard_error": 0.03569088210688647
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.36024167518232,
            "upper_bound": 66.5814234000138
          },
          "point_estimate": 66.45394958821205,
          "standard_error": 0.05784106274361284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.33465488714764,
            "upper_bound": 66.50440415919122
          },
          "point_estimate": 66.39513155026316,
          "standard_error": 0.05442390764850111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012380796036269282,
            "upper_bound": 0.2208566049070316
          },
          "point_estimate": 0.1025388682051522,
          "standard_error": 0.051508107874976915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.32633386668196,
            "upper_bound": 66.4793912608259
          },
          "point_estimate": 66.39111536453396,
          "standard_error": 0.03923635303800694
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06256103524097578,
            "upper_bound": 0.2820512410424876
          },
          "point_estimate": 0.19244303402709032,
          "standard_error": 0.06896048748763882
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154983.6884997656,
            "upper_bound": 155179.79958157524
          },
          "point_estimate": 155075.2488068448,
          "standard_error": 50.3198357291695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154950.01687763713,
            "upper_bound": 155194.364556962
          },
          "point_estimate": 155052.87074542895,
          "standard_error": 55.62589514365215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.379278103755222,
            "upper_bound": 282.71106692181735
          },
          "point_estimate": 122.75390834809107,
          "standard_error": 64.20129532434873
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154944.6692781227,
            "upper_bound": 155063.52015836726
          },
          "point_estimate": 155001.53140446052,
          "standard_error": 30.67122479788889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.56257897218927,
            "upper_bound": 218.76168753811984
          },
          "point_estimate": 167.82133295085708,
          "standard_error": 38.120010643316256
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53607.59445427729,
            "upper_bound": 53683.01845499719
          },
          "point_estimate": 53645.63859495715,
          "standard_error": 19.27577493885985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53595.79351032448,
            "upper_bound": 53684.94331366765
          },
          "point_estimate": 53648.673193215334,
          "standard_error": 20.24532533566002
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.6509910279879465,
            "upper_bound": 111.28729515883988
          },
          "point_estimate": 59.75586757482297,
          "standard_error": 26.311399914133535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53617.908616358734,
            "upper_bound": 53697.321756071986
          },
          "point_estimate": 53655.28683293108,
          "standard_error": 19.8297388460958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.470595013948984,
            "upper_bound": 85.12139068418062
          },
          "point_estimate": 64.25452393354323,
          "standard_error": 13.806326501410751
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247578.08109664576,
            "upper_bound": 248930.84284173907
          },
          "point_estimate": 248187.03441602312,
          "standard_error": 349.4006758960123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247318.9009009009,
            "upper_bound": 248652.25844594595
          },
          "point_estimate": 248147.58204633207,
          "standard_error": 339.84548753002895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 178.65530034175328,
            "upper_bound": 1658.036295699094
          },
          "point_estimate": 753.2324003098461,
          "standard_error": 359.1689151840741
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247661.26295634956,
            "upper_bound": 249724.80236899492
          },
          "point_estimate": 248681.23662688665,
          "standard_error": 545.6850560132841
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 460.65420185055893,
            "upper_bound": 1651.7986407275605
          },
          "point_estimate": 1168.048528137715,
          "standard_error": 350.51071650972034
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829466.2786904762,
            "upper_bound": 830570.3373333331
          },
          "point_estimate": 829996.8419285713,
          "standard_error": 281.8467147850272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829196.0222222222,
            "upper_bound": 830648.8736111111
          },
          "point_estimate": 829896.9877777777,
          "standard_error": 360.94837801447505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.6257888304122,
            "upper_bound": 1638.1635409167473
          },
          "point_estimate": 1026.964055101073,
          "standard_error": 363.3940628514169
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829194.6355223695,
            "upper_bound": 831080.3685923433
          },
          "point_estimate": 830123.5179220779,
          "standard_error": 506.24077513674024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502.9326392646855,
            "upper_bound": 1193.0845504148317
          },
          "point_estimate": 939.7745311386436,
          "standard_error": 177.98774767533274
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1893073.7588208336,
            "upper_bound": 1895916.0567857143
          },
          "point_estimate": 1894537.2529702384,
          "standard_error": 727.8958739063713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1893065.2642857144,
            "upper_bound": 1895962.5275
          },
          "point_estimate": 1894904.745833333,
          "standard_error": 783.7493009954825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500.4392661157147,
            "upper_bound": 4007.410807604177
          },
          "point_estimate": 1887.5264429896704,
          "standard_error": 940.884793374698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1894007.2184899773,
            "upper_bound": 1895572.9172616312
          },
          "point_estimate": 1894755.7463636363,
          "standard_error": 388.9074291134218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1159.3232394370589,
            "upper_bound": 3181.535134272029
          },
          "point_estimate": 2424.3289230922346,
          "standard_error": 521.7508647591392
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62951.303939724,
            "upper_bound": 63052.41635187926
          },
          "point_estimate": 62997.99242652905,
          "standard_error": 25.95741634399631
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62934.7426597582,
            "upper_bound": 63051.08171416235
          },
          "point_estimate": 62976.66570441648,
          "standard_error": 29.59654386240378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.06063751088735,
            "upper_bound": 139.40312692924903
          },
          "point_estimate": 68.5647678328211,
          "standard_error": 31.944604010796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62938.50055413479,
            "upper_bound": 63027.07420000959
          },
          "point_estimate": 62973.72327568804,
          "standard_error": 22.355978677709277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.6963642026474,
            "upper_bound": 111.34139033156973
          },
          "point_estimate": 86.82751556420322,
          "standard_error": 20.10882154871963
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217378.06859854885,
            "upper_bound": 217726.3629323753
          },
          "point_estimate": 217554.8732093548,
          "standard_error": 89.33394032703075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217323.1395463511,
            "upper_bound": 217854.3727810651
          },
          "point_estimate": 217523.3901098901,
          "standard_error": 150.58989459637692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.808909956016368,
            "upper_bound": 541.2782844732635
          },
          "point_estimate": 399.5804316634242,
          "standard_error": 135.1234268959488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217425.0323571268,
            "upper_bound": 217799.31179349465
          },
          "point_estimate": 217643.9540305848,
          "standard_error": 96.5663324796112
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.45426526564648,
            "upper_bound": 366.06067856895106
          },
          "point_estimate": 297.29025867067816,
          "standard_error": 48.13031987221524
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145540.1890756024,
            "upper_bound": 145833.4264121846
          },
          "point_estimate": 145681.76141940177,
          "standard_error": 74.80388127498996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145438.41173085436,
            "upper_bound": 145859.1015936255
          },
          "point_estimate": 145688.5392430279,
          "standard_error": 126.55602855897811
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.811328971696029,
            "upper_bound": 440.355584877998
          },
          "point_estimate": 366.6657392141343,
          "standard_error": 129.24216093239227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145495.99353284537,
            "upper_bound": 145813.5957986992
          },
          "point_estimate": 145655.26475914524,
          "standard_error": 80.57007444779845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.38369164304672,
            "upper_bound": 308.3243621504675
          },
          "point_estimate": 248.8360772809909,
          "standard_error": 42.90488547678439
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 944178.6260952384,
            "upper_bound": 945765.854751116
          },
          "point_estimate": 944916.765671627,
          "standard_error": 406.9120812575092
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 944302.1388888888,
            "upper_bound": 945595.935
          },
          "point_estimate": 944587.857142857,
          "standard_error": 318.4736583879772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.60401897888156,
            "upper_bound": 2123.491906883789
          },
          "point_estimate": 485.82050679147295,
          "standard_error": 560.2316827827404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 944447.0090958604,
            "upper_bound": 945410.9805288462
          },
          "point_estimate": 944834.7187012988,
          "standard_error": 251.07994917929383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500.1897305698524,
            "upper_bound": 1895.5549879619025
          },
          "point_estimate": 1357.7147473447671,
          "standard_error": 384.1011611010576
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63673.533482921965,
            "upper_bound": 63826.917910883945
          },
          "point_estimate": 63745.38170766503,
          "standard_error": 39.31911429099922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63648.80017452007,
            "upper_bound": 63824.65127150336
          },
          "point_estimate": 63730.02692941633,
          "standard_error": 40.5117552620075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.78484204599062,
            "upper_bound": 209.7731787016103
          },
          "point_estimate": 95.16338317962268,
          "standard_error": 46.86998125829928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63625.132095872235,
            "upper_bound": 63765.73783556463
          },
          "point_estimate": 63692.85252827452,
          "standard_error": 38.15460833696593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.70995040970493,
            "upper_bound": 177.35356113883338
          },
          "point_estimate": 131.07160830635115,
          "standard_error": 32.650472700482
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126678.7539838362,
            "upper_bound": 126822.95268820129
          },
          "point_estimate": 126750.92187944718,
          "standard_error": 37.003866556559224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126665.49655172414,
            "upper_bound": 126867.33409961686
          },
          "point_estimate": 126720.07844827586,
          "standard_error": 55.82265780210791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.9429150668188,
            "upper_bound": 208.4936035398776
          },
          "point_estimate": 123.86707396471115,
          "standard_error": 51.987301563306126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126701.34542624584,
            "upper_bound": 126859.2794785534
          },
          "point_estimate": 126774.50598298252,
          "standard_error": 41.15487622833196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.20022980811196,
            "upper_bound": 155.62473653156684
          },
          "point_estimate": 123.4612148251706,
          "standard_error": 21.54971881556943
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413588.93212232145,
            "upper_bound": 414154.2756468254
          },
          "point_estimate": 413867.41151190473,
          "standard_error": 144.5482120964486
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413576.9895833334,
            "upper_bound": 414181.29333333333
          },
          "point_estimate": 413780.3069444444,
          "standard_error": 169.50779829290974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.580370574449876,
            "upper_bound": 831.7995365659484
          },
          "point_estimate": 309.05352426318245,
          "standard_error": 201.36234639335393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413629.0652979066,
            "upper_bound": 414196.9021975309
          },
          "point_estimate": 413961.3231168831,
          "standard_error": 145.4870247897903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.80220867985383,
            "upper_bound": 632.763523681206
          },
          "point_estimate": 481.1931557301649,
          "standard_error": 101.37291181491491
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58887.03158762802,
            "upper_bound": 58983.8847558664
          },
          "point_estimate": 58935.714853435755,
          "standard_error": 24.873121605635625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58868.962011771,
            "upper_bound": 59016.15141787052
          },
          "point_estimate": 58929.98095906902,
          "standard_error": 34.48281741758154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.70620481868354,
            "upper_bound": 147.89980846976152
          },
          "point_estimate": 98.0989815649775,
          "standard_error": 35.258030530038575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58908.08128103556,
            "upper_bound": 59001.49263570768
          },
          "point_estimate": 58953.28351712493,
          "standard_error": 24.390652375659165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.26574902264856,
            "upper_bound": 101.62953651688376
          },
          "point_estimate": 82.79718026583559,
          "standard_error": 13.49075893402403
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22957.380323632813,
            "upper_bound": 23001.7119234375
          },
          "point_estimate": 22979.641278993055,
          "standard_error": 11.295701718714575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22946.541180555556,
            "upper_bound": 22999.890625
          },
          "point_estimate": 22987.7638125,
          "standard_error": 12.21960620166168
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.544869172574988,
            "upper_bound": 70.53538871649825
          },
          "point_estimate": 18.485091220260255,
          "standard_error": 17.25204369247942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22960.197779175855,
            "upper_bound": 22991.994352616857
          },
          "point_estimate": 22978.463087662338,
          "standard_error": 8.457228656187354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.64341852858395,
            "upper_bound": 50.26151522043082
          },
          "point_estimate": 37.56636959079782,
          "standard_error": 8.393630016962328
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2073067.9483081144,
            "upper_bound": 2076563.6811666663
          },
          "point_estimate": 2074929.529692982,
          "standard_error": 896.091873575783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2072821.035087719,
            "upper_bound": 2077243.6403508773
          },
          "point_estimate": 2075609.3486842103,
          "standard_error": 1155.8282960647578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 568.8924675319973,
            "upper_bound": 5011.102726561721
          },
          "point_estimate": 2611.2405832462155,
          "standard_error": 1086.443789198933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2073369.3022501776,
            "upper_bound": 2077278.1959637655
          },
          "point_estimate": 2075403.5380724536,
          "standard_error": 1028.5749249784417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412.4127054873245,
            "upper_bound": 3863.37771513068
          },
          "point_estimate": 2980.0114669808636,
          "standard_error": 641.7685160144424
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4146.708291420474,
            "upper_bound": 4151.272717213015
          },
          "point_estimate": 4149.083040311467,
          "standard_error": 1.1709826346212795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4146.515960560495,
            "upper_bound": 4152.556333004061
          },
          "point_estimate": 4148.438823186868,
          "standard_error": 1.69235582894383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3724390794736369,
            "upper_bound": 6.966867439332663
          },
          "point_estimate": 5.233280117824952,
          "standard_error": 1.7874861460466684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4145.492408912718,
            "upper_bound": 4150.299028505976
          },
          "point_estimate": 4147.626100033212,
          "standard_error": 1.2220715944158025
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.22177291663725,
            "upper_bound": 4.975572882149409
          },
          "point_estimate": 3.8977179071228294,
          "standard_error": 0.7436771362085702
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40903.48918530273,
            "upper_bound": 40987.0399696712
          },
          "point_estimate": 40942.41929740925,
          "standard_error": 21.534878572021768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40881.28587497991,
            "upper_bound": 40993.93180539933
          },
          "point_estimate": 40926.699387576555,
          "standard_error": 24.69216057844279
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.933150522291283,
            "upper_bound": 120.75122206884808
          },
          "point_estimate": 61.99724198144289,
          "standard_error": 26.916868595728783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40896.75737902022,
            "upper_bound": 40942.046155687465
          },
          "point_estimate": 40921.20074211503,
          "standard_error": 11.423268469530152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.520360377330093,
            "upper_bound": 93.12657566838236
          },
          "point_estimate": 72.04243842500938,
          "standard_error": 16.350081938227472
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49278.53240937405,
            "upper_bound": 49363.26236008948
          },
          "point_estimate": 49314.93415435325,
          "standard_error": 22.015698210005983
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49261.62703252032,
            "upper_bound": 49342.45586527294
          },
          "point_estimate": 49298.042818428185,
          "standard_error": 18.509348692725563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.405118024080174,
            "upper_bound": 88.41858383224354
          },
          "point_estimate": 59.91841265575053,
          "standard_error": 21.819556370106927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49290.31307322918,
            "upper_bound": 49333.80848685722
          },
          "point_estimate": 49312.16855312709,
          "standard_error": 11.33149769861846
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.55948663640214,
            "upper_bound": 106.38432906160132
          },
          "point_estimate": 73.49517306395028,
          "standard_error": 24.85181909803033
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48111.79116985544,
            "upper_bound": 48193.56842561098
          },
          "point_estimate": 48149.03731670446,
          "standard_error": 21.07277756465776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48100.84708994709,
            "upper_bound": 48178.79657186949
          },
          "point_estimate": 48136.319420823886,
          "standard_error": 26.346040163302124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.356480510583848,
            "upper_bound": 106.45098861937242
          },
          "point_estimate": 57.66313730960761,
          "standard_error": 24.395413694760965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48105.97471927533,
            "upper_bound": 48203.13072116832
          },
          "point_estimate": 48144.02571978286,
          "standard_error": 24.861274226863635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.932081212129425,
            "upper_bound": 98.22520835898014
          },
          "point_estimate": 70.67836281034349,
          "standard_error": 19.630366956419756
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14028.819977228795,
            "upper_bound": 14088.33233264902
          },
          "point_estimate": 14056.540239089216,
          "standard_error": 15.302721180531195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14018.538450010705,
            "upper_bound": 14123.37880539499
          },
          "point_estimate": 14034.50873795761,
          "standard_error": 24.49065919857185
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.208598555340787,
            "upper_bound": 78.61925173204052
          },
          "point_estimate": 25.23938188524503,
          "standard_error": 18.95478408290496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14020.04019029274,
            "upper_bound": 14042.351078093494
          },
          "point_estimate": 14029.345623701924,
          "standard_error": 5.832485778160474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.415804261712571,
            "upper_bound": 58.529174017404856
          },
          "point_estimate": 50.97907662831159,
          "standard_error": 9.46534728803024
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25585.13170342545,
            "upper_bound": 25642.214307405447
          },
          "point_estimate": 25614.721264807544,
          "standard_error": 14.602819032497704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25573.2218111346,
            "upper_bound": 25644.97322057787
          },
          "point_estimate": 25631.240209403,
          "standard_error": 19.43225245316489
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.335899669323663,
            "upper_bound": 82.83361280002232
          },
          "point_estimate": 30.62138372697396,
          "standard_error": 22.05654803754988
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25595.350067552044,
            "upper_bound": 25639.470212069093
          },
          "point_estimate": 25616.84405700008,
          "standard_error": 11.177155844454736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.740999694377475,
            "upper_bound": 62.66161703356384
          },
          "point_estimate": 48.815062788512634,
          "standard_error": 9.613137127782007
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17319.598868754307,
            "upper_bound": 17348.11123655487
          },
          "point_estimate": 17334.00027910242,
          "standard_error": 7.312925601408017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17312.054840247976,
            "upper_bound": 17355.736623748213
          },
          "point_estimate": 17334.37960644837,
          "standard_error": 9.946267772975911
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.9107736371885125,
            "upper_bound": 41.43856321281523
          },
          "point_estimate": 25.92695231033143,
          "standard_error": 9.767265602152053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.676719005533,
            "upper_bound": 17349.38263687446
          },
          "point_estimate": 17340.063323610106,
          "standard_error": 5.6220876875274515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.876972709665836,
            "upper_bound": 30.3951302315414
          },
          "point_estimate": 24.30208945331118,
          "standard_error": 4.229133445780371
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.10514487817,
            "upper_bound": 17341.812292303443
          },
          "point_estimate": 17334.389195373587,
          "standard_error": 3.763636689024175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.187589413446,
            "upper_bound": 17344.751470354473
          },
          "point_estimate": 17333.488627934086,
          "standard_error": 4.019309257572427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3948423572107005,
            "upper_bound": 21.583159452297615
          },
          "point_estimate": 8.740683850545595,
          "standard_error": 5.400945571897746
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17329.886524544538,
            "upper_bound": 17343.575404593255
          },
          "point_estimate": 17334.970692826486,
          "standard_error": 3.498715201035604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.205670538541677,
            "upper_bound": 16.499574992030574
          },
          "point_estimate": 12.534277086997225,
          "standard_error": 2.675453498593972
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17564.229283834688,
            "upper_bound": 17586.175397208986
          },
          "point_estimate": 17575.68037361811,
          "standard_error": 5.646029726113519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17560.474867085548,
            "upper_bound": 17592.71224884347
          },
          "point_estimate": 17577.534471832878,
          "standard_error": 8.703327408813848
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4187068202779987,
            "upper_bound": 32.63208832028978
          },
          "point_estimate": 21.29610232854044,
          "standard_error": 8.526487021392857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17571.394447129584,
            "upper_bound": 17589.873847236413
          },
          "point_estimate": 17581.138349036173,
          "standard_error": 4.838363516609333
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.85486156916736,
            "upper_bound": 22.99560242935673
          },
          "point_estimate": 18.875018417723332,
          "standard_error": 3.3195287644078073
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94453.67003174602,
            "upper_bound": 94534.09952494327
          },
          "point_estimate": 94494.36096310036,
          "standard_error": 20.61027306246647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94432.71688311688,
            "upper_bound": 94555.7178210678
          },
          "point_estimate": 94500.987012987,
          "standard_error": 45.536314571894685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.443401774805455,
            "upper_bound": 100.32430973402948
          },
          "point_estimate": 84.1909263840979,
          "standard_error": 29.413256639934406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94440.01585072328,
            "upper_bound": 94556.58813936872
          },
          "point_estimate": 94503.8506999494,
          "standard_error": 32.49503127211458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.2293551814527,
            "upper_bound": 77.54855767398028
          },
          "point_estimate": 68.5319361293754,
          "standard_error": 7.739696203395757
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104376.52597429189,
            "upper_bound": 104516.62109169748
          },
          "point_estimate": 104445.50372947456,
          "standard_error": 35.783478708935135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104349.30176518884,
            "upper_bound": 104513.55296934866
          },
          "point_estimate": 104444.57962164751,
          "standard_error": 40.96709788941939
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.49080045435346,
            "upper_bound": 211.8712048592298
          },
          "point_estimate": 88.5851709129245,
          "standard_error": 49.99192605796462
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104372.41812291568,
            "upper_bound": 104484.81144566037
          },
          "point_estimate": 104429.33769219286,
          "standard_error": 28.249545720132204
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.493874150958376,
            "upper_bound": 158.37683913246372
          },
          "point_estimate": 119.2188356108718,
          "standard_error": 25.719706653166018
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18466.76656584884,
            "upper_bound": 18480.297829828374
          },
          "point_estimate": 18473.45626248892,
          "standard_error": 3.459574738929064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18465.85211294416,
            "upper_bound": 18483.993836113124
          },
          "point_estimate": 18469.526029328823,
          "standard_error": 4.843458420224878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9892466448702262,
            "upper_bound": 20.69343825994805
          },
          "point_estimate": 11.12631073140384,
          "standard_error": 5.356701508263444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18468.88452261631,
            "upper_bound": 18479.349996491143
          },
          "point_estimate": 18473.95782187356,
          "standard_error": 2.6520963605115018
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.428630095600899,
            "upper_bound": 14.596457963726778
          },
          "point_estimate": 11.559033735426883,
          "standard_error": 2.0604948739828584
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17473.65816522575,
            "upper_bound": 17497.45468699968
          },
          "point_estimate": 17485.211154758093,
          "standard_error": 6.0583160563719165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17474.47212882912,
            "upper_bound": 17495.094792095515
          },
          "point_estimate": 17482.918347742554,
          "standard_error": 4.831925662634781
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7617446373142154,
            "upper_bound": 32.890291202823256
          },
          "point_estimate": 12.597280359930098,
          "standard_error": 7.604223323935203
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17477.22788068392,
            "upper_bound": 17494.546416373145
          },
          "point_estimate": 17487.406408672978,
          "standard_error": 4.50316760481463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.514924100495453,
            "upper_bound": 28.003840204063668
          },
          "point_estimate": 20.25439759850304,
          "standard_error": 5.217634128958183
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17337.488880249097,
            "upper_bound": 17366.084727591373
          },
          "point_estimate": 17351.37742071054,
          "standard_error": 7.301431296324747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17329.80816261712,
            "upper_bound": 17370.05330769754
          },
          "point_estimate": 17351.250327536924,
          "standard_error": 9.866516617298895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.395504714948014,
            "upper_bound": 42.13484613142369
          },
          "point_estimate": 25.110919009887834,
          "standard_error": 9.274484542325556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17334.58651439465,
            "upper_bound": 17361.45323499236
          },
          "point_estimate": 17345.566909412646,
          "standard_error": 6.9080657818221125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.536269379920702,
            "upper_bound": 30.915846654040724
          },
          "point_estimate": 24.363883878149583,
          "standard_error": 4.529394200738287
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17288.064795515045,
            "upper_bound": 17308.00603340734
          },
          "point_estimate": 17298.039506177032,
          "standard_error": 5.086845020039
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17289.325336414302,
            "upper_bound": 17311.757796807273
          },
          "point_estimate": 17294.354923882016,
          "standard_error": 5.941140655162383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.252113090367624,
            "upper_bound": 33.24798818629096
          },
          "point_estimate": 9.921306650692753,
          "standard_error": 8.047765652045497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17289.675747964902,
            "upper_bound": 17306.635563775537
          },
          "point_estimate": 17297.782610253686,
          "standard_error": 4.3486037889164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.907012975063692,
            "upper_bound": 22.211145917118458
          },
          "point_estimate": 16.969946747974998,
          "standard_error": 3.455202935849534
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17334.747306505065,
            "upper_bound": 17354.915288369877
          },
          "point_estimate": 17344.81986234055,
          "standard_error": 5.166523491167662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17329.507673975215,
            "upper_bound": 17357.22295042898
          },
          "point_estimate": 17347.61412059104,
          "standard_error": 7.53650745244631
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4005316815217856,
            "upper_bound": 28.913374748838887
          },
          "point_estimate": 23.67596028033511,
          "standard_error": 7.212622252020089
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17330.056006109808,
            "upper_bound": 17351.315384059853
          },
          "point_estimate": 17340.19496242556,
          "standard_error": 5.427652225744956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.264479852098283,
            "upper_bound": 21.409985499320737
          },
          "point_estimate": 17.231949371367392,
          "standard_error": 2.860798039621646
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15197.888954869,
            "upper_bound": 15217.456821976488
          },
          "point_estimate": 15207.74545948728,
          "standard_error": 5.0038733816590195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15194.6059972106,
            "upper_bound": 15220.675122036262
          },
          "point_estimate": 15210.341364481636,
          "standard_error": 5.806719874309957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4752334989138596,
            "upper_bound": 27.691190359150134
          },
          "point_estimate": 18.127665788349137,
          "standard_error": 7.895814779400077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15192.805391440235,
            "upper_bound": 15218.528854334496
          },
          "point_estimate": 15205.081638863228,
          "standard_error": 6.459597417177858
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.43801745296009,
            "upper_bound": 21.518609972111143
          },
          "point_estimate": 16.625849454174695,
          "standard_error": 3.301826440154301
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16345.629979274918,
            "upper_bound": 16395.94471628499
          },
          "point_estimate": 16371.147872110678,
          "standard_error": 12.999582612241795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16325.991917377638,
            "upper_bound": 16409.50276904655
          },
          "point_estimate": 16382.728371501273,
          "standard_error": 28.44222022543516
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0714841659755567,
            "upper_bound": 67.22159701044073
          },
          "point_estimate": 46.17742471453548,
          "standard_error": 20.566782463280383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16327.196458606875,
            "upper_bound": 16369.784030259942
          },
          "point_estimate": 16341.550076685777,
          "standard_error": 10.8893983541048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.9741584424075,
            "upper_bound": 48.54951030732038
          },
          "point_estimate": 43.31912801856331,
          "standard_error": 5.078367546000907
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17921.1368584885,
            "upper_bound": 17944.690288860642
          },
          "point_estimate": 17932.837471674946,
          "standard_error": 6.067775202957724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17923.88838976767,
            "upper_bound": 17948.42533483982
          },
          "point_estimate": 17929.280578898226,
          "standard_error": 4.8260871712511655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.37919507688746235,
            "upper_bound": 41.38909963716853
          },
          "point_estimate": 7.142881295171442,
          "standard_error": 9.942125820250125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17923.590104137413,
            "upper_bound": 17933.06988037568
          },
          "point_estimate": 17927.499076208023,
          "standard_error": 2.4516498859862508
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.5939384315992715,
            "upper_bound": 27.431790186454467
          },
          "point_estimate": 20.308407730398752,
          "standard_error": 4.859369260684013
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18382.9345204015,
            "upper_bound": 18403.100941565554
          },
          "point_estimate": 18392.24049530951,
          "standard_error": 5.179941011953883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18380.516992472756,
            "upper_bound": 18403.79231547017
          },
          "point_estimate": 18387.46511627907,
          "standard_error": 5.225996482119456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.304684252983087,
            "upper_bound": 28.031155297342245
          },
          "point_estimate": 12.316897910755037,
          "standard_error": 6.3812659236781295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18378.50502792896,
            "upper_bound": 18393.28516253312
          },
          "point_estimate": 18384.099251506836,
          "standard_error": 3.7419138429638616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.940056647680269,
            "upper_bound": 22.775887385888847
          },
          "point_estimate": 17.2790509646017,
          "standard_error": 4.172904898222005
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57683.864128369874,
            "upper_bound": 57786.33071018203
          },
          "point_estimate": 57731.58827009322,
          "standard_error": 26.25934264453438
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57677.86682539682,
            "upper_bound": 57761.0253968254
          },
          "point_estimate": 57734.295789241616,
          "standard_error": 23.062041760515164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.841182232689423,
            "upper_bound": 126.93367084171209
          },
          "point_estimate": 48.85508146598274,
          "standard_error": 29.92261763191385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57711.67098124098,
            "upper_bound": 57770.421326027245
          },
          "point_estimate": 57741.30490208205,
          "standard_error": 14.81729729812402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.508728082802815,
            "upper_bound": 124.16439298039774
          },
          "point_estimate": 87.59792668654086,
          "standard_error": 25.038356366764468
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881245.4110978836,
            "upper_bound": 882527.04801892
          },
          "point_estimate": 881895.0424650416,
          "standard_error": 326.4330743453501
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881071.6285714286,
            "upper_bound": 882698.1876984127
          },
          "point_estimate": 882049.8579931973,
          "standard_error": 402.8476092177646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.02277722589696,
            "upper_bound": 1867.1010985189373
          },
          "point_estimate": 1244.1740564035504,
          "standard_error": 465.1825912150352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881214.1413376576,
            "upper_bound": 882256.1479523809
          },
          "point_estimate": 881740.9226963513,
          "standard_error": 264.26217641274684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 562.4923604370886,
            "upper_bound": 1415.6348412377413
          },
          "point_estimate": 1088.775372287902,
          "standard_error": 215.68042242166672
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207597.72599047617,
            "upper_bound": 207921.7894680272
          },
          "point_estimate": 207768.50112199545,
          "standard_error": 82.90222149477265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207650.44095238097,
            "upper_bound": 207994.39581632655
          },
          "point_estimate": 207751.85485714284,
          "standard_error": 81.2304911023684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.09991651490713,
            "upper_bound": 463.47455239076805
          },
          "point_estimate": 196.76855050666276,
          "standard_error": 107.02451365316836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207664.19529374465,
            "upper_bound": 207920.4248754336
          },
          "point_estimate": 207792.99657142855,
          "standard_error": 64.95723989598206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.08954139821964,
            "upper_bound": 378.8556902341681
          },
          "point_estimate": 276.5510226013424,
          "standard_error": 69.67698646255228
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5543.5063411130795,
            "upper_bound": 5559.582205317496
          },
          "point_estimate": 5551.220255399766,
          "standard_error": 4.092765138647699
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5542.2258612769865,
            "upper_bound": 5558.292817162592
          },
          "point_estimate": 5551.338500995253,
          "standard_error": 4.1523226596306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.16091063520127,
            "upper_bound": 23.13423483044264
          },
          "point_estimate": 10.526919507644564,
          "standard_error": 4.951977175655706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5541.393946970789,
            "upper_bound": 5555.781857506851
          },
          "point_estimate": 5548.652532676327,
          "standard_error": 3.67035978093065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.023465896782067,
            "upper_bound": 18.780046536207788
          },
          "point_estimate": 13.692533351689136,
          "standard_error": 3.336474111651211
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5905.084226979047,
            "upper_bound": 5916.160326121664
          },
          "point_estimate": 5910.439271511908,
          "standard_error": 2.845966284793465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5902.867793992427,
            "upper_bound": 5918.796306864504
          },
          "point_estimate": 5908.537396733518,
          "standard_error": 4.422755121099143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3544252048177916,
            "upper_bound": 15.819353739070335
          },
          "point_estimate": 10.29708214775155,
          "standard_error": 3.473192641319176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5903.576878243819,
            "upper_bound": 5916.448503878316
          },
          "point_estimate": 5908.09540806495,
          "standard_error": 3.341358154674947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.464763260856581,
            "upper_bound": 11.577862128296523
          },
          "point_estimate": 9.485531905971332,
          "standard_error": 1.550626280329601
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14083.656008906646,
            "upper_bound": 14107.975758322717
          },
          "point_estimate": 14095.62166497412,
          "standard_error": 6.224185781942181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14080.803278617896,
            "upper_bound": 14112.121881866356
          },
          "point_estimate": 14090.244460754446,
          "standard_error": 8.18060775479212
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7129165047647805,
            "upper_bound": 35.803246785846056
          },
          "point_estimate": 21.783739341191655,
          "standard_error": 8.691179171703222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14081.742603196975,
            "upper_bound": 14100.313370085833
          },
          "point_estimate": 14090.340458145964,
          "standard_error": 4.743824927846203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.597926807018007,
            "upper_bound": 26.020330818741936
          },
          "point_estimate": 20.716197930328356,
          "standard_error": 3.666574939259324
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.977840142384544,
            "upper_bound": 36.03282131399416
          },
          "point_estimate": 36.00484599224227,
          "standard_error": 0.014130018783421915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.963973044294846,
            "upper_bound": 36.05675522843094
          },
          "point_estimate": 35.993245224406344,
          "standard_error": 0.02478577702834992
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008881230542495662,
            "upper_bound": 0.0736678025596316
          },
          "point_estimate": 0.05125009930396627,
          "standard_error": 0.01854976704633758
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.97128791929524,
            "upper_bound": 36.03188604920142
          },
          "point_estimate": 35.99219748244085,
          "standard_error": 0.01561957346454316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029495702439136167,
            "upper_bound": 0.05541810471208524
          },
          "point_estimate": 0.046988331912600864,
          "standard_error": 0.006623526098447444
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23331748.794047616,
            "upper_bound": 23366609.26457143
          },
          "point_estimate": 23347482.02571429,
          "standard_error": 9007.40669467979
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23328540.67857143,
            "upper_bound": 23364961.9
          },
          "point_estimate": 23338374.083333336,
          "standard_error": 8634.94542362875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4726.6963910854165,
            "upper_bound": 44566.69575378188
          },
          "point_estimate": 19440.8145448571,
          "standard_error": 10246.99331537172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23324610.136666667,
            "upper_bound": 23350090.696023516
          },
          "point_estimate": 23338238.596103895,
          "standard_error": 6730.1129951684525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10668.108466491089,
            "upper_bound": 40722.16000381186
          },
          "point_estimate": 30010.545697917783,
          "standard_error": 8104.378277664978
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81005818.2475,
            "upper_bound": 81098757.53333333
          },
          "point_estimate": 81047607.93333334,
          "standard_error": 23991.957012321574
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80991539.33333333,
            "upper_bound": 81102893.33333333
          },
          "point_estimate": 81017729.33333333,
          "standard_error": 26331.924515552822
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2832.507249712944,
            "upper_bound": 125029.13838027803
          },
          "point_estimate": 44834.07030403246,
          "standard_error": 29963.83321226175
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17530.68038177851,
            "upper_bound": 108125.22245084308
          },
          "point_estimate": 79937.63354969841,
          "standard_error": 21825.40685945647
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.046307761721234,
            "upper_bound": 9.057775689054523
          },
          "point_estimate": 9.051990670510367,
          "standard_error": 0.002940329330155842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044499471497984,
            "upper_bound": 9.058674086025093
          },
          "point_estimate": 9.052042200998056,
          "standard_error": 0.0040687182818024215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002732693748419749,
            "upper_bound": 0.016806148980674167
          },
          "point_estimate": 0.010507641562397889,
          "standard_error": 0.003536870786866742
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04546122443188,
            "upper_bound": 9.058109400826025
          },
          "point_estimate": 9.052084893916714,
          "standard_error": 0.0032090742633098636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0055767204265508965,
            "upper_bound": 0.0125334628130618
          },
          "point_estimate": 0.009811858819305751,
          "standard_error": 0.001798946985791335
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.03899540909965,
            "upper_bound": 9.050506887521829
          },
          "point_estimate": 9.045161985742286,
          "standard_error": 0.0029591644416012454
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.040778245889197,
            "upper_bound": 9.052583245363746
          },
          "point_estimate": 9.0456202387717,
          "standard_error": 0.0029202072486859687
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007404894806286739,
            "upper_bound": 0.015145601095404932
          },
          "point_estimate": 0.008751045955121089,
          "standard_error": 0.0034524650946382344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044133230281066,
            "upper_bound": 9.054663852776171
          },
          "point_estimate": 9.049126040460589,
          "standard_error": 0.002745105900776408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004282556995562317,
            "upper_bound": 0.013503567724408827
          },
          "point_estimate": 0.009869200197815356,
          "standard_error": 0.002550627283029142
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.040299513744312,
            "upper_bound": 9.057856419645216
          },
          "point_estimate": 9.047689390026935,
          "standard_error": 0.0045629244251218825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.040014649665352,
            "upper_bound": 9.049986661185908
          },
          "point_estimate": 9.045190795399623,
          "standard_error": 0.002786597480598774
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012925385195656051,
            "upper_bound": 0.015472149644621068
          },
          "point_estimate": 0.005960545428023984,
          "standard_error": 0.003488927873312149
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04068750902479,
            "upper_bound": 9.048891260169968
          },
          "point_estimate": 9.044810280030523,
          "standard_error": 0.0021894832037501243
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035057731527360224,
            "upper_bound": 0.02252344683088745
          },
          "point_estimate": 0.015172317215777262,
          "standard_error": 0.005775463226016931
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.80145526403479,
            "upper_bound": 8.814043223369746
          },
          "point_estimate": 8.807232932315259,
          "standard_error": 0.0032433902410618953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.798911563571103,
            "upper_bound": 8.81285197695794
          },
          "point_estimate": 8.804823116967185,
          "standard_error": 0.003543858345794254
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021020904205470616,
            "upper_bound": 0.016488308776413815
          },
          "point_estimate": 0.0092194399364633,
          "standard_error": 0.00354130219485829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.801529405406134,
            "upper_bound": 8.809521594678856
          },
          "point_estimate": 8.805552019503619,
          "standard_error": 0.0019943809762991826
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004780175695511172,
            "upper_bound": 0.014720467161819227
          },
          "point_estimate": 0.01081210090447937,
          "standard_error": 0.0027755139724479293
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.530966607519186,
            "upper_bound": 9.540490397040331
          },
          "point_estimate": 9.535232455622843,
          "standard_error": 0.002462353801558194
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.52944859135726,
            "upper_bound": 9.540187013858038
          },
          "point_estimate": 9.531475890455027,
          "standard_error": 0.0029485439656532888
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007444120454527922,
            "upper_bound": 0.011904288253373434
          },
          "point_estimate": 0.00396718420149648,
          "standard_error": 0.0032164736562546445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.530152026201112,
            "upper_bound": 9.535607389099797
          },
          "point_estimate": 9.531918181621554,
          "standard_error": 0.0014131975314070498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002837402258151898,
            "upper_bound": 0.011204874430182483
          },
          "point_estimate": 0.008184622594031072,
          "standard_error": 0.002291015011930235
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.45766762890521,
            "upper_bound": 10.469658156184874
          },
          "point_estimate": 10.46369875091378,
          "standard_error": 0.0030767272446790526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.45589923205502,
            "upper_bound": 10.472367042695296
          },
          "point_estimate": 10.464366019925231,
          "standard_error": 0.003951522683255844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030148442492103395,
            "upper_bound": 0.017513203321464648
          },
          "point_estimate": 0.010463164233128515,
          "standard_error": 0.00380932681293241
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.456225601526477,
            "upper_bound": 10.465609387054728
          },
          "point_estimate": 10.46198758498917,
          "standard_error": 0.0023891834314855457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005567713362248905,
            "upper_bound": 0.013228369498436754
          },
          "point_estimate": 0.010262739925694866,
          "standard_error": 0.0019718942119012184
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.065729807046035,
            "upper_bound": 8.074970577829001
          },
          "point_estimate": 8.070460181133088,
          "standard_error": 0.0023601439824786027
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.063592459852703,
            "upper_bound": 8.076812593081655
          },
          "point_estimate": 8.071737626187229,
          "standard_error": 0.0031079437498504068
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001692785686200241,
            "upper_bound": 0.013856549402508056
          },
          "point_estimate": 0.007343061663339963,
          "standard_error": 0.0030548392659602154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.06646424635326,
            "upper_bound": 8.073890426014088
          },
          "point_estimate": 8.070645423003343,
          "standard_error": 0.0018916910402134823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004267644473023738,
            "upper_bound": 0.009883048930038852
          },
          "point_estimate": 0.007868516879881424,
          "standard_error": 0.0014223328316337756
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.538814101390985,
            "upper_bound": 9.54730015433842
          },
          "point_estimate": 9.54317432976978,
          "standard_error": 0.002176064784427585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.536886505659457,
            "upper_bound": 9.548993980749117
          },
          "point_estimate": 9.543779844729912,
          "standard_error": 0.0029139464874965897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019143467404312195,
            "upper_bound": 0.013028953628104797
          },
          "point_estimate": 0.006956833142138541,
          "standard_error": 0.002853834436364087
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.536631344012658,
            "upper_bound": 9.548396708843942
          },
          "point_estimate": 9.543028222853511,
          "standard_error": 0.0031108047236363835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003974298679261878,
            "upper_bound": 0.009004084141291379
          },
          "point_estimate": 0.007243814845376642,
          "standard_error": 0.0012839580222752276
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.547008265290753,
            "upper_bound": 12.565489947080168
          },
          "point_estimate": 12.55498896817402,
          "standard_error": 0.004815927648585876
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.54488855019214,
            "upper_bound": 12.562457423009056
          },
          "point_estimate": 12.550201736442649,
          "standard_error": 0.004437878192606744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023730143443456675,
            "upper_bound": 0.020201423111932293
          },
          "point_estimate": 0.009892299214490682,
          "standard_error": 0.004455176126738162
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.547597961263037,
            "upper_bound": 12.558081710157165
          },
          "point_estimate": 12.55167963554266,
          "standard_error": 0.0026786098215997754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005512811622474041,
            "upper_bound": 0.023100520034904277
          },
          "point_estimate": 0.01601999529091912,
          "standard_error": 0.005313437852536984
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04925967470525,
            "upper_bound": 9.063788308846783
          },
          "point_estimate": 9.05631719741336,
          "standard_error": 0.0037123752963893887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04596082577045,
            "upper_bound": 9.064895732726146
          },
          "point_estimate": 9.056090055820423,
          "standard_error": 0.0047916089769408365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004120473153155401,
            "upper_bound": 0.021318561760274
          },
          "point_estimate": 0.011227750366435543,
          "standard_error": 0.004422764452979958
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045672601558778,
            "upper_bound": 9.062101739935942
          },
          "point_estimate": 9.054735615592788,
          "standard_error": 0.004246956611684024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006752729794574713,
            "upper_bound": 0.015978358149197548
          },
          "point_estimate": 0.012387595188898676,
          "standard_error": 0.0024055414764512445
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.217775979286174,
            "upper_bound": 10.253409697464338
          },
          "point_estimate": 10.235787486649162,
          "standard_error": 0.00916177191349128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.20759833190754,
            "upper_bound": 10.26115502076339
          },
          "point_estimate": 10.23920802727485,
          "standard_error": 0.018116713212522163
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038044473364602103,
            "upper_bound": 0.0470593872469674
          },
          "point_estimate": 0.03933397739634832,
          "standard_error": 0.01168601391537771
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.211643915790743,
            "upper_bound": 10.250181739700686
          },
          "point_estimate": 10.233687103528595,
          "standard_error": 0.009889739066288492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020325727047017753,
            "upper_bound": 0.035493794316232445
          },
          "point_estimate": 0.030424664559161717,
          "standard_error": 0.0038741321941387786
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.467784067043446,
            "upper_bound": 19.501938179302535
          },
          "point_estimate": 19.484401237318927,
          "standard_error": 0.00876175355357015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.46249390795795,
            "upper_bound": 19.51059538665053
          },
          "point_estimate": 19.474171962057063,
          "standard_error": 0.01401809524719029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002237833817564691,
            "upper_bound": 0.04633877589527969
          },
          "point_estimate": 0.03417861530803332,
          "standard_error": 0.012191883176452636
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.469083731235077,
            "upper_bound": 19.49088425734424
          },
          "point_estimate": 19.477452168384424,
          "standard_error": 0.0054959505834573695
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01735748222615451,
            "upper_bound": 0.03623668849392525
          },
          "point_estimate": 0.02917638660818836,
          "standard_error": 0.0049150184949867115
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99122.35534903336,
            "upper_bound": 99230.33805177112
          },
          "point_estimate": 99176.67537206436,
          "standard_error": 27.546086085894252
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99134.15747048138,
            "upper_bound": 99224.20817438692
          },
          "point_estimate": 99177.74029291554,
          "standard_error": 29.06004033055537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7497835289166884,
            "upper_bound": 154.727399705351
          },
          "point_estimate": 57.51344976232314,
          "standard_error": 36.22947056304108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99139.75818287655,
            "upper_bound": 99216.90296855636
          },
          "point_estimate": 99179.1976503061,
          "standard_error": 20.54350767090982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.26124504111524,
            "upper_bound": 125.50350925679604
          },
          "point_estimate": 91.79897026675565,
          "standard_error": 23.774161776639183
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52426.74641757713,
            "upper_bound": 52514.07692806036
          },
          "point_estimate": 52470.81136105957,
          "standard_error": 22.266807885654945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52405.35367278225,
            "upper_bound": 52529.09000721501
          },
          "point_estimate": 52466.45747955747,
          "standard_error": 28.625350736137506
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.113709085369427,
            "upper_bound": 136.25557293452042
          },
          "point_estimate": 70.55901152511643,
          "standard_error": 32.10092093568041
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52423.52497697279,
            "upper_bound": 52496.124562052646
          },
          "point_estimate": 52457.67003242068,
          "standard_error": 17.95706047614714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.28178306952379,
            "upper_bound": 93.89279224495864
          },
          "point_estimate": 74.42649146911187,
          "standard_error": 13.48774210073913
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143973.86960850743,
            "upper_bound": 144128.69299193018
          },
          "point_estimate": 144050.86919427192,
          "standard_error": 39.58306462969343
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143933.69486166007,
            "upper_bound": 144168.6947738252
          },
          "point_estimate": 144054.97801383398,
          "standard_error": 73.9141952656846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.81008677097637,
            "upper_bound": 215.5093750540291
          },
          "point_estimate": 160.00916265335326,
          "standard_error": 47.79397525451381
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143982.1603167999,
            "upper_bound": 144150.05212507606
          },
          "point_estimate": 144082.57006313844,
          "standard_error": 42.48227048982997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.90972879487799,
            "upper_bound": 155.09373548470998
          },
          "point_estimate": 132.09988948567144,
          "standard_error": 17.456354601173487
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397442.14403564954,
            "upper_bound": 398447.3444292184
          },
          "point_estimate": 397943.1156530365,
          "standard_error": 257.01172577776646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397269.3907004831,
            "upper_bound": 398405.9764492754
          },
          "point_estimate": 397978.81875,
          "standard_error": 227.9763189071149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.3158534744494,
            "upper_bound": 1600.6844956039358
          },
          "point_estimate": 385.13216348861465,
          "standard_error": 393.4258422795968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397405.06917332823,
            "upper_bound": 398043.02675585286
          },
          "point_estimate": 397773.9552230378,
          "standard_error": 160.35049055883593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325.67487843248557,
            "upper_bound": 1149.1101933433972
          },
          "point_estimate": 855.5786973783545,
          "standard_error": 194.8221157290069
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573701.1308004713,
            "upper_bound": 574871.2130096725
          },
          "point_estimate": 574260.4483525545,
          "standard_error": 300.42771935720776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573357.6171875,
            "upper_bound": 574979.0843098958
          },
          "point_estimate": 574176.8870659722,
          "standard_error": 469.3459299687896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.2625335007906,
            "upper_bound": 1811.4854668892565
          },
          "point_estimate": 1139.770351015037,
          "standard_error": 395.7155717892624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573667.0369985015,
            "upper_bound": 574507.333462733
          },
          "point_estimate": 574122.477150974,
          "standard_error": 211.37110071442075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 569.568754916276,
            "upper_bound": 1267.849187149818
          },
          "point_estimate": 1001.3886050474392,
          "standard_error": 185.31817547962217
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39674.933997404376,
            "upper_bound": 39725.17406800242
          },
          "point_estimate": 39700.56316879174,
          "standard_error": 12.850096254637783
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39674.25519125683,
            "upper_bound": 39733.069702489374
          },
          "point_estimate": 39703.90883424408,
          "standard_error": 14.976043368109208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.997481467291662,
            "upper_bound": 73.97978079042062
          },
          "point_estimate": 43.5991964026442,
          "standard_error": 15.903311533325082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39680.7228014867,
            "upper_bound": 39725.872218477794
          },
          "point_estimate": 39703.67456390604,
          "standard_error": 11.798709496726168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.45240980086494,
            "upper_bound": 56.846916079432134
          },
          "point_estimate": 42.76649215942413,
          "standard_error": 9.17280441832168
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98188.62493693692,
            "upper_bound": 98294.43933204631
          },
          "point_estimate": 98243.29357614755,
          "standard_error": 27.21974833295062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98156.0472972973,
            "upper_bound": 98320.61930501933
          },
          "point_estimate": 98248.09872372371,
          "standard_error": 41.880975168690135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.725833566077473,
            "upper_bound": 152.8310133677945
          },
          "point_estimate": 112.3378434560573,
          "standard_error": 38.62691431725555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98167.03501474006,
            "upper_bound": 98297.34648150454
          },
          "point_estimate": 98228.89408213408,
          "standard_error": 35.17549423333025
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.48226104336682,
            "upper_bound": 109.2261902487696
          },
          "point_estimate": 90.85386541202686,
          "standard_error": 14.33354609704291
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68252.58797470237,
            "upper_bound": 68415.20533834587
          },
          "point_estimate": 68329.32307121971,
          "standard_error": 41.82311184781545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68232.07226399332,
            "upper_bound": 68432.4336622807
          },
          "point_estimate": 68278.97587719298,
          "standard_error": 53.299920706875554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.82619003440394,
            "upper_bound": 227.0665492582427
          },
          "point_estimate": 87.68415337749784,
          "standard_error": 54.51270557768705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68208.61162328531,
            "upper_bound": 68379.17779362199
          },
          "point_estimate": 68268.74816912411,
          "standard_error": 43.527462259320856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.08780460648431,
            "upper_bound": 171.6713578685889
          },
          "point_estimate": 139.5032357237225,
          "standard_error": 27.843730890849887
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307650.00344542816,
            "upper_bound": 308288.4674576497
          },
          "point_estimate": 307960.93511871406,
          "standard_error": 163.58289542741744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307525.56512605044,
            "upper_bound": 308408.0151260504
          },
          "point_estimate": 307838.4596638655,
          "standard_error": 213.8220681822953
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.03152982427684,
            "upper_bound": 898.0369675860366
          },
          "point_estimate": 564.9504302641869,
          "standard_error": 212.5969589639068
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307721.0286451763,
            "upper_bound": 308221.7175130922
          },
          "point_estimate": 307957.61800720287,
          "standard_error": 126.37894358948674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299.59412172278763,
            "upper_bound": 687.6282971022433
          },
          "point_estimate": 545.253235430233,
          "standard_error": 99.69179218294391
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34935.23303756665,
            "upper_bound": 35018.67700941076
          },
          "point_estimate": 34972.29916899644,
          "standard_error": 21.6102331796326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34909.84985563041,
            "upper_bound": 35001.59446583253
          },
          "point_estimate": 34954.046291840445,
          "standard_error": 20.04590084858721
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4696943561060114,
            "upper_bound": 94.48709043031832
          },
          "point_estimate": 62.52153103631952,
          "standard_error": 25.78847907627251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34938.60720346906,
            "upper_bound": 34980.274272414776
          },
          "point_estimate": 34957.47710960839,
          "standard_error": 10.374158059263287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.90528278149304,
            "upper_bound": 100.58740409622904
          },
          "point_estimate": 71.81321957628006,
          "standard_error": 21.34225271992348
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65951.19262089492,
            "upper_bound": 66014.09480640899
          },
          "point_estimate": 65983.27696878691,
          "standard_error": 16.054591903484805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65937.42006957048,
            "upper_bound": 66015.222323049
          },
          "point_estimate": 65997.84264972777,
          "standard_error": 20.777670564902348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.713137181905289,
            "upper_bound": 88.54374867000422
          },
          "point_estimate": 42.313744076360635,
          "standard_error": 23.954041503250675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65925.0269453374,
            "upper_bound": 66023.06644549074
          },
          "point_estimate": 65971.47383977185,
          "standard_error": 25.752052806293012
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.19972120982353,
            "upper_bound": 68.84433940352605
          },
          "point_estimate": 53.171900718056314,
          "standard_error": 10.490546730139522
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170136.76806964842,
            "upper_bound": 170301.57047628323
          },
          "point_estimate": 170211.97769451863,
          "standard_error": 42.44408942095073
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170107.46717846015,
            "upper_bound": 170272.0605140187
          },
          "point_estimate": 170200.70420560747,
          "standard_error": 41.42879809640561
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.993711576852405,
            "upper_bound": 207.02287811590503
          },
          "point_estimate": 107.85890752826846,
          "standard_error": 46.2794878647915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170121.9972664475,
            "upper_bound": 170271.5536734063
          },
          "point_estimate": 170207.61674960554,
          "standard_error": 38.726128223505214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.43981444261057,
            "upper_bound": 196.2456182316242
          },
          "point_estimate": 141.58111731437432,
          "standard_error": 39.214351644871925
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34354.11919872084,
            "upper_bound": 34412.872738320715
          },
          "point_estimate": 34381.4148554368,
          "standard_error": 15.085246954102914
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34344.245833333334,
            "upper_bound": 34417.235400883845
          },
          "point_estimate": 34375.07022964015,
          "standard_error": 16.20467120949762
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.702050795231367,
            "upper_bound": 82.6563800879507
          },
          "point_estimate": 36.12291305897295,
          "standard_error": 19.54217439840889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34341.466416962925,
            "upper_bound": 34391.665832502134
          },
          "point_estimate": 34363.9043437623,
          "standard_error": 12.659622689643646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.01798742497138,
            "upper_bound": 65.65278843379365
          },
          "point_estimate": 50.253575179861,
          "standard_error": 11.717532525157164
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11352.129693216266,
            "upper_bound": 11387.078328560605
          },
          "point_estimate": 11369.2786202198,
          "standard_error": 8.962475122763097
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11349.860771401693,
            "upper_bound": 11390.54840299243
          },
          "point_estimate": 11362.947220915648,
          "standard_error": 11.828488966986814
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.670988013656666,
            "upper_bound": 52.14406542734723
          },
          "point_estimate": 26.025417675303164,
          "standard_error": 12.336625601781632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11358.760312206814,
            "upper_bound": 11386.580032333688
          },
          "point_estimate": 11376.270408425064,
          "standard_error": 7.126670998252188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.47685841134239,
            "upper_bound": 39.116232394720896
          },
          "point_estimate": 29.834754872134233,
          "standard_error": 6.258493311968537
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500230.1382311371,
            "upper_bound": 501579.5115601217
          },
          "point_estimate": 500829.19268699706,
          "standard_error": 349.1525244488278
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500078.3982115677,
            "upper_bound": 501189.0159817352
          },
          "point_estimate": 500698.3089041096,
          "standard_error": 267.5549342611505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.12102961295182,
            "upper_bound": 1456.468665580901
          },
          "point_estimate": 657.4902931217513,
          "standard_error": 350.59987031618516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499985.7101322115,
            "upper_bound": 500705.88507299504
          },
          "point_estimate": 500379.7453478029,
          "standard_error": 186.01751788476844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413.636381351734,
            "upper_bound": 1673.6074570238834
          },
          "point_estimate": 1169.7225567527012,
          "standard_error": 377.9014215434419
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971.822610584162,
            "upper_bound": 973.1866747359884
          },
          "point_estimate": 972.5195815607092,
          "standard_error": 0.34893123662441805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971.7292191435768,
            "upper_bound": 973.7823302427782
          },
          "point_estimate": 972.5414705295628,
          "standard_error": 0.45789820776870194
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2117300924413316,
            "upper_bound": 2.046476583976107
          },
          "point_estimate": 1.4745094839781128,
          "standard_error": 0.5327436051219641
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 972.2451685945985,
            "upper_bound": 973.1073567077704
          },
          "point_estimate": 972.6544411213418,
          "standard_error": 0.2147798242046894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6680323080481896,
            "upper_bound": 1.4650620112178026
          },
          "point_estimate": 1.1628743396289305,
          "standard_error": 0.2087191239790314
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247180.78445497248,
            "upper_bound": 247463.51158211855
          },
          "point_estimate": 247321.01987447363,
          "standard_error": 72.23385696619228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247120.38556851313,
            "upper_bound": 247478.52227891155
          },
          "point_estimate": 247313.52962018145,
          "standard_error": 82.65507780448596
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.05967975453111,
            "upper_bound": 394.02666157606507
          },
          "point_estimate": 201.4774779706701,
          "standard_error": 100.44864700828444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247270.42915285772,
            "upper_bound": 247500.297466619
          },
          "point_estimate": 247381.10138704831,
          "standard_error": 59.624157386524296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.95039353253584,
            "upper_bound": 316.2846776755778
          },
          "point_estimate": 241.2741215224035,
          "standard_error": 49.80553898960997
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195195.325395652,
            "upper_bound": 195585.1086482335
          },
          "point_estimate": 195380.35034092847,
          "standard_error": 100.00833611990863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195097.95101553167,
            "upper_bound": 195655.63279569897
          },
          "point_estimate": 195307.45545314896,
          "standard_error": 156.84907783351437
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.75780277002233,
            "upper_bound": 551.1246563446218
          },
          "point_estimate": 333.7549749617456,
          "standard_error": 138.04136876791824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195112.9125827241,
            "upper_bound": 195448.6235981624
          },
          "point_estimate": 195276.2978215333,
          "standard_error": 88.40265456998884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.7290419332891,
            "upper_bound": 414.7619709992731
          },
          "point_estimate": 332.51492545644277,
          "standard_error": 61.374823696464325
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174321.59175438597,
            "upper_bound": 174763.28044662793
          },
          "point_estimate": 174556.59682463735,
          "standard_error": 113.48167004681784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174309.82488038277,
            "upper_bound": 174833.13237639554
          },
          "point_estimate": 174572.64752791068,
          "standard_error": 112.2991504339006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.02444993308185,
            "upper_bound": 642.9805031302686
          },
          "point_estimate": 303.53022555226187,
          "standard_error": 164.74807488300672
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174387.28719831564,
            "upper_bound": 174719.96848073628
          },
          "point_estimate": 174573.48015907538,
          "standard_error": 83.34682417236976
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.11646148188586,
            "upper_bound": 496.6603018005051
          },
          "point_estimate": 378.7020408574979,
          "standard_error": 87.82704675512936
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/libc_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37381.91297778716,
            "upper_bound": 37452.38183722409
          },
          "point_estimate": 37415.62900642752,
          "standard_error": 18.02553757395314
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37369.57429770469,
            "upper_bound": 37443.53522895969
          },
          "point_estimate": 37418.32029804728,
          "standard_error": 18.14196298745987
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.39115161408483,
            "upper_bound": 102.73041008777784
          },
          "point_estimate": 40.716246735177045,
          "standard_error": 23.194188571880495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37366.149075240704,
            "upper_bound": 37426.56951268604
          },
          "point_estimate": 37400.007103482334,
          "standard_error": 15.399033139333394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.94975572487635,
            "upper_bound": 82.31710294107788
          },
          "point_estimate": 60.249761669465634,
          "standard_error": 14.809581625204055
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71619.96638132734,
            "upper_bound": 71715.4021238517
          },
          "point_estimate": 71666.1969653012,
          "standard_error": 24.448136736020768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71628.67093175853,
            "upper_bound": 71720.80228838584
          },
          "point_estimate": 71648.32955255592,
          "standard_error": 19.40294470830468
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.791520679950849,
            "upper_bound": 152.76354071308836
          },
          "point_estimate": 29.829152659411925,
          "standard_error": 35.77048863426959
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71638.25253501622,
            "upper_bound": 71713.56041927046
          },
          "point_estimate": 71673.2940791492,
          "standard_error": 19.579720402684806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.30895456900091,
            "upper_bound": 109.77823970397203
          },
          "point_estimate": 81.53999762824967,
          "standard_error": 20.22296873360679
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68677.72628799474,
            "upper_bound": 68744.39897996743
          },
          "point_estimate": 68710.16295916225,
          "standard_error": 17.172997143984677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68657.62717916405,
            "upper_bound": 68763.5568458007
          },
          "point_estimate": 68707.46384688091,
          "standard_error": 26.04936960389066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.935693431892229,
            "upper_bound": 100.62933230294344
          },
          "point_estimate": 66.19706897284907,
          "standard_error": 24.28182249719242
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68673.10741657432,
            "upper_bound": 68765.30166885082
          },
          "point_estimate": 68719.4173814843,
          "standard_error": 24.912789317109777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.261754673083104,
            "upper_bound": 69.81609710330875
          },
          "point_estimate": 57.14336254231049,
          "standard_error": 9.557271270054963
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82424.46142937944,
            "upper_bound": 82554.3976662492
          },
          "point_estimate": 82482.83934793148,
          "standard_error": 33.438243398132656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82420.32159017453,
            "upper_bound": 82523.74366515837
          },
          "point_estimate": 82463.71361236802,
          "standard_error": 27.361967798933104
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.033728246503877,
            "upper_bound": 158.0274353166319
          },
          "point_estimate": 69.04191707177043,
          "standard_error": 35.10617709226718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82433.95901701784,
            "upper_bound": 82485.78667853201
          },
          "point_estimate": 82460.93077510725,
          "standard_error": 13.08077466945913
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.106465060995234,
            "upper_bound": 156.59360153543545
          },
          "point_estimate": 111.2510909754139,
          "standard_error": 32.90674104808127
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321359.3610085631,
            "upper_bound": 321706.78488304093
          },
          "point_estimate": 321526.32146825397,
          "standard_error": 89.03715738024616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321278.8743734336,
            "upper_bound": 321778.07675438595
          },
          "point_estimate": 321468.75292397663,
          "standard_error": 151.9413321710817
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.62555773115496,
            "upper_bound": 486.1491915884336
          },
          "point_estimate": 324.0003939846656,
          "standard_error": 113.01557871214176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321326.56278491503,
            "upper_bound": 321712.91056941776
          },
          "point_estimate": 321526.5068580542,
          "standard_error": 98.44715243608184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.70740104581418,
            "upper_bound": 359.79158917441714
          },
          "point_estimate": 296.50009126967853,
          "standard_error": 46.28451552286494
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47904.176221991875,
            "upper_bound": 47981.96187951533
          },
          "point_estimate": 47940.25659719604,
          "standard_error": 20.04111469115277
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47896.818381706245,
            "upper_bound": 47984.0889676781
          },
          "point_estimate": 47915.66844662227,
          "standard_error": 21.142532548248884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.20654965691397,
            "upper_bound": 102.24502314520846
          },
          "point_estimate": 31.102204246637505,
          "standard_error": 27.27655265105375
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47908.21313149382,
            "upper_bound": 47937.05210462273
          },
          "point_estimate": 47919.203769317755,
          "standard_error": 7.32367885756601
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.74497613488086,
            "upper_bound": 84.44305984953924
          },
          "point_estimate": 66.76284139630265,
          "standard_error": 15.380188560966902
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34122.831819319144,
            "upper_bound": 34229.31240877878
          },
          "point_estimate": 34175.76157466002,
          "standard_error": 27.2670550280864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34116.08945962928,
            "upper_bound": 34268.95651003097
          },
          "point_estimate": 34153.70440622055,
          "standard_error": 37.0259377593816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.220544487583696,
            "upper_bound": 170.87276607572608
          },
          "point_estimate": 73.92912804659173,
          "standard_error": 41.39269787135525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34125.33764591972,
            "upper_bound": 34243.641494596486
          },
          "point_estimate": 34171.594961871306,
          "standard_error": 30.024700409696727
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.58115391380997,
            "upper_bound": 114.4787193684021
          },
          "point_estimate": 90.4166343452896,
          "standard_error": 16.281769271471546
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37104.167108313915,
            "upper_bound": 37160.06090332994
          },
          "point_estimate": 37134.222233386725,
          "standard_error": 14.33020143798659
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37106.62104994903,
            "upper_bound": 37174.41522256201
          },
          "point_estimate": 37144.47798165138,
          "standard_error": 18.60891460996708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.101476696467325,
            "upper_bound": 81.2280027322177
          },
          "point_estimate": 47.14247266967868,
          "standard_error": 16.564857919141502
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37121.02710707421,
            "upper_bound": 37167.69642218546
          },
          "point_estimate": 37146.12235063611,
          "standard_error": 11.837956877119366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.06699117862025,
            "upper_bound": 64.33087211874034
          },
          "point_estimate": 47.724882862082104,
          "standard_error": 11.659984111737794
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83424.95742181811,
            "upper_bound": 83508.54629440168
          },
          "point_estimate": 83470.94644486312,
          "standard_error": 21.548282741859367
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83445.5186353211,
            "upper_bound": 83520.25840978594
          },
          "point_estimate": 83476.0369266055,
          "standard_error": 23.786163429492913
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8908675409782068,
            "upper_bound": 104.5518619970548
          },
          "point_estimate": 48.98738521593147,
          "standard_error": 24.012602115716184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83463.4144967041,
            "upper_bound": 83523.63286934515
          },
          "point_estimate": 83491.58129393542,
          "standard_error": 15.303687344556776
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.4151043842769,
            "upper_bound": 101.7051573456611
          },
          "point_estimate": 71.95879988127025,
          "standard_error": 21.4135440000689
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65184.04409839564,
            "upper_bound": 65256.903268049144
          },
          "point_estimate": 65219.70588652786,
          "standard_error": 18.685983007734155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65156.952508960574,
            "upper_bound": 65271.711053507424
          },
          "point_estimate": 65215.874193548385,
          "standard_error": 31.191529046705305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.356235371695943,
            "upper_bound": 106.4063538367034
          },
          "point_estimate": 82.08539337542778,
          "standard_error": 24.992761851648343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65176.30917665397,
            "upper_bound": 65259.00845730839
          },
          "point_estimate": 65222.683503235115,
          "standard_error": 20.866765053724993
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.15149826928346,
            "upper_bound": 74.36090529287272
          },
          "point_estimate": 62.11057984798911,
          "standard_error": 9.392225618144336
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148783.11640481043,
            "upper_bound": 149026.9594437075
          },
          "point_estimate": 148902.15386491737,
          "standard_error": 62.52978923040938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148727.97295918368,
            "upper_bound": 149120.65782312927
          },
          "point_estimate": 148811.1224489796,
          "standard_error": 123.95765373237388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.180655222936167,
            "upper_bound": 320.3800567610692
          },
          "point_estimate": 206.5571899042827,
          "standard_error": 83.69773955286797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148724.24542040817,
            "upper_bound": 149004.56647934948
          },
          "point_estimate": 148848.80696527963,
          "standard_error": 73.42591956045453
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.55695751740055,
            "upper_bound": 236.70922218722268
          },
          "point_estimate": 208.3573472978308,
          "standard_error": 26.23734586828406
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198946.0012782982,
            "upper_bound": 199210.79634812212
          },
          "point_estimate": 199077.45937180155,
          "standard_error": 67.83460891622171
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198906.23922282935,
            "upper_bound": 199255.98797814208
          },
          "point_estimate": 199072.9538641686,
          "standard_error": 87.65323435696642
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.14421354844453,
            "upper_bound": 384.3504150616863
          },
          "point_estimate": 259.26874771036364,
          "standard_error": 84.05197820697335
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198998.55582119903,
            "upper_bound": 199282.51050319296
          },
          "point_estimate": 199157.61362571857,
          "standard_error": 73.35918731337394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.74740571948854,
            "upper_bound": 285.85578541657674
          },
          "point_estimate": 226.30507356663475,
          "standard_error": 40.212423955103645
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119512.32549159355,
            "upper_bound": 119717.85303884713
          },
          "point_estimate": 119602.87290544069,
          "standard_error": 52.971473703804165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119484.4144736842,
            "upper_bound": 119691.74698464911
          },
          "point_estimate": 119551.41233552631,
          "standard_error": 47.23168822601852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.600743740878055,
            "upper_bound": 245.9086864346981
          },
          "point_estimate": 104.32679674979504,
          "standard_error": 56.367625322081835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119500.44097115973,
            "upper_bound": 119612.35669045497
          },
          "point_estimate": 119555.55798017772,
          "standard_error": 28.320033817775787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.37580891833568,
            "upper_bound": 246.55168422655285
          },
          "point_estimate": 175.68074752750175,
          "standard_error": 53.262387078659785
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52673.25948464384,
            "upper_bound": 52797.21532830697
          },
          "point_estimate": 52731.732123618865,
          "standard_error": 31.847430597357604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52652.55643994211,
            "upper_bound": 52814.390427951206
          },
          "point_estimate": 52685.71215629522,
          "standard_error": 46.47520318923493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.095328455210538,
            "upper_bound": 177.83950668352742
          },
          "point_estimate": 88.12216109470207,
          "standard_error": 44.218808543003504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52649.55962000153,
            "upper_bound": 52741.358245587086
          },
          "point_estimate": 52686.48394760088,
          "standard_error": 23.225953030397175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.701485363374225,
            "upper_bound": 133.78613429330306
          },
          "point_estimate": 106.17507002559732,
          "standard_error": 20.827573489917818
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77342.91784160756,
            "upper_bound": 77497.40197264438
          },
          "point_estimate": 77419.61717190138,
          "standard_error": 39.48244561488136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77337.78723404255,
            "upper_bound": 77515.34340425531
          },
          "point_estimate": 77399.06361702128,
          "standard_error": 53.016101334137375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.589052819188993,
            "upper_bound": 223.01274061520056
          },
          "point_estimate": 149.37580280904243,
          "standard_error": 54.92613588138102
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77336.7667499643,
            "upper_bound": 77523.44335362934
          },
          "point_estimate": 77433.52776457585,
          "standard_error": 47.697172427338806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.88395191455989,
            "upper_bound": 173.00386270351433
          },
          "point_estimate": 132.1425067026237,
          "standard_error": 27.08042887772888
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40812.7945301409,
            "upper_bound": 40899.809892812555
          },
          "point_estimate": 40856.6938719012,
          "standard_error": 22.247773196971675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40803.89801029963,
            "upper_bound": 40915.04662921348
          },
          "point_estimate": 40857.03382022472,
          "standard_error": 29.385522897285465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.010523930361984,
            "upper_bound": 127.24021875601734
          },
          "point_estimate": 73.9467559905532,
          "standard_error": 26.696754398826897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40831.435592831025,
            "upper_bound": 40927.07607145798
          },
          "point_estimate": 40873.85131183423,
          "standard_error": 25.834569052648497
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.111461911602674,
            "upper_bound": 96.0268335078125
          },
          "point_estimate": 74.01633259983906,
          "standard_error": 14.440302254775755
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93563.62603939691,
            "upper_bound": 93691.40940659812
          },
          "point_estimate": 93628.14209766596,
          "standard_error": 32.588364099744155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93545.07476435303,
            "upper_bound": 93721.3735218509
          },
          "point_estimate": 93625.841879361,
          "standard_error": 39.61620678300739
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.448307689347335,
            "upper_bound": 201.9337371604693
          },
          "point_estimate": 108.02170049868162,
          "standard_error": 49.1364102529212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93582.7497451467,
            "upper_bound": 93722.40275161598
          },
          "point_estimate": 93668.26409374688,
          "standard_error": 35.59160686518947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.521623396082205,
            "upper_bound": 137.062591435887
          },
          "point_estimate": 108.74524464799454,
          "standard_error": 19.568961331446303
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319160.3460773807,
            "upper_bound": 1321254.9720684525
          },
          "point_estimate": 1320178.2749404763,
          "standard_error": 536.2934117410282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1318851.0595238097,
            "upper_bound": 1321200.4910714286
          },
          "point_estimate": 1320145.2053571427,
          "standard_error": 565.0923596635218
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.10427047107603,
            "upper_bound": 3092.8275363411935
          },
          "point_estimate": 1741.6335753299136,
          "standard_error": 720.5277414833207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319741.43355295,
            "upper_bound": 1320779.2847942456
          },
          "point_estimate": 1320302.79851577,
          "standard_error": 264.5955849405608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 894.3225766251252,
            "upper_bound": 2385.0652320870263
          },
          "point_estimate": 1787.9425945176065,
          "standard_error": 390.0808267988608
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156972.36596982757,
            "upper_bound": 157220.61465050286
          },
          "point_estimate": 157099.28097701148,
          "standard_error": 63.782256941082935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156888.97984913795,
            "upper_bound": 157297.7514367816
          },
          "point_estimate": 157147.138829023,
          "standard_error": 115.24741290400262
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.64628282805845,
            "upper_bound": 353.57373283920265
          },
          "point_estimate": 261.38897889390205,
          "standard_error": 84.56296399683299
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156987.20276895288,
            "upper_bound": 157271.8241800436
          },
          "point_estimate": 157146.22398119123,
          "standard_error": 74.04097178375896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.38429864952587,
            "upper_bound": 248.61555383030776
          },
          "point_estimate": 213.0548427167679,
          "standard_error": 29.917803051254644
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6787.961388540862,
            "upper_bound": 6808.023815493963
          },
          "point_estimate": 6798.181698611149,
          "standard_error": 5.139708738530749
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6787.575312558313,
            "upper_bound": 6816.359830814206
          },
          "point_estimate": 6795.504201210248,
          "standard_error": 7.465863549830445
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.536285406495906,
            "upper_bound": 32.79459525655507
          },
          "point_estimate": 15.92584601321022,
          "standard_error": 7.276187568232051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6793.39881622574,
            "upper_bound": 6814.979876775248
          },
          "point_estimate": 6804.563113878098,
          "standard_error": 5.546236971692622
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.042348804179603,
            "upper_bound": 21.90014602159526
          },
          "point_estimate": 17.171578976978605,
          "standard_error": 3.1326009670097994
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7422.731494826619,
            "upper_bound": 7431.903935515671
          },
          "point_estimate": 7427.389593405724,
          "standard_error": 2.3448770315187373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7419.644182861804,
            "upper_bound": 7433.40472669794
          },
          "point_estimate": 7427.746695900469,
          "standard_error": 3.4330294594291986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1729186157259115,
            "upper_bound": 13.63348342969028
          },
          "point_estimate": 8.681079449184827,
          "standard_error": 3.189516947821642
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7421.296097549758,
            "upper_bound": 7434.619620806317
          },
          "point_estimate": 7428.761491374218,
          "standard_error": 3.402940792312449
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.435475759393088,
            "upper_bound": 9.580201364329175
          },
          "point_estimate": 7.8003334413340175,
          "standard_error": 1.2839560992743395
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47596.726484272614,
            "upper_bound": 47637.39275571054
          },
          "point_estimate": 47616.8057129855,
          "standard_error": 10.408906131107264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47592.20292704237,
            "upper_bound": 47642.98845019659
          },
          "point_estimate": 47613.55225613181,
          "standard_error": 10.816492300904889
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.222450715218634,
            "upper_bound": 59.4422842657928
          },
          "point_estimate": 24.54002404991825,
          "standard_error": 14.854378573597192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47581.69979995861,
            "upper_bound": 47621.41682173794
          },
          "point_estimate": 47603.65294548178,
          "standard_error": 10.679951927171112
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.38795786164414,
            "upper_bound": 45.35909684926981
          },
          "point_estimate": 34.70203469614309,
          "standard_error": 7.053858920831099
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.8731566080335,
            "upper_bound": 116.0317283781663
          },
          "point_estimate": 115.9521481297306,
          "standard_error": 0.04057725564921718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.85836857397658,
            "upper_bound": 116.05145107277524
          },
          "point_estimate": 115.93986441656244,
          "standard_error": 0.04649502810328174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009537021780567258,
            "upper_bound": 0.23175463007007693
          },
          "point_estimate": 0.13639453911452637,
          "standard_error": 0.05669343726251383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.90488269131843,
            "upper_bound": 116.0853242051847
          },
          "point_estimate": 116.004130128252,
          "standard_error": 0.045677025269320805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07505377580407979,
            "upper_bound": 0.17259249497072593
          },
          "point_estimate": 0.13551457352019605,
          "standard_error": 0.025121946445343445
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.074866237893573,
            "upper_bound": 19.11463363811008
          },
          "point_estimate": 19.09366866475923,
          "standard_error": 0.01012613180392044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.07477906048597,
            "upper_bound": 19.108121200697667
          },
          "point_estimate": 19.091033664001927,
          "standard_error": 0.008865933002326078
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006680236300519388,
            "upper_bound": 0.05237496869673964
          },
          "point_estimate": 0.023048716714270355,
          "standard_error": 0.010990572124783129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.0773823426359,
            "upper_bound": 19.09710434900934
          },
          "point_estimate": 19.08809317863011,
          "standard_error": 0.005066955942855456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012735378843219522,
            "upper_bound": 0.04762675610199189
          },
          "point_estimate": 0.03387669228230651,
          "standard_error": 0.009268862521475636
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.39442385082689,
            "upper_bound": 19.45777024253254
          },
          "point_estimate": 19.425282288498178,
          "standard_error": 0.0160993127181045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.392094027353004,
            "upper_bound": 19.44949086766151
          },
          "point_estimate": 19.42772552465179,
          "standard_error": 0.016165180318768093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006668564811784248,
            "upper_bound": 0.0875033128364095
          },
          "point_estimate": 0.03409261495309407,
          "standard_error": 0.01946262516543337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.407334997656896,
            "upper_bound": 19.473776225885633
          },
          "point_estimate": 19.437065256766523,
          "standard_error": 0.01683010381885436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02183401145839247,
            "upper_bound": 0.0746543120554527
          },
          "point_estimate": 0.05352753524586363,
          "standard_error": 0.013658113926705163
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.62080768946318,
            "upper_bound": 18.661500353088822
          },
          "point_estimate": 18.64148030636384,
          "standard_error": 0.010397884331792609
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.61909189022589,
            "upper_bound": 18.661900861962863
          },
          "point_estimate": 18.64233397187861,
          "standard_error": 0.011395145027571498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005131165394696499,
            "upper_bound": 0.05844953594958684
          },
          "point_estimate": 0.02382455288380376,
          "standard_error": 0.013006580647751816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.63956162113332,
            "upper_bound": 18.68023388313399
          },
          "point_estimate": 18.66011877144624,
          "standard_error": 0.01056600377770421
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015708972314441214,
            "upper_bound": 0.046448223251708505
          },
          "point_estimate": 0.034611829291282856,
          "standard_error": 0.007780474657549481
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.94062970672735,
            "upper_bound": 18.313342355372313
          },
          "point_estimate": 18.119243899228984,
          "standard_error": 0.0955949140224591
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.88521941948444,
            "upper_bound": 18.38655588033633
          },
          "point_estimate": 18.06280665920874,
          "standard_error": 0.10054358946344027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.033751783316520255,
            "upper_bound": 0.5990449120116681
          },
          "point_estimate": 0.184034439693613,
          "standard_error": 0.14375405919933387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.88713100130789,
            "upper_bound": 18.438665578604567
          },
          "point_estimate": 18.124192553287717,
          "standard_error": 0.1423908306180059
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1475145579297849,
            "upper_bound": 0.405849341711356
          },
          "point_estimate": 0.31863648638254544,
          "standard_error": 0.06552507053094483
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.937920140105337,
            "upper_bound": 22.97761083193313
          },
          "point_estimate": 22.959610530686305,
          "standard_error": 0.010227839758094168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.939439811388898,
            "upper_bound": 22.983228745822604
          },
          "point_estimate": 22.975060866732164,
          "standard_error": 0.014044850990531123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006816739030071317,
            "upper_bound": 0.05660533522692188
          },
          "point_estimate": 0.019288662679373937,
          "standard_error": 0.014266742552932166
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.96316111906011,
            "upper_bound": 22.98433366301512
          },
          "point_estimate": 22.97561661719188,
          "standard_error": 0.005386656545887102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014421277435472009,
            "upper_bound": 0.04674232777387771
          },
          "point_estimate": 0.034129370619688675,
          "standard_error": 0.009126955873293909
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.389433247702407,
            "upper_bound": 26.4312813047374
          },
          "point_estimate": 26.4090403436987,
          "standard_error": 0.010782074298980916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.382194589684534,
            "upper_bound": 26.441556492807074
          },
          "point_estimate": 26.393939063802428,
          "standard_error": 0.015017456880510156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002401834376636384,
            "upper_bound": 0.055425033736847415
          },
          "point_estimate": 0.030856384004049567,
          "standard_error": 0.014071218586295672
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.386317779068857,
            "upper_bound": 26.43598366753671
          },
          "point_estimate": 26.415610860498894,
          "standard_error": 0.012816151459585846
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01670170663608032,
            "upper_bound": 0.04531206314326115
          },
          "point_estimate": 0.035861598152173414,
          "standard_error": 0.007274450549533158
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.16257842656089,
            "upper_bound": 24.210880157249463
          },
          "point_estimate": 24.18523293319972,
          "standard_error": 0.012463485750567072
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.15264355773684,
            "upper_bound": 24.22953785128412
          },
          "point_estimate": 24.169716955761732,
          "standard_error": 0.01842286115093451
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006039917545495983,
            "upper_bound": 0.06553612199100008
          },
          "point_estimate": 0.027328245021526525,
          "standard_error": 0.015212537701734207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.1577980257831,
            "upper_bound": 24.22788479731832
          },
          "point_estimate": 24.193988874511263,
          "standard_error": 0.017899234774159684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013584559315157556,
            "upper_bound": 0.05014763231839075
          },
          "point_estimate": 0.04152567090811403,
          "standard_error": 0.007957947369496897
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.989928985743727,
            "upper_bound": 29.046811570768256
          },
          "point_estimate": 29.0163137074088,
          "standard_error": 0.014557870328833704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.976464709835717,
            "upper_bound": 29.047133858681125
          },
          "point_estimate": 29.01182977382623,
          "standard_error": 0.014279151927713849
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007813304022944983,
            "upper_bound": 0.07779464837983925
          },
          "point_estimate": 0.03547308909542663,
          "standard_error": 0.020278979766048616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.987416344594138,
            "upper_bound": 29.04384796691959
          },
          "point_estimate": 29.01621905785655,
          "standard_error": 0.014510920556254909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020772975834651285,
            "upper_bound": 0.0644322723035725
          },
          "point_estimate": 0.048421866673818016,
          "standard_error": 0.011743838724181523
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.58542714875741,
            "upper_bound": 37.65429925180318
          },
          "point_estimate": 37.61855640644606,
          "standard_error": 0.01765118417489017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.55870782008067,
            "upper_bound": 37.664351139899864
          },
          "point_estimate": 37.611228993101406,
          "standard_error": 0.022853325663567847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008610903430707794,
            "upper_bound": 0.09456463901580084
          },
          "point_estimate": 0.07831339159162784,
          "standard_error": 0.02581244414555201
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.604542052271704,
            "upper_bound": 37.67869032280704
          },
          "point_estimate": 37.642848224880936,
          "standard_error": 0.018914783210382227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030593428912686613,
            "upper_bound": 0.07486613092428586
          },
          "point_estimate": 0.05868650807031346,
          "standard_error": 0.011511843778425594
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.82756549965636,
            "upper_bound": 20.882136867254268
          },
          "point_estimate": 20.855475339687068,
          "standard_error": 0.01402099725862522
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.807218049022385,
            "upper_bound": 20.890138363654508
          },
          "point_estimate": 20.866143946145648,
          "standard_error": 0.01999048444860421
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007618826543703042,
            "upper_bound": 0.08078447827310696
          },
          "point_estimate": 0.044236178398815766,
          "standard_error": 0.018396614831137116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.82461677325104,
            "upper_bound": 20.88509666355493
          },
          "point_estimate": 20.856509799230135,
          "standard_error": 0.015315601924694942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02621226453883708,
            "upper_bound": 0.05814420594384431
          },
          "point_estimate": 0.046684872502062186,
          "standard_error": 0.008065809371501982
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.32130196367393,
            "upper_bound": 23.35569032645251
          },
          "point_estimate": 23.338517422717995,
          "standard_error": 0.008814481170957823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.31234654594906,
            "upper_bound": 23.36711410604403
          },
          "point_estimate": 23.335919434822316,
          "standard_error": 0.016435513851509775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017527408726764988,
            "upper_bound": 0.0463704193215608
          },
          "point_estimate": 0.04391221955203464,
          "standard_error": 0.012191286928896752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.30761016645094,
            "upper_bound": 23.355363132457867
          },
          "point_estimate": 23.326793069881713,
          "standard_error": 0.012319516937567393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019197897978036876,
            "upper_bound": 0.034316788747488544
          },
          "point_estimate": 0.029281499627908603,
          "standard_error": 0.003898209235517581
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.8400874872642,
            "upper_bound": 32.91316351869266
          },
          "point_estimate": 32.87690118037445,
          "standard_error": 0.018684046486806875
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.833034978185665,
            "upper_bound": 32.920911349200196
          },
          "point_estimate": 32.87389458855469,
          "standard_error": 0.02215388285638497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006647509249302082,
            "upper_bound": 0.1122302036224333
          },
          "point_estimate": 0.05173189909979854,
          "standard_error": 0.025222462476855875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.83872889000604,
            "upper_bound": 32.91023770447069
          },
          "point_estimate": 32.8709367400393,
          "standard_error": 0.01873292181394273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03377224797852626,
            "upper_bound": 0.07968704718862196
          },
          "point_estimate": 0.06194451105116744,
          "standard_error": 0.01194800304136429
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 891080.4164186991,
            "upper_bound": 892728.3452071234
          },
          "point_estimate": 891850.8758846304,
          "standard_error": 421.4889536300829
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 890711.3008130081,
            "upper_bound": 892400.382791328
          },
          "point_estimate": 892026.9238675958,
          "standard_error": 480.77733114798434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.923363835842,
            "upper_bound": 2302.073358886123
          },
          "point_estimate": 1111.9901591200448,
          "standard_error": 555.0885006566033
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 891164.7844421117,
            "upper_bound": 892268.4138418878
          },
          "point_estimate": 891778.5625593918,
          "standard_error": 279.5475892370371
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648.067331493386,
            "upper_bound": 1936.6347508013037
          },
          "point_estimate": 1405.7442979378934,
          "standard_error": 362.1579843499423
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166872.1677195282,
            "upper_bound": 167311.59123088684
          },
          "point_estimate": 167072.95751310614,
          "standard_error": 113.32960046212744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166834.347706422,
            "upper_bound": 167321.02162516385
          },
          "point_estimate": 166981.77691131498,
          "standard_error": 100.43798527611736
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.12558620607192,
            "upper_bound": 560.196772898543
          },
          "point_estimate": 210.03340939042948,
          "standard_error": 136.9466603482693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166913.59186361404,
            "upper_bound": 167058.87021143665
          },
          "point_estimate": 166991.04545454544,
          "standard_error": 37.05893135709226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.00672263473324,
            "upper_bound": 492.58472737475387
          },
          "point_estimate": 377.4464392858867,
          "standard_error": 95.704508725707
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697587.6949205974,
            "upper_bound": 698696.4855518868
          },
          "point_estimate": 698104.5943081761,
          "standard_error": 284.16921412305044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697391.8238993711,
            "upper_bound": 698641.8578616353
          },
          "point_estimate": 698021.7555031446,
          "standard_error": 336.1005024427685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.9443333913343,
            "upper_bound": 1534.9982270879784
          },
          "point_estimate": 827.9050386035245,
          "standard_error": 323.8262612011617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697557.8929903908,
            "upper_bound": 699315.64353338
          },
          "point_estimate": 698361.3280078412,
          "standard_error": 501.6428952450076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443.8369424980588,
            "upper_bound": 1271.234958944102
          },
          "point_estimate": 943.6544876865358,
          "standard_error": 227.43265358377633
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379564.2073373016,
            "upper_bound": 381483.6409755291
          },
          "point_estimate": 380528.6896139219,
          "standard_error": 492.8949190583464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378726.7708333333,
            "upper_bound": 381895.07607886905
          },
          "point_estimate": 381002.58449074073,
          "standard_error": 922.4312836336876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157.12814165483033,
            "upper_bound": 2705.6459403679646
          },
          "point_estimate": 2374.4860010909847,
          "standard_error": 727.270583609877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 379914.0741738506,
            "upper_bound": 382162.15545468655
          },
          "point_estimate": 381313.778517316,
          "standard_error": 575.5343613830953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1084.4269314835176,
            "upper_bound": 1909.236246197181
          },
          "point_estimate": 1639.1076594031235,
          "standard_error": 214.30574623426855
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580320.0817776833,
            "upper_bound": 581305.4858987151
          },
          "point_estimate": 580802.4112396069,
          "standard_error": 252.4751420668221
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579974.9444444445,
            "upper_bound": 581565.7096560847
          },
          "point_estimate": 580819.6242063493,
          "standard_error": 452.2120972907933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.772421818009455,
            "upper_bound": 1451.7599611309906
          },
          "point_estimate": 1240.9359818579117,
          "standard_error": 414.39051201171344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580326.588466948,
            "upper_bound": 581439.8572681628
          },
          "point_estimate": 580879.9885384457,
          "standard_error": 284.59809632808117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 519.6777177719761,
            "upper_bound": 994.9519385286484
          },
          "point_estimate": 840.0803897530942,
          "standard_error": 121.08426821627307
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228690.4690676101,
            "upper_bound": 229295.12015373865
          },
          "point_estimate": 228954.53384346617,
          "standard_error": 156.17866340229298
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228593.51257861633,
            "upper_bound": 229133.2711390636
          },
          "point_estimate": 228842.92243186585,
          "standard_error": 154.24196834891245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.17468359637112,
            "upper_bound": 696.4928317857093
          },
          "point_estimate": 392.5486995507789,
          "standard_error": 140.0768940078378
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228711.09171597633,
            "upper_bound": 229129.0210691824
          },
          "point_estimate": 228889.24917095483,
          "standard_error": 108.58110361840946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.216965722669,
            "upper_bound": 746.1399612281834
          },
          "point_estimate": 520.2793312549983,
          "standard_error": 167.1211792633992
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390202.8205141843,
            "upper_bound": 390973.19149696047
          },
          "point_estimate": 390601.48256079026,
          "standard_error": 196.7611363484355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390139.88120567374,
            "upper_bound": 391077.02260638296
          },
          "point_estimate": 390669.7226443769,
          "standard_error": 236.49396457734937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.31681502757046,
            "upper_bound": 1100.9918840704888
          },
          "point_estimate": 632.773612436183,
          "standard_error": 250.34821482226573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390274.31684449775,
            "upper_bound": 391168.27122225193
          },
          "point_estimate": 390755.1105277701,
          "standard_error": 227.69546772541332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328.64188731033937,
            "upper_bound": 839.7611282604598
          },
          "point_estimate": 653.492674290556,
          "standard_error": 129.8504337572486
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384671.90202447784,
            "upper_bound": 385248.5999024644
          },
          "point_estimate": 384964.3646896407,
          "standard_error": 147.73354805634352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384629.4242105263,
            "upper_bound": 385344.41776315786
          },
          "point_estimate": 384952.10735171265,
          "standard_error": 154.98302327639294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.472031406540093,
            "upper_bound": 880.5221718149925
          },
          "point_estimate": 373.07872627123817,
          "standard_error": 218.58280390774905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384513.5627043561,
            "upper_bound": 385304.2876186311
          },
          "point_estimate": 384872.4209706083,
          "standard_error": 203.91844076089447
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 259.90107143956686,
            "upper_bound": 631.8119617159962
          },
          "point_estimate": 492.1844543542887,
          "standard_error": 95.93953454194076
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 331163.28332462115,
            "upper_bound": 331711.4750937951
          },
          "point_estimate": 331412.0057150072,
          "standard_error": 140.6145037598756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 331148.4290909091,
            "upper_bound": 331629.5535353535
          },
          "point_estimate": 331271.5396103896,
          "standard_error": 144.66408514217113
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.848341749831553,
            "upper_bound": 669.729345079588
          },
          "point_estimate": 254.2076023505656,
          "standard_error": 172.7154490625479
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 331172.5589668318,
            "upper_bound": 331643.3171224439
          },
          "point_estimate": 331388.21166469896,
          "standard_error": 120.909419653296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.5063980164871,
            "upper_bound": 655.3566562366917
          },
          "point_estimate": 469.1226383787125,
          "standard_error": 134.34033004427303
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289619.0043276014,
            "upper_bound": 290453.53144052345
          },
          "point_estimate": 290067.2726949484,
          "standard_error": 213.52760405608907
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289695.4135802469,
            "upper_bound": 290495.74632936507
          },
          "point_estimate": 290167.4232804233,
          "standard_error": 186.19291569738172
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.58006286871364,
            "upper_bound": 1089.9932499080194
          },
          "point_estimate": 562.6548613071873,
          "standard_error": 257.9529999518795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289852.3621031746,
            "upper_bound": 290511.25762484845
          },
          "point_estimate": 290194.3117295403,
          "standard_error": 174.0816767526701
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292.9424897407015,
            "upper_bound": 987.2127071093796
          },
          "point_estimate": 712.5484542535115,
          "standard_error": 189.97733428024375
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317337.4098560387,
            "upper_bound": 317802.6698019324
          },
          "point_estimate": 317580.70382125606,
          "standard_error": 119.60462450917156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317090.75,
            "upper_bound": 317915.9077294686
          },
          "point_estimate": 317726.47739130433,
          "standard_error": 202.3635373322706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.191816464708126,
            "upper_bound": 636.3999149190117
          },
          "point_estimate": 327.49516096843394,
          "standard_error": 156.0895562347326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317353.070424957,
            "upper_bound": 317800.2449795502
          },
          "point_estimate": 317623.66215697344,
          "standard_error": 117.30359767763498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.789346580768,
            "upper_bound": 466.80074723744985
          },
          "point_estimate": 399.3047301680117,
          "standard_error": 64.28210697120102
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166420.39085960714,
            "upper_bound": 166890.16721889272
          },
          "point_estimate": 166617.43408766395,
          "standard_error": 123.2425497902344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166378.70811770676,
            "upper_bound": 166706.86044520547
          },
          "point_estimate": 166504.06009458576,
          "standard_error": 72.47642019982267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.98707571573221,
            "upper_bound": 429.28716087180055
          },
          "point_estimate": 142.6907696585134,
          "standard_error": 114.9049722746478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166431.47853067258,
            "upper_bound": 166679.87396546802
          },
          "point_estimate": 166528.26676154896,
          "standard_error": 66.71990869505701
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.80951549176147,
            "upper_bound": 606.6531868257908
          },
          "point_estimate": 412.2342242978095,
          "standard_error": 152.88654359914528
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221431.5394507576,
            "upper_bound": 222238.6739181818
          },
          "point_estimate": 221766.85862626263,
          "standard_error": 213.60953791485747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221330.18333333335,
            "upper_bound": 221964.43181818185
          },
          "point_estimate": 221548.35454545455,
          "standard_error": 164.8145838885752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.05550048569431,
            "upper_bound": 722.829936076306
          },
          "point_estimate": 406.59136169063714,
          "standard_error": 164.54089860167775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221521.4376846325,
            "upper_bound": 222066.8663698204
          },
          "point_estimate": 221718.26647776467,
          "standard_error": 140.40319542534036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.13388242875843,
            "upper_bound": 1051.6137565105655
          },
          "point_estimate": 708.9522549830683,
          "standard_error": 270.50028176039206
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/libc_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84325.90340199518,
            "upper_bound": 84436.46035545795
          },
          "point_estimate": 84374.87895223364,
          "standard_error": 28.596744180891903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84311.74651972158,
            "upper_bound": 84426.86403712297
          },
          "point_estimate": 84346.6555168858,
          "standard_error": 27.404300149603692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.36671412060504,
            "upper_bound": 130.83126834239118
          },
          "point_estimate": 52.18391672630598,
          "standard_error": 31.335401699418753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84318.06287892676,
            "upper_bound": 84403.30255053498
          },
          "point_estimate": 84351.81107059993,
          "standard_error": 21.66324595575122
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.288974914909897,
            "upper_bound": 130.23625543540155
          },
          "point_estimate": 95.57957117509652,
          "standard_error": 27.78321171271559
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1110438.2109178286,
            "upper_bound": 1111507.0955396823
          },
          "point_estimate": 1110989.4966859412,
          "standard_error": 275.05701265749747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1110104.0696428572,
            "upper_bound": 1111862.2571428572
          },
          "point_estimate": 1111151.8190476191,
          "standard_error": 436.4995170674396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.36031914332528,
            "upper_bound": 1634.922118474534
          },
          "point_estimate": 1033.3891256536667,
          "standard_error": 387.8101228081414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1110344.289167038,
            "upper_bound": 1111622.544163522
          },
          "point_estimate": 1110920.8988497218,
          "standard_error": 328.0443541886121
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 508.68452611844606,
            "upper_bound": 1106.0663189040922
          },
          "point_estimate": 917.4171696096432,
          "standard_error": 151.81665634350193
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2231.169231300002,
            "upper_bound": 2235.312367382349
          },
          "point_estimate": 2233.1840411272674,
          "standard_error": 1.0588629511732466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2230.448844402941,
            "upper_bound": 2235.629730360686
          },
          "point_estimate": 2233.457107005286,
          "standard_error": 1.370015061507517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3794327585260391,
            "upper_bound": 5.952482275553225
          },
          "point_estimate": 4.1853446797907905,
          "standard_error": 1.5184748368288552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2229.592483688608,
            "upper_bound": 2234.7332305756727
          },
          "point_estimate": 2231.835032342468,
          "standard_error": 1.3402716788602815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8615124923253712,
            "upper_bound": 4.552313857957813
          },
          "point_estimate": 3.53729757230964,
          "standard_error": 0.6939708184959601
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40925.287179275256,
            "upper_bound": 41436.32899754665
          },
          "point_estimate": 41106.122619851994,
          "standard_error": 153.03800820843816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40913.96570945946,
            "upper_bound": 40982.78026463964
          },
          "point_estimate": 40960.939872908624,
          "standard_error": 43.11053502537496
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.826563038451905,
            "upper_bound": 122.80446818014347
          },
          "point_estimate": 24.4474856301554,
          "standard_error": 52.76881771221524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40945.65811230485,
            "upper_bound": 41524.88290998903
          },
          "point_estimate": 41108.08139113139,
          "standard_error": 164.9226055460136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.144645365493997,
            "upper_bound": 781.1292302128522
          },
          "point_estimate": 508.5146272396108,
          "standard_error": 273.0581204931288
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48785.41030882964,
            "upper_bound": 48889.74435960972
          },
          "point_estimate": 48838.63276910719,
          "standard_error": 26.62340522234104
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48781.96896782842,
            "upper_bound": 48905.038334078046
          },
          "point_estimate": 48844.437380314055,
          "standard_error": 36.24967528094598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.720720800987324,
            "upper_bound": 152.95144693828308
          },
          "point_estimate": 77.97673449029787,
          "standard_error": 33.51396035664553
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48792.272189528274,
            "upper_bound": 48906.7833880208
          },
          "point_estimate": 48850.4245952439,
          "standard_error": 30.31425569921883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.45131522183044,
            "upper_bound": 115.58522663505212
          },
          "point_estimate": 88.61227097229956,
          "standard_error": 17.750804502317372
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49231.57029570761,
            "upper_bound": 49345.95076611261
          },
          "point_estimate": 49286.68548954361,
          "standard_error": 29.362579099868597
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49212.371801705754,
            "upper_bound": 49372.84746720941
          },
          "point_estimate": 49248.08697421981,
          "standard_error": 44.90662867493039
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.37986186818451,
            "upper_bound": 160.50426595192397
          },
          "point_estimate": 100.41638988618053,
          "standard_error": 38.84611430069726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49223.32279015124,
            "upper_bound": 49330.17369826749
          },
          "point_estimate": 49268.27037304622,
          "standard_error": 28.063537318409416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.223328486529574,
            "upper_bound": 118.21359882019344
          },
          "point_estimate": 97.48389047508972,
          "standard_error": 16.527622906273614
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13989.155550162212,
            "upper_bound": 14019.533094000875
          },
          "point_estimate": 14002.610501224995,
          "standard_error": 7.823721035290672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13988.20438799076,
            "upper_bound": 14009.463441963904
          },
          "point_estimate": 13998.730360350452,
          "standard_error": 6.097561203168434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.872349240978843,
            "upper_bound": 32.31041518368632
          },
          "point_estimate": 14.424019974869692,
          "standard_error": 6.976336911580891
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13992.6283846574,
            "upper_bound": 14007.8021046634
          },
          "point_estimate": 14001.641350489388,
          "standard_error": 3.887844317989659
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.975230673801498,
            "upper_bound": 37.66670922786231
          },
          "point_estimate": 26.070111645655043,
          "standard_error": 8.735253849517422
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27115.64915122633,
            "upper_bound": 27164.385631897767
          },
          "point_estimate": 27140.62128216982,
          "standard_error": 12.424274169366992
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27114.316488258235,
            "upper_bound": 27175.51929300473
          },
          "point_estimate": 27141.727781926813,
          "standard_error": 13.230784246592084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.5487957761935895,
            "upper_bound": 73.48226083135171
          },
          "point_estimate": 32.43075180163682,
          "standard_error": 17.32471891820427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27125.154727078425,
            "upper_bound": 27177.832897997923
          },
          "point_estimate": 27150.316918033423,
          "standard_error": 13.35162836167038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.81092667240638,
            "upper_bound": 55.20096089918305
          },
          "point_estimate": 41.339764772305145,
          "standard_error": 9.245173948429146
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17587.52076925736,
            "upper_bound": 17622.83376493377
          },
          "point_estimate": 17603.969240461593,
          "standard_error": 9.053923455577404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17579.250242013553,
            "upper_bound": 17625.11743035388
          },
          "point_estimate": 17593.108301064858,
          "standard_error": 14.034108472001346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.519534666459591,
            "upper_bound": 51.76254535402099
          },
          "point_estimate": 24.873813086955696,
          "standard_error": 13.397193621811413
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17585.77170729832,
            "upper_bound": 17620.70171943422
          },
          "point_estimate": 17602.64446889026,
          "standard_error": 9.000219292662361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.981928051484344,
            "upper_bound": 38.875228171845485
          },
          "point_estimate": 30.206967642714687,
          "standard_error": 6.318351183776894
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17327.85759572277,
            "upper_bound": 17360.060185208218
          },
          "point_estimate": 17345.196518519777,
          "standard_error": 8.267786346350423
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17321.930820610687,
            "upper_bound": 17365.364477311283
          },
          "point_estimate": 17354.98814408397,
          "standard_error": 9.721432169453976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.322239284732907,
            "upper_bound": 43.999832424953816
          },
          "point_estimate": 15.777143146664,
          "standard_error": 11.895282711535652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17336.680623008786,
            "upper_bound": 17357.88637907798
          },
          "point_estimate": 17350.496030782197,
          "standard_error": 5.458848012123156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.333999641510792,
            "upper_bound": 35.148033055627096
          },
          "point_estimate": 27.47332660550745,
          "standard_error": 6.123098001560826
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17334.654024927077,
            "upper_bound": 17345.586207965396
          },
          "point_estimate": 17339.543615619197,
          "standard_error": 2.8212509446910743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17333.031980906922,
            "upper_bound": 17344.143102625298
          },
          "point_estimate": 17336.646479713603,
          "standard_error": 3.022454311476613
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1743147166202978,
            "upper_bound": 13.427449145859372
          },
          "point_estimate": 5.710850467424312,
          "standard_error": 3.248363763991624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17334.749670911857,
            "upper_bound": 17346.13632842389
          },
          "point_estimate": 17339.848761739453,
          "standard_error": 2.8487757814324013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3228183927889305,
            "upper_bound": 12.907349553339722
          },
          "point_estimate": 9.414205581064463,
          "standard_error": 2.6354728788794355
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23074.371321875667,
            "upper_bound": 23114.827478100087
          },
          "point_estimate": 23094.30211588009,
          "standard_error": 10.344071155501291
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23064.33243157225,
            "upper_bound": 23118.658099936347
          },
          "point_estimate": 23095.859495013792,
          "standard_error": 12.071975930388817
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.27470409158788,
            "upper_bound": 65.47239970650445
          },
          "point_estimate": 29.619609308645806,
          "standard_error": 15.134494299823976
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23061.571060301765,
            "upper_bound": 23099.66678556903
          },
          "point_estimate": 23079.52142650475,
          "standard_error": 10.126775180416429
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.995273020223944,
            "upper_bound": 43.775255013368565
          },
          "point_estimate": 34.39767220200775,
          "standard_error": 6.3743213794079345
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18064.36072327635,
            "upper_bound": 18086.493440913604
          },
          "point_estimate": 18075.42403035892,
          "standard_error": 5.638169435333311
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18060.35283018868,
            "upper_bound": 18088.921797418072
          },
          "point_estimate": 18077.452631578948,
          "standard_error": 7.199428691006908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6364082273985305,
            "upper_bound": 34.14901081875747
          },
          "point_estimate": 19.011363267247976,
          "standard_error": 7.616051507459173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18058.177337179564,
            "upper_bound": 18080.21503354281
          },
          "point_estimate": 18067.885869046542,
          "standard_error": 5.592073664354199
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.61026302428558,
            "upper_bound": 23.964145038636612
          },
          "point_estimate": 18.804713360705975,
          "standard_error": 3.465592909660168
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18490.85340284842,
            "upper_bound": 18516.49199207964
          },
          "point_estimate": 18503.353085307368,
          "standard_error": 6.579136768297156
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18485.76085961343,
            "upper_bound": 18518.689216683622
          },
          "point_estimate": 18502.484130213634,
          "standard_error": 9.034865033083136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.570212454468884,
            "upper_bound": 37.12293730634958
          },
          "point_estimate": 19.509462166355146,
          "standard_error": 8.199744584156354
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18496.37958052856,
            "upper_bound": 18521.541498122104
          },
          "point_estimate": 18509.94107489662,
          "standard_error": 6.279699789460543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.25168746605974,
            "upper_bound": 28.190448592702797
          },
          "point_estimate": 21.926937479697617,
          "standard_error": 4.1693389278812605
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17476.414431965834,
            "upper_bound": 17513.596676466997
          },
          "point_estimate": 17494.014329292513,
          "standard_error": 9.49407203390362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17476.015485356802,
            "upper_bound": 17506.029836381134
          },
          "point_estimate": 17490.59731312159,
          "standard_error": 6.929707293301116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6219539975142736,
            "upper_bound": 49.145676678020656
          },
          "point_estimate": 16.854952704455517,
          "standard_error": 12.71296227731258
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17481.509423392483,
            "upper_bound": 17496.837862789973
          },
          "point_estimate": 17488.698096321386,
          "standard_error": 3.826856719858308
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.747842132156448,
            "upper_bound": 43.83502714386475
          },
          "point_estimate": 31.50852705900065,
          "standard_error": 8.37970339612945
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17347.45476152611,
            "upper_bound": 17363.43777739913
          },
          "point_estimate": 17354.82826661517,
          "standard_error": 4.097313480683223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17344.18552344602,
            "upper_bound": 17362.72312340967
          },
          "point_estimate": 17351.431416984735,
          "standard_error": 4.69726490328715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3174147269302037,
            "upper_bound": 22.01050480881876
          },
          "point_estimate": 13.325147676125097,
          "standard_error": 4.909383721247659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17346.169125386714,
            "upper_bound": 17362.212759845745
          },
          "point_estimate": 17353.532229602457,
          "standard_error": 4.038077510871824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.44447569258888,
            "upper_bound": 18.46433074206621
          },
          "point_estimate": 13.695813623594049,
          "standard_error": 3.340271896182503
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17530.959974990594,
            "upper_bound": 17562.37182320728
          },
          "point_estimate": 17545.927858601917,
          "standard_error": 8.020526829608709
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17523.14320270924,
            "upper_bound": 17560.902273826803
          },
          "point_estimate": 17547.19432579999,
          "standard_error": 11.11466522712784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.245333985251921,
            "upper_bound": 48.72576010898414
          },
          "point_estimate": 27.209260010408,
          "standard_error": 10.468991120584452
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17526.488615331353,
            "upper_bound": 17558.07970054952
          },
          "point_estimate": 17541.805547911208,
          "standard_error": 8.250209860242403
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.264276357487764,
            "upper_bound": 35.640184672525145
          },
          "point_estimate": 26.83240133550722,
          "standard_error": 5.951659264565107
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17346.53206902036,
            "upper_bound": 17384.289705788804
          },
          "point_estimate": 17365.297239412943,
          "standard_error": 9.598105872002945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17350.332856234098,
            "upper_bound": 17378.048664122136
          },
          "point_estimate": 17364.48878816794,
          "standard_error": 7.498067937982266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.594599024143323,
            "upper_bound": 52.06695172352671
          },
          "point_estimate": 20.54572802264328,
          "standard_error": 10.470421130645445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17355.764231741283,
            "upper_bound": 17374.397091315866
          },
          "point_estimate": 17364.86830697928,
          "standard_error": 4.865433982103107
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.586094874442352,
            "upper_bound": 44.30023236354113
          },
          "point_estimate": 31.973632062891227,
          "standard_error": 8.85263014293029
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15211.07813914729,
            "upper_bound": 15238.295279063555
          },
          "point_estimate": 15224.427502974528,
          "standard_error": 6.956732006914012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15206.445077238042,
            "upper_bound": 15241.258657872697
          },
          "point_estimate": 15221.13707426019,
          "standard_error": 8.512774405543492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.301099429546742,
            "upper_bound": 41.28754816356358
          },
          "point_estimate": 21.32633829914575,
          "standard_error": 9.19576251531208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15207.539050888245,
            "upper_bound": 15233.864817916745
          },
          "point_estimate": 15220.342430768562,
          "standard_error": 6.8607143153899965
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.66639849250069,
            "upper_bound": 29.41798474348635
          },
          "point_estimate": 23.16218832889896,
          "standard_error": 4.310211795894963
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16242.155562373331,
            "upper_bound": 16278.678423983132
          },
          "point_estimate": 16258.4452199015,
          "standard_error": 9.424449548053827
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16238.71640625,
            "upper_bound": 16270.327326388888
          },
          "point_estimate": 16252.123370535715,
          "standard_error": 7.422987145485816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2256063880480803,
            "upper_bound": 41.8845330767146
          },
          "point_estimate": 19.333923908986403,
          "standard_error": 9.314467154746456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16245.702659313723,
            "upper_bound": 16261.50095027561
          },
          "point_estimate": 16253.926667439704,
          "standard_error": 3.967051779790759
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.030196097452755,
            "upper_bound": 44.8829022794175
          },
          "point_estimate": 31.59652177773619,
          "standard_error": 9.85612063994678
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17713.095858635057,
            "upper_bound": 17735.960147596626
          },
          "point_estimate": 17724.4413620795,
          "standard_error": 5.858880126519504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17708.717486604968,
            "upper_bound": 17739.695973372298
          },
          "point_estimate": 17725.055393191535,
          "standard_error": 7.194502855402558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.1324555525477935,
            "upper_bound": 32.99703406266955
          },
          "point_estimate": 19.508605693594554,
          "standard_error": 7.817771798171862
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17720.145570975474,
            "upper_bound": 17739.002511146216
          },
          "point_estimate": 17727.642231514223,
          "standard_error": 4.799187833936605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.497799017570385,
            "upper_bound": 25.059640532471345
          },
          "point_estimate": 19.551997409343407,
          "standard_error": 3.6993292048892257
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18409.34371443433,
            "upper_bound": 18439.877900556683
          },
          "point_estimate": 18423.957818966486,
          "standard_error": 7.855729219340036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18401.97932692308,
            "upper_bound": 18443.38891700405
          },
          "point_estimate": 18422.9834682861,
          "standard_error": 10.712395414414054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.145540103529833,
            "upper_bound": 44.46325157566559
          },
          "point_estimate": 28.937053901749707,
          "standard_error": 9.92418770032772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18399.443821028908,
            "upper_bound": 18426.2175055033
          },
          "point_estimate": 18410.27335822073,
          "standard_error": 6.766965822978597
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.96234911227092,
            "upper_bound": 33.73930820063228
          },
          "point_estimate": 26.20246538060444,
          "standard_error": 5.1971722952406365
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68346.59814536342,
            "upper_bound": 68428.30027127717
          },
          "point_estimate": 68388.47652986633,
          "standard_error": 20.870596612664183
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68337.99138471179,
            "upper_bound": 68443.27923976608
          },
          "point_estimate": 68390.41306390977,
          "standard_error": 28.22936707198986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.317529004372856,
            "upper_bound": 114.96921822205336
          },
          "point_estimate": 68.3665383695802,
          "standard_error": 26.125090525095843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68382.8886630101,
            "upper_bound": 68444.5100564819
          },
          "point_estimate": 68417.30541451031,
          "standard_error": 15.600275142636889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.52174883950472,
            "upper_bound": 88.33735005010061
          },
          "point_estimate": 69.50289612568461,
          "standard_error": 12.985292473812954
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218706.6569444444,
            "upper_bound": 1221564.0493095238
          },
          "point_estimate": 1220115.321791005,
          "standard_error": 728.5703641530193
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218097.7444444443,
            "upper_bound": 1221713.4722222222
          },
          "point_estimate": 1220093.019047619,
          "standard_error": 844.8241365403434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 643.3804360777945,
            "upper_bound": 4226.475200520585
          },
          "point_estimate": 2176.129765699329,
          "standard_error": 991.0948083846522
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218939.3351340175,
            "upper_bound": 1221584.8579135537
          },
          "point_estimate": 1220281.4037229435,
          "standard_error": 688.8817445368329
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1310.1361436360778,
            "upper_bound": 3116.7526489838947
          },
          "point_estimate": 2424.7149645210316,
          "standard_error": 467.70293734489263
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205081.45724604948,
            "upper_bound": 205519.41498328844
          },
          "point_estimate": 205297.9430005197,
          "standard_error": 112.00200281897816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204982.8591269841,
            "upper_bound": 205649.95238095237
          },
          "point_estimate": 205289.09470663263,
          "standard_error": 184.09542541663467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.74268982595277,
            "upper_bound": 637.2211009787785
          },
          "point_estimate": 471.68515784618035,
          "standard_error": 138.43868288119788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205002.8381482522,
            "upper_bound": 205498.73211895773
          },
          "point_estimate": 205229.19431045145,
          "standard_error": 126.0949455720262
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.61224129136397,
            "upper_bound": 460.3828179804907
          },
          "point_estimate": 374.06328123881855,
          "standard_error": 59.432774806869794
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7397.297029278229,
            "upper_bound": 7416.57155431112
          },
          "point_estimate": 7407.079649168433,
          "standard_error": 4.951404168298023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7392.744691710903,
            "upper_bound": 7422.770985241789
          },
          "point_estimate": 7406.14300054444,
          "standard_error": 8.40576476761786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.655455619173902,
            "upper_bound": 28.668921877713657
          },
          "point_estimate": 23.72452683142526,
          "standard_error": 7.821994569422487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7405.158214874408,
            "upper_bound": 7419.002985968362
          },
          "point_estimate": 7411.94438334226,
          "standard_error": 3.5342810938515403
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.921288091872114,
            "upper_bound": 19.52576942070924
          },
          "point_estimate": 16.500492337090954,
          "standard_error": 2.404720221729308
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7491.940562405976,
            "upper_bound": 7531.696756487026
          },
          "point_estimate": 7507.341979407359,
          "standard_error": 10.922948621420186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7487.307743134421,
            "upper_bound": 7508.814485970587
          },
          "point_estimate": 7497.616276068553,
          "standard_error": 6.1042827368620145
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.688660909803444,
            "upper_bound": 24.746430793370617
          },
          "point_estimate": 14.219100446163145,
          "standard_error": 5.67882206000525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7488.711349823183,
            "upper_bound": 7504.030363200412
          },
          "point_estimate": 7497.462133860358,
          "standard_error": 3.924369792876157
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.066092204980679,
            "upper_bound": 55.57418624699163
          },
          "point_estimate": 36.3609830139524,
          "standard_error": 16.461310416098776
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14291.055446193424,
            "upper_bound": 14305.027768842969
          },
          "point_estimate": 14297.68926523348,
          "standard_error": 3.5610652160211775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14289.588443396226,
            "upper_bound": 14304.329697327044
          },
          "point_estimate": 14296.390560422282,
          "standard_error": 4.443225318923424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.13298581118762,
            "upper_bound": 19.64663508752211
          },
          "point_estimate": 10.76181673111022,
          "standard_error": 4.314190931417899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14290.757484764912,
            "upper_bound": 14310.703952888558
          },
          "point_estimate": 14299.51063464837,
          "standard_error": 5.456430074505544
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.726452909832314,
            "upper_bound": 16.02535369487405
          },
          "point_estimate": 11.844684342683678,
          "standard_error": 2.7906332995604286
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.13597821335986,
            "upper_bound": 43.377018804965864
          },
          "point_estimate": 43.25146319776837,
          "standard_error": 0.061867697463037226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.09278993937001,
            "upper_bound": 43.393763988741185
          },
          "point_estimate": 43.220200347841256,
          "standard_error": 0.07496588173985631
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03129832821367961,
            "upper_bound": 0.3522638164239361
          },
          "point_estimate": 0.17302375157699634,
          "standard_error": 0.07895417098876438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.16548697305078,
            "upper_bound": 43.43436965749856
          },
          "point_estimate": 43.30250243928035,
          "standard_error": 0.06747322892353216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10391654260999823,
            "upper_bound": 0.2623560544564348
          },
          "point_estimate": 0.2058563653220345,
          "standard_error": 0.040778556394326315
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.41387984551003,
            "upper_bound": 15.469281105502796
          },
          "point_estimate": 15.436609246706132,
          "standard_error": 0.01474072664443198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.409167819617782,
            "upper_bound": 15.444641841854573
          },
          "point_estimate": 15.42171150183382,
          "standard_error": 0.010769579656696676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022163586042429884,
            "upper_bound": 0.04482278513367385
          },
          "point_estimate": 0.01989710124305142,
          "standard_error": 0.011330131258352796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.408444891083231,
            "upper_bound": 15.426160073481109
          },
          "point_estimate": 15.415486880535664,
          "standard_error": 0.0045153092620379715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01192668741551592,
            "upper_bound": 0.07300260382613209
          },
          "point_estimate": 0.04902049398481694,
          "standard_error": 0.01933043252199484
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.266283895433409,
            "upper_bound": 15.29859655211699
          },
          "point_estimate": 15.28353549367576,
          "standard_error": 0.008306014470985345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.268915512302977,
            "upper_bound": 15.30347329236994
          },
          "point_estimate": 15.28674144207523,
          "standard_error": 0.008281732081397302
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004086756451624156,
            "upper_bound": 0.04364149143778932
          },
          "point_estimate": 0.024293842008522464,
          "standard_error": 0.010815217194615898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.280295323672943,
            "upper_bound": 15.305256921323297
          },
          "point_estimate": 15.293663273712268,
          "standard_error": 0.00630137117759184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01263122666579965,
            "upper_bound": 0.03726565727061476
          },
          "point_estimate": 0.027690585117626523,
          "standard_error": 0.006745497222986006
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.293664287254838,
            "upper_bound": 15.313786192406424
          },
          "point_estimate": 15.303308051698377,
          "standard_error": 0.005153808866211882
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.287585665543912,
            "upper_bound": 15.318583985613312
          },
          "point_estimate": 15.301203138501691,
          "standard_error": 0.00662177799272929
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035247382402603094,
            "upper_bound": 0.03171419358042386
          },
          "point_estimate": 0.01654677373352641,
          "standard_error": 0.007214117209183563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.29181774202916,
            "upper_bound": 15.315694853125317
          },
          "point_estimate": 15.303545009813195,
          "standard_error": 0.006075337495050393
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008685173799956005,
            "upper_bound": 0.02156102058513946
          },
          "point_estimate": 0.017243010596867284,
          "standard_error": 0.0032561342173877275
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.03556978649319,
            "upper_bound": 15.054095838902258
          },
          "point_estimate": 15.045227923941331,
          "standard_error": 0.004748544667163833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.032836654751124,
            "upper_bound": 15.056959629601897
          },
          "point_estimate": 15.049776186885,
          "standard_error": 0.005627120023844171
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010401824447386185,
            "upper_bound": 0.0264695343109481
          },
          "point_estimate": 0.010992104123278236,
          "standard_error": 0.006662745304000893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.04329415261803,
            "upper_bound": 15.06032705994538
          },
          "point_estimate": 15.052769807945417,
          "standard_error": 0.004503334170703912
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007057642312366388,
            "upper_bound": 0.01993869431297956
          },
          "point_estimate": 0.015859765037612893,
          "standard_error": 0.0031163341303551196
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.724848097515245,
            "upper_bound": 16.773020598261798
          },
          "point_estimate": 16.748062542108446,
          "standard_error": 0.01231472293051283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.71789385706779,
            "upper_bound": 16.778510354429386
          },
          "point_estimate": 16.746345592156985,
          "standard_error": 0.012730498980846315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020922875773102256,
            "upper_bound": 0.07928722677700509
          },
          "point_estimate": 0.03287219828188647,
          "standard_error": 0.020239839658101652
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.714974469002605,
            "upper_bound": 16.747304232714608
          },
          "point_estimate": 16.73397355499756,
          "standard_error": 0.008307759431375156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020497503662997497,
            "upper_bound": 0.05216727674089956
          },
          "point_estimate": 0.04112398390971115,
          "standard_error": 0.008148051473934968
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.42947892026512,
            "upper_bound": 17.494089078952083
          },
          "point_estimate": 17.457902011710324,
          "standard_error": 0.016728953574164325
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.42243901159276,
            "upper_bound": 17.490155167887785
          },
          "point_estimate": 17.43426595794203,
          "standard_error": 0.015176564434146004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034395367676107306,
            "upper_bound": 0.07660332781256926
          },
          "point_estimate": 0.032803237912008634,
          "standard_error": 0.0181072343015706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.42355276191823,
            "upper_bound": 17.453132629007165
          },
          "point_estimate": 17.43680161444742,
          "standard_error": 0.007498483669848846
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01528805879325696,
            "upper_bound": 0.07631859168186571
          },
          "point_estimate": 0.05567348221199677,
          "standard_error": 0.01659376893662052
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.945481732241264,
            "upper_bound": 15.07219118987198
          },
          "point_estimate": 14.99929609015326,
          "standard_error": 0.03295523975998637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.928713463211688,
            "upper_bound": 15.034636311799462
          },
          "point_estimate": 14.963217541367865,
          "standard_error": 0.02850279394824184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014153778333835111,
            "upper_bound": 0.12526835559260163
          },
          "point_estimate": 0.054217609317061766,
          "standard_error": 0.029484472490538753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.940829199302003,
            "upper_bound": 15.027288507106674
          },
          "point_estimate": 14.98111425397124,
          "standard_error": 0.022728774138573635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03221166054951676,
            "upper_bound": 0.15984226120806627
          },
          "point_estimate": 0.1100120027742264,
          "standard_error": 0.03819141703257362
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.30472883986043,
            "upper_bound": 17.430038782226784
          },
          "point_estimate": 17.367420575532673,
          "standard_error": 0.03189361087221233
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.288462376544405,
            "upper_bound": 17.45593428520391
          },
          "point_estimate": 17.352551198748507,
          "standard_error": 0.03900196508036523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0075322254296552255,
            "upper_bound": 0.1805974630345792
          },
          "point_estimate": 0.10608507709195326,
          "standard_error": 0.041358686776609664
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.325469947893886,
            "upper_bound": 17.416116141526878
          },
          "point_estimate": 17.371141314560376,
          "standard_error": 0.0232640186230878
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05688981089743107,
            "upper_bound": 0.13752143893694382
          },
          "point_estimate": 0.1062795862038201,
          "standard_error": 0.020748323162879875
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.641321747764614,
            "upper_bound": 19.69207609606894
          },
          "point_estimate": 19.66492320466597,
          "standard_error": 0.013091110900851476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.641001330800982,
            "upper_bound": 19.69004471418929
          },
          "point_estimate": 19.6487868267764,
          "standard_error": 0.01378805624111546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011370996991468792,
            "upper_bound": 0.0691076205464216
          },
          "point_estimate": 0.02302618458960213,
          "standard_error": 0.018324399086255334
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.644389445730404,
            "upper_bound": 19.709615097541462
          },
          "point_estimate": 19.6707000728077,
          "standard_error": 0.01745096099764914
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017302310789436278,
            "upper_bound": 0.058911312463697435
          },
          "point_estimate": 0.04372324711648715,
          "standard_error": 0.01108797241784638
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.498605314798972,
            "upper_bound": 16.637031015153703
          },
          "point_estimate": 16.552755589945967,
          "standard_error": 0.03777944018429512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.485708931034672,
            "upper_bound": 16.55757718916901
          },
          "point_estimate": 16.514018330372433,
          "standard_error": 0.022459190831650325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008452942456718331,
            "upper_bound": 0.09143924790325934
          },
          "point_estimate": 0.059301678772080445,
          "standard_error": 0.02126249264802787
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.49076593477645,
            "upper_bound": 16.61845112716828
          },
          "point_estimate": 16.536344599141806,
          "standard_error": 0.03316158298919581
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026230569584194455,
            "upper_bound": 0.1918260169832032
          },
          "point_estimate": 0.1260819866316459,
          "standard_error": 0.056025489598545954
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.63266202453911,
            "upper_bound": 19.68533105200723
          },
          "point_estimate": 19.656035882343893,
          "standard_error": 0.013601052209516623
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.62514259403128,
            "upper_bound": 19.683406284970467
          },
          "point_estimate": 19.643004122081365,
          "standard_error": 0.012633691370035505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004985321200349871,
            "upper_bound": 0.05819280440726776
          },
          "point_estimate": 0.023302813215891897,
          "standard_error": 0.013562519792672438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.63042511570561,
            "upper_bound": 19.65478800006637
          },
          "point_estimate": 19.63925524736639,
          "standard_error": 0.0062601913706900394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011948071903649571,
            "upper_bound": 0.060572172713082274
          },
          "point_estimate": 0.04531957299027073,
          "standard_error": 0.013025594789557443
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.182044952408592,
            "upper_bound": 26.209249209960404
          },
          "point_estimate": 26.195235629232677,
          "standard_error": 0.0069477570117474656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.17896104492339,
            "upper_bound": 26.217609056972364
          },
          "point_estimate": 26.18919016495888,
          "standard_error": 0.008989743288518991
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008270587608281687,
            "upper_bound": 0.04107755681370536
          },
          "point_estimate": 0.02286857582079845,
          "standard_error": 0.010126169800809251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.175356225731377,
            "upper_bound": 26.20356986822643
          },
          "point_estimate": 26.189106969865023,
          "standard_error": 0.007238141198131038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011732058998215724,
            "upper_bound": 0.029431797910138426
          },
          "point_estimate": 0.023211299148118363,
          "standard_error": 0.004393715199893942
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132702.9098056316,
            "upper_bound": 132807.8743391083
          },
          "point_estimate": 132757.2544014309,
          "standard_error": 26.963397728866635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132676.3886861314,
            "upper_bound": 132827.78842544317
          },
          "point_estimate": 132786.08079886454,
          "standard_error": 39.20578465678864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.882946862741852,
            "upper_bound": 150.4994538062091
          },
          "point_estimate": 81.12892569471302,
          "standard_error": 35.37072778044342
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132706.28510588256,
            "upper_bound": 132806.4005834979
          },
          "point_estimate": 132769.64569153474,
          "standard_error": 26.887812849005915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.2079320190498,
            "upper_bound": 110.18232875312744
          },
          "point_estimate": 89.87812127320376,
          "standard_error": 15.563680478147766
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53374.96377946618,
            "upper_bound": 53468.45165442976
          },
          "point_estimate": 53419.595384297136,
          "standard_error": 23.834126725091327
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53383.76868167728,
            "upper_bound": 53474.03915810083
          },
          "point_estimate": 53393.66746643592,
          "standard_error": 23.31658950390987
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.037216784063452,
            "upper_bound": 138.84716661756374
          },
          "point_estimate": 25.69712957315383,
          "standard_error": 37.87503399163047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53387.45733007103,
            "upper_bound": 53503.688952215605
          },
          "point_estimate": 53439.36339988939,
          "standard_error": 34.03554742804427
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.795573332143476,
            "upper_bound": 104.0276399259497
          },
          "point_estimate": 78.96637841313802,
          "standard_error": 17.899766039664275
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199563.2856667968,
            "upper_bound": 199865.83364462983
          },
          "point_estimate": 199710.45253382772,
          "standard_error": 77.05974825845865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199520.50594262296,
            "upper_bound": 199913.85755919857
          },
          "point_estimate": 199699.6029143898,
          "standard_error": 75.52627111763461
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.25133704358993,
            "upper_bound": 520.9255435932698
          },
          "point_estimate": 150.67862601344785,
          "standard_error": 137.40944097724528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199490.5286986428,
            "upper_bound": 199784.49299244033
          },
          "point_estimate": 199623.88064722164,
          "standard_error": 77.21599177590335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.79160765879529,
            "upper_bound": 329.238805077658
          },
          "point_estimate": 257.2296028709968,
          "standard_error": 51.36746270323584
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 663841.9989504329,
            "upper_bound": 665229.3457085136
          },
          "point_estimate": 664479.8321803751,
          "standard_error": 358.2174498657098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 663669.4204545454,
            "upper_bound": 665220.794949495
          },
          "point_estimate": 664122.9706060607,
          "standard_error": 328.1639680567615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.83576568158028,
            "upper_bound": 1904.5445031572071
          },
          "point_estimate": 709.7889610351133,
          "standard_error": 435.7418433184317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 663777.4114961271,
            "upper_bound": 665368.9665535956
          },
          "point_estimate": 664469.481511216,
          "standard_error": 422.49265617443046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413.2931193499041,
            "upper_bound": 1582.6850837208303
          },
          "point_estimate": 1198.181064693892,
          "standard_error": 306.9038512984545
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1338397.0809627976,
            "upper_bound": 1342634.5928061227
          },
          "point_estimate": 1340514.3771215987,
          "standard_error": 1087.3470873974006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1337612.9642857143,
            "upper_bound": 1343917.5052083335
          },
          "point_estimate": 1340146.1167091837,
          "standard_error": 2078.46463890309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350.10715878439817,
            "upper_bound": 5677.577769202916
          },
          "point_estimate": 4188.132022520884,
          "standard_error": 1491.7510451319174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1338433.423997868,
            "upper_bound": 1343713.9594155843
          },
          "point_estimate": 1341334.9075139146,
          "standard_error": 1382.346000345481
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2357.731361063454,
            "upper_bound": 4347.747926579332
          },
          "point_estimate": 3624.820860721455,
          "standard_error": 512.5405971800918
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53234.730144452566,
            "upper_bound": 53312.3806189693
          },
          "point_estimate": 53272.65834632878,
          "standard_error": 19.901153774665577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53219.92562134503,
            "upper_bound": 53334.148538011694
          },
          "point_estimate": 53259.20333820663,
          "standard_error": 34.108224288437405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.033267911905244,
            "upper_bound": 107.75760467804926
          },
          "point_estimate": 67.52191621352749,
          "standard_error": 29.082572782833584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53213.86439911694,
            "upper_bound": 53317.26466371152
          },
          "point_estimate": 53264.13266119845,
          "standard_error": 26.573030283119152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.193202399737544,
            "upper_bound": 80.13467713182548
          },
          "point_estimate": 66.27138931411343,
          "standard_error": 10.319925941661037
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163338.09675261582,
            "upper_bound": 163989.01680605378
          },
          "point_estimate": 163632.90086216098,
          "standard_error": 167.69623014156667
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163279.24170403587,
            "upper_bound": 164000.5927690583
          },
          "point_estimate": 163412.062406577,
          "standard_error": 162.4011228908997
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.4402900359856,
            "upper_bound": 830.0985005655156
          },
          "point_estimate": 256.9828596640986,
          "standard_error": 190.30375944039656
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163217.6314776144,
            "upper_bound": 164066.2617699797
          },
          "point_estimate": 163517.27475394562,
          "standard_error": 222.180982415878
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.65238967955543,
            "upper_bound": 720.3797912793386
          },
          "point_estimate": 558.2909872672739,
          "standard_error": 146.31499291752246
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113551.37055353436,
            "upper_bound": 113729.12178089308
          },
          "point_estimate": 113640.48113472284,
          "standard_error": 45.45519134717231
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113550.95911214952,
            "upper_bound": 113777.98719280028
          },
          "point_estimate": 113615.80560747664,
          "standard_error": 61.467155489883424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.80421197833142,
            "upper_bound": 281.86574434166624
          },
          "point_estimate": 105.04365147621918,
          "standard_error": 65.42033735148294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113582.81314662582,
            "upper_bound": 113735.81483694036
          },
          "point_estimate": 113652.66032285472,
          "standard_error": 39.724145329248856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.80210337426513,
            "upper_bound": 195.35357784400787
          },
          "point_estimate": 151.3526212866277,
          "standard_error": 28.893541362386017
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673279.2418479938,
            "upper_bound": 675842.1728549383
          },
          "point_estimate": 674371.3241512345,
          "standard_error": 669.2311380110029
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 672944.5796296296,
            "upper_bound": 674963.7907407407
          },
          "point_estimate": 673811.6392746913,
          "standard_error": 606.4124077896394
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.44312646645224,
            "upper_bound": 2573.4406360253047
          },
          "point_estimate": 1401.514910905168,
          "standard_error": 575.6071306860855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673393.2747357213,
            "upper_bound": 674366.022466629
          },
          "point_estimate": 673796.3398268399,
          "standard_error": 249.13485946582955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 748.2026495744678,
            "upper_bound": 3289.054003929051
          },
          "point_estimate": 2242.8974141853005,
          "standard_error": 794.82076157901
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47704.23213943641,
            "upper_bound": 47815.67320174479
          },
          "point_estimate": 47752.57329943891,
          "standard_error": 28.97476051873416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47687.22503285151,
            "upper_bound": 47789.78373850197
          },
          "point_estimate": 47723.424472811465,
          "standard_error": 24.53352548498297
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.890488751962364,
            "upper_bound": 118.61290829350884
          },
          "point_estimate": 57.74493342291758,
          "standard_error": 28.68012671721075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47713.33318327087,
            "upper_bound": 47765.64798227299
          },
          "point_estimate": 47740.81136918272,
          "standard_error": 13.505410788690346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.904547503100233,
            "upper_bound": 137.63972577264434
          },
          "point_estimate": 96.1821215233956,
          "standard_error": 31.442346547095823
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99488.61255061692,
            "upper_bound": 100122.71935806444
          },
          "point_estimate": 99798.99697133317,
          "standard_error": 162.13229054311512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99342.28161592504,
            "upper_bound": 100362.9690649666
          },
          "point_estimate": 99529.54963570127,
          "standard_error": 332.7959486251703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.11839866636745,
            "upper_bound": 844.1076423228432
          },
          "point_estimate": 394.7287402599206,
          "standard_error": 220.055906318605
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99459.81343566495,
            "upper_bound": 100275.13637653414
          },
          "point_estimate": 99946.94186360088,
          "standard_error": 207.7777403712057
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 324.0661438792047,
            "upper_bound": 610.1781264107345
          },
          "point_estimate": 541.8229150596804,
          "standard_error": 69.81941878261924
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 314491.20513409964,
            "upper_bound": 317440.7886056377
          },
          "point_estimate": 315754.9753133552,
          "standard_error": 772.8686038936501
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 314229.0833333333,
            "upper_bound": 316958.474137931
          },
          "point_estimate": 314474.12198275863,
          "standard_error": 736.0515605871582
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.62004688792174,
            "upper_bound": 3438.635373291072
          },
          "point_estimate": 482.5360193643223,
          "standard_error": 877.7958069364039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 314344.234297366,
            "upper_bound": 316035.63466211525
          },
          "point_estimate": 314950.1606359158,
          "standard_error": 446.51986448805695
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322.0766518562122,
            "upper_bound": 3691.003119694138
          },
          "point_estimate": 2568.5061989531255,
          "standard_error": 886.3022381770426
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48076.081832372765,
            "upper_bound": 48329.31594121105
          },
          "point_estimate": 48194.00985365751,
          "standard_error": 65.61754288089422
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48035.94520030234,
            "upper_bound": 48472.4253968254
          },
          "point_estimate": 48081.36970899471,
          "standard_error": 108.44533769705208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.187684651005911,
            "upper_bound": 341.57093254701056
          },
          "point_estimate": 102.00108050393304,
          "standard_error": 85.07673145145895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48062.71543191264,
            "upper_bound": 48351.247535552175
          },
          "point_estimate": 48200.416666666664,
          "standard_error": 74.13654929329287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.50681425515127,
            "upper_bound": 252.0701344801306
          },
          "point_estimate": 218.25054540501696,
          "standard_error": 39.696395375755834
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/regex_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18704.244427118836,
            "upper_bound": 18820.8139753952
          },
          "point_estimate": 18756.250721305223,
          "standard_error": 30.03627774466552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18677.37231595092,
            "upper_bound": 18789.763352079073
          },
          "point_estimate": 18749.482183026583,
          "standard_error": 28.60102309182358
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.070988258619956,
            "upper_bound": 137.60963293271018
          },
          "point_estimate": 75.49020246686813,
          "standard_error": 30.35518918595392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18723.646066601,
            "upper_bound": 18791.676448457485
          },
          "point_estimate": 18764.28511539585,
          "standard_error": 17.582387513294478
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.505466632676864,
            "upper_bound": 141.56311921687862
          },
          "point_estimate": 100.29134361396306,
          "standard_error": 30.081088569005043
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1070724.4975993524,
            "upper_bound": 1072438.4928189777
          },
          "point_estimate": 1071577.1382387956,
          "standard_error": 439.5063225876012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1070531.3878676472,
            "upper_bound": 1072596.0147058824
          },
          "point_estimate": 1071759.7580882353,
          "standard_error": 491.13551511116873
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.685489225992,
            "upper_bound": 2549.3275400345196
          },
          "point_estimate": 1334.1553521227895,
          "standard_error": 625.8234505354679
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1070799.924392583,
            "upper_bound": 1071857.012266518
          },
          "point_estimate": 1071413.679220779,
          "standard_error": 268.1303054453317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785.4427406742426,
            "upper_bound": 1888.1656287909257
          },
          "point_estimate": 1463.660429853511,
          "standard_error": 282.84964477914764
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2107.1380628920288,
            "upper_bound": 2111.403655510938
          },
          "point_estimate": 2109.112801298323,
          "standard_error": 1.0972649768288776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2105.9942324497524,
            "upper_bound": 2111.3084959704825
          },
          "point_estimate": 2108.652881347703,
          "standard_error": 1.3503313706215292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.34501147945645444,
            "upper_bound": 5.771608508078218
          },
          "point_estimate": 3.939463477977719,
          "standard_error": 1.4188015272821557
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2107.9065129023743,
            "upper_bound": 2110.9038916726945
          },
          "point_estimate": 2109.527421171895,
          "standard_error": 0.7537566955955203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.595900011146005,
            "upper_bound": 4.775542944027863
          },
          "point_estimate": 3.653384395561685,
          "standard_error": 0.827535720215573
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46952.245951765726,
            "upper_bound": 47027.34695176572
          },
          "point_estimate": 46987.55095391903,
          "standard_error": 19.254031896285444
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46940.833333333336,
            "upper_bound": 47048.4984496124
          },
          "point_estimate": 46971.04118217055,
          "standard_error": 23.4237682868319
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.181811506811568,
            "upper_bound": 96.8923139387317
          },
          "point_estimate": 59.7136773313506,
          "standard_error": 27.569920275895697
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46953.74551436933,
            "upper_bound": 47027.795984632125
          },
          "point_estimate": 46989.14540085238,
          "standard_error": 20.200697074094304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.455090881318537,
            "upper_bound": 81.87731203491485
          },
          "point_estimate": 64.19150199058396,
          "standard_error": 13.429819323638284
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49254.38791475792,
            "upper_bound": 49305.104956817646
          },
          "point_estimate": 49280.1164761151,
          "standard_error": 12.983660643971591
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49235.732021709635,
            "upper_bound": 49311.56468792402
          },
          "point_estimate": 49287.30885615214,
          "standard_error": 16.396999191818942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.513525997890254,
            "upper_bound": 76.1622424786971
          },
          "point_estimate": 49.70292797896945,
          "standard_error": 21.94670865535305
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49274.934693630275,
            "upper_bound": 49308.112456406736
          },
          "point_estimate": 49292.13717950977,
          "standard_error": 8.252296240575681
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.12269829642104,
            "upper_bound": 53.64653118626904
          },
          "point_estimate": 43.209224700845155,
          "standard_error": 7.509895309920513
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52074.85921043417,
            "upper_bound": 52143.399601352736
          },
          "point_estimate": 52110.34176897588,
          "standard_error": 17.60186238216188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52073.93328550932,
            "upper_bound": 52159.4981348637
          },
          "point_estimate": 52110.167106476736,
          "standard_error": 20.538150884074305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7340954681339023,
            "upper_bound": 98.36032848330558
          },
          "point_estimate": 63.42922170030637,
          "standard_error": 24.006961055031123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52086.306842826736,
            "upper_bound": 52148.58848226801
          },
          "point_estimate": 52120.482658517954,
          "standard_error": 16.192765476542068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.879769940320724,
            "upper_bound": 75.37864375621464
          },
          "point_estimate": 58.847829525680005,
          "standard_error": 11.520202497016133
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19985.642589728453,
            "upper_bound": 20041.103542568544
          },
          "point_estimate": 20011.444262035944,
          "standard_error": 14.199979090897518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19990.363691460057,
            "upper_bound": 20035.0891184573
          },
          "point_estimate": 20001.670661157023,
          "standard_error": 10.74756614854248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1457387895804183,
            "upper_bound": 78.30241484952458
          },
          "point_estimate": 18.308322794796023,
          "standard_error": 17.843307026278147
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19991.97738522782,
            "upper_bound": 20014.05392326794
          },
          "point_estimate": 20000.960067260563,
          "standard_error": 5.712286921285641
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.879806656873823,
            "upper_bound": 66.04245118662983
          },
          "point_estimate": 47.304291915218016,
          "standard_error": 13.50932347395325
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23033.500046547146,
            "upper_bound": 23067.268067777175
          },
          "point_estimate": 23050.06727123443,
          "standard_error": 8.572949191258397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23030.551858893115,
            "upper_bound": 23068.324580164764
          },
          "point_estimate": 23045.833428390368,
          "standard_error": 10.359071128269042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.5040696247090795,
            "upper_bound": 48.97512517614297
          },
          "point_estimate": 24.746750820161324,
          "standard_error": 10.714329178390074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23036.07464125368,
            "upper_bound": 23080.42695144807
          },
          "point_estimate": 23057.041946899742,
          "standard_error": 11.735573067860155
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.85395548362612,
            "upper_bound": 37.88014243093624
          },
          "point_estimate": 28.50453315871427,
          "standard_error": 6.282657463341494
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17673.427307960443,
            "upper_bound": 17690.751162103945
          },
          "point_estimate": 17681.97005450559,
          "standard_error": 4.453711825654773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17668.40502825644,
            "upper_bound": 17698.221546692606
          },
          "point_estimate": 17681.399650075662,
          "standard_error": 7.1972920916448775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6143482213382097,
            "upper_bound": 24.785415537597792
          },
          "point_estimate": 19.69237735548533,
          "standard_error": 5.924746283605171
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17672.18529074587,
            "upper_bound": 17686.95446720664
          },
          "point_estimate": 17678.405134165445,
          "standard_error": 3.748401097125908
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.920966149004489,
            "upper_bound": 17.702234987042665
          },
          "point_estimate": 14.8244735226344,
          "standard_error": 2.1986229343386987
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17498.200829296267,
            "upper_bound": 17521.477258240433
          },
          "point_estimate": 17509.684781249045,
          "standard_error": 5.992716467607195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17491.462320702205,
            "upper_bound": 17532.82514450867
          },
          "point_estimate": 17503.679362785577,
          "standard_error": 10.745063245498654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.79565113685172,
            "upper_bound": 31.81590983977951
          },
          "point_estimate": 23.36576844355487,
          "standard_error": 7.76150793193539
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17493.291012798785,
            "upper_bound": 17512.711182819756
          },
          "point_estimate": 17500.461529414708,
          "standard_error": 4.914312686790295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.648660812063063,
            "upper_bound": 23.31971649344088
          },
          "point_estimate": 19.985005816085486,
          "standard_error": 2.7228126572123865
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17307.611355444613,
            "upper_bound": 17327.09222818596
          },
          "point_estimate": 17317.240107763842,
          "standard_error": 5.001199560816472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17302.246978371502,
            "upper_bound": 17333.578761132318
          },
          "point_estimate": 17315.653625954197,
          "standard_error": 8.366977027437141
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6590318768328536,
            "upper_bound": 27.561827452790723
          },
          "point_estimate": 20.16138580618356,
          "standard_error": 6.066482625651347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17308.220076074784,
            "upper_bound": 17331.524975513763
          },
          "point_estimate": 17320.568413552097,
          "standard_error": 5.925417606549693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.33121231631804,
            "upper_bound": 20.04707080115962
          },
          "point_estimate": 16.640026326843575,
          "standard_error": 2.4787939582611513
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25556.7657364744,
            "upper_bound": 25617.765825222166
          },
          "point_estimate": 25584.17576338587,
          "standard_error": 15.75756324057162
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25554.663677062377,
            "upper_bound": 25611.062376760565
          },
          "point_estimate": 25563.54566216745,
          "standard_error": 14.234230299724482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.318780217197243,
            "upper_bound": 74.02903281365886
          },
          "point_estimate": 13.800467428115002,
          "standard_error": 20.28206479611863
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25552.29496266857,
            "upper_bound": 25586.078151595084
          },
          "point_estimate": 25564.819714651545,
          "standard_error": 8.677379571349631
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.691469087007656,
            "upper_bound": 68.99287005857457
          },
          "point_estimate": 52.51336682769238,
          "standard_error": 14.32778270772324
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26367.68302464009,
            "upper_bound": 26414.836488279805
          },
          "point_estimate": 26389.133141034516,
          "standard_error": 12.151902262288347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26359.64690614618,
            "upper_bound": 26407.47965116279
          },
          "point_estimate": 26384.240661337208,
          "standard_error": 12.929138680664847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.524004821224151,
            "upper_bound": 59.85967463979524
          },
          "point_estimate": 35.45841325130085,
          "standard_error": 12.346913403146836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26362.62925700526,
            "upper_bound": 26397.856267593314
          },
          "point_estimate": 26380.571655089097,
          "standard_error": 9.03621162206075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.466993566771883,
            "upper_bound": 56.153322857528146
          },
          "point_estimate": 40.369619114340665,
          "standard_error": 11.220395791031187
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41351.31023642298,
            "upper_bound": 41424.01429314121
          },
          "point_estimate": 41385.15762695701,
          "standard_error": 18.637203079069344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41340.83872437358,
            "upper_bound": 41429.027050113895
          },
          "point_estimate": 41361.66055157826,
          "standard_error": 23.45550142491808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.755107452763099,
            "upper_bound": 99.69492764349732
          },
          "point_estimate": 49.72145126703987,
          "standard_error": 24.976922597621822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41328.45159949056,
            "upper_bound": 41369.5761197369
          },
          "point_estimate": 41347.272177025705,
          "standard_error": 10.656361928964952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.15664355610194,
            "upper_bound": 82.40312686649652
          },
          "point_estimate": 62.35341331464547,
          "standard_error": 14.426592871828332
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17611.720389894414,
            "upper_bound": 17624.981463716624
          },
          "point_estimate": 17618.257559254642,
          "standard_error": 3.3807066034602045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17608.85949612403,
            "upper_bound": 17626.121850775195
          },
          "point_estimate": 17620.118156492248,
          "standard_error": 4.675834130575179
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4347722371229834,
            "upper_bound": 19.64977287062297
          },
          "point_estimate": 13.564486532440045,
          "standard_error": 4.609330192591768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17612.213313800276,
            "upper_bound": 17625.615319965196
          },
          "point_estimate": 17618.793059750325,
          "standard_error": 3.423532297385005
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.37853402755061,
            "upper_bound": 14.442269646161725
          },
          "point_estimate": 11.300683681912112,
          "standard_error": 2.0821249480939357
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25489.278216462393,
            "upper_bound": 25538.067834036563
          },
          "point_estimate": 25513.180111512957,
          "standard_error": 12.505308932287647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25476.872773089544,
            "upper_bound": 25556.38129395218
          },
          "point_estimate": 25504.604003415712,
          "standard_error": 20.957151899401683
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.884476775819614,
            "upper_bound": 67.95129978518445
          },
          "point_estimate": 46.74508085084119,
          "standard_error": 15.186946905694946
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25482.795309676814,
            "upper_bound": 25538.725983240183
          },
          "point_estimate": 25509.4499881272,
          "standard_error": 15.086100671018457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.33344680656032,
            "upper_bound": 49.76163855410975
          },
          "point_estimate": 41.629345732597606,
          "standard_error": 6.177024524696743
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68858.7641967292,
            "upper_bound": 69004.36669017556
          },
          "point_estimate": 68930.70073765932,
          "standard_error": 37.24034974185573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68832.10833333334,
            "upper_bound": 69028.08077651515
          },
          "point_estimate": 68921.75316220238,
          "standard_error": 45.99802553313293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.09648477186086,
            "upper_bound": 214.53597183044667
          },
          "point_estimate": 125.0903480443555,
          "standard_error": 48.003147284622706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68873.25294417492,
            "upper_bound": 68979.55910875676
          },
          "point_estimate": 68914.21280991736,
          "standard_error": 27.245547180631423
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.21563608754346,
            "upper_bound": 156.70762258978343
          },
          "point_estimate": 123.746636678599,
          "standard_error": 22.462896584648036
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37120.274312007015,
            "upper_bound": 37186.40887509738
          },
          "point_estimate": 37152.72213068458,
          "standard_error": 16.933996275872758
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37098.853345018986,
            "upper_bound": 37185.778161213355
          },
          "point_estimate": 37158.289834696654,
          "standard_error": 24.044222026688693
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.870163119338759,
            "upper_bound": 107.7052159048334
          },
          "point_estimate": 42.345372126549655,
          "standard_error": 24.60593810578328
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37127.848397621936,
            "upper_bound": 37170.07356480644
          },
          "point_estimate": 37148.82692481343,
          "standard_error": 10.680197859752909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.613923068631788,
            "upper_bound": 72.8559336059828
          },
          "point_estimate": 56.29862548140444,
          "standard_error": 10.88737522527259
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59585.85217593676,
            "upper_bound": 59692.17717538382
          },
          "point_estimate": 59635.83562190996,
          "standard_error": 27.31898512709296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59568.93469945355,
            "upper_bound": 59681.60737704918
          },
          "point_estimate": 59628.701229508195,
          "standard_error": 35.51645298419322
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.688616396329875,
            "upper_bound": 148.11403283493425
          },
          "point_estimate": 82.34674867466262,
          "standard_error": 32.44133711945912
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59566.69148858375,
            "upper_bound": 59656.977791073354
          },
          "point_estimate": 59619.34669363424,
          "standard_error": 23.292225411900898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.45554199849689,
            "upper_bound": 123.8990182116968
          },
          "point_estimate": 91.23268612289289,
          "standard_error": 22.42822545725299
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26132.484745513186,
            "upper_bound": 26155.085091948626
          },
          "point_estimate": 26143.0806802108,
          "standard_error": 5.826775556492558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26127.628857245854,
            "upper_bound": 26162.287671232876
          },
          "point_estimate": 26136.6845259553,
          "standard_error": 8.549512230581739
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9281684053921064,
            "upper_bound": 30.748058091457175
          },
          "point_estimate": 14.29427599893324,
          "standard_error": 7.311374314752229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26129.11065417548,
            "upper_bound": 26150.749589922645
          },
          "point_estimate": 26136.362949091283,
          "standard_error": 5.5847980294721715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.162106555485654,
            "upper_bound": 23.56716705155441
          },
          "point_estimate": 19.485191156916695,
          "standard_error": 3.5477901754919623
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22170.249488233498,
            "upper_bound": 22190.92851738658
          },
          "point_estimate": 22180.22270952187,
          "standard_error": 5.300137215030365
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22166.81769559902,
            "upper_bound": 22193.691116544414
          },
          "point_estimate": 22178.184121162725,
          "standard_error": 6.492028551972152
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.6965613469400855,
            "upper_bound": 29.340451086687136
          },
          "point_estimate": 18.608019229544663,
          "standard_error": 6.727514216644103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22171.51926601427,
            "upper_bound": 22185.42758926913
          },
          "point_estimate": 22177.317478487283,
          "standard_error": 3.5342866783851346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.268859355333657,
            "upper_bound": 22.851242842298156
          },
          "point_estimate": 17.685229209819948,
          "standard_error": 3.5200269743470485
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28784.438121430467,
            "upper_bound": 28807.980045931192
          },
          "point_estimate": 28796.298362803685,
          "standard_error": 6.0189827559198275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28782.713165210484,
            "upper_bound": 28809.60484511517
          },
          "point_estimate": 28798.507982525814,
          "standard_error": 6.9984081173218975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7511054978895366,
            "upper_bound": 35.46451494093612
          },
          "point_estimate": 19.20331368078175,
          "standard_error": 7.432611112657408
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28788.357348087764,
            "upper_bound": 28806.09876025571
          },
          "point_estimate": 28797.540600146476,
          "standard_error": 4.516491960237413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.939236395392248,
            "upper_bound": 26.604988195949755
          },
          "point_estimate": 20.05338351153233,
          "standard_error": 4.31367242787485
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-freq_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413489.82302290766,
            "upper_bound": 414585.1348773449
          },
          "point_estimate": 413940.6617049964,
          "standard_error": 287.95495115225833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413449.38295454544,
            "upper_bound": 414026.61130275973
          },
          "point_estimate": 413776.4210858586,
          "standard_error": 149.62584445723536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.20879621803185,
            "upper_bound": 891.3459930238586
          },
          "point_estimate": 348.4716456315946,
          "standard_error": 217.86225315696507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413631.1304950918,
            "upper_bound": 413931.726575611
          },
          "point_estimate": 413780.7565820543,
          "standard_error": 75.12903267635822
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.9171802711892,
            "upper_bound": 1437.6320114784255
          },
          "point_estimate": 962.3099844159924,
          "standard_error": 384.1801423368156
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-repeated_ra"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2670100.2708333335,
            "upper_bound": 2673763.4919419643
          },
          "point_estimate": 2671969.5029166667,
          "standard_error": 938.6379286345946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2670146.3333333335,
            "upper_bound": 2673718.617857143
          },
          "point_estimate": 2671922.845238095,
          "standard_error": 946.7533178382738
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.1525671565559,
            "upper_bound": 5549.018701485209
          },
          "point_estimate": 2606.0511349833764,
          "standard_error": 1212.5763551354644
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2670864.2192546586,
            "upper_bound": 2673366.8167989417
          },
          "point_estimate": 2672346.0131725417,
          "standard_error": 658.8669112445613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1337.3236456397162,
            "upper_bound": 4226.709679947355
          },
          "point_estimate": 3132.895080601481,
          "standard_error": 753.3995247703522
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117710.35964131608,
            "upper_bound": 117933.9046700827
          },
          "point_estimate": 117806.07997752604,
          "standard_error": 58.15359920873361
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117672.84449838188,
            "upper_bound": 117872.22204243076
          },
          "point_estimate": 117775.32389428264,
          "standard_error": 52.08317931481658
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.465086953727532,
            "upper_bound": 222.8546879852866
          },
          "point_estimate": 136.1062561354063,
          "standard_error": 55.04680140977353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117663.06760939918,
            "upper_bound": 117832.44964264412
          },
          "point_estimate": 117740.24539990754,
          "standard_error": 44.14831991061782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.27651097980322,
            "upper_bound": 281.1378719014132
          },
          "point_estimate": 193.6959381689716,
          "standard_error": 65.99086578274147
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9144.624437324355,
            "upper_bound": 9163.043946746331
          },
          "point_estimate": 9153.505267749551,
          "standard_error": 4.71345868330238
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9139.427175283734,
            "upper_bound": 9163.620521227407
          },
          "point_estimate": 9154.36001396145,
          "standard_error": 7.910921610839618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.398696535808198,
            "upper_bound": 30.09266780959218
          },
          "point_estimate": 17.26244308571214,
          "standard_error": 6.804975727481927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9148.127356825047,
            "upper_bound": 9172.926674810846
          },
          "point_estimate": 9161.755354809124,
          "standard_error": 6.542932178835384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.435500419654195,
            "upper_bound": 19.88224702713263
          },
          "point_estimate": 15.67228119996779,
          "standard_error": 2.826696782268596
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8932.641870969512,
            "upper_bound": 8956.427606570993
          },
          "point_estimate": 8944.950342687003,
          "standard_error": 6.124999338470887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8931.05207870256,
            "upper_bound": 8962.396173658399
          },
          "point_estimate": 8945.800034302021,
          "standard_error": 9.09445908540885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.859783809441864,
            "upper_bound": 35.38087173870323
          },
          "point_estimate": 21.91518611706269,
          "standard_error": 7.726818183185513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8940.772322629873,
            "upper_bound": 8961.429556574207
          },
          "point_estimate": 8951.574836820379,
          "standard_error": 5.245812594843782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.336310175779984,
            "upper_bound": 25.728244586440535
          },
          "point_estimate": 20.36755075298738,
          "standard_error": 3.7480134998384247
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14148.117819152272,
            "upper_bound": 14169.021178232884
          },
          "point_estimate": 14157.79071519778,
          "standard_error": 5.369309717284858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14144.713627172723,
            "upper_bound": 14166.326207635371
          },
          "point_estimate": 14154.993000908971,
          "standard_error": 5.671222295893298
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.56170115241897,
            "upper_bound": 26.748684193018008
          },
          "point_estimate": 14.367721654738087,
          "standard_error": 6.2049953111639
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14141.63197652768,
            "upper_bound": 14160.095603891654
          },
          "point_estimate": 14150.537252540991,
          "standard_error": 4.726300206896098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.578497735259562,
            "upper_bound": 24.77212316295486
          },
          "point_estimate": 17.908843124275364,
          "standard_error": 4.823314565852154
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.600547053052935,
            "upper_bound": 51.90824856046762
          },
          "point_estimate": 51.73144700204756,
          "standard_error": 0.0802738534025806
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.54454583628511,
            "upper_bound": 51.837622689046825
          },
          "point_estimate": 51.65818733955682,
          "standard_error": 0.06859016372080355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02714210162498012,
            "upper_bound": 0.3086577424977902
          },
          "point_estimate": 0.1832860175265129,
          "standard_error": 0.07114641635488818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.545487261374944,
            "upper_bound": 51.689330093706175
          },
          "point_estimate": 51.59558899646024,
          "standard_error": 0.036973482354401704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0862087091494427,
            "upper_bound": 0.3896095005822134
          },
          "point_estimate": 0.26718660447095477,
          "standard_error": 0.09384278996718286
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.710261585352917,
            "upper_bound": 28.74545508111482
          },
          "point_estimate": 28.72788218066197,
          "standard_error": 0.009021580850626572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.703033771729004,
            "upper_bound": 28.755195936024755
          },
          "point_estimate": 28.728741247217265,
          "standard_error": 0.015001532999262726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005759209825077262,
            "upper_bound": 0.04962027786608498
          },
          "point_estimate": 0.03605938659003067,
          "standard_error": 0.010694342548540564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.71373353203598,
            "upper_bound": 28.7496224664962
          },
          "point_estimate": 28.732608624777495,
          "standard_error": 0.00921299941635019
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018773745127502607,
            "upper_bound": 0.036631328390238894
          },
          "point_estimate": 0.03008979599139652,
          "standard_error": 0.004587072842029571
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.952449371324064,
            "upper_bound": 28.990143498901976
          },
          "point_estimate": 28.97058326473051,
          "standard_error": 0.009634257583578347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.943396802633316,
            "upper_bound": 28.993817228448936
          },
          "point_estimate": 28.967164067425003,
          "standard_error": 0.01519767870063192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004027297013955247,
            "upper_bound": 0.058165834796179566
          },
          "point_estimate": 0.03724011013189431,
          "standard_error": 0.01244313094260021
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.95492526507179,
            "upper_bound": 29.00960346956013
          },
          "point_estimate": 28.97984568909779,
          "standard_error": 0.014481108247050158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018381698069371257,
            "upper_bound": 0.04085716129713029
          },
          "point_estimate": 0.03205521508670443,
          "standard_error": 0.006052661601664051
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.658791374880582,
            "upper_bound": 27.68045095388088
          },
          "point_estimate": 27.670288686314148,
          "standard_error": 0.005571240495906161
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.65583435765168,
            "upper_bound": 27.68407469402845
          },
          "point_estimate": 27.674836413129825,
          "standard_error": 0.006306122245794801
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037659246816626,
            "upper_bound": 0.029789968798583644
          },
          "point_estimate": 0.01375496993084274,
          "standard_error": 0.006724624804116901
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.64955350981429,
            "upper_bound": 27.684642427959098
          },
          "point_estimate": 27.66731792611251,
          "standard_error": 0.00950087647522689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007307172199635536,
            "upper_bound": 0.02341851048680969
          },
          "point_estimate": 0.018555821423738628,
          "standard_error": 0.004089056619635393
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.73959547258061,
            "upper_bound": 27.783278527870625
          },
          "point_estimate": 27.76176342761506,
          "standard_error": 0.011201557714274429
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.73553765426576,
            "upper_bound": 27.782944248557484
          },
          "point_estimate": 27.766167938721154,
          "standard_error": 0.011933705295267622
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006673848475300011,
            "upper_bound": 0.06139198008810612
          },
          "point_estimate": 0.026849013346556073,
          "standard_error": 0.014588376779443845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.75326460279133,
            "upper_bound": 27.789329740784485
          },
          "point_estimate": 27.770280164614764,
          "standard_error": 0.009157863311354828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016842429992581818,
            "upper_bound": 0.04964471799603544
          },
          "point_estimate": 0.0373778849344838,
          "standard_error": 0.008177110465067918
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.03615204188706,
            "upper_bound": 28.077977432520605
          },
          "point_estimate": 28.056237586183492,
          "standard_error": 0.010692957271015775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.031201719069873,
            "upper_bound": 28.07909186984789
          },
          "point_estimate": 28.052763908875296,
          "standard_error": 0.01037482669068914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004359747420054874,
            "upper_bound": 0.06260446922274901
          },
          "point_estimate": 0.02559681082337551,
          "standard_error": 0.015980859038240793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.05003936309752,
            "upper_bound": 28.069280782613504
          },
          "point_estimate": 28.056906724738376,
          "standard_error": 0.00491857109068192
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01728911329255679,
            "upper_bound": 0.04669313963722456
          },
          "point_estimate": 0.0355854057888302,
          "standard_error": 0.007655586382948418
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.86475054204215,
            "upper_bound": 44.03207983324566
          },
          "point_estimate": 43.94019094174804,
          "standard_error": 0.042905660054141835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.83755224155533,
            "upper_bound": 44.058167386581985
          },
          "point_estimate": 43.891486701253186,
          "standard_error": 0.044294146883198826
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02236922621138723,
            "upper_bound": 0.20068400039242695
          },
          "point_estimate": 0.07765189240850451,
          "standard_error": 0.0424601526335759
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.85683700877515,
            "upper_bound": 43.91925086847001
          },
          "point_estimate": 43.88690912521662,
          "standard_error": 0.01592382768703996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.040560785353998326,
            "upper_bound": 0.18037798977551417
          },
          "point_estimate": 0.1428668384977172,
          "standard_error": 0.036876685153703216
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.8733177646836,
            "upper_bound": 36.95834059693652
          },
          "point_estimate": 36.91226323480818,
          "standard_error": 0.021739313073399207
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.8759793530423,
            "upper_bound": 36.94601199006207
          },
          "point_estimate": 36.89101876719656,
          "standard_error": 0.021201718141932575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0056209669808207985,
            "upper_bound": 0.10545645943900792
          },
          "point_estimate": 0.040249220089555234,
          "standard_error": 0.027406728231066255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.87959488210149,
            "upper_bound": 36.93636187983964
          },
          "point_estimate": 36.90940487784973,
          "standard_error": 0.015063270342881952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02750292931177467,
            "upper_bound": 0.10195186094380816
          },
          "point_estimate": 0.07236038872650884,
          "standard_error": 0.020925633851533215
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.68172130787913,
            "upper_bound": 29.7387537767943
          },
          "point_estimate": 29.710464813554132,
          "standard_error": 0.014584844758726704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.68074400142792,
            "upper_bound": 29.742620242841745
          },
          "point_estimate": 29.706749459294308,
          "standard_error": 0.016771232983741717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009772210286870356,
            "upper_bound": 0.08158235229185468
          },
          "point_estimate": 0.044301706138348274,
          "standard_error": 0.017861056899904783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.683380529888545,
            "upper_bound": 29.72447605581301
          },
          "point_estimate": 29.699754378848095,
          "standard_error": 0.01037949660907359
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02367851053482755,
            "upper_bound": 0.06464189412667681
          },
          "point_estimate": 0.04862474185201087,
          "standard_error": 0.010524364143748148
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.379632289791616,
            "upper_bound": 39.446072577747685
          },
          "point_estimate": 39.412247185435945,
          "standard_error": 0.01698432846264864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.35768311141202,
            "upper_bound": 39.46690769577973
          },
          "point_estimate": 39.40257029113067,
          "standard_error": 0.02554454689215587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010118418588134538,
            "upper_bound": 0.09919578654855364
          },
          "point_estimate": 0.06873153018577556,
          "standard_error": 0.022178465174857817
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.3783277618359,
            "upper_bound": 39.45819504400812
          },
          "point_estimate": 39.41333859965357,
          "standard_error": 0.020176352169812783
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03369860030269009,
            "upper_bound": 0.06849560664422472
          },
          "point_estimate": 0.05653862033736377,
          "standard_error": 0.008843950492276374
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.91410371394862,
            "upper_bound": 33.9556110445976
          },
          "point_estimate": 33.93462505001833,
          "standard_error": 0.010628624762970688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.903407324938954,
            "upper_bound": 33.95666626105864
          },
          "point_estimate": 33.93939142545977,
          "standard_error": 0.011966612419820126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003282170811803086,
            "upper_bound": 0.0605912580318286
          },
          "point_estimate": 0.03380831944129428,
          "standard_error": 0.015453027044112775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.92535444182039,
            "upper_bound": 33.972816335439035
          },
          "point_estimate": 33.94889473600591,
          "standard_error": 0.012382622828367364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0179026099364588,
            "upper_bound": 0.04649002916852693
          },
          "point_estimate": 0.03533069504216425,
          "standard_error": 0.00731613508084527
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.886192100852675,
            "upper_bound": 27.98105660928779
          },
          "point_estimate": 27.930596752295667,
          "standard_error": 0.024264411446607743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.877568488499954,
            "upper_bound": 27.98896501524467
          },
          "point_estimate": 27.903167382145767,
          "standard_error": 0.03068351849474946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011177123045945326,
            "upper_bound": 0.1362637374794788
          },
          "point_estimate": 0.06041795755632677,
          "standard_error": 0.03294256942594441
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.91331199175706,
            "upper_bound": 28.00299600210675
          },
          "point_estimate": 27.95973542934425,
          "standard_error": 0.02250791745866991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.037972323580419055,
            "upper_bound": 0.10816381230009312
          },
          "point_estimate": 0.08097640329146848,
          "standard_error": 0.019021716467289545
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.18043666618422,
            "upper_bound": 46.28901989225013
          },
          "point_estimate": 46.23697627381418,
          "standard_error": 0.027854441607966924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.1730037086415,
            "upper_bound": 46.31728787734502
          },
          "point_estimate": 46.24445640803169,
          "standard_error": 0.03131102833948655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016743062103156545,
            "upper_bound": 0.15448086506048234
          },
          "point_estimate": 0.08940846770132338,
          "standard_error": 0.0372292433135764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.1583745932678,
            "upper_bound": 46.24691048709935
          },
          "point_estimate": 46.20916756557096,
          "standard_error": 0.022812031298871077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04545217189396672,
            "upper_bound": 0.1196985784607267
          },
          "point_estimate": 0.09255793624257788,
          "standard_error": 0.01912996779420017
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46481.392277284926,
            "upper_bound": 46565.42954796963
          },
          "point_estimate": 46517.71842966892,
          "standard_error": 21.85464059769139
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46465.72812633376,
            "upper_bound": 46547.55054417414
          },
          "point_estimate": 46500.36737089202,
          "standard_error": 19.93086007884651
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.51560699059397,
            "upper_bound": 87.77627786548068
          },
          "point_estimate": 46.18333720825159,
          "standard_error": 23.03123662136066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46479.894729881635,
            "upper_bound": 46515.87516292209
          },
          "point_estimate": 46498.52256015431,
          "standard_error": 9.0760549563332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.30790415382512,
            "upper_bound": 103.37859680578728
          },
          "point_estimate": 72.90530072173605,
          "standard_error": 23.645368720098897
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49192.90993048699,
            "upper_bound": 49268.343971200586
          },
          "point_estimate": 49229.69472087796,
          "standard_error": 19.343948772276587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49162.21544715447,
            "upper_bound": 49273.324622531945
          },
          "point_estimate": 49229.21156278229,
          "standard_error": 24.751868742830908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.167664212299147,
            "upper_bound": 112.60439713705252
          },
          "point_estimate": 75.43028707547987,
          "standard_error": 29.35491664105859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49168.76684372936,
            "upper_bound": 49258.31040015421
          },
          "point_estimate": 49203.76581846338,
          "standard_error": 22.91468600335113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.08981750640718,
            "upper_bound": 82.0033494856967
          },
          "point_estimate": 64.50987421728641,
          "standard_error": 12.154337148828182
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52036.82054282075,
            "upper_bound": 52145.08086324446
          },
          "point_estimate": 52086.514013792235,
          "standard_error": 27.829959698672344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52014.48328557784,
            "upper_bound": 52152.94679441261
          },
          "point_estimate": 52073.2893982808,
          "standard_error": 25.60191258007756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6217495918573586,
            "upper_bound": 137.7203683286134
          },
          "point_estimate": 56.07694852687921,
          "standard_error": 36.92588179324285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52054.20585347094,
            "upper_bound": 52189.81806086336
          },
          "point_estimate": 52115.25921184832,
          "standard_error": 34.691159507780625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.55947073744404,
            "upper_bound": 119.41852134561752
          },
          "point_estimate": 92.89732163656387,
          "standard_error": 22.673991129820873
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19959.955798631316,
            "upper_bound": 19982.435127909033
          },
          "point_estimate": 19971.541668697748,
          "standard_error": 5.761390991996554
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19950.941478627778,
            "upper_bound": 19985.82879288204
          },
          "point_estimate": 19976.679347826088,
          "standard_error": 8.869832573417805
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.873047424227157,
            "upper_bound": 32.3363730637807
          },
          "point_estimate": 14.9362053828749,
          "standard_error": 7.780671412966604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19954.5767975776,
            "upper_bound": 19984.2589482248
          },
          "point_estimate": 19971.448152727844,
          "standard_error": 7.863702860603089
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.266931736561846,
            "upper_bound": 22.991214412795745
          },
          "point_estimate": 19.16703129729688,
          "standard_error": 3.0970698231767755
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22964.46715005619,
            "upper_bound": 23008.681735672144
          },
          "point_estimate": 22985.646384900567,
          "standard_error": 11.377875070809356
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22956.897387273493,
            "upper_bound": 23021.47977243995
          },
          "point_estimate": 22973.587165507794,
          "standard_error": 16.644436825699405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.507966114662578,
            "upper_bound": 61.77934111558632
          },
          "point_estimate": 25.947666740660814,
          "standard_error": 15.367300988032833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22961.508346299717,
            "upper_bound": 23016.67716253138
          },
          "point_estimate": 22988.48662386918,
          "standard_error": 15.4021007510515
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.967412455820085,
            "upper_bound": 45.673863121081375
          },
          "point_estimate": 37.867438647308376,
          "standard_error": 6.633968004133376
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17638.83976233819,
            "upper_bound": 17661.563084476133
          },
          "point_estimate": 17649.956796521034,
          "standard_error": 5.7929323366044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17633.56796116505,
            "upper_bound": 17663.448810679613
          },
          "point_estimate": 17651.24448826861,
          "standard_error": 7.074717438774592
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.8312194586828303,
            "upper_bound": 33.01480250610019
          },
          "point_estimate": 20.669059978926548,
          "standard_error": 7.764571198829972
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17637.71510385997,
            "upper_bound": 17653.399195715258
          },
          "point_estimate": 17646.21231874921,
          "standard_error": 4.089669274753487
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.968966272093576,
            "upper_bound": 25.223075531476713
          },
          "point_estimate": 19.30358522574062,
          "standard_error": 3.923100280602516
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17453.620316602628,
            "upper_bound": 17480.6878422762
          },
          "point_estimate": 17467.893963431834,
          "standard_error": 6.98006517714273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17446.273695164906,
            "upper_bound": 17483.94065916472
          },
          "point_estimate": 17477.006658261285,
          "standard_error": 9.400443433383469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9824971511451308,
            "upper_bound": 36.770619614561646
          },
          "point_estimate": 17.93737084389021,
          "standard_error": 9.39100676504116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17461.90726456079,
            "upper_bound": 17485.473827435606
          },
          "point_estimate": 17475.85191187295,
          "standard_error": 5.930290118794945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.95586377836793,
            "upper_bound": 29.13476368886995
          },
          "point_estimate": 23.248075686905928,
          "standard_error": 4.477364769346196
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17306.477243763904,
            "upper_bound": 17343.382251290914
          },
          "point_estimate": 17323.16405968647,
          "standard_error": 9.517614726878
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17302.79371888571,
            "upper_bound": 17339.16871226565
          },
          "point_estimate": 17318.07600095329,
          "standard_error": 8.21429603154849
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9743266093754337,
            "upper_bound": 46.86496396779424
          },
          "point_estimate": 16.732524996869905,
          "standard_error": 11.299240349217596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17306.14448112242,
            "upper_bound": 17337.75836490212
          },
          "point_estimate": 17318.72678617855,
          "standard_error": 8.194486966264277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.329026304756429,
            "upper_bound": 43.755080146821975
          },
          "point_estimate": 31.75521182406198,
          "standard_error": 9.006311151645598
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25304.894935916993,
            "upper_bound": 25356.5620542365
          },
          "point_estimate": 25328.412110795205,
          "standard_error": 13.286276243651455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25289.13564944134,
            "upper_bound": 25349.60111731843
          },
          "point_estimate": 25320.494929391683,
          "standard_error": 15.731411807301566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.589899674100156,
            "upper_bound": 69.23916158500705
          },
          "point_estimate": 44.82305054152084,
          "standard_error": 15.164634252778434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25304.872863258977,
            "upper_bound": 25331.937167811146
          },
          "point_estimate": 25318.141137633316,
          "standard_error": 6.855828077378626
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.78017976990354,
            "upper_bound": 60.81842725767362
          },
          "point_estimate": 44.299511375322695,
          "standard_error": 11.86183441347258
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26428.135524242425,
            "upper_bound": 26454.657990227275
          },
          "point_estimate": 26441.767072121213,
          "standard_error": 6.81870992394062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26425.181333333338,
            "upper_bound": 26463.872981818186
          },
          "point_estimate": 26441.005212121217,
          "standard_error": 11.17748456760308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.623474539730781,
            "upper_bound": 40.80267880288378
          },
          "point_estimate": 29.637829412000855,
          "standard_error": 9.570240153221093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26430.267760265735,
            "upper_bound": 26461.6627977068
          },
          "point_estimate": 26448.843024321133,
          "standard_error": 8.132722378620574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.632848985519876,
            "upper_bound": 27.88324752194369
          },
          "point_estimate": 22.799099239551666,
          "standard_error": 3.6561358878436296
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41161.06463483955,
            "upper_bound": 41256.168940645104
          },
          "point_estimate": 41203.859090814905,
          "standard_error": 24.467851074901088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41150.63233068483,
            "upper_bound": 41245.45329171396
          },
          "point_estimate": 41174.93853485397,
          "standard_error": 23.39003427885622
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.167808019080674,
            "upper_bound": 120.06787925316462
          },
          "point_estimate": 37.52453107425433,
          "standard_error": 28.844333870123265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41164.01183088912,
            "upper_bound": 41228.98918434827
          },
          "point_estimate": 41191.46440438109,
          "standard_error": 16.593090358096223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.84954158791648,
            "upper_bound": 109.36193555538148
          },
          "point_estimate": 81.56866246267315,
          "standard_error": 22.19161721692751
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17383.090322589225,
            "upper_bound": 17400.6429704537
          },
          "point_estimate": 17391.217892684283,
          "standard_error": 4.5254540070644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17383.265677357587,
            "upper_bound": 17402.9230825488
          },
          "point_estimate": 17386.83200694112,
          "standard_error": 4.011237971160836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8631622932008003,
            "upper_bound": 23.382506669603757
          },
          "point_estimate": 5.848329930876677,
          "standard_error": 5.684725385906872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17387.19383957921,
            "upper_bound": 17400.81564631873
          },
          "point_estimate": 17392.330655940517,
          "standard_error": 3.489050395892314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5612960221358168,
            "upper_bound": 19.59195752082607
          },
          "point_estimate": 15.06826336833788,
          "standard_error": 3.7639242715620074
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25428.987073643508,
            "upper_bound": 25458.48204861459
          },
          "point_estimate": 25442.539337465656,
          "standard_error": 7.558576239043588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25420.30927353422,
            "upper_bound": 25457.22154369807
          },
          "point_estimate": 25440.238244569024,
          "standard_error": 9.163607424062311
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.954775117362,
            "upper_bound": 41.24417892497232
          },
          "point_estimate": 25.187156851371917,
          "standard_error": 8.689105958968543
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25429.360239371817,
            "upper_bound": 25447.059269979585
          },
          "point_estimate": 25438.07573967728,
          "standard_error": 4.447896873488104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.86100621342707,
            "upper_bound": 34.41395094093533
          },
          "point_estimate": 25.140951957895503,
          "standard_error": 6.458323522083433
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68829.13419011544,
            "upper_bound": 68932.05510732325
          },
          "point_estimate": 68882.3755844156,
          "standard_error": 26.29830985243833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68831.81605113637,
            "upper_bound": 68949.3896780303
          },
          "point_estimate": 68886.11229482322,
          "standard_error": 30.59569779783531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.031699608837505,
            "upper_bound": 152.92061684004736
          },
          "point_estimate": 76.25323347577445,
          "standard_error": 34.45922792807638
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68799.59132416993,
            "upper_bound": 68916.22901770526
          },
          "point_estimate": 68870.7122245179,
          "standard_error": 30.0156541364327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.09355954342775,
            "upper_bound": 116.85120910929928
          },
          "point_estimate": 87.66925055636155,
          "standard_error": 19.337221209764586
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36999.90543300998,
            "upper_bound": 37051.38853611393
          },
          "point_estimate": 37026.11476638571,
          "standard_error": 13.22974764271627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36992.28168870803,
            "upper_bound": 37066.17332146491
          },
          "point_estimate": 37027.06964830693,
          "standard_error": 22.07470456842537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.40289724663698,
            "upper_bound": 70.7824598827242
          },
          "point_estimate": 54.77586639020405,
          "standard_error": 16.705610467887677
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37003.42955599864,
            "upper_bound": 37067.11756380203
          },
          "point_estimate": 37035.2092904044,
          "standard_error": 16.72852544588549
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.28696868797219,
            "upper_bound": 54.27009159353286
          },
          "point_estimate": 43.931505680013856,
          "standard_error": 7.033074285815271
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59403.591124319166,
            "upper_bound": 59496.47092931062
          },
          "point_estimate": 59447.35662050005,
          "standard_error": 23.738886249959936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59400.07422969187,
            "upper_bound": 59491.56590413943
          },
          "point_estimate": 59431.16412490922,
          "standard_error": 25.177424109335792
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.84746937975742,
            "upper_bound": 127.22791166281934
          },
          "point_estimate": 49.42952012462869,
          "standard_error": 28.786972466589894
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59399.24963035806,
            "upper_bound": 59445.638487459575
          },
          "point_estimate": 59419.616547831254,
          "standard_error": 11.905592036465505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.786341105818174,
            "upper_bound": 109.4609161712038
          },
          "point_estimate": 79.24041656983543,
          "standard_error": 20.401593917061422
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26082.1364421282,
            "upper_bound": 26107.938998431557
          },
          "point_estimate": 26095.515079736477,
          "standard_error": 6.605219724453292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26076.01867350612,
            "upper_bound": 26113.15715314203
          },
          "point_estimate": 26095.519402447804,
          "standard_error": 8.983659936511797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.646497559454157,
            "upper_bound": 39.45891873790936
          },
          "point_estimate": 24.187567194775013,
          "standard_error": 9.667057791142405
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26092.894819109657,
            "upper_bound": 26116.472973610627
          },
          "point_estimate": 26104.039682851348,
          "standard_error": 6.16012787929551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.554290092310763,
            "upper_bound": 27.483755352676308
          },
          "point_estimate": 22.01970333865645,
          "standard_error": 4.025041778315389
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21953.564810805394,
            "upper_bound": 21978.03500719621
          },
          "point_estimate": 21965.2574858054,
          "standard_error": 6.28749097536965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21948.36518126888,
            "upper_bound": 21981.423081570996
          },
          "point_estimate": 21963.589603893925,
          "standard_error": 8.882061479913201
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.6846351887376905,
            "upper_bound": 33.60623643861711
          },
          "point_estimate": 24.505821058893105,
          "standard_error": 7.684677489794506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21953.376304047782,
            "upper_bound": 21967.219615592745
          },
          "point_estimate": 21960.5644147997,
          "standard_error": 3.481609913132087
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.35731813132214,
            "upper_bound": 26.422249854142738
          },
          "point_estimate": 20.984305994827128,
          "standard_error": 3.867538491337409
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28693.85038252649,
            "upper_bound": 28736.561009501187
          },
          "point_estimate": 28715.48011373776,
          "standard_error": 10.985939726292123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28682.452296120347,
            "upper_bound": 28747.199348992697
          },
          "point_estimate": 28715.635740677903,
          "standard_error": 18.357612157634268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.914585464947008,
            "upper_bound": 60.76201748816112
          },
          "point_estimate": 38.595390334852034,
          "standard_error": 14.759019309828252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28700.439911079626,
            "upper_bound": 28735.440715552573
          },
          "point_estimate": 28718.235818654823,
          "standard_error": 8.986034911304744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.591060393978264,
            "upper_bound": 43.99902599871602
          },
          "point_estimate": 36.5962508228159,
          "standard_error": 5.495664746689386
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413753.92368465906,
            "upper_bound": 414436.09315025253
          },
          "point_estimate": 414040.5480077561,
          "standard_error": 178.1963856701824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413646.65625,
            "upper_bound": 414162.70119949494
          },
          "point_estimate": 413929.7147727273,
          "standard_error": 171.6553123718182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.655082712131044,
            "upper_bound": 699.4439151392025
          },
          "point_estimate": 293.906491088931,
          "standard_error": 167.74198672272976
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413697.72894385026,
            "upper_bound": 414104.76216909866
          },
          "point_estimate": 413934.1860094451,
          "standard_error": 104.74676695758588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.2218135677213,
            "upper_bound": 874.7988258257062
          },
          "point_estimate": 592.1370350781743,
          "standard_error": 213.8621479494771
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2668086.614251275,
            "upper_bound": 2670362.6299829935
          },
          "point_estimate": 2669282.7635034015,
          "standard_error": 583.7362162697115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2668066.685714286,
            "upper_bound": 2670867.380952381
          },
          "point_estimate": 2669475.6726190476,
          "standard_error": 771.1029104594198
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338.09141721231543,
            "upper_bound": 3247.5293423449275
          },
          "point_estimate": 2038.844126303462,
          "standard_error": 715.4149136284869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2668390.9631161243,
            "upper_bound": 2670375.836271154
          },
          "point_estimate": 2669394.044341373,
          "standard_error": 506.634585776884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 991.8497465798788,
            "upper_bound": 2520.3370558395086
          },
          "point_estimate": 1939.1325045984408,
          "standard_error": 407.23269864939465
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117622.32291262138,
            "upper_bound": 117822.04628936147
          },
          "point_estimate": 117728.90788847792,
          "standard_error": 51.475961269552734
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117600.64886731392,
            "upper_bound": 117865.33090614888
          },
          "point_estimate": 117782.72715364464,
          "standard_error": 65.7245569088231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.341339338170265,
            "upper_bound": 274.9312652916029
          },
          "point_estimate": 126.7590514442861,
          "standard_error": 65.66214301808151
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117751.18542128184,
            "upper_bound": 117853.37449992853
          },
          "point_estimate": 117805.98417181525,
          "standard_error": 26.446061909381857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.11771665167355,
            "upper_bound": 211.523426397046
          },
          "point_estimate": 172.04613123487712,
          "standard_error": 35.098009532719715
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9097.994171197546,
            "upper_bound": 9125.276518790704
          },
          "point_estimate": 9111.44779057378,
          "standard_error": 6.959988155955954
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9092.70112923463,
            "upper_bound": 9129.314219991636
          },
          "point_estimate": 9109.588728565452,
          "standard_error": 9.123407636422444
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.831017887926384,
            "upper_bound": 39.90267047519146
          },
          "point_estimate": 23.456862870883803,
          "standard_error": 8.362293764610296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9094.63790971406,
            "upper_bound": 9132.768530081648
          },
          "point_estimate": 9111.113189395295,
          "standard_error": 9.884016175630002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.979109710978925,
            "upper_bound": 29.567409566416472
          },
          "point_estimate": 23.24720386028084,
          "standard_error": 4.294121954828464
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8881.624318293281,
            "upper_bound": 8925.264316729528
          },
          "point_estimate": 8899.08859809119,
          "standard_error": 11.78914045670614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8873.195479810365,
            "upper_bound": 8902.693177483516
          },
          "point_estimate": 8891.391552231486,
          "standard_error": 8.525259724418833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.31507370210905117,
            "upper_bound": 34.85002681303355
          },
          "point_estimate": 17.893593919695796,
          "standard_error": 8.456769476392955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8878.44807468647,
            "upper_bound": 8899.617485467374
          },
          "point_estimate": 8888.968638178889,
          "standard_error": 5.567729968945918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.798773737936456,
            "upper_bound": 59.2446201723652
          },
          "point_estimate": 39.159103061493454,
          "standard_error": 16.597717780788308
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14124.642198705502,
            "upper_bound": 14141.06786361535
          },
          "point_estimate": 14132.791441701338,
          "standard_error": 4.203727970623746
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14121.275080906147,
            "upper_bound": 14143.8652815534
          },
          "point_estimate": 14132.758038834952,
          "standard_error": 6.8217808887699425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.972061170535981,
            "upper_bound": 23.963800071646528
          },
          "point_estimate": 15.59973459683591,
          "standard_error": 5.272145426715914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14123.539345934309,
            "upper_bound": 14141.573977876256
          },
          "point_estimate": 14132.038223679234,
          "standard_error": 4.559093154440566
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.475426738166167,
            "upper_bound": 17.344598629747377
          },
          "point_estimate": 14.003149732359976,
          "standard_error": 2.2765368108903568
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.95409428800568,
            "upper_bound": 30.21897243674388
          },
          "point_estimate": 30.083020836277576,
          "standard_error": 0.06799494611328849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.93803642671872,
            "upper_bound": 30.24195187739438
          },
          "point_estimate": 30.04572256558011,
          "standard_error": 0.07868900070082417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05868747893274434,
            "upper_bound": 0.3868926206945066
          },
          "point_estimate": 0.18273652080456204,
          "standard_error": 0.0889088534788349
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.98414287670492,
            "upper_bound": 30.101129604394274
          },
          "point_estimate": 30.028769598487187,
          "standard_error": 0.0300379173621776
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11303712691321384,
            "upper_bound": 0.2953313516296331
          },
          "point_estimate": 0.22660863304423395,
          "standard_error": 0.04646868282680433
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26582970.50005208,
            "upper_bound": 26605767.385833334
          },
          "point_estimate": 26594404.154583335,
          "standard_error": 5837.915850716841
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26578901.177083336,
            "upper_bound": 26612817.333333332
          },
          "point_estimate": 26591223.075,
          "standard_error": 8251.441551240467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1642.6929720852654,
            "upper_bound": 33105.09836226518
          },
          "point_estimate": 26130.58979108948,
          "standard_error": 8649.301381821422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26586448.764305174,
            "upper_bound": 26611080.825648416
          },
          "point_estimate": 26598667.36623377,
          "standard_error": 6247.908093552902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11452.028480858806,
            "upper_bound": 24016.216861835394
          },
          "point_estimate": 19431.043469771957,
          "standard_error": 3226.6375724121885
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76738616.14583333,
            "upper_bound": 76844554.97166666
          },
          "point_estimate": 76789514.8,
          "standard_error": 27070.566560088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76693789.0,
            "upper_bound": 76837214.66666666
          },
          "point_estimate": 76791959.66666667,
          "standard_error": 32821.13731013521
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11175.344401594297,
            "upper_bound": 156723.17221762222
          },
          "point_estimate": 91339.77207838753,
          "standard_error": 40277.425433450066
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47956.14488878305,
            "upper_bound": 118183.23412052415
          },
          "point_estimate": 90107.4207658103,
          "standard_error": 19189.11800287264
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.753463790318994,
            "upper_bound": 5.837095377470871
          },
          "point_estimate": 5.789904519209024,
          "standard_error": 0.02166804463368577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.743058569793877,
            "upper_bound": 5.803262405096161
          },
          "point_estimate": 5.787509501082125,
          "standard_error": 0.01797635272106399
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004431284479190704,
            "upper_bound": 0.0948714180985746
          },
          "point_estimate": 0.028073496208102636,
          "standard_error": 0.022611087446558865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.760743361629169,
            "upper_bound": 5.792553354007337
          },
          "point_estimate": 5.77662539464663,
          "standard_error": 0.007984953476217182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02524607877521178,
            "upper_bound": 0.1051382428946998
          },
          "point_estimate": 0.0724341942437524,
          "standard_error": 0.024531438260117795
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.131996579783172,
            "upper_bound": 5.135941420091839
          },
          "point_estimate": 5.133938386282255,
          "standard_error": 0.0010082976069255916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.130700696126498,
            "upper_bound": 5.136482268598628
          },
          "point_estimate": 5.134187319719402,
          "standard_error": 0.0016807157516201825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007955361344056442,
            "upper_bound": 0.005840436007566442
          },
          "point_estimate": 0.004088216969787892,
          "standard_error": 0.0012703463126571256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.131736544898255,
            "upper_bound": 5.137744665675392
          },
          "point_estimate": 5.134968159167859,
          "standard_error": 0.001583654481272063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020873330757646582,
            "upper_bound": 0.0041541293144178116
          },
          "point_estimate": 0.003368356470093291,
          "standard_error": 0.0005364595290882571
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.377927653250368,
            "upper_bound": 5.388139518613096
          },
          "point_estimate": 5.38281865178898,
          "standard_error": 0.002620245384218762
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.375509924482553,
            "upper_bound": 5.38994935948375
          },
          "point_estimate": 5.382238582260841,
          "standard_error": 0.003169090087239373
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013738446650724272,
            "upper_bound": 0.015962231779616736
          },
          "point_estimate": 0.008504443896086433,
          "standard_error": 0.0037258158570198778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.3779122753307025,
            "upper_bound": 5.390215952248672
          },
          "point_estimate": 5.383807827587919,
          "standard_error": 0.0032075447062085575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004508171091652527,
            "upper_bound": 0.010895085756208876
          },
          "point_estimate": 0.008733228310077803,
          "standard_error": 0.001626435365840375
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.233960275965945,
            "upper_bound": 5.2918719433452175
          },
          "point_estimate": 5.2630080251184586,
          "standard_error": 0.01489866064383056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.221563397586768,
            "upper_bound": 5.318074053902573
          },
          "point_estimate": 5.2630991343167075,
          "standard_error": 0.023277259357342334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00968634788295968,
            "upper_bound": 0.0854428095690562
          },
          "point_estimate": 0.06849365002162458,
          "standard_error": 0.01977155629718534
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2423864820609385,
            "upper_bound": 5.298472872961003
          },
          "point_estimate": 5.27336240256199,
          "standard_error": 0.01416808795667843
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03047090359293391,
            "upper_bound": 0.06002488734349117
          },
          "point_estimate": 0.04966357920471087,
          "standard_error": 0.007557201372094698
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.603613748331279,
            "upper_bound": 4.634717010175244
          },
          "point_estimate": 4.619102712801064,
          "standard_error": 0.007905125618959256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.5996299973193855,
            "upper_bound": 4.633887999852647
          },
          "point_estimate": 4.618455747583798,
          "standard_error": 0.006042654308920993
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005544674904828459,
            "upper_bound": 0.05279607961209015
          },
          "point_estimate": 0.009224228509200438,
          "standard_error": 0.014461857750525086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.612939068186055,
            "upper_bound": 4.6302262022743355
          },
          "point_estimate": 4.621461928052675,
          "standard_error": 0.004343429290894232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011910358995402964,
            "upper_bound": 0.03526896905575833
          },
          "point_estimate": 0.026384349563847595,
          "standard_error": 0.005965802358793806
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.521555968927853,
            "upper_bound": 18.581116542138247
          },
          "point_estimate": 18.548202584851627,
          "standard_error": 0.015289559527266789
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.51731506015565,
            "upper_bound": 18.570501908731853
          },
          "point_estimate": 18.53941804361908,
          "standard_error": 0.0103173094965043
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001975693751705907,
            "upper_bound": 0.0826023525908319
          },
          "point_estimate": 0.021700504645938583,
          "standard_error": 0.020036157927115023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.510656974449695,
            "upper_bound": 18.561995353912817
          },
          "point_estimate": 18.532993944784337,
          "standard_error": 0.012864366789907777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01381696285787441,
            "upper_bound": 0.07057319409978616
          },
          "point_estimate": 0.0509132635416465,
          "standard_error": 0.0151278620373828
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.367600423947534,
            "upper_bound": 7.374753945493683
          },
          "point_estimate": 7.371200735816457,
          "standard_error": 0.0018319875979292395
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.365829417684897,
            "upper_bound": 7.376567650044109
          },
          "point_estimate": 7.371327371906698,
          "standard_error": 0.0023469685920705103
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013999180548993572,
            "upper_bound": 0.010943231564610354
          },
          "point_estimate": 0.006205325435011285,
          "standard_error": 0.0025791579545276105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.37116634023153,
            "upper_bound": 7.376535644363212
          },
          "point_estimate": 7.3735473824243245,
          "standard_error": 0.0013752477234692975
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003496546307549386,
            "upper_bound": 0.00765046581144741
          },
          "point_estimate": 0.006114610761844023,
          "standard_error": 0.001070679428343723
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.112573837726459,
            "upper_bound": 6.128334444406802
          },
          "point_estimate": 6.119614287081054,
          "standard_error": 0.004073593389756706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.111243928858695,
            "upper_bound": 6.129436817191836
          },
          "point_estimate": 6.113980089870132,
          "standard_error": 0.003684738380120237
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010809869907381275,
            "upper_bound": 0.01649303544260519
          },
          "point_estimate": 0.006423108077709789,
          "standard_error": 0.003897369157240292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.112369503044794,
            "upper_bound": 6.124100266725657
          },
          "point_estimate": 6.116505366330099,
          "standard_error": 0.003066312923190448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029596332274907933,
            "upper_bound": 0.017570373051238656
          },
          "point_estimate": 0.01359090686050336,
          "standard_error": 0.0039164768488434785
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.769377726959249,
            "upper_bound": 9.78220891382516
          },
          "point_estimate": 9.776111440065502,
          "standard_error": 0.0032900594476113253
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.764413950456849,
            "upper_bound": 9.783211204652986
          },
          "point_estimate": 9.780519137229309,
          "standard_error": 0.004738395653356942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008826075215447784,
            "upper_bound": 0.017384978616861276
          },
          "point_estimate": 0.004522420520153105,
          "standard_error": 0.004813708634850227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.777524359045518,
            "upper_bound": 9.784807609969148
          },
          "point_estimate": 9.78164740846658,
          "standard_error": 0.0018298200166147153
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004957354856424501,
            "upper_bound": 0.013296842867398727
          },
          "point_estimate": 0.010984861462034637,
          "standard_error": 0.0019787034244666693
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.335145202118514,
            "upper_bound": 7.34712030145728
          },
          "point_estimate": 7.340777592434111,
          "standard_error": 0.003053318773251428
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.333617260683461,
            "upper_bound": 7.34768157286937
          },
          "point_estimate": 7.339660194576533,
          "standard_error": 0.0035039777359307327
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011054899019118324,
            "upper_bound": 0.016255210853234895
          },
          "point_estimate": 0.009082486082457543,
          "standard_error": 0.0037944260957561617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.33672217441339,
            "upper_bound": 7.3448354399204865
          },
          "point_estimate": 7.3403526629866285,
          "standard_error": 0.0020276161022056353
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004689691150855599,
            "upper_bound": 0.01371598667118146
          },
          "point_estimate": 0.010165676179186308,
          "standard_error": 0.0024386991893144312
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.281391485402448,
            "upper_bound": 4.313420058034726
          },
          "point_estimate": 4.298040065715729,
          "standard_error": 0.0081363209728069
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.285873343454395,
            "upper_bound": 4.3189952296632885
          },
          "point_estimate": 4.295023628660086,
          "standard_error": 0.008955403062684435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025627013274221,
            "upper_bound": 0.04971879822837109
          },
          "point_estimate": 0.013679940037982142,
          "standard_error": 0.012165840353014986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.289155793336897,
            "upper_bound": 4.321478845613116
          },
          "point_estimate": 4.30792980888664,
          "standard_error": 0.008316163518110372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013241996914821868,
            "upper_bound": 0.036760970619963695
          },
          "point_estimate": 0.027100563916955627,
          "standard_error": 0.006374311051727796
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.64399756438209,
            "upper_bound": 15.67209366547373
          },
          "point_estimate": 15.657642095389203,
          "standard_error": 0.00721311353281222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.636469066262691,
            "upper_bound": 15.677723123225372
          },
          "point_estimate": 15.653682296628626,
          "standard_error": 0.01180891158522624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036371521571900898,
            "upper_bound": 0.039633614267235016
          },
          "point_estimate": 0.026432691402339295,
          "standard_error": 0.008953669657849998
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.648174255257656,
            "upper_bound": 15.677637103049223
          },
          "point_estimate": 15.661577882454733,
          "standard_error": 0.007498478645041928
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014301156532660895,
            "upper_bound": 0.02960454994468121
          },
          "point_estimate": 0.02402685482570497,
          "standard_error": 0.003973072514124457
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228436.2062950395,
            "upper_bound": 1231535.2639444442
          },
          "point_estimate": 1229828.4531137566,
          "standard_error": 799.0433144572545
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227657.5083333333,
            "upper_bound": 1230938.9777777777
          },
          "point_estimate": 1229428.9793650792,
          "standard_error": 856.023395518084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 505.1606410316387,
            "upper_bound": 3831.458401977832
          },
          "point_estimate": 2432.5532559801827,
          "standard_error": 873.7810968437755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227790.3540526074,
            "upper_bound": 1230821.2383084577
          },
          "point_estimate": 1229132.915411255,
          "standard_error": 800.2913037276371
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1092.203404200981,
            "upper_bound": 3712.082800032086
          },
          "point_estimate": 2658.5605514359313,
          "standard_error": 759.120731941109
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1371449.5440886242,
            "upper_bound": 1373349.5079279102
          },
          "point_estimate": 1372462.67196649,
          "standard_error": 487.1612943275951
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1371431.6697530863,
            "upper_bound": 1373794.8783068785
          },
          "point_estimate": 1372672.5925925926,
          "standard_error": 619.0223224658705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399.80779290210785,
            "upper_bound": 2657.8739037393234
          },
          "point_estimate": 1451.9544753336677,
          "standard_error": 572.6581299647789
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1372074.4138530795,
            "upper_bound": 1373369.822861859
          },
          "point_estimate": 1372664.1357383358,
          "standard_error": 332.37088954815806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 790.5936759057996,
            "upper_bound": 2156.1311682209653
          },
          "point_estimate": 1625.3180850185763,
          "standard_error": 377.01961113706216
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1271303.7976477833,
            "upper_bound": 1273781.4835632185
          },
          "point_estimate": 1272450.4805582925,
          "standard_error": 635.0993452898822
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1271153.5316091953,
            "upper_bound": 1273496.9315270935
          },
          "point_estimate": 1272194.206896552,
          "standard_error": 506.6306857024853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.316613254816456,
            "upper_bound": 3407.1212481321445
          },
          "point_estimate": 1083.2096945623427,
          "standard_error": 826.2782274124406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1271871.5968660414,
            "upper_bound": 1272878.1542395474
          },
          "point_estimate": 1272355.2115539634,
          "standard_error": 253.123530225335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785.3195482744759,
            "upper_bound": 2883.7064008550515
          },
          "point_estimate": 2118.6982276666413,
          "standard_error": 557.1317389314444
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332845.34097968554,
            "upper_bound": 333528.7901158258
          },
          "point_estimate": 333181.40663280914,
          "standard_error": 175.25344520821136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332721.06028833555,
            "upper_bound": 333753.59403669724
          },
          "point_estimate": 332955.935970948,
          "standard_error": 301.42513414010926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.64098237413338,
            "upper_bound": 933.3578916864324
          },
          "point_estimate": 640.1417825801383,
          "standard_error": 249.1722911684167
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332864.36274105543,
            "upper_bound": 333402.7977709029
          },
          "point_estimate": 333122.2867866079,
          "standard_error": 144.66888593700804
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 361.7743021333721,
            "upper_bound": 701.3585127913481
          },
          "point_estimate": 583.1092211238916,
          "standard_error": 86.7295398522368
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256432.90676056335,
            "upper_bound": 256913.41419768613
          },
          "point_estimate": 256658.45558266263,
          "standard_error": 123.00182790675991
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256305.96126760563,
            "upper_bound": 256952.650528169
          },
          "point_estimate": 256570.73274647887,
          "standard_error": 152.52864181160274
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.87305539340308,
            "upper_bound": 655.6627849793724
          },
          "point_estimate": 438.9867153050097,
          "standard_error": 159.2763306091357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256346.16116395345,
            "upper_bound": 256803.5096578987
          },
          "point_estimate": 256544.16623376624,
          "standard_error": 116.63054315792108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.53691041807855,
            "upper_bound": 527.6181050694674
          },
          "point_estimate": 410.74234308786714,
          "standard_error": 86.07980838638255
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447562.3706402439,
            "upper_bound": 447916.164224158
          },
          "point_estimate": 447748.5905749129,
          "standard_error": 90.49548151111334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447516.5737804878,
            "upper_bound": 448011.4207317073
          },
          "point_estimate": 447816.0136469222,
          "standard_error": 128.4546562472501
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.83495933913366,
            "upper_bound": 522.682484500994
          },
          "point_estimate": 299.7949306531804,
          "standard_error": 118.39857891581526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447449.3930411738,
            "upper_bound": 447935.411745828
          },
          "point_estimate": 447700.4327842889,
          "standard_error": 131.02485348859037
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138.3388894801879,
            "upper_bound": 368.48130066969907
          },
          "point_estimate": 303.12685832905333,
          "standard_error": 56.263015333239345
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236177.8221489899,
            "upper_bound": 236555.5036032197
          },
          "point_estimate": 236367.47478535352,
          "standard_error": 96.79959168850216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236065.55762987013,
            "upper_bound": 236639.46753246753
          },
          "point_estimate": 236405.0956168831,
          "standard_error": 154.8019287500158
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.144682838547,
            "upper_bound": 525.0162087472552
          },
          "point_estimate": 362.3221619766016,
          "standard_error": 117.81092629840506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236141.16233889153,
            "upper_bound": 236429.75584415585
          },
          "point_estimate": 236278.973283859,
          "standard_error": 72.59946899723225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.07592418088865,
            "upper_bound": 393.4180022079657
          },
          "point_estimate": 321.8279257995815,
          "standard_error": 49.90021703958371
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639981.0515419801,
            "upper_bound": 641672.4811459205
          },
          "point_estimate": 640748.0937921193,
          "standard_error": 436.7028929694573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639858.7313596492,
            "upper_bound": 641438.2210526316
          },
          "point_estimate": 640338.6200779728,
          "standard_error": 431.97544876303505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.74958285827636,
            "upper_bound": 2135.9909941399824
          },
          "point_estimate": 861.0097162929425,
          "standard_error": 510.37385965274393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 640149.0259784076,
            "upper_bound": 640813.9657667305
          },
          "point_estimate": 640436.5240829346,
          "standard_error": 166.63039193571407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575.8773254144984,
            "upper_bound": 2014.9539546212195
          },
          "point_estimate": 1450.5386233406682,
          "standard_error": 405.1103620176673
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117946.68279852734,
            "upper_bound": 118102.67228709288
          },
          "point_estimate": 118024.19099734593,
          "standard_error": 39.64856257301812
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117917.10606060606,
            "upper_bound": 118112.41087662338
          },
          "point_estimate": 118031.90143784788,
          "standard_error": 46.36665523555698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.924319539762426,
            "upper_bound": 233.05982199873355
          },
          "point_estimate": 125.19866823182473,
          "standard_error": 53.37191167690141
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117927.01882850564,
            "upper_bound": 118176.35013703028
          },
          "point_estimate": 118075.11503626245,
          "standard_error": 64.32633874400895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.94238310638205,
            "upper_bound": 171.36796877047308
          },
          "point_estimate": 132.28657857464833,
          "standard_error": 25.802856974875677
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124210.89437185114,
            "upper_bound": 124428.89325231596
          },
          "point_estimate": 124322.98473427596,
          "standard_error": 56.08125184265745
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124160.06097838451,
            "upper_bound": 124481.44823663254
          },
          "point_estimate": 124391.44636762555,
          "standard_error": 92.24796915762492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.807146954858544,
            "upper_bound": 288.77943978782764
          },
          "point_estimate": 211.32218123120632,
          "standard_error": 75.12610572318447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124219.85060162924,
            "upper_bound": 124400.83814649514
          },
          "point_estimate": 124324.11918797926,
          "standard_error": 46.952223638321705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.94263941572724,
            "upper_bound": 229.73958021111633
          },
          "point_estimate": 186.89153323127175,
          "standard_error": 30.30725426379653
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200652.6410012716,
            "upper_bound": 200999.25735267036
          },
          "point_estimate": 200805.84954989917,
          "standard_error": 89.66008912941446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200585.4534990792,
            "upper_bound": 200918.41804788212
          },
          "point_estimate": 200755.88985354733,
          "standard_error": 85.71342097135215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.699947010145113,
            "upper_bound": 398.53116647435223
          },
          "point_estimate": 231.08559506866064,
          "standard_error": 96.18958989454724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200633.7512044528,
            "upper_bound": 200810.99862253733
          },
          "point_estimate": 200713.5894668867,
          "standard_error": 45.40741982964666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.5902615713203,
            "upper_bound": 420.0756409970696
          },
          "point_estimate": 298.8818586765255,
          "standard_error": 89.99877895077364
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272184.68162214966,
            "upper_bound": 272382.7258374792
          },
          "point_estimate": 272275.944736733,
          "standard_error": 50.85047688841561
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272155.52313432837,
            "upper_bound": 272426.58404850744
          },
          "point_estimate": 272214.3805970149,
          "standard_error": 60.472105601193654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.15038862246405,
            "upper_bound": 261.05996326523
          },
          "point_estimate": 126.75192508177568,
          "standard_error": 58.91074760801471
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272194.0241997862,
            "upper_bound": 272349.94492582546
          },
          "point_estimate": 272260.4732700136,
          "standard_error": 39.67720569368663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.90451041846707,
            "upper_bound": 211.75738160969104
          },
          "point_estimate": 169.5568761916361,
          "standard_error": 38.345978844998925
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302854.15568927914,
            "upper_bound": 303584.0634090254
          },
          "point_estimate": 303180.5286199659,
          "standard_error": 187.6014713275856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302818.4419421487,
            "upper_bound": 303426.67107438017
          },
          "point_estimate": 303096.9849632691,
          "standard_error": 148.76318612495183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.44326656267808,
            "upper_bound": 823.4878959587037
          },
          "point_estimate": 396.6089711406061,
          "standard_error": 183.44735455639577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303019.3882171614,
            "upper_bound": 303326.9531840033
          },
          "point_estimate": 303190.1260276913,
          "standard_error": 80.56971957153357
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.44613708404125,
            "upper_bound": 895.9608234425535
          },
          "point_estimate": 625.7054357652471,
          "standard_error": 199.496626508366
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323548.39676639985,
            "upper_bound": 324706.4506045618
          },
          "point_estimate": 324075.1178037646,
          "standard_error": 298.26172119650636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323474.44776302855,
            "upper_bound": 324843.203539823
          },
          "point_estimate": 323625.13982300885,
          "standard_error": 316.6757471706763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.262409095901866,
            "upper_bound": 1483.5162117154584
          },
          "point_estimate": 476.8018554731021,
          "standard_error": 358.2745783229082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323475.7127646853,
            "upper_bound": 324449.14689379587
          },
          "point_estimate": 323788.4390759683,
          "standard_error": 264.7650346636905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297.18320025737194,
            "upper_bound": 1270.611673939929
          },
          "point_estimate": 995.4891300331374,
          "standard_error": 253.72786039851957
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406679.57798677246,
            "upper_bound": 407354.8519166667
          },
          "point_estimate": 407016.81060978834,
          "standard_error": 171.79013836244258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406701.84305555554,
            "upper_bound": 407353.1
          },
          "point_estimate": 406973.1075,
          "standard_error": 205.8645929042904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.48925398457483,
            "upper_bound": 1005.6272450354162
          },
          "point_estimate": 482.02188366463673,
          "standard_error": 223.2241335407496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406608.3528419509,
            "upper_bound": 407096.9528339039
          },
          "point_estimate": 406858.90132756135,
          "standard_error": 123.83573560516824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.1778591447125,
            "upper_bound": 770.0600928157606
          },
          "point_estimate": 571.6168270506332,
          "standard_error": 133.93322581901424
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277183.1808439355,
            "upper_bound": 277598.03140158125
          },
          "point_estimate": 277395.43127741426,
          "standard_error": 106.02485901744666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277086.3893129771,
            "upper_bound": 277637.2877589967
          },
          "point_estimate": 277500.0171755725,
          "standard_error": 140.87933705578328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.34367651318591,
            "upper_bound": 605.4742435890333
          },
          "point_estimate": 343.63286269700905,
          "standard_error": 146.86144867201767
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277330.1345852525,
            "upper_bound": 277759.27002512605
          },
          "point_estimate": 277569.8746108853,
          "standard_error": 112.0709710524824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191.7306515197047,
            "upper_bound": 450.8788488211176
          },
          "point_estimate": 352.87373679680593,
          "standard_error": 66.59047117072694
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121968.30871764144,
            "upper_bound": 122192.0980611484
          },
          "point_estimate": 122081.26464791733,
          "standard_error": 57.435900586166646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121945.35810162991,
            "upper_bound": 122258.37751677852
          },
          "point_estimate": 122056.17570842654,
          "standard_error": 89.57702881638123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.68938288792161,
            "upper_bound": 331.6119847167374
          },
          "point_estimate": 195.96190524580948,
          "standard_error": 75.8518624125153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121976.88378222522,
            "upper_bound": 122143.45749440716
          },
          "point_estimate": 122056.41576745402,
          "standard_error": 42.56601265326618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.21683512432972,
            "upper_bound": 238.42904965894903
          },
          "point_estimate": 190.7709326468596,
          "standard_error": 32.06682746277911
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205839.42099946196,
            "upper_bound": 206258.9407826036
          },
          "point_estimate": 206037.87726190477,
          "standard_error": 107.7161612939388
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205767.08286252353,
            "upper_bound": 206329.7570621469
          },
          "point_estimate": 205933.8702448211,
          "standard_error": 153.8475033806333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.844617440356295,
            "upper_bound": 593.0728513776751
          },
          "point_estimate": 267.2029264426639,
          "standard_error": 141.14538420047418
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205789.01131080263,
            "upper_bound": 206011.00310640273
          },
          "point_estimate": 205888.34485288724,
          "standard_error": 57.719387185723214
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.28187342728427,
            "upper_bound": 453.9964362062863
          },
          "point_estimate": 358.3293570409209,
          "standard_error": 72.09378783972119
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152567.52664574617,
            "upper_bound": 152779.55155773065
          },
          "point_estimate": 152679.11150129506,
          "standard_error": 54.49837361590474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152515.36631799163,
            "upper_bound": 152816.2993126121
          },
          "point_estimate": 152729.61837517435,
          "standard_error": 71.98144389270553
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.08464894096383,
            "upper_bound": 311.9787358838616
          },
          "point_estimate": 145.6992556560004,
          "standard_error": 69.66694399200927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152630.2671431892,
            "upper_bound": 152798.81375159422
          },
          "point_estimate": 152704.736803782,
          "standard_error": 42.88688171350119
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.77342556309029,
            "upper_bound": 225.4082824282278
          },
          "point_estimate": 181.80699883057224,
          "standard_error": 36.05403315944695
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47645.88010696862,
            "upper_bound": 47732.99717009404
          },
          "point_estimate": 47685.56397688738,
          "standard_error": 22.384159605535466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47630.788608562696,
            "upper_bound": 47745.74435707004
          },
          "point_estimate": 47658.7393434438,
          "standard_error": 26.0325050187821
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.798399349579164,
            "upper_bound": 112.57110427058348
          },
          "point_estimate": 45.50112088288184,
          "standard_error": 24.813748807509313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47633.72312887413,
            "upper_bound": 47705.25704336513
          },
          "point_estimate": 47660.136961073,
          "standard_error": 18.43024432761924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.96164152548606,
            "upper_bound": 93.35807847611376
          },
          "point_estimate": 74.82040032610108,
          "standard_error": 17.83916648812305
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1145739.252172619,
            "upper_bound": 1146992.390684896
          },
          "point_estimate": 1146304.485714286,
          "standard_error": 323.7671391127552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1145547.4375,
            "upper_bound": 1147120.4541666666
          },
          "point_estimate": 1145938.738541667,
          "standard_error": 319.1575629253221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.94297036714852,
            "upper_bound": 1541.3930051348827
          },
          "point_estimate": 583.5305005777627,
          "standard_error": 333.92758779634386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1145674.340551181,
            "upper_bound": 1146891.5851862512
          },
          "point_estimate": 1146109.8728896105,
          "standard_error": 313.4649093361657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303.0844603392201,
            "upper_bound": 1388.757890316128
          },
          "point_estimate": 1078.739891761546,
          "standard_error": 285.21781815847913
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104447.15148638478,
            "upper_bound": 104621.43307072158
          },
          "point_estimate": 104530.44827437968,
          "standard_error": 44.45833350002684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104385.7643678161,
            "upper_bound": 104609.3586566092
          },
          "point_estimate": 104559.24487547892,
          "standard_error": 71.81682971252165
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.381155956728049,
            "upper_bound": 284.31759813722397
          },
          "point_estimate": 186.34508300636568,
          "standard_error": 79.73786374285483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104474.3398954792,
            "upper_bound": 104606.22579743565
          },
          "point_estimate": 104552.84153605015,
          "standard_error": 33.43649419552126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.38478463554857,
            "upper_bound": 196.5631769125941
          },
          "point_estimate": 148.04071621880396,
          "standard_error": 31.657877855840567
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26520.40592362991,
            "upper_bound": 26551.53167694884
          },
          "point_estimate": 26534.982871662272,
          "standard_error": 8.001310348474403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26517.58642232403,
            "upper_bound": 26557.77281341108
          },
          "point_estimate": 26527.07148931001,
          "standard_error": 10.033039132976286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.04980277047756,
            "upper_bound": 42.66419158949965
          },
          "point_estimate": 14.186166665052754,
          "standard_error": 10.776131419942834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26518.70169811232,
            "upper_bound": 26536.856484064367
          },
          "point_estimate": 26525.771544053616,
          "standard_error": 4.73080011972628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.14074187201841,
            "upper_bound": 33.450158710694616
          },
          "point_estimate": 26.5767953962192,
          "standard_error": 5.848283574113065
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25873.770742026616,
            "upper_bound": 25938.54263361585
          },
          "point_estimate": 25903.58350114027,
          "standard_error": 16.61939952284075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25865.61454610436,
            "upper_bound": 25928.72674359236
          },
          "point_estimate": 25896.04568739576,
          "standard_error": 16.69217999669074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.893303323636696,
            "upper_bound": 85.34731405304375
          },
          "point_estimate": 44.36556656760643,
          "standard_error": 18.093115952926286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25863.378174937778,
            "upper_bound": 25933.15922087205
          },
          "point_estimate": 25895.49977999127,
          "standard_error": 17.947631010153827
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.784632988849182,
            "upper_bound": 76.24263062416324
          },
          "point_estimate": 55.452920108110106,
          "standard_error": 14.647797660235844
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550562.3661056999,
            "upper_bound": 551450.6389400252
          },
          "point_estimate": 550974.4525450937,
          "standard_error": 228.5630855912813
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550436.225,
            "upper_bound": 551609.6136363636
          },
          "point_estimate": 550694.8481060606,
          "standard_error": 279.32652614000403
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.86869717931467,
            "upper_bound": 1180.4636955577669
          },
          "point_estimate": 396.06853349874024,
          "standard_error": 298.81131500835505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550383.8127074945,
            "upper_bound": 551387.8554658181
          },
          "point_estimate": 550800.6920897284,
          "standard_error": 275.27353900399714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233.5267367815848,
            "upper_bound": 967.976956566357
          },
          "point_estimate": 764.3409732964928,
          "standard_error": 169.56493406895592
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1127.9205407661973,
            "upper_bound": 1129.201752044224
          },
          "point_estimate": 1128.5701560143889,
          "standard_error": 0.3286794792116238
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1127.4873666770218,
            "upper_bound": 1129.4151235965028
          },
          "point_estimate": 1128.9733360515688,
          "standard_error": 0.5218219337404894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11030493140604508,
            "upper_bound": 1.770530192238713
          },
          "point_estimate": 1.2953794799536078,
          "standard_error": 0.48909843822692833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1127.3597337254398,
            "upper_bound": 1129.1359536826617
          },
          "point_estimate": 1128.071430992121,
          "standard_error": 0.4562523338504574
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6574985319800872,
            "upper_bound": 1.3169673254289325
          },
          "point_estimate": 1.0935065818375769,
          "standard_error": 0.1674107323712347
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.49795759371254,
            "upper_bound": 34.55933006500757
          },
          "point_estimate": 34.529262016463264,
          "standard_error": 0.015732296865595158
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.481778221513004,
            "upper_bound": 34.57739699985683
          },
          "point_estimate": 34.53862024802487,
          "standard_error": 0.02831555064582169
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008808447367660061,
            "upper_bound": 0.08726628836911254
          },
          "point_estimate": 0.060498587316945386,
          "standard_error": 0.022480195407898855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.47168804023647,
            "upper_bound": 34.54197889889529
          },
          "point_estimate": 34.500158194883724,
          "standard_error": 0.017853875030387746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.032175854638304066,
            "upper_bound": 0.06278787032651081
          },
          "point_estimate": 0.05246270592864704,
          "standard_error": 0.007919598211980251
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.8837545003567,
            "upper_bound": 37.92116613981382
          },
          "point_estimate": 37.90209297072273,
          "standard_error": 0.009602248475587336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.87391004832632,
            "upper_bound": 37.9285643011471
          },
          "point_estimate": 37.900683013767264,
          "standard_error": 0.014775810225784563
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006302744938051588,
            "upper_bound": 0.05459956393719337
          },
          "point_estimate": 0.03577439596428997,
          "standard_error": 0.011731002345815035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.87451102170194,
            "upper_bound": 37.920172682395915
          },
          "point_estimate": 37.89849949146952,
          "standard_error": 0.01187610701910588
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0186869762637228,
            "upper_bound": 0.039601199171364114
          },
          "point_estimate": 0.031997234965440095,
          "standard_error": 0.005381211387735629
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.545109581630815,
            "upper_bound": 26.574318459547925
          },
          "point_estimate": 26.55953528633618,
          "standard_error": 0.007475428524803408
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.539431355575314,
            "upper_bound": 26.585956143839784
          },
          "point_estimate": 26.55202931087819,
          "standard_error": 0.013175242588882209
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00626421270932073,
            "upper_bound": 0.04009362933702048
          },
          "point_estimate": 0.02979797835445555,
          "standard_error": 0.009629097065699989
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.54056211903449,
            "upper_bound": 26.572708286254503
          },
          "point_estimate": 26.552562074728648,
          "standard_error": 0.008198455361304069
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015745380619507636,
            "upper_bound": 0.02951337416368743
          },
          "point_estimate": 0.024822292893058703,
          "standard_error": 0.0035323202817100488
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.80918605348828,
            "upper_bound": 32.84539970460573
          },
          "point_estimate": 32.82681946955425,
          "standard_error": 0.00926014783746378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.80168252578727,
            "upper_bound": 32.844200870337446
          },
          "point_estimate": 32.827226904948375,
          "standard_error": 0.01064144919498096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007505549127094614,
            "upper_bound": 0.05328995745876529
          },
          "point_estimate": 0.02661601239352712,
          "standard_error": 0.01196402752714208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.81861507785616,
            "upper_bound": 32.85433165493176
          },
          "point_estimate": 32.832642218196824,
          "standard_error": 0.00925090596133809
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015578537523994032,
            "upper_bound": 0.041098278068146935
          },
          "point_estimate": 0.03093638273284585,
          "standard_error": 0.006717984100998668
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.61169930933952,
            "upper_bound": 40.675171392660424
          },
          "point_estimate": 40.64322527794483,
          "standard_error": 0.016278827555374405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.59485824410912,
            "upper_bound": 40.69695965465895
          },
          "point_estimate": 40.646343528206984,
          "standard_error": 0.02729743272094253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00642346375215214,
            "upper_bound": 0.09059084491934048
          },
          "point_estimate": 0.075687774296861,
          "standard_error": 0.023226849872248365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.60276412566951,
            "upper_bound": 40.69023539474804
          },
          "point_estimate": 40.65140643407757,
          "standard_error": 0.02270822772610896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03324635133281819,
            "upper_bound": 0.0645937294542706
          },
          "point_estimate": 0.05424048522680645,
          "standard_error": 0.007844018309916732
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.40545143727289,
            "upper_bound": 73.56088406975809
          },
          "point_estimate": 73.46683331227571,
          "standard_error": 0.04220613965441024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.39517794711757,
            "upper_bound": 73.47013658325638
          },
          "point_estimate": 73.42727446788302,
          "standard_error": 0.020194058001996463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00685280641266848,
            "upper_bound": 0.09754518534020912
          },
          "point_estimate": 0.04459513991389519,
          "standard_error": 0.025716373231018896
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.39188343037192,
            "upper_bound": 73.46112692289634
          },
          "point_estimate": 73.4237018662203,
          "standard_error": 0.017638754648010332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026758332494176144,
            "upper_bound": 0.21464887455610668
          },
          "point_estimate": 0.1411514754401825,
          "standard_error": 0.0621416403833983
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.02015219909286,
            "upper_bound": 66.13292213165329
          },
          "point_estimate": 66.07504811735711,
          "standard_error": 0.02885679886461304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.98458922482382,
            "upper_bound": 66.160803364401
          },
          "point_estimate": 66.06709670379632,
          "standard_error": 0.04497610374417657
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0254259499918784,
            "upper_bound": 0.1623684802817551
          },
          "point_estimate": 0.1306275393494637,
          "standard_error": 0.035170844833351464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.02513171839526,
            "upper_bound": 66.18058773014155
          },
          "point_estimate": 66.12143290181062,
          "standard_error": 0.039374457511219645
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05693862916567673,
            "upper_bound": 0.116444468282173
          },
          "point_estimate": 0.09610913047640626,
          "standard_error": 0.015057879936087424
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.71408574821585,
            "upper_bound": 56.78923917268941
          },
          "point_estimate": 56.74948133827128,
          "standard_error": 0.019358700689621425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.702071323069646,
            "upper_bound": 56.80875605817094
          },
          "point_estimate": 56.71282698113709,
          "standard_error": 0.03088613519187757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030280267357316805,
            "upper_bound": 0.1024899904256692
          },
          "point_estimate": 0.03431043621564336,
          "standard_error": 0.02943069968671801
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.70246114063628,
            "upper_bound": 56.77504637617325
          },
          "point_estimate": 56.735665645090855,
          "standard_error": 0.02012912418990217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031078535561467377,
            "upper_bound": 0.07676433858908707
          },
          "point_estimate": 0.06461971520825965,
          "standard_error": 0.011557627758665966
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.730843582006,
            "upper_bound": 108.8520753446658
          },
          "point_estimate": 108.79016541966315,
          "standard_error": 0.030948030996149685
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.71893620973226,
            "upper_bound": 108.85646770342454
          },
          "point_estimate": 108.78079294082885,
          "standard_error": 0.02948019834863752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017452312273361175,
            "upper_bound": 0.17556675399891922
          },
          "point_estimate": 0.06771511262448172,
          "standard_error": 0.04347853481838899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.75076945976446,
            "upper_bound": 108.86560570090874
          },
          "point_estimate": 108.8097276558437,
          "standard_error": 0.030286539426967487
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.048889955613431776,
            "upper_bound": 0.13630372594786216
          },
          "point_estimate": 0.10300994572746729,
          "standard_error": 0.02218819454181048
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.79327474634015,
            "upper_bound": 45.86143441073598
          },
          "point_estimate": 45.82584774920177,
          "standard_error": 0.017325773083285378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.80032775675819,
            "upper_bound": 45.847145007681135
          },
          "point_estimate": 45.819355752018794,
          "standard_error": 0.014662993777655276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007340331501319397,
            "upper_bound": 0.08840696008647679
          },
          "point_estimate": 0.03493172348150238,
          "standard_error": 0.018897005473507603
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.802709685949075,
            "upper_bound": 45.83282010802888
          },
          "point_estimate": 45.8192775366037,
          "standard_error": 0.0076167531885017155
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018452576223289444,
            "upper_bound": 0.08163803469485213
          },
          "point_estimate": 0.05766341991447422,
          "standard_error": 0.016838142617031458
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.15571395026257,
            "upper_bound": 50.32250527300741
          },
          "point_estimate": 50.21935173256284,
          "standard_error": 0.04675191709917028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.145182214737304,
            "upper_bound": 50.21869852778775
          },
          "point_estimate": 50.164770108369126,
          "standard_error": 0.020797788561835166
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00452367430747549,
            "upper_bound": 0.08447794751805703
          },
          "point_estimate": 0.03964965397154491,
          "standard_error": 0.02281705612198044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.142595588067856,
            "upper_bound": 50.21661549861421
          },
          "point_estimate": 50.16986010483264,
          "standard_error": 0.019062432062110612
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02117924264972648,
            "upper_bound": 0.23877152030592644
          },
          "point_estimate": 0.15604239456837687,
          "standard_error": 0.0738942867936988
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.3902899979928,
            "upper_bound": 93.68685783073296
          },
          "point_estimate": 93.5466522734932,
          "standard_error": 0.07565373926937408
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.4669858068113,
            "upper_bound": 93.67340565164122
          },
          "point_estimate": 93.54968176975753,
          "standard_error": 0.054989261620904575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.037335422480185135,
            "upper_bound": 0.3681319480797205
          },
          "point_estimate": 0.13643940030510482,
          "standard_error": 0.07932785328356191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.51910620868168,
            "upper_bound": 93.65003096332534
          },
          "point_estimate": 93.5767108073051,
          "standard_error": 0.03469499404231491
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07581333444252178,
            "upper_bound": 0.36453101471095295
          },
          "point_estimate": 0.2511925263142904,
          "standard_error": 0.07541378688369456
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1086734.1274509802,
            "upper_bound": 1088600.9124509804
          },
          "point_estimate": 1087577.455147059,
          "standard_error": 481.16665315359165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1086328.8970588236,
            "upper_bound": 1088531.950980392
          },
          "point_estimate": 1086981.7389705882,
          "standard_error": 559.7782835119302
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.7544864258379,
            "upper_bound": 2506.067079331963
          },
          "point_estimate": 995.0589639516568,
          "standard_error": 576.7942782476777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1086540.1628437538,
            "upper_bound": 1087651.6112877585
          },
          "point_estimate": 1087063.8434682963,
          "standard_error": 284.04526198065474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 571.5145031594577,
            "upper_bound": 2168.8833748514494
          },
          "point_estimate": 1602.7730674823829,
          "standard_error": 425.0435784812285
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1364832.8297305994,
            "upper_bound": 1366731.1475562167
          },
          "point_estimate": 1365779.2002087005,
          "standard_error": 485.89458854415693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1364266.991769547,
            "upper_bound": 1367285.7222222222
          },
          "point_estimate": 1365883.4030864197,
          "standard_error": 790.8480418083055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485.7093608215202,
            "upper_bound": 2648.5763850770513
          },
          "point_estimate": 2227.516772305615,
          "standard_error": 582.370634058959
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1364727.4966329967,
            "upper_bound": 1366495.869220945
          },
          "point_estimate": 1365591.638961039,
          "standard_error": 450.1816213039651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1005.9434283904748,
            "upper_bound": 1954.231442706593
          },
          "point_estimate": 1618.2494916736475,
          "standard_error": 241.62513466892585
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412510.5133882784,
            "upper_bound": 1415156.4319887822
          },
          "point_estimate": 1413811.9781318684,
          "standard_error": 676.6529371552733
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1411933.8173076925,
            "upper_bound": 1415554.2519230768
          },
          "point_estimate": 1413601.6923076925,
          "standard_error": 971.6353209751662
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 488.8730855516963,
            "upper_bound": 3822.296694448504
          },
          "point_estimate": 2440.0102903351103,
          "standard_error": 838.2664129723674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1412024.734035386,
            "upper_bound": 1414685.1409296014
          },
          "point_estimate": 1413428.8308691308,
          "standard_error": 684.587761287806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1305.414011126938,
            "upper_bound": 2873.5523646929164
          },
          "point_estimate": 2258.6317698969724,
          "standard_error": 405.81072042189106
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1493049.1420809526,
            "upper_bound": 1495927.9096583335
          },
          "point_estimate": 1494407.3929571428,
          "standard_error": 738.0101025701794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1492541.7392857145,
            "upper_bound": 1496239.8275
          },
          "point_estimate": 1493600.109333333,
          "standard_error": 1102.7952511384333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 434.8969806790821,
            "upper_bound": 4230.632877591278
          },
          "point_estimate": 2482.824926420984,
          "standard_error": 972.5655860149157
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1493264.7191534392,
            "upper_bound": 1495117.5440141074
          },
          "point_estimate": 1494063.1708051949,
          "standard_error": 471.4754038011439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303.258469476173,
            "upper_bound": 3139.275774983185
          },
          "point_estimate": 2458.49613641182,
          "standard_error": 485.925728432692
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1325179.0793452382,
            "upper_bound": 1327806.2301573132
          },
          "point_estimate": 1326273.420547052,
          "standard_error": 692.6939705764165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1324882.0625,
            "upper_bound": 1326855.0484693877
          },
          "point_estimate": 1325719.1857142858,
          "standard_error": 418.56364292272366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 178.02319183938752,
            "upper_bound": 2350.956424928918
          },
          "point_estimate": 710.7120961325452,
          "standard_error": 608.4066476819215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1325225.118248832,
            "upper_bound": 1326583.9680025857
          },
          "point_estimate": 1325835.2505565863,
          "standard_error": 343.57918723888696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.3111935222456,
            "upper_bound": 3379.0212410426675
          },
          "point_estimate": 2301.4592982310214,
          "standard_error": 861.5705738826296
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384839.38052631577,
            "upper_bound": 385320.4166448621
          },
          "point_estimate": 385057.8255137844,
          "standard_error": 123.50892112984684
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384771.69824561407,
            "upper_bound": 385273.0898496241
          },
          "point_estimate": 384908.9929824561,
          "standard_error": 166.03572017892452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.715664440057633,
            "upper_bound": 662.393712450735
          },
          "point_estimate": 380.2429354598427,
          "standard_error": 160.55391721743695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384763.18447233,
            "upper_bound": 385391.2479048859
          },
          "point_estimate": 385003.9397949419,
          "standard_error": 162.53485656977918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.2333441630114,
            "upper_bound": 567.8645812966267
          },
          "point_estimate": 411.81953662062926,
          "standard_error": 110.50495874639152
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456554.28538298607,
            "upper_bound": 457005.8936828125
          },
          "point_estimate": 456776.1040347222,
          "standard_error": 115.44423840387398
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456469.085,
            "upper_bound": 457103.421875
          },
          "point_estimate": 456750.11875,
          "standard_error": 145.94947330887982
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.31345240584643,
            "upper_bound": 713.8626210763533
          },
          "point_estimate": 325.50714078361034,
          "standard_error": 158.38907136575244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456405.0672704173,
            "upper_bound": 457232.00498426973
          },
          "point_estimate": 456852.5457142857,
          "standard_error": 217.5779802377578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 213.9628734000548,
            "upper_bound": 483.38421433798067
          },
          "point_estimate": 385.13318997794113,
          "standard_error": 69.55732259472013
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 742180.9674781341,
            "upper_bound": 743838.3074263039
          },
          "point_estimate": 742861.0007831228,
          "standard_error": 440.84197556221915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 741825.0076530612,
            "upper_bound": 743088.274829932
          },
          "point_estimate": 742462.8038548753,
          "standard_error": 293.0522343863922
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.895701325958424,
            "upper_bound": 1356.9783221672974
          },
          "point_estimate": 880.2767147291107,
          "standard_error": 360.5003458161281
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 742082.6304575519,
            "upper_bound": 742649.4308991501
          },
          "point_estimate": 742386.3841505434,
          "standard_error": 145.30620195196633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 363.9944042802837,
            "upper_bound": 2192.87005381725
          },
          "point_estimate": 1469.697479263463,
          "standard_error": 579.027495515612
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 866506.2919769699,
            "upper_bound": 867853.4589125096
          },
          "point_estimate": 867131.6243093349,
          "standard_error": 345.64870857400825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 866386.8775510204,
            "upper_bound": 867601.119047619
          },
          "point_estimate": 867090.8544973545,
          "standard_error": 344.6500205516603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.78439664837825,
            "upper_bound": 1738.678671215611
          },
          "point_estimate": 874.3907900320479,
          "standard_error": 392.46317232148505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 866511.3882126842,
            "upper_bound": 867292.8157713455
          },
          "point_estimate": 866940.4881261595,
          "standard_error": 198.08360965871245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484.623535503586,
            "upper_bound": 1603.5590573336833
          },
          "point_estimate": 1154.3058433300323,
          "standard_error": 308.1193523306484
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 722049.4788772175,
            "upper_bound": 723330.0289355742
          },
          "point_estimate": 722652.6788935574,
          "standard_error": 328.77901121780167
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721896.3137254902,
            "upper_bound": 723566.8622782447
          },
          "point_estimate": 722397.0531045751,
          "standard_error": 334.90898873146375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.44293076498465,
            "upper_bound": 1862.6167527163043
          },
          "point_estimate": 738.8144515892785,
          "standard_error": 434.2703409059084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721915.8401671635,
            "upper_bound": 723175.3966271047
          },
          "point_estimate": 722401.6462948816,
          "standard_error": 326.3260147259579
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443.92210738598015,
            "upper_bound": 1401.5060551391307
          },
          "point_estimate": 1095.599699537154,
          "standard_error": 239.20332552926223
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306215.6796528611,
            "upper_bound": 306746.04266611644
          },
          "point_estimate": 306461.47905862343,
          "standard_error": 135.58033172738146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306216.7082833133,
            "upper_bound": 306635.9947478991
          },
          "point_estimate": 306397.5992997199,
          "standard_error": 106.14134200416962
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.59149081830393,
            "upper_bound": 647.3394348309172
          },
          "point_estimate": 253.6576972613444,
          "standard_error": 142.23729924972025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306309.139422805,
            "upper_bound": 307114.84180877986
          },
          "point_estimate": 306689.042955364,
          "standard_error": 229.83400400198911
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.63871175026603,
            "upper_bound": 647.5703252915054
          },
          "point_estimate": 454.361874891082,
          "standard_error": 134.1649021720071
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619333.2544371468,
            "upper_bound": 620572.0704808314
          },
          "point_estimate": 619928.9024616627,
          "standard_error": 315.9394873793691
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619078.0152542372,
            "upper_bound": 620538.2290960453
          },
          "point_estimate": 619868.1405367232,
          "standard_error": 297.48392469681005
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.6972778403704,
            "upper_bound": 1819.5689812555183
          },
          "point_estimate": 639.9245212661577,
          "standard_error": 468.8185623579738
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 619735.3368905629,
            "upper_bound": 620696.3723655122
          },
          "point_estimate": 620116.1055690072,
          "standard_error": 243.33663265614695
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 500.6157421487995,
            "upper_bound": 1404.722668661352
          },
          "point_estimate": 1060.0243945870732,
          "standard_error": 236.54647014067305
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183783.42309027776,
            "upper_bound": 184202.37033495668
          },
          "point_estimate": 183982.536504329,
          "standard_error": 107.36779307709202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183695.43308080808,
            "upper_bound": 184256.15572390571
          },
          "point_estimate": 183946.18416305917,
          "standard_error": 137.66325457851266
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.07808363588002,
            "upper_bound": 616.5640962507587
          },
          "point_estimate": 350.8589685437382,
          "standard_error": 139.32991120072032
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183778.8702804711,
            "upper_bound": 183988.61091750383
          },
          "point_estimate": 183879.8621015348,
          "standard_error": 53.92402937560386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.1489645127919,
            "upper_bound": 442.73359869982625
          },
          "point_estimate": 356.6572085292454,
          "standard_error": 69.24821437114605
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153596.6659971079,
            "upper_bound": 153841.85894697503
          },
          "point_estimate": 153716.77070285176,
          "standard_error": 62.842726655437716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153548.88601694914,
            "upper_bound": 153896.00847457626
          },
          "point_estimate": 153689.21121704334,
          "standard_error": 82.10343614249798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.434663796240848,
            "upper_bound": 375.404172904474
          },
          "point_estimate": 257.3218732706074,
          "standard_error": 98.69426191492364
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153572.73358983226,
            "upper_bound": 153776.80339790648
          },
          "point_estimate": 153659.7475786925,
          "standard_error": 51.68113102225555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.32769786918672,
            "upper_bound": 256.41091553196765
          },
          "point_estimate": 209.76247486845008,
          "standard_error": 34.98282996061184
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422124.19659501,
            "upper_bound": 422988.43568007665
          },
          "point_estimate": 422508.2877891808,
          "standard_error": 222.4701355574426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422129.1810344828,
            "upper_bound": 422687.3159095056
          },
          "point_estimate": 422388.6906130269,
          "standard_error": 176.61243402972588
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.70408000343908,
            "upper_bound": 901.2305596034256
          },
          "point_estimate": 409.55392323660385,
          "standard_error": 216.88798337187023
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422269.6155248367,
            "upper_bound": 422716.6602299019
          },
          "point_estimate": 422528.902582475,
          "standard_error": 112.56630545441038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215.29840614688183,
            "upper_bound": 1068.2124209520675
          },
          "point_estimate": 739.2139150094378,
          "standard_error": 245.83760005363797
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 887.3905579812665,
            "upper_bound": 888.708376978803
          },
          "point_estimate": 888.0877064409691,
          "standard_error": 0.33785129656975216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 887.3375207337301,
            "upper_bound": 888.8510669643341
          },
          "point_estimate": 888.3590990584447,
          "standard_error": 0.45373393711168886
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06518651703963114,
            "upper_bound": 1.959135011483055
          },
          "point_estimate": 0.9370568354145636,
          "standard_error": 0.4514630611451343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 887.7933245400523,
            "upper_bound": 888.7619492661754
          },
          "point_estimate": 888.3255241246216,
          "standard_error": 0.24704116761804773
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5534952578254135,
            "upper_bound": 1.505587976223965
          },
          "point_estimate": 1.1242281028589929,
          "standard_error": 0.2589175469680233
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 301986067.69500005,
            "upper_bound": 302335921.3
          },
          "point_estimate": 302169899.5,
          "standard_error": 89393.9804987892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302010170.0,
            "upper_bound": 302391461.0
          },
          "point_estimate": 302162549.5,
          "standard_error": 93221.99812899675
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44145.89681625366,
            "upper_bound": 497332.2307705879
          },
          "point_estimate": 242645.2768921852,
          "standard_error": 112624.91773628084
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139896.2801886534,
            "upper_bound": 403962.81885673024
          },
          "point_estimate": 297957.54318032553,
          "standard_error": 71091.62018875766
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/stud/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266108630.4,
            "upper_bound": 266405614.4
          },
          "point_estimate": 266258174.9,
          "standard_error": 76020.81590624826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266024156.0,
            "upper_bound": 266475331.5
          },
          "point_estimate": 266314661.0,
          "standard_error": 111453.72714910917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26566.70892834663,
            "upper_bound": 441802.1900564432
          },
          "point_estimate": 330016.37594103813,
          "standard_error": 108933.68405799646
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152363.12579455876,
            "upper_bound": 310525.2096333458
          },
          "point_estimate": 253543.62083890024,
          "standard_error": 40736.483989689965
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159238.36033374924,
            "upper_bound": 159450.20684555004
          },
          "point_estimate": 159333.727808796,
          "standard_error": 54.80268273318841
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159219.0471771678,
            "upper_bound": 159445.03537117905
          },
          "point_estimate": 159281.3294213974,
          "standard_error": 48.42453046144797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.031237151409017,
            "upper_bound": 244.80552648352628
          },
          "point_estimate": 96.28860986258802,
          "standard_error": 60.668825451862595
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159227.28761581948,
            "upper_bound": 159430.20319462864
          },
          "point_estimate": 159306.80282425,
          "standard_error": 52.19629684893261
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.85260761458116,
            "upper_bound": 236.88227036023105
          },
          "point_estimate": 182.16420800747375,
          "standard_error": 49.083939993727824
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224452.8344513032,
            "upper_bound": 224847.04915016904
          },
          "point_estimate": 224641.33463011956,
          "standard_error": 100.68272199534556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224379.0655006859,
            "upper_bound": 224858.30041152265
          },
          "point_estimate": 224667.77354497355,
          "standard_error": 129.67014757725798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.056624578118345,
            "upper_bound": 584.400060406739
          },
          "point_estimate": 328.06998656242246,
          "standard_error": 125.8235911848154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224522.40335283623,
            "upper_bound": 224752.80408938864
          },
          "point_estimate": 224637.12531665864,
          "standard_error": 58.813394392567886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.41600830375094,
            "upper_bound": 444.4257471930976
          },
          "point_estimate": 333.9695254074823,
          "standard_error": 73.10689747404146
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224501.4875342936,
            "upper_bound": 224922.30775402943
          },
          "point_estimate": 224704.6674554184,
          "standard_error": 107.56316209803364
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224341.5061728395,
            "upper_bound": 224960.04444444444
          },
          "point_estimate": 224736.71635802469,
          "standard_error": 183.8553596190084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.00539046344991,
            "upper_bound": 695.6134347697315
          },
          "point_estimate": 446.4741015385121,
          "standard_error": 167.47171352490545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224420.5263138264,
            "upper_bound": 224827.8669268837
          },
          "point_estimate": 224639.0368606702,
          "standard_error": 104.85086720084529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.1150105902297,
            "upper_bound": 449.2870697399502
          },
          "point_estimate": 358.05196653645675,
          "standard_error": 62.72386067986502
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135125.41378739598,
            "upper_bound": 135658.93899628252
          },
          "point_estimate": 135357.15583171064,
          "standard_error": 137.72338970532047
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135025.6311205523,
            "upper_bound": 135530.42936802973
          },
          "point_estimate": 135227.92973977694,
          "standard_error": 146.0050494424495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.657852368815046,
            "upper_bound": 619.1004042875835
          },
          "point_estimate": 301.89266338875177,
          "standard_error": 156.65328498498388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135044.02876280362,
            "upper_bound": 135249.2686802974
          },
          "point_estimate": 135129.8251146623,
          "standard_error": 53.16938289517785
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.66701102256314,
            "upper_bound": 653.9766950895088
          },
          "point_estimate": 458.8542722118949,
          "standard_error": 145.137517108248
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96027.92160955364,
            "upper_bound": 96196.9502282322
          },
          "point_estimate": 96106.8992414248,
          "standard_error": 43.33529335085569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95999.18205804747,
            "upper_bound": 96182.70422163588
          },
          "point_estimate": 96083.11015831136,
          "standard_error": 65.80287812955986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.951778235634045,
            "upper_bound": 254.8456025072565
          },
          "point_estimate": 142.18226654568025,
          "standard_error": 61.397831084682856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96015.54882605842,
            "upper_bound": 96140.63129909609
          },
          "point_estimate": 96067.51707500944,
          "standard_error": 32.09566058416177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.28092889253168,
            "upper_bound": 194.55991427382563
          },
          "point_estimate": 144.34448159052982,
          "standard_error": 34.44872971331084
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84223.4323403215,
            "upper_bound": 84405.80022222683
          },
          "point_estimate": 84306.76609932605,
          "standard_error": 46.97706049138977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84213.56612529003,
            "upper_bound": 84411.25551044084
          },
          "point_estimate": 84258.03546569441,
          "standard_error": 40.31171697577696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.562081848813037,
            "upper_bound": 242.91979180100273
          },
          "point_estimate": 86.87369978018019,
          "standard_error": 58.86147388639816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84228.35833553747,
            "upper_bound": 84336.5104096395
          },
          "point_estimate": 84274.82278603068,
          "standard_error": 27.64624530994265
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.33168134987094,
            "upper_bound": 208.3989293188999
          },
          "point_estimate": 156.00950323551203,
          "standard_error": 41.148086121264996
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66184.9912177986,
            "upper_bound": 66231.89298724956
          },
          "point_estimate": 66209.43204527712,
          "standard_error": 12.015156754920792
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66177.80783242258,
            "upper_bound": 66237.81238615664
          },
          "point_estimate": 66218.82103825136,
          "standard_error": 13.669679300819292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.105157923933813,
            "upper_bound": 65.87915429489446
          },
          "point_estimate": 34.16798819121114,
          "standard_error": 15.340428533838493
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66168.45266846335,
            "upper_bound": 66236.00678070578
          },
          "point_estimate": 66201.79474368982,
          "standard_error": 17.436896269957817
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.81789059647614,
            "upper_bound": 51.32783930112389
          },
          "point_estimate": 39.93703965372829,
          "standard_error": 8.232441489705927
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132172.22354060606,
            "upper_bound": 132472.65715878786
          },
          "point_estimate": 132315.6881933622,
          "standard_error": 76.71373089452062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132091.0646969697,
            "upper_bound": 132488.60904040403
          },
          "point_estimate": 132310.19225974026,
          "standard_error": 107.180589489133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.27590960048657,
            "upper_bound": 456.52115056782986
          },
          "point_estimate": 265.3761659431736,
          "standard_error": 94.83240494449171
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132117.0389188211,
            "upper_bound": 132416.53205871454
          },
          "point_estimate": 132280.34290436836,
          "standard_error": 75.96552707581273
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.96474798483374,
            "upper_bound": 330.9900508206353
          },
          "point_estimate": 255.16337712646,
          "standard_error": 51.91449022111741
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78435.27792048064,
            "upper_bound": 78593.83171077598
          },
          "point_estimate": 78507.37651659297,
          "standard_error": 40.833272104177986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78415.19805615551,
            "upper_bound": 78578.67722582194
          },
          "point_estimate": 78473.50435822277,
          "standard_error": 38.16876791842381
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.493028083399995,
            "upper_bound": 204.316686091883
          },
          "point_estimate": 91.41681817725096,
          "standard_error": 48.871813795853114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78458.57278962593,
            "upper_bound": 78556.6280703472
          },
          "point_estimate": 78509.41528147878,
          "standard_error": 24.7023611052911
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.04912098990755,
            "upper_bound": 184.1421143231977
          },
          "point_estimate": 135.82047220732926,
          "standard_error": 35.6456710287897
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77931.81828683833,
            "upper_bound": 78025.20355439743
          },
          "point_estimate": 77978.87791453776,
          "standard_error": 22.77720947100344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77965.89771101574,
            "upper_bound": 77993.29163090128
          },
          "point_estimate": 77976.81584235984,
          "standard_error": 8.821646581487993
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1298739428794957,
            "upper_bound": 121.53638282083942
          },
          "point_estimate": 16.343249513148994,
          "standard_error": 24.204476559142996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77970.02375133024,
            "upper_bound": 78020.70251296293
          },
          "point_estimate": 77992.19633799676,
          "standard_error": 12.793330387881058
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.80471924057583,
            "upper_bound": 108.22873977013369
          },
          "point_estimate": 75.84017971579989,
          "standard_error": 25.768165622569892
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83412.15150392722,
            "upper_bound": 83513.97494636015
          },
          "point_estimate": 83465.19590421456,
          "standard_error": 25.980583167586513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83410.53459770115,
            "upper_bound": 83541.42873563219
          },
          "point_estimate": 83475.32107279694,
          "standard_error": 29.74859497206127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.306446324828469,
            "upper_bound": 142.97376159964526
          },
          "point_estimate": 88.81597508987534,
          "standard_error": 36.86146301863075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83412.93722815737,
            "upper_bound": 83506.4077017674
          },
          "point_estimate": 83450.14989102852,
          "standard_error": 24.1176294199015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.08243757277328,
            "upper_bound": 112.77438772732005
          },
          "point_estimate": 86.64550254815914,
          "standard_error": 18.08377949452395
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73702.97894608315,
            "upper_bound": 73813.87279169478
          },
          "point_estimate": 73754.382916506,
          "standard_error": 28.38647191554709
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73680.61009253903,
            "upper_bound": 73794.79014844805
          },
          "point_estimate": 73748.05742521367,
          "standard_error": 29.356125874998437
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.385068324778043,
            "upper_bound": 149.52235821589875
          },
          "point_estimate": 69.92564728083256,
          "standard_error": 32.396372695973895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73708.78966969493,
            "upper_bound": 73764.24870083117
          },
          "point_estimate": 73732.02318208107,
          "standard_error": 14.1054653599309
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.06500219873968,
            "upper_bound": 130.20421845054466
          },
          "point_estimate": 95.0412157625328,
          "standard_error": 24.687721060736827
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233487.10839870773,
            "upper_bound": 233799.57133848444
          },
          "point_estimate": 233629.2914435795,
          "standard_error": 80.57108169386775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233424.34971509973,
            "upper_bound": 233762.30064102565
          },
          "point_estimate": 233579.4676816239,
          "standard_error": 84.31160615488571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.83138456654975,
            "upper_bound": 403.1854597650999
          },
          "point_estimate": 231.2158350243517,
          "standard_error": 94.95449196045227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233478.97056330295,
            "upper_bound": 233890.66039576367
          },
          "point_estimate": 233695.06583416584,
          "standard_error": 104.60718712385956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.64668010112176,
            "upper_bound": 361.7336538252419
          },
          "point_estimate": 268.88672447768596,
          "standard_error": 69.91160683743368
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152739.76271634456,
            "upper_bound": 152986.03664375373
          },
          "point_estimate": 152852.66998970578,
          "standard_error": 62.98682744512536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152727.26447001396,
            "upper_bound": 152933.64351464435
          },
          "point_estimate": 152831.73998804542,
          "standard_error": 68.51341839587704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.626706386708143,
            "upper_bound": 294.34239833087315
          },
          "point_estimate": 150.22549345569166,
          "standard_error": 75.53417520931659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152755.9212939109,
            "upper_bound": 152890.4260272503
          },
          "point_estimate": 152824.99702222465,
          "standard_error": 35.154002908104886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.87670046305347,
            "upper_bound": 297.1533973633064
          },
          "point_estimate": 210.24363605953175,
          "standard_error": 60.65853265135617
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73284.21409798026,
            "upper_bound": 73453.49597470238
          },
          "point_estimate": 73361.64518841205,
          "standard_error": 43.23650118607882
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73284.6863407258,
            "upper_bound": 73452.82782258064
          },
          "point_estimate": 73329.29769105223,
          "standard_error": 37.84546269075732
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.296494462417106,
            "upper_bound": 216.01293729404208
          },
          "point_estimate": 91.67939337236623,
          "standard_error": 61.18127249333314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73285.60940062876,
            "upper_bound": 73415.29555346754
          },
          "point_estimate": 73341.61738060327,
          "standard_error": 33.58376907849141
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.57926975711163,
            "upper_bound": 199.5828099550405
          },
          "point_estimate": 144.52073599913078,
          "standard_error": 39.55394677959447
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228415.16026205447,
            "upper_bound": 229102.31069339623
          },
          "point_estimate": 228762.445172956,
          "standard_error": 175.7495664617823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228252.32955974844,
            "upper_bound": 229223.26572327045
          },
          "point_estimate": 228843.5863731656,
          "standard_error": 232.33143310648413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.90902845255009,
            "upper_bound": 994.0071320384154
          },
          "point_estimate": 679.8534055873944,
          "standard_error": 244.8384642506061
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228425.15304115697,
            "upper_bound": 229107.91785588863
          },
          "point_estimate": 228725.0285550927,
          "standard_error": 176.03288255719573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.9869190312816,
            "upper_bound": 734.5130237029225
          },
          "point_estimate": 585.1490726523946,
          "standard_error": 102.83052344567716
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59566.10531876301,
            "upper_bound": 59664.07722293781
          },
          "point_estimate": 59617.46808346345,
          "standard_error": 25.121506913956154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59556.438251366126,
            "upper_bound": 59688.270856102
          },
          "point_estimate": 59628.38178278688,
          "standard_error": 37.35199065659515
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.914243123222537,
            "upper_bound": 145.81337755883513
          },
          "point_estimate": 82.97148632750866,
          "standard_error": 32.199469399835216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59621.90142320351,
            "upper_bound": 59684.274304576815
          },
          "point_estimate": 59659.72878858846,
          "standard_error": 15.985039552291262
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.458343290487754,
            "upper_bound": 107.148789960501
          },
          "point_estimate": 83.74535826432593,
          "standard_error": 16.461309157527626
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65795.12080897942,
            "upper_bound": 65931.71714586556
          },
          "point_estimate": 65856.74167605008,
          "standard_error": 35.23606278977888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65759.92599277978,
            "upper_bound": 65908.73818119305
          },
          "point_estimate": 65836.7288407541,
          "standard_error": 35.02133864137923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.377745624693905,
            "upper_bound": 169.36768933968654
          },
          "point_estimate": 104.61696420766404,
          "standard_error": 39.973792416922336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65769.32157377104,
            "upper_bound": 65844.07252275433
          },
          "point_estimate": 65801.24493412724,
          "standard_error": 19.157120933988327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.58626331213867,
            "upper_bound": 161.59931214934358
          },
          "point_estimate": 117.4159204995959,
          "standard_error": 32.305781421671824
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69794.68409704322,
            "upper_bound": 69939.8750659789
          },
          "point_estimate": 69862.4304077933,
          "standard_error": 37.28422380797631
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69763.06877799104,
            "upper_bound": 69964.66176156965
          },
          "point_estimate": 69837.7202837949,
          "standard_error": 51.74727175034419
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.125179049607977,
            "upper_bound": 210.11801902398463
          },
          "point_estimate": 103.48206673289376,
          "standard_error": 53.09466895258206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69800.15972743393,
            "upper_bound": 69887.67085214295
          },
          "point_estimate": 69854.32346386819,
          "standard_error": 22.210405509577164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.59471199436678,
            "upper_bound": 155.2206899932808
          },
          "point_estimate": 124.03610591324232,
          "standard_error": 25.778646121953592
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94870.8726443284,
            "upper_bound": 95055.13639873182
          },
          "point_estimate": 94956.4265916532,
          "standard_error": 47.20914721361062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94843.38355091384,
            "upper_bound": 95037.62364789256
          },
          "point_estimate": 94937.45637510878,
          "standard_error": 52.254568425085175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.4196406582175,
            "upper_bound": 248.9300194709436
          },
          "point_estimate": 139.4419284904645,
          "standard_error": 52.53079446273241
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94842.22795183888,
            "upper_bound": 94994.5574586597
          },
          "point_estimate": 94912.3098165542,
          "standard_error": 38.61404092949877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.27530133044348,
            "upper_bound": 211.4316196261478
          },
          "point_estimate": 157.43354696415966,
          "standard_error": 38.47406953174031
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66494.76634064596,
            "upper_bound": 66596.02134035577
          },
          "point_estimate": 66543.46745248252,
          "standard_error": 25.87301991790775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66462.42797074954,
            "upper_bound": 66579.76693943879
          },
          "point_estimate": 66555.74451553929,
          "standard_error": 27.779847953125195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.751555232514209,
            "upper_bound": 153.27224234284478
          },
          "point_estimate": 49.67203595543555,
          "standard_error": 38.68829792435737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66484.29501939782,
            "upper_bound": 66565.59368576221
          },
          "point_estimate": 66526.51575773404,
          "standard_error": 21.607026805985097
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.031825633298574,
            "upper_bound": 116.48473113704392
          },
          "point_estimate": 86.27819543025169,
          "standard_error": 20.280791390641205
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94797.23637152778,
            "upper_bound": 94926.99162582157
          },
          "point_estimate": 94865.30119316305,
          "standard_error": 33.111408336757414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94818.14409722222,
            "upper_bound": 94925.47433035714
          },
          "point_estimate": 94863.81380208331,
          "standard_error": 26.601962628992155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.235782840280546,
            "upper_bound": 174.0872556983995
          },
          "point_estimate": 78.0467128722115,
          "standard_error": 40.08505097910395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94835.13425245098,
            "upper_bound": 94907.01040500012
          },
          "point_estimate": 94872.5249594156,
          "standard_error": 18.305247195462364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.96037637015183,
            "upper_bound": 156.4247279630087
          },
          "point_estimate": 110.81007967962697,
          "standard_error": 30.99402710391026
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19592.516429443032,
            "upper_bound": 19631.147840085578
          },
          "point_estimate": 19611.60810059192,
          "standard_error": 9.93767654898962
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19587.426672962763,
            "upper_bound": 19641.247751394138
          },
          "point_estimate": 19603.45014947875,
          "standard_error": 12.722684687948917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3370168050663205,
            "upper_bound": 60.38962156683698
          },
          "point_estimate": 37.64164219915011,
          "standard_error": 14.340137185723313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19598.15877126011,
            "upper_bound": 19626.311936730624
          },
          "point_estimate": 19607.800786369593,
          "standard_error": 7.23382860338102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.637626086034448,
            "upper_bound": 41.42049296453163
          },
          "point_estimate": 33.17738700835123,
          "standard_error": 5.857456869857937
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19803.59972123612,
            "upper_bound": 19827.415837472803
          },
          "point_estimate": 19815.93168810087,
          "standard_error": 6.091464881143182
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19805.636264336423,
            "upper_bound": 19825.183287820862
          },
          "point_estimate": 19819.05660158383,
          "standard_error": 5.624208762273822
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3379128722074305,
            "upper_bound": 34.08539792026066
          },
          "point_estimate": 9.275652725821612,
          "standard_error": 7.911742696697384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19806.35938442359,
            "upper_bound": 19832.04121270614
          },
          "point_estimate": 19817.959740969025,
          "standard_error": 6.477454887938594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.783960913274563,
            "upper_bound": 27.909323830416007
          },
          "point_estimate": 20.279664863503104,
          "standard_error": 5.34926829617712
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269212.267722339,
            "upper_bound": 269806.5400729458
          },
          "point_estimate": 269501.29620681604,
          "standard_error": 152.17865909242497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269201.8676470588,
            "upper_bound": 269890.36662581697
          },
          "point_estimate": 269347.91024159663,
          "standard_error": 197.4014068763218
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.99645567157158,
            "upper_bound": 829.766933185426
          },
          "point_estimate": 438.9254482736582,
          "standard_error": 222.633937842964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269310.24891586654,
            "upper_bound": 269803.6395109871
          },
          "point_estimate": 269531.4467341482,
          "standard_error": 130.79104157233826
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257.3723323465239,
            "upper_bound": 661.3436991242992
          },
          "point_estimate": 505.98978443745136,
          "standard_error": 105.10339976297078
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572.2206655103566,
            "upper_bound": 572.9571450135332
          },
          "point_estimate": 572.5776807834861,
          "standard_error": 0.1891731766611023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572.0217687395063,
            "upper_bound": 573.2031394655494
          },
          "point_estimate": 572.4151320434967,
          "standard_error": 0.3437341754694787
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05543826432657183,
            "upper_bound": 1.0515714109782863
          },
          "point_estimate": 0.7407811966587585,
          "standard_error": 0.26052994438987187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572.2256643146936,
            "upper_bound": 573.0392647637595
          },
          "point_estimate": 572.619938118653,
          "standard_error": 0.21208265164782372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3743757699869142,
            "upper_bound": 0.7448331658346968
          },
          "point_estimate": 0.6302829235970601,
          "standard_error": 0.09261060997600074
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.12936219809861,
            "upper_bound": 44.20644861334726
          },
          "point_estimate": 44.16401852735154,
          "standard_error": 0.01984810139166615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.1136731632603,
            "upper_bound": 44.19894822483588
          },
          "point_estimate": 44.14866138719641,
          "standard_error": 0.023270894771010457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01064657182330664,
            "upper_bound": 0.09783778903210716
          },
          "point_estimate": 0.0522478671610457,
          "standard_error": 0.02193448105923456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.13208175689963,
            "upper_bound": 44.18980956027565
          },
          "point_estimate": 44.163578603294035,
          "standard_error": 0.014578271613534704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02787704569100313,
            "upper_bound": 0.09124383307797992
          },
          "point_estimate": 0.06615371766513836,
          "standard_error": 0.018195792580650377
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.362009890493155,
            "upper_bound": 47.428343648331634
          },
          "point_estimate": 47.39344858226265,
          "standard_error": 0.017062475703005363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.34597753014585,
            "upper_bound": 47.44662016293677
          },
          "point_estimate": 47.37073101470461,
          "standard_error": 0.02495681652977641
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007379780605935711,
            "upper_bound": 0.0907299395516658
          },
          "point_estimate": 0.04441528459836398,
          "standard_error": 0.021774845678598005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.351021517038454,
            "upper_bound": 47.40119555231323
          },
          "point_estimate": 47.36939550390811,
          "standard_error": 0.012901601779675769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02668436540697765,
            "upper_bound": 0.06946619781748414
          },
          "point_estimate": 0.05697465251673323,
          "standard_error": 0.010373289274010397
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.36216466268513,
            "upper_bound": 27.406970611389653
          },
          "point_estimate": 27.381497863411756,
          "standard_error": 0.011626899148399192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.355435559896215,
            "upper_bound": 27.39482510740821
          },
          "point_estimate": 27.36677120060669,
          "standard_error": 0.009641632600877772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017189402305165817,
            "upper_bound": 0.04823504537444965
          },
          "point_estimate": 0.017665690734236808,
          "standard_error": 0.011793219854214191
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.36173651984797,
            "upper_bound": 27.38346831996032
          },
          "point_estimate": 27.36996249258497,
          "standard_error": 0.005521606880119734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007063202706979253,
            "upper_bound": 0.05483450521444387
          },
          "point_estimate": 0.03889248806428088,
          "standard_error": 0.012795976202131187
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.36084759065532,
            "upper_bound": 27.38111311033481
          },
          "point_estimate": 27.37136884329023,
          "standard_error": 0.005199748235931704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.35889103890038,
            "upper_bound": 27.383857164859485
          },
          "point_estimate": 27.372405286194272,
          "standard_error": 0.005145216405696999
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017169119566828788,
            "upper_bound": 0.032767322413243906
          },
          "point_estimate": 0.01252700291149922,
          "standard_error": 0.0086233187433909
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.37122246225454,
            "upper_bound": 27.38844963060995
          },
          "point_estimate": 27.380502288689296,
          "standard_error": 0.004450506738217277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008530210031126265,
            "upper_bound": 0.022290968918434996
          },
          "point_estimate": 0.017378697826451288,
          "standard_error": 0.0035375287067059536
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.80395192067653,
            "upper_bound": 36.875942467246354
          },
          "point_estimate": 36.838127295563325,
          "standard_error": 0.018447569429018335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.79659058807806,
            "upper_bound": 36.87745330250856
          },
          "point_estimate": 36.82396926109791,
          "standard_error": 0.02007736126363492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008157760095643946,
            "upper_bound": 0.10006924216934188
          },
          "point_estimate": 0.05092965658776015,
          "standard_error": 0.022656972332683822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.81017061849502,
            "upper_bound": 36.84457125218691
          },
          "point_estimate": 36.82468056476223,
          "standard_error": 0.008751705812525752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028430422095224207,
            "upper_bound": 0.08162006637854761
          },
          "point_estimate": 0.06149501502369247,
          "standard_error": 0.014068785782545666
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.83793334200678,
            "upper_bound": 50.882995414630784
          },
          "point_estimate": 50.85847328946473,
          "standard_error": 0.0115168084194224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.84223659332924,
            "upper_bound": 50.86610892714446
          },
          "point_estimate": 50.85459224444041,
          "standard_error": 0.006314043243233391
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003189008332339225,
            "upper_bound": 0.0503430739182472
          },
          "point_estimate": 0.017696560743045684,
          "standard_error": 0.011089365477742755
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.84188751715794,
            "upper_bound": 50.900795048318024
          },
          "point_estimate": 50.866154106728395,
          "standard_error": 0.01597333949997856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009518573800217284,
            "upper_bound": 0.05611865592435936
          },
          "point_estimate": 0.03851738791146735,
          "standard_error": 0.012785110772265455
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.60866620048662,
            "upper_bound": 68.68095758892083
          },
          "point_estimate": 68.64270514132667,
          "standard_error": 0.0185282378914159
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.59246927512561,
            "upper_bound": 68.67952047046471
          },
          "point_estimate": 68.63516833201453,
          "standard_error": 0.025317346459346347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0163195741164742,
            "upper_bound": 0.10633844237945006
          },
          "point_estimate": 0.06318869148411083,
          "standard_error": 0.02188473647123544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.59444029184804,
            "upper_bound": 68.67666224971367
          },
          "point_estimate": 68.62784868001867,
          "standard_error": 0.02081165708676714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031709776878261736,
            "upper_bound": 0.08112730321614553
          },
          "point_estimate": 0.06166221345068158,
          "standard_error": 0.013565679664649725
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.155530901887786,
            "upper_bound": 45.19916180893718
          },
          "point_estimate": 45.17564754392448,
          "standard_error": 0.011261985579601948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.146723714010825,
            "upper_bound": 45.19884667794244
          },
          "point_estimate": 45.170176810304326,
          "standard_error": 0.009868842820577024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000701031671033675,
            "upper_bound": 0.05509938682357156
          },
          "point_estimate": 0.022615697976820025,
          "standard_error": 0.01596069951854738
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.16455564828525,
            "upper_bound": 45.195572961386496
          },
          "point_estimate": 45.1761613814724,
          "standard_error": 0.007962780106892439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013972906549304226,
            "upper_bound": 0.04948688940434033
          },
          "point_estimate": 0.03768978459205878,
          "standard_error": 0.009438761264141749
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.04707183512865,
            "upper_bound": 66.83718637132641
          },
          "point_estimate": 65.97437061929138,
          "standard_error": 0.4434856124251064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.02857586074575,
            "upper_bound": 66.96597839664928
          },
          "point_estimate": 66.79494920640752,
          "standard_error": 0.714836676526011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018844795333728976,
            "upper_bound": 2.2411486396241793
          },
          "point_estimate": 0.2905681876182245,
          "standard_error": 0.6007917514073554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.10890814795573,
            "upper_bound": 66.80221062064284
          },
          "point_estimate": 64.91820271249068,
          "standard_error": 0.6005670247656044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1432406067531308,
            "upper_bound": 1.6698814982149497
          },
          "point_estimate": 1.4740964187138126,
          "standard_error": 0.2958098462058891
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.13805028834889,
            "upper_bound": 53.20763850332498
          },
          "point_estimate": 53.17379263417884,
          "standard_error": 0.017805864060091835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.11676000602284,
            "upper_bound": 53.23210990800555
          },
          "point_estimate": 53.18386159851748,
          "standard_error": 0.028168472069931776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012935107516283585,
            "upper_bound": 0.09996001980231504
          },
          "point_estimate": 0.07630601225808312,
          "standard_error": 0.02295924794720285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.11885475235693,
            "upper_bound": 53.225928808247915
          },
          "point_estimate": 53.17108469408653,
          "standard_error": 0.02727613816050298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03444734672335777,
            "upper_bound": 0.07090755551313165
          },
          "point_estimate": 0.0593571579889529,
          "standard_error": 0.00906305658683366
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.2331764553745,
            "upper_bound": 39.325875823367845
          },
          "point_estimate": 39.276028528845046,
          "standard_error": 0.02387025123309232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.21197429195124,
            "upper_bound": 39.3335456632802
          },
          "point_estimate": 39.253175526329635,
          "standard_error": 0.02589921452720058
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007950759576653355,
            "upper_bound": 0.1259404972835928
          },
          "point_estimate": 0.06695046663273055,
          "standard_error": 0.030319967820418205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.23573373374239,
            "upper_bound": 39.34475354691765
          },
          "point_estimate": 39.286887625430026,
          "standard_error": 0.028079988712875623
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0285724149021044,
            "upper_bound": 0.1001713917170048
          },
          "point_estimate": 0.07961352984580336,
          "standard_error": 0.017971347031890524
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.67947133079556,
            "upper_bound": 65.84218856171854
          },
          "point_estimate": 65.76473109659352,
          "standard_error": 0.04160559590067771
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.68334233026593,
            "upper_bound": 65.86883951646016
          },
          "point_estimate": 65.76447716147783,
          "standard_error": 0.04088916977770696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015643118457637432,
            "upper_bound": 0.2312455614332829
          },
          "point_estimate": 0.116359043571008,
          "standard_error": 0.05685762797293475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.74814545438926,
            "upper_bound": 65.83965244937448
          },
          "point_estimate": 65.79631667007003,
          "standard_error": 0.023173622619320915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05970842481863544,
            "upper_bound": 0.1853714192665241
          },
          "point_estimate": 0.138353477806552,
          "standard_error": 0.03232447444322985
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252108.19819553915,
            "upper_bound": 252504.7517776478
          },
          "point_estimate": 252293.26122304323,
          "standard_error": 101.95772563228527
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252080.2551724138,
            "upper_bound": 252525.23663793103
          },
          "point_estimate": 252206.3863163656,
          "standard_error": 99.41691805540086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.140348044976555,
            "upper_bound": 553.6503025845286
          },
          "point_estimate": 229.78813543196287,
          "standard_error": 127.8291564552432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252132.3346803161,
            "upper_bound": 252332.57141339523
          },
          "point_estimate": 252217.63795790417,
          "standard_error": 51.46433990017435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.0173851540761,
            "upper_bound": 450.8598028635446
          },
          "point_estimate": 340.3245096368626,
          "standard_error": 81.35067815861525
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170262.61185061565,
            "upper_bound": 170452.97731568018
          },
          "point_estimate": 170351.29587987688,
          "standard_error": 49.008020491923766
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170215.4885773624,
            "upper_bound": 170467.85257009347
          },
          "point_estimate": 170292.74892912773,
          "standard_error": 76.64656698732227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.102490601985,
            "upper_bound": 299.88505790025613
          },
          "point_estimate": 131.226572378215,
          "standard_error": 73.19509331507348
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170233.96278530973,
            "upper_bound": 170367.64945695535
          },
          "point_estimate": 170282.543536837,
          "standard_error": 34.344576326414355
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.18410650075883,
            "upper_bound": 212.65887462831773
          },
          "point_estimate": 163.2668918156518,
          "standard_error": 35.54940899413149
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322143.6462457595,
            "upper_bound": 322506.3775495154
          },
          "point_estimate": 322325.38106089336,
          "standard_error": 92.87408352523792
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322058.17477876105,
            "upper_bound": 322565.6814159292
          },
          "point_estimate": 322325.1257743363,
          "standard_error": 135.66871891635935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.37653662612206,
            "upper_bound": 527.2448517014604
          },
          "point_estimate": 376.21466345362273,
          "standard_error": 117.15072600889428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 322272.5853178688,
            "upper_bound": 322521.3716704678
          },
          "point_estimate": 322394.5982300885,
          "standard_error": 63.235598007117034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.33445206299191,
            "upper_bound": 385.44247519178367
          },
          "point_estimate": 309.8483872339976,
          "standard_error": 51.54569743647234
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356528.81819794595,
            "upper_bound": 357457.6977750058
          },
          "point_estimate": 356945.2760383598,
          "standard_error": 239.2616090065583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356325.2,
            "upper_bound": 357421.0067401961
          },
          "point_estimate": 356730.795751634,
          "standard_error": 269.4270563953199
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.57955703276032,
            "upper_bound": 1158.0259533380156
          },
          "point_estimate": 578.3740856141538,
          "standard_error": 286.28511319797735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356429.95765848004,
            "upper_bound": 357281.19174896233
          },
          "point_estimate": 356750.42869875225,
          "standard_error": 218.39394895283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298.72787591070454,
            "upper_bound": 1083.6839378336674
          },
          "point_estimate": 797.1948618370839,
          "standard_error": 217.37784361969085
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499008.86592392373,
            "upper_bound": 499735.1057609262
          },
          "point_estimate": 499381.14472874533,
          "standard_error": 186.1167167513433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498982.4566536203,
            "upper_bound": 499863.74520547944
          },
          "point_estimate": 499418.38070776255,
          "standard_error": 285.68940970802635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.98254006016948,
            "upper_bound": 1091.9628062987242
          },
          "point_estimate": 583.3764333073644,
          "standard_error": 258.00069765873343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499036.4896809662,
            "upper_bound": 499953.064088101
          },
          "point_estimate": 499472.169542786,
          "standard_error": 241.01559749344088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.86189844605497,
            "upper_bound": 797.8305126008026
          },
          "point_estimate": 621.8819462491876,
          "standard_error": 116.57648750726727
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107334.0519036733,
            "upper_bound": 107484.43095378565
          },
          "point_estimate": 107405.22707449547,
          "standard_error": 38.4980502049967
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107297.20001638807,
            "upper_bound": 107477.43756145528
          },
          "point_estimate": 107397.05250737464,
          "standard_error": 47.819804560764695
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.60073311237892,
            "upper_bound": 219.82861143649373
          },
          "point_estimate": 114.02337142701526,
          "standard_error": 46.111264955996155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107348.57707257434,
            "upper_bound": 107451.48519258625
          },
          "point_estimate": 107391.1525801632,
          "standard_error": 26.213223159647825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.80785646997882,
            "upper_bound": 169.43831597892913
          },
          "point_estimate": 128.30081343626665,
          "standard_error": 28.14209159577823
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163286.51129596413,
            "upper_bound": 163517.28074022173
          },
          "point_estimate": 163404.66695031675,
          "standard_error": 59.026848563847906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163188.98430493273,
            "upper_bound": 163566.82386646737
          },
          "point_estimate": 163431.88774289985,
          "standard_error": 88.48242793655352
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.79944421867828,
            "upper_bound": 345.2812452153333
          },
          "point_estimate": 257.0883757945173,
          "standard_error": 83.42728480729484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163267.5170164612,
            "upper_bound": 163576.0946911381
          },
          "point_estimate": 163455.59175353794,
          "standard_error": 78.74108541021731
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.95543552892248,
            "upper_bound": 237.0342335580023
          },
          "point_estimate": 197.2265893528083,
          "standard_error": 30.14949573676952
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307884.2430420198,
            "upper_bound": 308303.22607697744
          },
          "point_estimate": 308088.3420433145,
          "standard_error": 107.35722297492016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307775.38559322036,
            "upper_bound": 308470.656779661
          },
          "point_estimate": 308028.6345338983,
          "standard_error": 174.06605127796064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.14980076515263,
            "upper_bound": 578.2391185478097
          },
          "point_estimate": 429.8047900389553,
          "standard_error": 133.37875917914172
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307801.82443502825,
            "upper_bound": 308370.55718443054
          },
          "point_estimate": 308096.4157825226,
          "standard_error": 148.60445149585098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.17495131845973,
            "upper_bound": 429.8445281123682
          },
          "point_estimate": 357.7175598471682,
          "standard_error": 55.47674366233595
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271351.5851442194,
            "upper_bound": 272075.46068970626
          },
          "point_estimate": 271678.6651193437,
          "standard_error": 186.38537030630096
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271252.3982542644,
            "upper_bound": 272090.26492537314
          },
          "point_estimate": 271439.54160447756,
          "standard_error": 216.65366855422343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.122590364045026,
            "upper_bound": 884.2137729588347
          },
          "point_estimate": 351.4460363911962,
          "standard_error": 247.15837509104392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271285.6544730336,
            "upper_bound": 271664.66858424386
          },
          "point_estimate": 271445.145086257,
          "standard_error": 99.78487409338746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.82376591279723,
            "upper_bound": 829.5069412921582
          },
          "point_estimate": 624.0408769893927,
          "standard_error": 166.33703121137629
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147182.89689791788,
            "upper_bound": 147398.43928475035
          },
          "point_estimate": 147294.81288268746,
          "standard_error": 55.05126201092906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147181.8507422402,
            "upper_bound": 147485.06882591094
          },
          "point_estimate": 147274.0641025641,
          "standard_error": 94.25920892832605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.42465926337584,
            "upper_bound": 369.39961241957127
          },
          "point_estimate": 201.91183401655485,
          "standard_error": 91.92199520506912
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147216.43059733562,
            "upper_bound": 147397.179389032
          },
          "point_estimate": 147288.17744360902,
          "standard_error": 46.1109430092462
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.18633543311724,
            "upper_bound": 235.3302334415491
          },
          "point_estimate": 183.85962368710415,
          "standard_error": 35.17195472781346
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157424.81154504226,
            "upper_bound": 157720.07166908882
          },
          "point_estimate": 157564.03119236583,
          "standard_error": 75.70090919262405
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157387.03535353535,
            "upper_bound": 157749.68807118808
          },
          "point_estimate": 157477.53951762524,
          "standard_error": 90.73311335574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.79776200175443,
            "upper_bound": 402.8363596592006
          },
          "point_estimate": 165.58659200976086,
          "standard_error": 111.97471036844658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157455.8126239099,
            "upper_bound": 157653.81124061436
          },
          "point_estimate": 157540.22940349692,
          "standard_error": 50.99070843461994
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.25579075770094,
            "upper_bound": 312.90072413708475
          },
          "point_estimate": 252.04594646577496,
          "standard_error": 49.566817654101925
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150502.97087300767,
            "upper_bound": 150612.5624018021
          },
          "point_estimate": 150557.79795192182,
          "standard_error": 28.028111595607932
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150448.05578512396,
            "upper_bound": 150636.83778696053
          },
          "point_estimate": 150556.1194903581,
          "standard_error": 42.67819782307898
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.76547873448478,
            "upper_bound": 166.14922019074865
          },
          "point_estimate": 135.76281298229452,
          "standard_error": 42.68167671598497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150505.58901125222,
            "upper_bound": 150629.78874674515
          },
          "point_estimate": 150582.69258344962,
          "standard_error": 31.41812715297788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.935230866555315,
            "upper_bound": 112.45922585728688
          },
          "point_estimate": 93.50334278071846,
          "standard_error": 14.052717821312743
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91307.69148678907,
            "upper_bound": 91456.70657697217
          },
          "point_estimate": 91388.8953339116,
          "standard_error": 38.16904611314003
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91345.22975991068,
            "upper_bound": 91481.76005025124
          },
          "point_estimate": 91390.1775125628,
          "standard_error": 35.46235326719063
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.492138579827156,
            "upper_bound": 179.1315586037112
          },
          "point_estimate": 81.55075923057974,
          "standard_error": 39.88326623317904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91359.27125511951,
            "upper_bound": 91443.5130612577
          },
          "point_estimate": 91394.09985642496,
          "standard_error": 21.471570254826307
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.690160514296934,
            "upper_bound": 180.11473051389498
          },
          "point_estimate": 127.94904250321224,
          "standard_error": 37.33886886761417
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28419.245332616632,
            "upper_bound": 28459.974000645343
          },
          "point_estimate": 28440.36651944723,
          "standard_error": 10.464064224876456
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28408.073690383113,
            "upper_bound": 28471.013682564502
          },
          "point_estimate": 28446.449830596823,
          "standard_error": 16.5701505332171
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.657854435484143,
            "upper_bound": 60.018665236254456
          },
          "point_estimate": 36.87962604579769,
          "standard_error": 14.841946111910357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28432.941089180003,
            "upper_bound": 28466.4093330121
          },
          "point_estimate": 28449.99951463705,
          "standard_error": 8.4868848551066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.00665448143675,
            "upper_bound": 42.08172238637812
          },
          "point_estimate": 34.82354060000585,
          "standard_error": 5.824841042126529
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1987470.405827067,
            "upper_bound": 1990704.8260025056
          },
          "point_estimate": 1988910.7681077693,
          "standard_error": 832.787468105712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1986811.144736842,
            "upper_bound": 1990235.0921052631
          },
          "point_estimate": 1988041.1409774437,
          "standard_error": 1069.5573520456378
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315.48166808338215,
            "upper_bound": 4344.850259705754
          },
          "point_estimate": 2290.0066409232168,
          "standard_error": 973.3557850403322
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1987926.038158502,
            "upper_bound": 1990007.7675012776
          },
          "point_estimate": 1989313.6500341764,
          "standard_error": 525.3433071945777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1172.093016505106,
            "upper_bound": 3889.03022870266
          },
          "point_estimate": 2774.732319478592,
          "standard_error": 802.4035893739648
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3983.824314957486,
            "upper_bound": 3987.400500436931
          },
          "point_estimate": 3985.670597230269,
          "standard_error": 0.9152899155451112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3984.059446819394,
            "upper_bound": 3988.267976542804
          },
          "point_estimate": 3985.427045653842,
          "standard_error": 1.047058579711231
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5710175375441486,
            "upper_bound": 5.602281699290546
          },
          "point_estimate": 2.4532488699067474,
          "standard_error": 1.2514901199116255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3983.577168993453,
            "upper_bound": 3987.8889749163577
          },
          "point_estimate": 3985.852964179304,
          "standard_error": 1.094179463262526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.596104151910626,
            "upper_bound": 4.062026396576176
          },
          "point_estimate": 3.059394125844931,
          "standard_error": 0.669668748533168
        }
      }
    },
    "memrchr1/krate/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/krate/empty/never",
        "directory_name": "memrchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8554047702597793,
            "upper_bound": 0.8563456485877838
          },
          "point_estimate": 0.8558860236933732,
          "standard_error": 0.00024167135029376437
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8552115493383186,
            "upper_bound": 0.8565969255085738
          },
          "point_estimate": 0.8558256557011645,
          "standard_error": 0.00036917953725846766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00016166426879961256,
            "upper_bound": 0.0013987843197079035
          },
          "point_estimate": 0.001024975174041535,
          "standard_error": 0.00033569313056699886
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8556297215725721,
            "upper_bound": 0.8565222609177177
          },
          "point_estimate": 0.8561239699183056,
          "standard_error": 0.0002289728847035496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004761024119314255,
            "upper_bound": 0.0009862461875794484
          },
          "point_estimate": 0.0008064810182721047,
          "standard_error": 0.0001306087610485409
        }
      }
    },
    "memrchr1/krate/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/common",
        "directory_name": "memrchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224612.4508269033,
            "upper_bound": 224999.156116696
          },
          "point_estimate": 224806.5941463355,
          "standard_error": 99.33939697135334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224581.9,
            "upper_bound": 225108.50848765433
          },
          "point_estimate": 224745.26140260632,
          "standard_error": 145.16645755750125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.20209852287569,
            "upper_bound": 579.9065103064855
          },
          "point_estimate": 324.4323797339919,
          "standard_error": 128.88281802773832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224647.41388697183,
            "upper_bound": 225039.18085756007
          },
          "point_estimate": 224819.53833573833,
          "standard_error": 100.48569087700834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.42259051660687,
            "upper_bound": 411.6077489922119
          },
          "point_estimate": 330.85867503854246,
          "standard_error": 55.70277377453155
        }
      }
    },
    "memrchr1/krate/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/never",
        "directory_name": "memrchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8612.681616538877,
            "upper_bound": 8622.791243427728
          },
          "point_estimate": 8618.440051313466,
          "standard_error": 2.659040146314474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8615.811689787368,
            "upper_bound": 8624.25219350249
          },
          "point_estimate": 8620.62819803441,
          "standard_error": 1.9954254924131287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6155721651842616,
            "upper_bound": 10.427999224992933
          },
          "point_estimate": 5.2602063448389815,
          "standard_error": 2.2909508155475304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8614.37560150282,
            "upper_bound": 8623.36275430214
          },
          "point_estimate": 8620.070370085215,
          "standard_error": 2.319577732489714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.813382025928389,
            "upper_bound": 12.90623619716724
          },
          "point_estimate": 8.865296701803626,
          "standard_error": 3.0808491882722424
        }
      }
    },
    "memrchr1/krate/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/rare",
        "directory_name": "memrchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9704.903391263892,
            "upper_bound": 9732.914083520804
          },
          "point_estimate": 9718.210263481018,
          "standard_error": 7.156300169455431
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9700.727560670948,
            "upper_bound": 9735.211248661672
          },
          "point_estimate": 9714.429278831449,
          "standard_error": 8.983888940686894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5973685121508994,
            "upper_bound": 39.082920908566976
          },
          "point_estimate": 20.25774619638405,
          "standard_error": 9.137640211869703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9704.957230366324,
            "upper_bound": 9735.66735207934
          },
          "point_estimate": 9723.337866737118,
          "standard_error": 7.874561444451538
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.356697703829466,
            "upper_bound": 30.95252493960556
          },
          "point_estimate": 23.91164035269423,
          "standard_error": 5.039270367010306
        }
      }
    },
    "memrchr1/krate/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/uncommon",
        "directory_name": "memrchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80761.69215555732,
            "upper_bound": 80999.84085374906
          },
          "point_estimate": 80874.63469173117,
          "standard_error": 60.746080531268106
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80737.22964612719,
            "upper_bound": 81007.2340386043
          },
          "point_estimate": 80854.2353749072,
          "standard_error": 66.97952551877692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.675715417598138,
            "upper_bound": 331.690973528528
          },
          "point_estimate": 155.9432479515791,
          "standard_error": 74.80917072487564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80699.4023356819,
            "upper_bound": 80856.69056157529
          },
          "point_estimate": 80778.22573684667,
          "standard_error": 40.04634903367501
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.54960115842132,
            "upper_bound": 269.44743221297296
          },
          "point_estimate": 202.86230700447024,
          "standard_error": 46.41430867179753
        }
      }
    },
    "memrchr1/krate/huge/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/verycommon",
        "directory_name": "memrchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463538.3151348453,
            "upper_bound": 464189.9789572785
          },
          "point_estimate": 463825.65293449874,
          "standard_error": 168.33700130584856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463380.15063291136,
            "upper_bound": 464046.36265822785
          },
          "point_estimate": 463739.0623869801,
          "standard_error": 209.0147252070257
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.79777319970703,
            "upper_bound": 847.7605176708105
          },
          "point_estimate": 504.180708580673,
          "standard_error": 202.39628733405837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463463.0820384599,
            "upper_bound": 463929.2541323227
          },
          "point_estimate": 463628.58188393887,
          "standard_error": 118.73697322125302
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.12178175422375,
            "upper_bound": 798.751483917923
          },
          "point_estimate": 561.3528325263503,
          "standard_error": 171.40946540466797
        }
      }
    },
    "memrchr1/krate/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/common",
        "directory_name": "memrchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.2413314860658,
            "upper_bound": 227.39688219015073
          },
          "point_estimate": 227.31286712681845,
          "standard_error": 0.03977530907657381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.2500750976269,
            "upper_bound": 227.3632374169754
          },
          "point_estimate": 227.29804401972564,
          "standard_error": 0.024424574495761985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012380051222099389,
            "upper_bound": 0.196576937783701
          },
          "point_estimate": 0.05433611254540727,
          "standard_error": 0.04731300519331777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.27408993610297,
            "upper_bound": 227.4740789687756
          },
          "point_estimate": 227.36599589206227,
          "standard_error": 0.0520979169077732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03535910950210635,
            "upper_bound": 0.18847216533343372
          },
          "point_estimate": 0.13256118847846474,
          "standard_error": 0.0395748662793637
        }
      }
    },
    "memrchr1/krate/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/never",
        "directory_name": "memrchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.809229686694124,
            "upper_bound": 8.83670824093251
          },
          "point_estimate": 8.819826725652153,
          "standard_error": 0.007612495519212662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.806505609120665,
            "upper_bound": 8.818923562467582
          },
          "point_estimate": 8.813860194256334,
          "standard_error": 0.0034669231516995247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002144603813402719,
            "upper_bound": 0.015228410192693468
          },
          "point_estimate": 0.0066399634610746616,
          "standard_error": 0.003731039964402272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.808214599145577,
            "upper_bound": 8.81704973602514
          },
          "point_estimate": 8.81308841431726,
          "standard_error": 0.002257907224769879
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004272641292597617,
            "upper_bound": 0.03876302923203525
          },
          "point_estimate": 0.025338087969287777,
          "standard_error": 0.01178982933130084
        }
      }
    },
    "memrchr1/krate/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/rare",
        "directory_name": "memrchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.692716825772091,
            "upper_bound": 13.70618412601195
          },
          "point_estimate": 13.699437576411189,
          "standard_error": 0.0034475148784054825
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.68869989919908,
            "upper_bound": 13.707641480752574
          },
          "point_estimate": 13.700399892623016,
          "standard_error": 0.003973553289062219
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000887009441855368,
            "upper_bound": 0.021138821054506325
          },
          "point_estimate": 0.011886954860284711,
          "standard_error": 0.006151355396046712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.689244445099774,
            "upper_bound": 13.700268455653577
          },
          "point_estimate": 13.695716237919829,
          "standard_error": 0.0028895922786800698
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006512190331661332,
            "upper_bound": 0.01441728374183274
          },
          "point_estimate": 0.011497012075454774,
          "standard_error": 0.0020695439746840484
        }
      }
    },
    "memrchr1/krate/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/uncommon",
        "directory_name": "memrchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.88868001503099,
            "upper_bound": 46.94550827092831
          },
          "point_estimate": 46.91593127070502,
          "standard_error": 0.014473440050712124
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.88012876402906,
            "upper_bound": 46.944098256788514
          },
          "point_estimate": 46.91122153107587,
          "standard_error": 0.015046961200363015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006215197836020376,
            "upper_bound": 0.08108445144703519
          },
          "point_estimate": 0.04203064907721671,
          "standard_error": 0.01869814429509404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.87215023438251,
            "upper_bound": 46.92564945286093
          },
          "point_estimate": 46.897137294648616,
          "standard_error": 0.013921472754076071
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022861867800585058,
            "upper_bound": 0.06477940518962137
          },
          "point_estimate": 0.04822089222839314,
          "standard_error": 0.011207358233398622
        }
      }
    },
    "memrchr1/krate/small/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/verycommon",
        "directory_name": "memrchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496.5961129058294,
            "upper_bound": 497.46716092973
          },
          "point_estimate": 497.0340303704861,
          "standard_error": 0.22219543813859768
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496.4158305995991,
            "upper_bound": 497.51621787862223
          },
          "point_estimate": 497.1034520229634,
          "standard_error": 0.28276033567372727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10448925265561004,
            "upper_bound": 1.2281673633788366
          },
          "point_estimate": 0.5665241700952803,
          "standard_error": 0.3051732298576251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496.5461050480585,
            "upper_bound": 497.1991982406176
          },
          "point_estimate": 496.91034271087034,
          "standard_error": 0.16644042099179868
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3884317258935567,
            "upper_bound": 0.9507315834689511
          },
          "point_estimate": 0.7410813868327865,
          "standard_error": 0.14150090457863984
        }
      }
    },
    "memrchr1/krate/tiny/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/common",
        "directory_name": "memrchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.47751278355656,
            "upper_bound": 51.57418199677935
          },
          "point_estimate": 51.52577404330113,
          "standard_error": 0.024834594758243147
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.433711484097685,
            "upper_bound": 51.60649766548819
          },
          "point_estimate": 51.52651479547535,
          "standard_error": 0.041179876620010006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016861435537588038,
            "upper_bound": 0.14105741493761362
          },
          "point_estimate": 0.1280863939907944,
          "standard_error": 0.0336226971337589
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.44624370403479,
            "upper_bound": 51.521528037985846
          },
          "point_estimate": 51.47825236098859,
          "standard_error": 0.01941758596580436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0513780024364587,
            "upper_bound": 0.09773685238686862
          },
          "point_estimate": 0.08279752157353626,
          "standard_error": 0.011682985104855005
        }
      }
    },
    "memrchr1/krate/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/never",
        "directory_name": "memrchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.530666421466627,
            "upper_bound": 4.56685597693382
          },
          "point_estimate": 4.550822475097117,
          "standard_error": 0.009307504825780952
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.541409948659512,
            "upper_bound": 4.566467235801973
          },
          "point_estimate": 4.5554997080045885,
          "standard_error": 0.007546524460000074
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004145129537012011,
            "upper_bound": 0.03852667049093093
          },
          "point_estimate": 0.018574966628934944,
          "standard_error": 0.00866364621044577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.532334139091928,
            "upper_bound": 4.562035149047613
          },
          "point_estimate": 4.549761152160562,
          "standard_error": 0.007520306776706077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00960807615542668,
            "upper_bound": 0.04479254639124365
          },
          "point_estimate": 0.03106667414314398,
          "standard_error": 0.010326970352408728
        }
      }
    },
    "memrchr1/krate/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/rare",
        "directory_name": "memrchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.595345371537185,
            "upper_bound": 7.619179924107079
          },
          "point_estimate": 7.607209506515349,
          "standard_error": 0.006090321511850788
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.591047545056465,
            "upper_bound": 7.622616551013448
          },
          "point_estimate": 7.608664867462121,
          "standard_error": 0.006997658701740571
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001318661747129089,
            "upper_bound": 0.03457110734122373
          },
          "point_estimate": 0.017646457354868375,
          "standard_error": 0.008746363023756485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.590757008835444,
            "upper_bound": 7.612581065686381
          },
          "point_estimate": 7.601571439616295,
          "standard_error": 0.00546048817474197
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0104733434199474,
            "upper_bound": 0.02623385471675159
          },
          "point_estimate": 0.020224743946882312,
          "standard_error": 0.004026441658696507
        }
      }
    },
    "memrchr1/krate/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/uncommon",
        "directory_name": "memrchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.566987131821133,
            "upper_bound": 28.591208575974424
          },
          "point_estimate": 28.5790706341906,
          "standard_error": 0.006227767536736898
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.559647357185405,
            "upper_bound": 28.598910850877587
          },
          "point_estimate": 28.577077928532965,
          "standard_error": 0.009709231050243404
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004655009804355557,
            "upper_bound": 0.036728597440094934
          },
          "point_estimate": 0.023048571008135843,
          "standard_error": 0.007562170834494667
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.570061821816193,
            "upper_bound": 28.594602090033113
          },
          "point_estimate": 28.582029873866983,
          "standard_error": 0.0062699966668851295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012724142606491368,
            "upper_bound": 0.02507088877123434
          },
          "point_estimate": 0.02077308454281774,
          "standard_error": 0.0031421501005375898
        }
      }
    },
    "memrchr1/libc/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/libc/empty/never",
        "directory_name": "memrchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4948936641378362,
            "upper_bound": 0.5023032741578926
          },
          "point_estimate": 0.4985886866228101,
          "standard_error": 0.0018939857354223432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4923871904248035,
            "upper_bound": 0.5032463925732412
          },
          "point_estimate": 0.49947671631711976,
          "standard_error": 0.00265541695381054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011852359868818425,
            "upper_bound": 0.011032888977283335
          },
          "point_estimate": 0.006722900033780775,
          "standard_error": 0.0024904645654229745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4961752933905878,
            "upper_bound": 0.5010462865232247
          },
          "point_estimate": 0.4988906262346493,
          "standard_error": 0.001242887294867366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003618971858864533,
            "upper_bound": 0.007907007782295571
          },
          "point_estimate": 0.006298951473390497,
          "standard_error": 0.0010966080237266962
        }
      }
    },
    "memrchr1/libc/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/common",
        "directory_name": "memrchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262052.37483528175,
            "upper_bound": 262643.4697883693
          },
          "point_estimate": 262335.88142486016,
          "standard_error": 151.90859721706468
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 261934.3705035971,
            "upper_bound": 262682.8820143885
          },
          "point_estimate": 262282.8723521183,
          "standard_error": 204.94499819584448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.28978567155713,
            "upper_bound": 885.611361831215
          },
          "point_estimate": 554.8715730987055,
          "standard_error": 180.41166616212325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262100.4412470024,
            "upper_bound": 262610.3730072392
          },
          "point_estimate": 262374.3410632533,
          "standard_error": 130.88839768785363
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 268.69951623472974,
            "upper_bound": 662.5307959455849
          },
          "point_estimate": 503.89086047172265,
          "standard_error": 106.30237030348322
        }
      }
    },
    "memrchr1/libc/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/never",
        "directory_name": "memrchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9442.360972511273,
            "upper_bound": 9454.598082191582
          },
          "point_estimate": 9448.576737484103,
          "standard_error": 3.1337886153052015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9439.125260145682,
            "upper_bound": 9456.745672910163
          },
          "point_estimate": 9450.244218984855,
          "standard_error": 3.962296333532093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5921364961884348,
            "upper_bound": 18.31504653228332
          },
          "point_estimate": 10.108956814978374,
          "standard_error": 4.659074171828959
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9440.0994544001,
            "upper_bound": 9452.962694270082
          },
          "point_estimate": 9445.51424922632,
          "standard_error": 3.3352563162258453
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.807531394661365,
            "upper_bound": 13.05047008861125
          },
          "point_estimate": 10.435579159838207,
          "standard_error": 1.845940006116075
        }
      }
    },
    "memrchr1/libc/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/rare",
        "directory_name": "memrchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10582.515833479973,
            "upper_bound": 10595.086281584065
          },
          "point_estimate": 10587.757567437178,
          "standard_error": 3.3185089460914736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10581.037463556851,
            "upper_bound": 10590.795127030404
          },
          "point_estimate": 10585.205850340137,
          "standard_error": 2.497626007720434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2967875749051451,
            "upper_bound": 11.271505922341664
          },
          "point_estimate": 5.308536375143266,
          "standard_error": 2.9072844843924286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10581.930326223724,
            "upper_bound": 10590.369068463213
          },
          "point_estimate": 10585.80550301011,
          "standard_error": 2.236049086552729
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.819574836635739,
            "upper_bound": 16.275588456192782
          },
          "point_estimate": 11.055289951996867,
          "standard_error": 4.129808943264095
        }
      }
    },
    "memrchr1/libc/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/uncommon",
        "directory_name": "memrchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79148.85518357487,
            "upper_bound": 79256.23699055384
          },
          "point_estimate": 79199.51821359558,
          "standard_error": 27.382852181960743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79125.07934782609,
            "upper_bound": 79248.48540372672
          },
          "point_estimate": 79206.38304347826,
          "standard_error": 34.98800526142207
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.88378495497952,
            "upper_bound": 157.62723589719613
          },
          "point_estimate": 87.75400786235407,
          "standard_error": 35.08057199450828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79143.77000330633,
            "upper_bound": 79218.82262546413
          },
          "point_estimate": 79177.89676453981,
          "standard_error": 19.01896260821927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.30286603956763,
            "upper_bound": 123.27850971767
          },
          "point_estimate": 91.57069750313237,
          "standard_error": 21.359247822870017
        }
      }
    },
    "memrchr1/libc/huge/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/verycommon",
        "directory_name": "memrchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546523.242906361,
            "upper_bound": 547572.5536839612
          },
          "point_estimate": 547019.3821085051,
          "standard_error": 267.3884148040319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546431.328358209,
            "upper_bound": 547476.1276119403
          },
          "point_estimate": 547027.6683250414,
          "standard_error": 211.7204517851197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.791899869131544,
            "upper_bound": 1397.5277654874567
          },
          "point_estimate": 487.22060179785115,
          "standard_error": 450.9193023678987
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 546559.9646780294,
            "upper_bound": 547160.6706402552
          },
          "point_estimate": 546885.6916069005,
          "standard_error": 151.72777576547125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384.0638010738725,
            "upper_bound": 1207.0077873864495
          },
          "point_estimate": 890.435097422548,
          "standard_error": 218.15434398138183
        }
      }
    },
    "memrchr1/libc/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/common",
        "directory_name": "memrchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.34642109126528,
            "upper_bound": 238.54471719800296
          },
          "point_estimate": 238.451414338383,
          "standard_error": 0.05123981300711097
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.31448507382208,
            "upper_bound": 238.59540078987385
          },
          "point_estimate": 238.480901079786,
          "standard_error": 0.08042383469400735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013483327667695373,
            "upper_bound": 0.283329688691384
          },
          "point_estimate": 0.15702945610481991,
          "standard_error": 0.08110638527603321
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.38402957704875,
            "upper_bound": 238.55877473188937
          },
          "point_estimate": 238.48938109971684,
          "standard_error": 0.044764089052610874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08827608966867262,
            "upper_bound": 0.21452243225850012
          },
          "point_estimate": 0.17073364750975986,
          "standard_error": 0.033232252210637934
        }
      }
    },
    "memrchr1/libc/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/never",
        "directory_name": "memrchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.98059439336548,
            "upper_bound": 8.013041597066273
          },
          "point_estimate": 7.994505055623523,
          "standard_error": 0.008497480010706344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.976097325989247,
            "upper_bound": 8.003999089866854
          },
          "point_estimate": 7.98630366130736,
          "standard_error": 0.007691881011534582
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003138030363401232,
            "upper_bound": 0.033227194961585445
          },
          "point_estimate": 0.017935290941779168,
          "standard_error": 0.007448199707171381
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.979666522070622,
            "upper_bound": 7.99703116928966
          },
          "point_estimate": 7.988959440846264,
          "standard_error": 0.004385108975299433
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009022597202138032,
            "upper_bound": 0.041077034968964136
          },
          "point_estimate": 0.028272034093523066,
          "standard_error": 0.00973359502291878
        }
      }
    },
    "memrchr1/libc/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/rare",
        "directory_name": "memrchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.317549393147887,
            "upper_bound": 14.335346949663366
          },
          "point_estimate": 14.325841717781095,
          "standard_error": 0.0045337393846819445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.318221442410024,
            "upper_bound": 14.33071174455748
          },
          "point_estimate": 14.3255858212175,
          "standard_error": 0.003390839716408756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022645589138758104,
            "upper_bound": 0.02196816385033544
          },
          "point_estimate": 0.008982365775722898,
          "standard_error": 0.004704369356713466
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.319009403846882,
            "upper_bound": 14.32919415851792
          },
          "point_estimate": 14.323531954301638,
          "standard_error": 0.002587637148874584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004898428474551121,
            "upper_bound": 0.021656526491272025
          },
          "point_estimate": 0.015091463659253969,
          "standard_error": 0.00444249898533633
        }
      }
    },
    "memrchr1/libc/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/uncommon",
        "directory_name": "memrchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.773636169845105,
            "upper_bound": 47.86150822899657
          },
          "point_estimate": 47.811331385196326,
          "standard_error": 0.022892212173852535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.76476401143109,
            "upper_bound": 47.827778245092
          },
          "point_estimate": 47.800642922139176,
          "standard_error": 0.019688594095367256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0059845976785887125,
            "upper_bound": 0.08476274966524168
          },
          "point_estimate": 0.05011086581321359,
          "standard_error": 0.019392774494343217
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.7745174694391,
            "upper_bound": 47.81229573553908
          },
          "point_estimate": 47.791803763031425,
          "standard_error": 0.009566437237190152
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02472504909509543,
            "upper_bound": 0.11162544505504222
          },
          "point_estimate": 0.07630253880348865,
          "standard_error": 0.026983893390285
        }
      }
    },
    "memrchr1/libc/small/verycommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/verycommon",
        "directory_name": "memrchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.3357580709694,
            "upper_bound": 581.2830948674267
          },
          "point_estimate": 580.7526617855393,
          "standard_error": 0.24562938715893903
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.2918450821244,
            "upper_bound": 581.0937858162374
          },
          "point_estimate": 580.511776751667,
          "standard_error": 0.18903071174452385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.060487937508521064,
            "upper_bound": 1.072171097473045
          },
          "point_estimate": 0.3792744891762045,
          "standard_error": 0.2738856703289919
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.4097671262967,
            "upper_bound": 581.2858197707917
          },
          "point_estimate": 580.857360939693,
          "standard_error": 0.23413219010485167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22019168442066295,
            "upper_bound": 1.133080342903474
          },
          "point_estimate": 0.8174039604402998,
          "standard_error": 0.2485578399495253
        }
      }
    },
    "memrchr1/libc/tiny/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/common",
        "directory_name": "memrchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.967659956802585,
            "upper_bound": 50.032655487278454
          },
          "point_estimate": 49.99905203225351,
          "standard_error": 0.016669967272834513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.94889193527682,
            "upper_bound": 50.042323926825446
          },
          "point_estimate": 49.98905219059239,
          "standard_error": 0.027762619975395197
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006645141433077979,
            "upper_bound": 0.09442132877571174
          },
          "point_estimate": 0.060700966469472165,
          "standard_error": 0.02354545443146843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.966127404140934,
            "upper_bound": 50.024518166120565
          },
          "point_estimate": 49.99305444965179,
          "standard_error": 0.01490176999655501
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03168680641923617,
            "upper_bound": 0.06962476810628274
          },
          "point_estimate": 0.05579609746842575,
          "standard_error": 0.00991670110543553
        }
      }
    },
    "memrchr1/libc/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/never",
        "directory_name": "memrchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.178785969596038,
            "upper_bound": 3.1859139635335603
          },
          "point_estimate": 3.181766315641849,
          "standard_error": 0.001878859339465068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.17882053010137,
            "upper_bound": 3.1831236418993014
          },
          "point_estimate": 3.179864518483163,
          "standard_error": 0.0013364605305523243
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00024772140406442326,
            "upper_bound": 0.006277552798144326
          },
          "point_estimate": 0.002238894142292105,
          "standard_error": 0.0016474636738750092
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1784556191939273,
            "upper_bound": 3.1825342543926456
          },
          "point_estimate": 3.180892280842414,
          "standard_error": 0.0010426944291317066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001631201522416409,
            "upper_bound": 0.00927501273146271
          },
          "point_estimate": 0.00625838085633376,
          "standard_error": 0.0023938694985726327
        }
      }
    },
    "memrchr1/libc/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/rare",
        "directory_name": "memrchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.87048843373831,
            "upper_bound": 5.879264543295426
          },
          "point_estimate": 5.874638586737949,
          "standard_error": 0.0022565267315475483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.868284032138453,
            "upper_bound": 5.880026496943591
          },
          "point_estimate": 5.874031159798054,
          "standard_error": 0.0027776786031743854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018027879565170764,
            "upper_bound": 0.013340257524792207
          },
          "point_estimate": 0.006669193239664618,
          "standard_error": 0.002987980228152027
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.867408500876074,
            "upper_bound": 5.876283083799352
          },
          "point_estimate": 5.870622407195284,
          "standard_error": 0.0022789779854010817
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036826956847888256,
            "upper_bound": 0.009609624228199248
          },
          "point_estimate": 0.007508242259443043,
          "standard_error": 0.001539826415136098
        }
      }
    },
    "memrchr1/libc/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/uncommon",
        "directory_name": "memrchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.70783680036203,
            "upper_bound": 18.72463712622218
          },
          "point_estimate": 18.71605909992082,
          "standard_error": 0.004293373922561064
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.70526374032482,
            "upper_bound": 18.72567153646844
          },
          "point_estimate": 18.71593746600798,
          "standard_error": 0.005134468578123742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025109890755773156,
            "upper_bound": 0.024847177547865026
          },
          "point_estimate": 0.015128299012683838,
          "standard_error": 0.005856776017658759
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.702314435104302,
            "upper_bound": 18.725028397046415
          },
          "point_estimate": 18.710655000646856,
          "standard_error": 0.005759622608518979
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007900274389422124,
            "upper_bound": 0.01810853723899667
          },
          "point_estimate": 0.014322899996815394,
          "standard_error": 0.0026242414486394596
        }
      }
    },
    "memrchr2/krate/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr2/krate/empty/never",
        "directory_name": "memrchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9837543278817492,
            "upper_bound": 0.985224723001384
          },
          "point_estimate": 0.9844952631388482,
          "standard_error": 0.0003757635942870461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9836523337077712,
            "upper_bound": 0.9853447167428264
          },
          "point_estimate": 0.9845997976915032,
          "standard_error": 0.00038426515008726934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00004651446987885681,
            "upper_bound": 0.002358066978019014
          },
          "point_estimate": 0.001065321647811854,
          "standard_error": 0.0006473545181182333
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9831455051831712,
            "upper_bound": 0.98494901021345
          },
          "point_estimate": 0.984018338174971,
          "standard_error": 0.0004607547988605648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006786619210434103,
            "upper_bound": 0.0015977295732670233
          },
          "point_estimate": 0.0012525529665767651,
          "standard_error": 0.00023876216590250457
        }
      }
    },
    "memrchr2/krate/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/common",
        "directory_name": "memrchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445437.4627067,
            "upper_bound": 445849.21991427126
          },
          "point_estimate": 445634.8541884436,
          "standard_error": 104.78115362539204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445502.4297328688,
            "upper_bound": 445756.19207317074
          },
          "point_estimate": 445610.4463922765,
          "standard_error": 54.34869241009808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.876996892800394,
            "upper_bound": 516.3371374185695
          },
          "point_estimate": 92.34473378738862,
          "standard_error": 136.55504220344824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445207.8278449874,
            "upper_bound": 445699.1399575773
          },
          "point_estimate": 445437.5993031359,
          "standard_error": 132.2547534931008
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.37628334424817,
            "upper_bound": 492.9933098551399
          },
          "point_estimate": 349.33916404684254,
          "standard_error": 102.32374468695755
        }
      }
    },
    "memrchr2/krate/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/never",
        "directory_name": "memrchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12389.05655574484,
            "upper_bound": 12404.22162618642
          },
          "point_estimate": 12396.017779968091,
          "standard_error": 3.891701219919225
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12386.69465076661,
            "upper_bound": 12402.622793867122
          },
          "point_estimate": 12392.59491766042,
          "standard_error": 4.036045494219577
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2355055907866828,
            "upper_bound": 19.86289951788956
          },
          "point_estimate": 9.983241132243508,
          "standard_error": 4.451508296962645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12387.81396621178,
            "upper_bound": 12399.43966806857
          },
          "point_estimate": 12391.658738467666,
          "standard_error": 2.9992324462331723
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.574847427037103,
            "upper_bound": 17.60710939111863
          },
          "point_estimate": 12.962040098044378,
          "standard_error": 3.3151900040189433
        }
      }
    },
    "memrchr2/krate/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/rare",
        "directory_name": "memrchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16868.30181527189,
            "upper_bound": 16892.57229919457
          },
          "point_estimate": 16880.28193432134,
          "standard_error": 6.221216699916557
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16860.142832295256,
            "upper_bound": 16894.814123376622
          },
          "point_estimate": 16884.516179653678,
          "standard_error": 9.77599923911824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.975662429418011,
            "upper_bound": 38.34020035828278
          },
          "point_estimate": 27.01493725229979,
          "standard_error": 8.959752479774911
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16859.279369864827,
            "upper_bound": 16901.53946210691
          },
          "point_estimate": 16878.743799243428,
          "standard_error": 11.565213248978816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.73859404825036,
            "upper_bound": 25.90427043070651
          },
          "point_estimate": 20.775136750219385,
          "standard_error": 3.475636389665733
        }
      }
    },
    "memrchr2/krate/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/uncommon",
        "directory_name": "memrchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165575.91862089586,
            "upper_bound": 166337.15679467007
          },
          "point_estimate": 165947.8551810176,
          "standard_error": 194.67179621639303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165379.26744944553,
            "upper_bound": 166547.5003805175
          },
          "point_estimate": 165860.51512557076,
          "standard_error": 276.8340530469611
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.90593047422772,
            "upper_bound": 1127.6459073775416
          },
          "point_estimate": 703.8767892160257,
          "standard_error": 244.2901820862747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165694.40059974103,
            "upper_bound": 166401.2712587232
          },
          "point_estimate": 166011.22089782363,
          "standard_error": 178.94008965266872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 362.5054498306285,
            "upper_bound": 795.9176698148574
          },
          "point_estimate": 647.1845586939129,
          "standard_error": 109.64382165036818
        }
      }
    },
    "memrchr2/krate/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/common",
        "directory_name": "memrchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456.995323749219,
            "upper_bound": 457.3803322591414
          },
          "point_estimate": 457.1896893124992,
          "standard_error": 0.0985005048797115
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456.8597320526843,
            "upper_bound": 457.4521139509447
          },
          "point_estimate": 457.2161831605906,
          "standard_error": 0.13070102088994642
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05565504684770048,
            "upper_bound": 0.588372612663785
          },
          "point_estimate": 0.3549213613867333,
          "standard_error": 0.13875490232894483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 456.9264801149438,
            "upper_bound": 457.3093809099485
          },
          "point_estimate": 457.1153661394613,
          "standard_error": 0.0973099520634122
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18368201230951375,
            "upper_bound": 0.41431090559702144
          },
          "point_estimate": 0.3276522807688979,
          "standard_error": 0.05842406860350465
        }
      }
    },
    "memrchr2/krate/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/never",
        "directory_name": "memrchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.963343996895246,
            "upper_bound": 12.980775053472962
          },
          "point_estimate": 12.97243029612616,
          "standard_error": 0.004487145929382053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.952986907104316,
            "upper_bound": 12.983922586751891
          },
          "point_estimate": 12.977661249068406,
          "standard_error": 0.0075790342736759
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015948462333217105,
            "upper_bound": 0.023494745590838553
          },
          "point_estimate": 0.01113776545580296,
          "standard_error": 0.00596637823785209
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.959000919499925,
            "upper_bound": 12.980468431953744
          },
          "point_estimate": 12.970158298661266,
          "standard_error": 0.005757952473287881
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007658698474212022,
            "upper_bound": 0.017579962246541105
          },
          "point_estimate": 0.014974408770455395,
          "standard_error": 0.00234186253386769
        }
      }
    },
    "memrchr2/krate/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/rare",
        "directory_name": "memrchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.534577970974944,
            "upper_bound": 23.55366055116684
          },
          "point_estimate": 23.54411622631899,
          "standard_error": 0.004882654470903035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.533655819196134,
            "upper_bound": 23.55691484708942
          },
          "point_estimate": 23.54145057870753,
          "standard_error": 0.0062143025794655325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002802397311708649,
            "upper_bound": 0.028402313765401355
          },
          "point_estimate": 0.0160780954969232,
          "standard_error": 0.006266021767399524
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.53560921318475,
            "upper_bound": 23.554154815864266
          },
          "point_estimate": 23.54505061298439,
          "standard_error": 0.0048501164519306915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008895811911640619,
            "upper_bound": 0.02098536751022054
          },
          "point_estimate": 0.016266662899985507,
          "standard_error": 0.003120785098919799
        }
      }
    },
    "memrchr2/krate/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/uncommon",
        "directory_name": "memrchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.40378202916725,
            "upper_bound": 96.50670955157844
          },
          "point_estimate": 96.45454972326,
          "standard_error": 0.026386686501352067
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.38292893583656,
            "upper_bound": 96.51741211491372
          },
          "point_estimate": 96.45327066623408,
          "standard_error": 0.030552194050828815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012014021625677492,
            "upper_bound": 0.15901553774969412
          },
          "point_estimate": 0.08075215510252964,
          "standard_error": 0.038079977184477205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.37998877756542,
            "upper_bound": 96.4903194780006
          },
          "point_estimate": 96.43015746688236,
          "standard_error": 0.028801851591368065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04818610164320356,
            "upper_bound": 0.11321800864737384
          },
          "point_estimate": 0.08792164421488728,
          "standard_error": 0.017071466559407788
        }
      }
    },
    "memrchr2/krate/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/never",
        "directory_name": "memrchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.840914671379605,
            "upper_bound": 4.850806386450257
          },
          "point_estimate": 4.8449735727346965,
          "standard_error": 0.0026168506152262033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.839774993728515,
            "upper_bound": 4.846023589668822
          },
          "point_estimate": 4.842662356675012,
          "standard_error": 0.0016205053381089204
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004459730802481598,
            "upper_bound": 0.007837710307882823
          },
          "point_estimate": 0.0035569060794062544,
          "standard_error": 0.002008069921414525
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.838928936710003,
            "upper_bound": 4.845813927947024
          },
          "point_estimate": 4.842154110265785,
          "standard_error": 0.0018525924563865584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002208095152109158,
            "upper_bound": 0.013012031636540822
          },
          "point_estimate": 0.008707294138310779,
          "standard_error": 0.003450131454780972
        }
      }
    },
    "memrchr2/krate/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/rare",
        "directory_name": "memrchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.228471701415923,
            "upper_bound": 15.30602992400054
          },
          "point_estimate": 15.267794257273843,
          "standard_error": 0.01983579543131279
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.210973748845156,
            "upper_bound": 15.330906551796186
          },
          "point_estimate": 15.270467582662404,
          "standard_error": 0.03584080645354565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01066085490744603,
            "upper_bound": 0.1071504148276476
          },
          "point_estimate": 0.07974018535444805,
          "standard_error": 0.025659830855486395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.19920004454353,
            "upper_bound": 15.31768083888133
          },
          "point_estimate": 15.2637579734079,
          "standard_error": 0.03112132343170088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04282322742887486,
            "upper_bound": 0.07885546862801102
          },
          "point_estimate": 0.06622426095630651,
          "standard_error": 0.009287702651443871
        }
      }
    },
    "memrchr2/krate/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/uncommon",
        "directory_name": "memrchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.51029509162443,
            "upper_bound": 52.59953358101118
          },
          "point_estimate": 52.55834606540831,
          "standard_error": 0.02295080019321993
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.51633219079768,
            "upper_bound": 52.62735900859503
          },
          "point_estimate": 52.57155982815999,
          "standard_error": 0.02882355982100689
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008843822347279363,
            "upper_bound": 0.12715408393561017
          },
          "point_estimate": 0.08230417857198603,
          "standard_error": 0.02743214288381954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.53175549995369,
            "upper_bound": 52.59410695149526
          },
          "point_estimate": 52.560641638327226,
          "standard_error": 0.015828243072745416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03598996444983892,
            "upper_bound": 0.1037031349677717
          },
          "point_estimate": 0.07658797406135107,
          "standard_error": 0.019421930733808875
        }
      }
    },
    "memrchr3/krate/empty/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr3/krate/empty/never",
        "directory_name": "memrchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2224179871216498,
            "upper_bound": 1.2289213421683358
          },
          "point_estimate": 1.2248608973634076,
          "standard_error": 0.0018449713097642296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2221867847580803,
            "upper_bound": 1.2245754246629508
          },
          "point_estimate": 1.222857026552311,
          "standard_error": 0.0007040901678745274
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002573330602567892,
            "upper_bound": 0.0029307480638270044
          },
          "point_estimate": 0.001213388332489518,
          "standard_error": 0.0008516139108138499
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2226422794314675,
            "upper_bound": 1.2234774393962138
          },
          "point_estimate": 1.2229990354694495,
          "standard_error": 0.00021215176676740927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007241057665359993,
            "upper_bound": 0.00943358274375392
          },
          "point_estimate": 0.006145391053119675,
          "standard_error": 0.0029858228825888787
        }
      }
    },
    "memrchr3/krate/huge/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/common",
        "directory_name": "memrchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685250.1779835391,
            "upper_bound": 686239.8583950618
          },
          "point_estimate": 685711.8143797766,
          "standard_error": 253.6086388168974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685094.4917695473,
            "upper_bound": 686227.3657407407
          },
          "point_estimate": 685557.5462962963,
          "standard_error": 283.05990300148056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.03682201679936,
            "upper_bound": 1343.8987585483833
          },
          "point_estimate": 649.1534785986809,
          "standard_error": 295.10537355989317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685187.6571842927,
            "upper_bound": 686075.7884990254
          },
          "point_estimate": 685575.9668109668,
          "standard_error": 222.50469322656164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.05789153305767,
            "upper_bound": 1136.2139333132145
          },
          "point_estimate": 847.0202012557618,
          "standard_error": 206.5653417693309
        }
      }
    },
    "memrchr3/krate/huge/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/never",
        "directory_name": "memrchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16480.189730692153,
            "upper_bound": 16522.342431009685
          },
          "point_estimate": 16496.94537656964,
          "standard_error": 11.428492209342403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16476.309037238876,
            "upper_bound": 16499.471071752952
          },
          "point_estimate": 16487.550131193864,
          "standard_error": 6.475978655990225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.9576503267601275,
            "upper_bound": 29.03658999312806
          },
          "point_estimate": 13.471961014232246,
          "standard_error": 6.491611382673651
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16476.503297556945,
            "upper_bound": 16493.160434355363
          },
          "point_estimate": 16485.930150866392,
          "standard_error": 4.315601023196945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.20774679225532,
            "upper_bound": 57.83536148510419
          },
          "point_estimate": 38.073452128045275,
          "standard_error": 16.50928481585983
        }
      }
    },
    "memrchr3/krate/huge/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/rare",
        "directory_name": "memrchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22038.33035047937,
            "upper_bound": 22087.846668688093
          },
          "point_estimate": 22061.892275693783,
          "standard_error": 12.65524531701946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22031.27539417829,
            "upper_bound": 22081.51728320194
          },
          "point_estimate": 22058.710834849404,
          "standard_error": 12.721028417847371
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3446610261261585,
            "upper_bound": 72.13289426213478
          },
          "point_estimate": 31.432134129657783,
          "standard_error": 17.10963099354229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22030.09080684093,
            "upper_bound": 22062.328419691967
          },
          "point_estimate": 22047.936959826104,
          "standard_error": 8.364531460153836
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.698983691620153,
            "upper_bound": 57.62310822557871
          },
          "point_estimate": 42.23655354866913,
          "standard_error": 10.555667769478411
        }
      }
    },
    "memrchr3/krate/huge/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/uncommon",
        "directory_name": "memrchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220398.79935295825,
            "upper_bound": 220620.5592969517
          },
          "point_estimate": 220506.15852886005,
          "standard_error": 56.74047871661884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220340.66464646463,
            "upper_bound": 220624.84606060607
          },
          "point_estimate": 220511.59064935063,
          "standard_error": 89.33855999461038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.65091768738002,
            "upper_bound": 363.4324271841518
          },
          "point_estimate": 196.9103923223194,
          "standard_error": 77.80680424662252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220363.833313783,
            "upper_bound": 220603.89703865236
          },
          "point_estimate": 220501.02258953167,
          "standard_error": 62.72470038329999
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.24494809235811,
            "upper_bound": 244.65735302296548
          },
          "point_estimate": 189.82799103417497,
          "standard_error": 36.471794290411424
        }
      }
    },
    "memrchr3/krate/small/common": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/common",
        "directory_name": "memrchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 711.5021212738984,
            "upper_bound": 712.6503060657607
          },
          "point_estimate": 712.0551228026113,
          "standard_error": 0.29375245890822177
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 711.3694358549485,
            "upper_bound": 712.8338682740984
          },
          "point_estimate": 711.7698144179584,
          "standard_error": 0.356203037536462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04281427188356992,
            "upper_bound": 1.662468194938537
          },
          "point_estimate": 0.6888105011511506,
          "standard_error": 0.4637126845635197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 711.0501830477036,
            "upper_bound": 712.2065793287935
          },
          "point_estimate": 711.4761314581771,
          "standard_error": 0.29550362742651654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4720595016346736,
            "upper_bound": 1.2170643659788902
          },
          "point_estimate": 0.9768542854995184,
          "standard_error": 0.1830325204769324
        }
      }
    },
    "memrchr3/krate/small/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/never",
        "directory_name": "memrchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.9089918447824,
            "upper_bound": 15.942872881904748
          },
          "point_estimate": 15.925672720522885,
          "standard_error": 0.008678694505856259
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.90053451296022,
            "upper_bound": 15.950417694520072
          },
          "point_estimate": 15.923091156149637,
          "standard_error": 0.016035490843708363
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007878652975913593,
            "upper_bound": 0.04612559722613509
          },
          "point_estimate": 0.03697840183381972,
          "standard_error": 0.010617517263303268
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.899019397062164,
            "upper_bound": 15.933565981154969
          },
          "point_estimate": 15.912972184080289,
          "standard_error": 0.008869824447303436
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01880364180030872,
            "upper_bound": 0.034278459090819974
          },
          "point_estimate": 0.028940376754017276,
          "standard_error": 0.003954694687132372
        }
      }
    },
    "memrchr3/krate/small/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/rare",
        "directory_name": "memrchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.03971674503164,
            "upper_bound": 35.11778719756236
          },
          "point_estimate": 35.076487670833885,
          "standard_error": 0.019971109565965928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.02956959696363,
            "upper_bound": 35.11778189692082
          },
          "point_estimate": 35.06747270634379,
          "standard_error": 0.02212603898128916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013696702232565526,
            "upper_bound": 0.1115967093418891
          },
          "point_estimate": 0.05932753936029861,
          "standard_error": 0.023445923664342443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.049301763782836,
            "upper_bound": 35.09480389640585
          },
          "point_estimate": 35.073064129466076,
          "standard_error": 0.011500619983100474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03066749667864244,
            "upper_bound": 0.09050114733865404
          },
          "point_estimate": 0.06660127942326836,
          "standard_error": 0.016096217021991266
        }
      }
    },
    "memrchr3/krate/small/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/uncommon",
        "directory_name": "memrchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.09747893581533,
            "upper_bound": 155.4638548333007
          },
          "point_estimate": 155.26253399216588,
          "standard_error": 0.09428359559565974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.06980560410585,
            "upper_bound": 155.45398809803612
          },
          "point_estimate": 155.14043084181242,
          "standard_error": 0.09032133181705136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02490651209805839,
            "upper_bound": 0.45413181359245613
          },
          "point_estimate": 0.1409403088987613,
          "standard_error": 0.1140542519671811
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.08477833628024,
            "upper_bound": 155.27521601809977
          },
          "point_estimate": 155.1780279966095,
          "standard_error": 0.05122627075696258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09758617250447434,
            "upper_bound": 0.4182533178883818
          },
          "point_estimate": 0.31264764764416625,
          "standard_error": 0.08471004119891389
        }
      }
    },
    "memrchr3/krate/tiny/never": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/never",
        "directory_name": "memrchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.619391457585065,
            "upper_bound": 5.624932932503291
          },
          "point_estimate": 5.6224770429714965,
          "standard_error": 0.001423501230388269
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.621509631604836,
            "upper_bound": 5.624462036638144
          },
          "point_estimate": 5.622868085163012,
          "standard_error": 0.0006971101667896147
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000044324268332258625,
            "upper_bound": 0.005697235649207704
          },
          "point_estimate": 0.0018560749127025497,
          "standard_error": 0.0014152319607030712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.622521710846228,
            "upper_bound": 5.626482958190462
          },
          "point_estimate": 5.624137116739451,
          "standard_error": 0.0010693937658819183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009240741543118516,
            "upper_bound": 0.0069202990139950755
          },
          "point_estimate": 0.004750226193179138,
          "standard_error": 0.001648613141141007
        }
      }
    },
    "memrchr3/krate/tiny/rare": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/rare",
        "directory_name": "memrchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.301395143058404,
            "upper_bound": 15.353874824644564
          },
          "point_estimate": 15.328439462878451,
          "standard_error": 0.01342953543388878
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.292550978974056,
            "upper_bound": 15.36964938226259
          },
          "point_estimate": 15.326478299535294,
          "standard_error": 0.022132100587477214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005063741624214171,
            "upper_bound": 0.08473994823160827
          },
          "point_estimate": 0.05923359483258757,
          "standard_error": 0.019567510491726843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.302502510645729,
            "upper_bound": 15.36655471608034
          },
          "point_estimate": 15.341473879015403,
          "standard_error": 0.01653433892561186
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026722244533715623,
            "upper_bound": 0.055521700643370725
          },
          "point_estimate": 0.04475750557588417,
          "standard_error": 0.007569666735440534
        }
      }
    },
    "memrchr3/krate/tiny/uncommon": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/uncommon",
        "directory_name": "memrchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.47808650297726,
            "upper_bound": 86.64579624590247
          },
          "point_estimate": 86.55767834381403,
          "standard_error": 0.04306985269901605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.45428833398259,
            "upper_bound": 86.68369565217391
          },
          "point_estimate": 86.50061532590809,
          "standard_error": 0.06194513927508409
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009978528287642213,
            "upper_bound": 0.22917206950415916
          },
          "point_estimate": 0.1403250675875105,
          "standard_error": 0.057765020526377574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.42175114973192,
            "upper_bound": 86.53400977843498
          },
          "point_estimate": 86.46258316702973,
          "standard_error": 0.029091939071563815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07568595461352254,
            "upper_bound": 0.18256709431950288
          },
          "point_estimate": 0.14404788044688,
          "standard_error": 0.027890459519563635
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1326508.24718254,
            "upper_bound": 1328608.297385204
          },
          "point_estimate": 1327641.3148455217,
          "standard_error": 538.3505488524168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1326528.1178571428,
            "upper_bound": 1328887.4267857145
          },
          "point_estimate": 1327841.933354592,
          "standard_error": 558.7321125668976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.87336836348248,
            "upper_bound": 2867.858097894661
          },
          "point_estimate": 1415.281960855813,
          "standard_error": 681.0293707515841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1327536.969633001,
            "upper_bound": 1328800.3088669952
          },
          "point_estimate": 1328153.6324675323,
          "standard_error": 324.2734584254391
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 716.8569522423046,
            "upper_bound": 2391.4909620891754
          },
          "point_estimate": 1792.1070607204545,
          "standard_error": 439.9761768788816
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1517573.0168769842,
            "upper_bound": 1520029.1891537698
          },
          "point_estimate": 1518851.0555059523,
          "standard_error": 629.2386810628986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1517047.9702380951,
            "upper_bound": 1520231.7725694445
          },
          "point_estimate": 1519397.638888889,
          "standard_error": 785.4569897371825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.19156039005657,
            "upper_bound": 3413.8202852258696
          },
          "point_estimate": 1527.8736937081885,
          "standard_error": 880.5535705321793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1517789.2280454517,
            "upper_bound": 1519911.7830615942
          },
          "point_estimate": 1518896.431926407,
          "standard_error": 547.6666557683238
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 957.467968667388,
            "upper_bound": 2643.6068079826355
          },
          "point_estimate": 2082.8709335542153,
          "standard_error": 411.8211984184656
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1423897.1790018314,
            "upper_bound": 1425604.27375
          },
          "point_estimate": 1424792.102815934,
          "standard_error": 435.8756042359579
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424029.545673077,
            "upper_bound": 1425911.412820513
          },
          "point_estimate": 1425044.2344322344,
          "standard_error": 468.4493361897246
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.76303137064855,
            "upper_bound": 2363.249151890138
          },
          "point_estimate": 1347.9610920304615,
          "standard_error": 588.192413403881
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424346.0620317063,
            "upper_bound": 1426108.6384615384
          },
          "point_estimate": 1425300.4557442558,
          "standard_error": 446.9540503139109
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 647.4864353382225,
            "upper_bound": 1966.6710532045083
          },
          "point_estimate": 1450.2991158922962,
          "standard_error": 348.9937891113389
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 763047.0230992065,
            "upper_bound": 764451.384139137
          },
          "point_estimate": 763677.8003869047,
          "standard_error": 364.2497714148108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762886.4260416667,
            "upper_bound": 764531.0260416666
          },
          "point_estimate": 763211.7633928572,
          "standard_error": 398.67171358916454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.92953553419832,
            "upper_bound": 1885.119431636645
          },
          "point_estimate": 535.3674020578476,
          "standard_error": 439.9855252650708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762923.8740126227,
            "upper_bound": 763955.8739458869
          },
          "point_estimate": 763350.2750541125,
          "standard_error": 268.1078680200305
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321.491793082851,
            "upper_bound": 1668.2375007882404
          },
          "point_estimate": 1220.7117635985917,
          "standard_error": 341.16986114101667
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266047.3287979377,
            "upper_bound": 266383.8086250507
          },
          "point_estimate": 266231.699188101,
          "standard_error": 86.59229468346462
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266047.5031282586,
            "upper_bound": 266442.48403284675
          },
          "point_estimate": 266311.8873682076,
          "standard_error": 96.91461093395658
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.60885417082861,
            "upper_bound": 419.2359849658128
          },
          "point_estimate": 157.49107603606765,
          "standard_error": 107.6880109654309
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266090.4173714892,
            "upper_bound": 266393.5349354295
          },
          "point_estimate": 266260.014181439,
          "standard_error": 76.69972271240042
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.3675737308786,
            "upper_bound": 386.6717216344639
          },
          "point_estimate": 288.75347529516154,
          "standard_error": 77.18553375951753
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476389.4960965523,
            "upper_bound": 477412.4527417026
          },
          "point_estimate": 476884.0328437436,
          "standard_error": 262.15019544504236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476316.6549165121,
            "upper_bound": 477426.3540764791
          },
          "point_estimate": 476774.30584415584,
          "standard_error": 325.4462034813818
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.84423961850845,
            "upper_bound": 1462.6410331237878
          },
          "point_estimate": 742.0564153972858,
          "standard_error": 336.2653284279168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476507.17728836613,
            "upper_bound": 478007.9678476863
          },
          "point_estimate": 477383.21022094786,
          "standard_error": 387.65219803341733
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 426.9554017746824,
            "upper_bound": 1155.04416197974
          },
          "point_estimate": 872.6649201606133,
          "standard_error": 189.81746831074096
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245893.32504698893,
            "upper_bound": 246475.36194766196
          },
          "point_estimate": 246114.41138567135,
          "standard_error": 163.11694600109436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245864.6302364865,
            "upper_bound": 246084.4695945946
          },
          "point_estimate": 245956.8404118404,
          "standard_error": 59.39489911697872
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.54000831208354,
            "upper_bound": 270.3630171933264
          },
          "point_estimate": 128.73450632936002,
          "standard_error": 88.71488762561694
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245834.9822910765,
            "upper_bound": 245984.07659581167
          },
          "point_estimate": 245905.90603720603,
          "standard_error": 39.95543301778997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.44750032738898,
            "upper_bound": 834.1010959390134
          },
          "point_estimate": 544.3895807105125,
          "standard_error": 260.2422822309081
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 693153.664654088,
            "upper_bound": 694219.5507794811
          },
          "point_estimate": 693665.6967138365,
          "standard_error": 272.82297703095475
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692979.2948113207,
            "upper_bound": 694285.3018867925
          },
          "point_estimate": 693481.1654874214,
          "standard_error": 400.6479053747306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.9991272924521,
            "upper_bound": 1661.1441735276776
          },
          "point_estimate": 1024.878702087736,
          "standard_error": 372.01367554055435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692970.946270558,
            "upper_bound": 694235.972658999
          },
          "point_estimate": 693562.7549620192,
          "standard_error": 318.80783309070216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 492.61990621534414,
            "upper_bound": 1191.6837378453354
          },
          "point_estimate": 909.690687720549,
          "standard_error": 187.04137254218645
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2304.6475971168006,
            "upper_bound": 2308.365413370494
          },
          "point_estimate": 2306.7058081403493,
          "standard_error": 0.9552015995100176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2305.5552275757,
            "upper_bound": 2308.9177394781946
          },
          "point_estimate": 2306.975479273789,
          "standard_error": 0.9653270613738956
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.43865778501385055,
            "upper_bound": 4.262883317958163
          },
          "point_estimate": 2.483436298511747,
          "standard_error": 0.9387982917050732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2306.284638674759,
            "upper_bound": 2309.2027758335175
          },
          "point_estimate": 2308.0268096532427,
          "standard_error": 0.7359152116286313
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.249624546395161,
            "upper_bound": 4.521306379230541
          },
          "point_estimate": 3.1888518432989477,
          "standard_error": 0.9728194261750408
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718.9775921939574,
            "upper_bound": 720.5361204786985
          },
          "point_estimate": 719.7687568528854,
          "standard_error": 0.39909996814428145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718.8075733124765,
            "upper_bound": 721.1146070530464
          },
          "point_estimate": 719.6001419057613,
          "standard_error": 0.6590005157363957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.29459321853878245,
            "upper_bound": 2.405221074212557
          },
          "point_estimate": 1.5929055694075478,
          "standard_error": 0.5495958106393193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 719.0229387904242,
            "upper_bound": 720.0352627152782
          },
          "point_estimate": 719.3952493499808,
          "standard_error": 0.2552567397290401
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8180315522858391,
            "upper_bound": 1.647137836128857
          },
          "point_estimate": 1.3274100556154975,
          "standard_error": 0.21897291361493057
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.4849437388977,
            "upper_bound": 140.63282078905087
          },
          "point_estimate": 140.56027850879016,
          "standard_error": 0.037883047536472945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.47194910660704,
            "upper_bound": 140.66646791980435
          },
          "point_estimate": 140.55399959623782,
          "standard_error": 0.055532173825438375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03339687654861361,
            "upper_bound": 0.2215998342290882
          },
          "point_estimate": 0.12876981172745547,
          "standard_error": 0.04860660573707916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.5099352659195,
            "upper_bound": 140.6676782299049
          },
          "point_estimate": 140.5854946400948,
          "standard_error": 0.041912130714571394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07326408291136821,
            "upper_bound": 0.1613392431203419
          },
          "point_estimate": 0.12610625828462715,
          "standard_error": 0.023270869668527244
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.03370615417432,
            "upper_bound": 52.07558419745444
          },
          "point_estimate": 52.05440118840277,
          "standard_error": 0.010721879977744087
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.018637684954946,
            "upper_bound": 52.09300960000025
          },
          "point_estimate": 52.05550036223582,
          "standard_error": 0.020278428482190587
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004509433095619611,
            "upper_bound": 0.05905667220345497
          },
          "point_estimate": 0.05474544815303846,
          "standard_error": 0.0170182261523081
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.020953518174466,
            "upper_bound": 52.081383082444525
          },
          "point_estimate": 52.04826155923012,
          "standard_error": 0.015661765556981642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02300567757491853,
            "upper_bound": 0.04123009408653498
          },
          "point_estimate": 0.03581709078336627,
          "standard_error": 0.004589160331452337
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.31302168214376,
            "upper_bound": 80.47488199742905
          },
          "point_estimate": 80.38788894548898,
          "standard_error": 0.041658730257831775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.27851332428926,
            "upper_bound": 80.48256212577115
          },
          "point_estimate": 80.32299068676183,
          "standard_error": 0.06749342363199196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006801524574187721,
            "upper_bound": 0.2645571900453553
          },
          "point_estimate": 0.07682266401270481,
          "standard_error": 0.06886370441120432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.28480146543828,
            "upper_bound": 80.4024502304325
          },
          "point_estimate": 80.32738863103901,
          "standard_error": 0.03077501553342259
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06725261480905805,
            "upper_bound": 0.18087882105823716
          },
          "point_estimate": 0.13833028109448922,
          "standard_error": 0.030740347852389428
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289286.75980599644,
            "upper_bound": 289806.2692218915
          },
          "point_estimate": 289537.2014770723,
          "standard_error": 133.06789201980416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289190.9623015873,
            "upper_bound": 289837.8561507936
          },
          "point_estimate": 289539.39695767197,
          "standard_error": 160.42892655931706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.90144639232076,
            "upper_bound": 767.8810824784687
          },
          "point_estimate": 417.9639553574712,
          "standard_error": 166.32646228414853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289218.2408243198,
            "upper_bound": 289590.8984222318
          },
          "point_estimate": 289424.4865594723,
          "standard_error": 94.17886981788824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.93479804887087,
            "upper_bound": 579.9003379777599
          },
          "point_estimate": 443.6931188684494,
          "standard_error": 93.17298008438844
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.70182562749293,
            "upper_bound": 62.800790744998295
          },
          "point_estimate": 62.75053324888192,
          "standard_error": 0.02537067808038235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.65519046140198,
            "upper_bound": 62.82144536339678
          },
          "point_estimate": 62.74451158753209,
          "standard_error": 0.036429073056458344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009466869792948888,
            "upper_bound": 0.14194677877564577
          },
          "point_estimate": 0.12324475666071678,
          "standard_error": 0.03838911827639174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.721586670863154,
            "upper_bound": 62.83415318770891
          },
          "point_estimate": 62.788233639739154,
          "standard_error": 0.02881000441900863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04994553097275184,
            "upper_bound": 0.10366066055063088
          },
          "point_estimate": 0.08450238719843944,
          "standard_error": 0.013749787811995944
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.61948127377144,
            "upper_bound": 105.79963276723215
          },
          "point_estimate": 105.70740021109712,
          "standard_error": 0.04623233747096299
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.56620722599588,
            "upper_bound": 105.86118386559322
          },
          "point_estimate": 105.6620548238034,
          "standard_error": 0.07139068391593371
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015655054880193964,
            "upper_bound": 0.25353228525528776
          },
          "point_estimate": 0.14825226524947424,
          "standard_error": 0.06056161359091147
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.6251484885926,
            "upper_bound": 105.73434947973148
          },
          "point_estimate": 105.67016112375664,
          "standard_error": 0.027389194381439677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08884360307927452,
            "upper_bound": 0.1843309441210541
          },
          "point_estimate": 0.1541523610273803,
          "standard_error": 0.0237963025681366
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128482.92273164506,
            "upper_bound": 128658.17275772268
          },
          "point_estimate": 128564.48762339448,
          "standard_error": 44.870312749571866
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128447.9497447978,
            "upper_bound": 128643.67712014134
          },
          "point_estimate": 128539.73775870772,
          "standard_error": 67.69696071454463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.86594611417345,
            "upper_bound": 263.7272204356813
          },
          "point_estimate": 145.25445812793672,
          "standard_error": 62.51458042244818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128463.5628260138,
            "upper_bound": 128613.76035258286
          },
          "point_estimate": 128525.56653664356,
          "standard_error": 38.37846362430093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.16132008145817,
            "upper_bound": 201.75731399366256
          },
          "point_estimate": 150.1971449500605,
          "standard_error": 35.72946629002832
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.59315262283492,
            "upper_bound": 60.67930329003953
          },
          "point_estimate": 60.63843435152661,
          "standard_error": 0.022084142247241548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.59104279367111,
            "upper_bound": 60.6925715199936
          },
          "point_estimate": 60.65034362409446,
          "standard_error": 0.0280550422851673
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022697291342174405,
            "upper_bound": 0.12302902589494824
          },
          "point_estimate": 0.07003394404714464,
          "standard_error": 0.025283730143254073
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.63207468495321,
            "upper_bound": 60.70202733626107
          },
          "point_estimate": 60.66637991184773,
          "standard_error": 0.017821476591266504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03778166303259873,
            "upper_bound": 0.096536836779597
          },
          "point_estimate": 0.073392058685846,
          "standard_error": 0.015783763747156947
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.6037302621937,
            "upper_bound": 102.73145333892914
          },
          "point_estimate": 102.66979477760154,
          "standard_error": 0.0325964507380695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.62431573040712,
            "upper_bound": 102.74893762066436
          },
          "point_estimate": 102.66014531498664,
          "standard_error": 0.03115248342416051
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01774985318590628,
            "upper_bound": 0.1837156210074551
          },
          "point_estimate": 0.07068523303196346,
          "standard_error": 0.04219528950892995
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.6548016097845,
            "upper_bound": 102.73416237532918
          },
          "point_estimate": 102.68815856391367,
          "standard_error": 0.020255882654886013
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045572154271874665,
            "upper_bound": 0.14832269112590957
          },
          "point_estimate": 0.10850712005544792,
          "standard_error": 0.026590206668566544
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.2555018982324,
            "upper_bound": 254.8351878706668
          },
          "point_estimate": 254.53233334130425,
          "standard_error": 0.14779111650005858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.211228631808,
            "upper_bound": 254.85791503637213
          },
          "point_estimate": 254.43657160393084,
          "standard_error": 0.16019124710591667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1139215099144074,
            "upper_bound": 0.8263630659057747
          },
          "point_estimate": 0.35613108710810787,
          "standard_error": 0.18824724695831943
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.27403150421944,
            "upper_bound": 254.57477039078947
          },
          "point_estimate": 254.41287522136184,
          "standard_error": 0.07635011716817533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2263572544570163,
            "upper_bound": 0.6610930067513341
          },
          "point_estimate": 0.49301600615388,
          "standard_error": 0.11475410800972312
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 519.3497867452836,
            "upper_bound": 520.1573060390529
          },
          "point_estimate": 519.7332524285334,
          "standard_error": 0.20643985996136657
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 519.2390095041303,
            "upper_bound": 520.2224693519303
          },
          "point_estimate": 519.5891051786315,
          "standard_error": 0.21862352391776213
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.058513285850162994,
            "upper_bound": 1.1172296067921228
          },
          "point_estimate": 0.611457712266644,
          "standard_error": 0.336209808789121
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 519.4202376970092,
            "upper_bound": 519.9541746204477
          },
          "point_estimate": 519.6271092737995,
          "standard_error": 0.13488396271769823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3264087780808446,
            "upper_bound": 0.8841980320270475
          },
          "point_estimate": 0.6895887726969938,
          "standard_error": 0.14101276525400677
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.011684322040594,
            "upper_bound": 32.06693600763823
          },
          "point_estimate": 32.037470004559495,
          "standard_error": 0.014197471380004135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.003310786971824,
            "upper_bound": 32.067537030215846
          },
          "point_estimate": 32.02921379879881,
          "standard_error": 0.01270473874547853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005651253280111444,
            "upper_bound": 0.079431834830065
          },
          "point_estimate": 0.03129183108256917,
          "standard_error": 0.018505258237993292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.00798563156685,
            "upper_bound": 32.09251221943618
          },
          "point_estimate": 32.05244914784536,
          "standard_error": 0.021496081641937698
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01752956924689135,
            "upper_bound": 0.0630465177621987
          },
          "point_estimate": 0.04723665434162518,
          "standard_error": 0.011635088335921131
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32807.67661101083,
            "upper_bound": 32852.21067915162
          },
          "point_estimate": 32829.54793969543,
          "standard_error": 11.420659252352236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32790.27085840353,
            "upper_bound": 32866.49548736462
          },
          "point_estimate": 32825.775688176895,
          "standard_error": 17.233359614162666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.095888207358053,
            "upper_bound": 65.80635653327121
          },
          "point_estimate": 56.18740379113499,
          "standard_error": 16.594374885893366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32795.08489063675,
            "upper_bound": 32844.96916703296
          },
          "point_estimate": 32814.17076749965,
          "standard_error": 12.779184042400024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.69846876963712,
            "upper_bound": 45.58593417273084
          },
          "point_estimate": 38.07187519014507,
          "standard_error": 5.754425313124268
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.15689030647465,
            "upper_bound": 143.9814799256846
          },
          "point_estimate": 143.52599042540913,
          "standard_error": 0.2137799740561995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.0560865141064,
            "upper_bound": 143.88096660453806
          },
          "point_estimate": 143.27986105263636,
          "standard_error": 0.1807294073809932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04557911685817555,
            "upper_bound": 0.9908362731512164
          },
          "point_estimate": 0.33686048078331915,
          "standard_error": 0.23305364552807667
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.15331987117858,
            "upper_bound": 143.4887868943385
          },
          "point_estimate": 143.27967288235635,
          "standard_error": 0.08553885349111527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19198343293782344,
            "upper_bound": 0.9696133567628712
          },
          "point_estimate": 0.7127941407994276,
          "standard_error": 0.20783136166963667
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978498.0705353228,
            "upper_bound": 980136.0722646464
          },
          "point_estimate": 979293.7869830828,
          "standard_error": 417.26231478896767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978306.023731203,
            "upper_bound": 980056.5372807018
          },
          "point_estimate": 979193.403508772,
          "standard_error": 350.86654339794796
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.22150849846696,
            "upper_bound": 2447.982266408057
          },
          "point_estimate": 787.7385294359208,
          "standard_error": 681.3590986157066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978533.9923286524,
            "upper_bound": 980303.614359974
          },
          "point_estimate": 979506.2961722488,
          "standard_error": 453.9732696441472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 640.6512871519968,
            "upper_bound": 1846.6696495821673
          },
          "point_estimate": 1388.8793136234829,
          "standard_error": 313.95776738546806
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.4243166566423,
            "upper_bound": 1987.8884584715463
          },
          "point_estimate": 1986.6059120843736,
          "standard_error": 0.6329837350665108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.813182787468,
            "upper_bound": 1988.4758762097435
          },
          "point_estimate": 1985.7074129258024,
          "standard_error": 1.0734318708755637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2219484163303868,
            "upper_bound": 3.4015248474791115
          },
          "point_estimate": 1.9369125483128136,
          "standard_error": 0.8318458020224624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.0970140840711,
            "upper_bound": 1987.2598652745949
          },
          "point_estimate": 1985.8966006818375,
          "standard_error": 0.5599740366652441
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1983858274511836,
            "upper_bound": 2.5720680239585527
          },
          "point_estimate": 2.113731257356042,
          "standard_error": 0.3471326398085138
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.23568664554416,
            "upper_bound": 45.29144826572072
          },
          "point_estimate": 45.26189727527704,
          "standard_error": 0.014305454709098076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.22853664396889,
            "upper_bound": 45.29008471161077
          },
          "point_estimate": 45.24921254346715,
          "standard_error": 0.017059483185268558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069638447406969945,
            "upper_bound": 0.07789555780094923
          },
          "point_estimate": 0.05129887029354599,
          "standard_error": 0.0179809986299926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.22095205052745,
            "upper_bound": 45.27235647073421
          },
          "point_estimate": 45.24653491187436,
          "standard_error": 0.013573741777026836
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02247122202905621,
            "upper_bound": 0.06428885299568385
          },
          "point_estimate": 0.047587258764239034,
          "standard_error": 0.011544456611747076
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.92830297437948,
            "upper_bound": 49.007099627791824
          },
          "point_estimate": 48.96940924787209,
          "standard_error": 0.0202248625696147
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.9118007998215,
            "upper_bound": 49.02977349230998
          },
          "point_estimate": 48.98619427152919,
          "standard_error": 0.0349560550244735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007427729925210612,
            "upper_bound": 0.12252516621273345
          },
          "point_estimate": 0.06817386817685278,
          "standard_error": 0.03314975063591207
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.952877570844365,
            "upper_bound": 49.01974395266477
          },
          "point_estimate": 48.99186711624144,
          "standard_error": 0.01713822904971313
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03804708685276724,
            "upper_bound": 0.08272102295505002
          },
          "point_estimate": 0.0675238815082316,
          "standard_error": 0.011635429875903635
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.454043100413266,
            "upper_bound": 35.49848233481602
          },
          "point_estimate": 35.475237025634385,
          "standard_error": 0.011399991183190655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.446310236657354,
            "upper_bound": 35.50247984698372
          },
          "point_estimate": 35.47128932064824,
          "standard_error": 0.014892461950035496
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007428130806887308,
            "upper_bound": 0.06384932953338311
          },
          "point_estimate": 0.03161173323788161,
          "standard_error": 0.014225940254538829
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.44645909364015,
            "upper_bound": 35.49144083254891
          },
          "point_estimate": 35.46615347331576,
          "standard_error": 0.01152149000945839
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019253976290635244,
            "upper_bound": 0.04899408943931178
          },
          "point_estimate": 0.03795848048369773,
          "standard_error": 0.007724988702506629
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.089743700852075,
            "upper_bound": 40.20755971901706
          },
          "point_estimate": 40.142970186586034,
          "standard_error": 0.0303280398384676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.0580588366504,
            "upper_bound": 40.18639807088617
          },
          "point_estimate": 40.139075583829865,
          "standard_error": 0.03115724232285375
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007367105824786419,
            "upper_bound": 0.14326865863351054
          },
          "point_estimate": 0.09823172687342488,
          "standard_error": 0.03758849847297771
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.0867471419286,
            "upper_bound": 40.170361682983035
          },
          "point_estimate": 40.12733761491524,
          "standard_error": 0.02110383860104953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04226641043649898,
            "upper_bound": 0.1399204627803799
          },
          "point_estimate": 0.10124681235611294,
          "standard_error": 0.028394422553599967
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.20406283558703,
            "upper_bound": 48.32588023332908
          },
          "point_estimate": 48.25874913265157,
          "standard_error": 0.03150171362834829
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.18139078620205,
            "upper_bound": 48.32152416643712
          },
          "point_estimate": 48.21392926623413,
          "standard_error": 0.03991715005353718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006066946385187304,
            "upper_bound": 0.1654024176528416
          },
          "point_estimate": 0.07789810056035557,
          "standard_error": 0.03904193622903005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.20529866871459,
            "upper_bound": 48.279529957532326
          },
          "point_estimate": 48.231658626437216,
          "standard_error": 0.019153170161929862
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04551021890998764,
            "upper_bound": 0.146198708113701
          },
          "point_estimate": 0.10553294126180564,
          "standard_error": 0.029302851825824417
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.45775154499857,
            "upper_bound": 74.65757118478604
          },
          "point_estimate": 74.55174157618765,
          "standard_error": 0.051303749277404734
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.42182733673282,
            "upper_bound": 74.67457298187423
          },
          "point_estimate": 74.49955882882765,
          "standard_error": 0.06512467455858865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03038755324954725,
            "upper_bound": 0.28393569257983575
          },
          "point_estimate": 0.1286448622426956,
          "standard_error": 0.06537502347412649
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.48019180174228,
            "upper_bound": 74.69438237122138
          },
          "point_estimate": 74.56531080750118,
          "standard_error": 0.05527829684536146
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07583113867791387,
            "upper_bound": 0.2137017775306011
          },
          "point_estimate": 0.1707926546045317,
          "standard_error": 0.03407299530016879
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.63409555283963,
            "upper_bound": 88.80531767672369
          },
          "point_estimate": 88.70878977180992,
          "standard_error": 0.044454964874219294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.61431156329184,
            "upper_bound": 88.77758570438387
          },
          "point_estimate": 88.64658049897173,
          "standard_error": 0.0471887195789898
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017360802171032884,
            "upper_bound": 0.20403014555983412
          },
          "point_estimate": 0.08581226387171899,
          "standard_error": 0.04861893266554754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.60929531393992,
            "upper_bound": 88.67426235120342
          },
          "point_estimate": 88.63145114052772,
          "standard_error": 0.01666899232154369
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05203396293198242,
            "upper_bound": 0.2104780638509763
          },
          "point_estimate": 0.14812233068669844,
          "standard_error": 0.046617956360791926
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.7776701738105,
            "upper_bound": 62.855933226327835
          },
          "point_estimate": 62.81777506219133,
          "standard_error": 0.020017318324388123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.74764721987316,
            "upper_bound": 62.86997008052039
          },
          "point_estimate": 62.82792824341379,
          "standard_error": 0.029456159928772065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017715496170000233,
            "upper_bound": 0.114472013582105
          },
          "point_estimate": 0.07514901403770334,
          "standard_error": 0.026317569698910387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.76483689230237,
            "upper_bound": 62.83482534809369
          },
          "point_estimate": 62.803435624317096,
          "standard_error": 0.01764568372771627
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03758721470286809,
            "upper_bound": 0.08084642606839783
          },
          "point_estimate": 0.06658674558313141,
          "standard_error": 0.010727827231761713
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.6507937981738,
            "upper_bound": 105.71920681577508
          },
          "point_estimate": 105.6823012762172,
          "standard_error": 0.017599140929727936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.6400365854726,
            "upper_bound": 105.70521386746957
          },
          "point_estimate": 105.68335064384402,
          "standard_error": 0.018408283396750713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008475909781862054,
            "upper_bound": 0.08655902438721993
          },
          "point_estimate": 0.036017103506628435,
          "standard_error": 0.020273216006336425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.64089776691578,
            "upper_bound": 105.7048304162766
          },
          "point_estimate": 105.66568026309652,
          "standard_error": 0.016273595177728965
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023900803326104596,
            "upper_bound": 0.08253549865077271
          },
          "point_estimate": 0.058681526462961944,
          "standard_error": 0.016693799121944228
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.75437937179311,
            "upper_bound": 63.845451991808545
          },
          "point_estimate": 63.794277012834655,
          "standard_error": 0.023623398177824773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.742089384799876,
            "upper_bound": 63.83287122666559
          },
          "point_estimate": 63.75894016121185,
          "standard_error": 0.023080507270425925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007743273539164789,
            "upper_bound": 0.10332253489560445
          },
          "point_estimate": 0.03527412952272466,
          "standard_error": 0.02641803679661213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.746033087182845,
            "upper_bound": 63.82039245686248
          },
          "point_estimate": 63.78031285417345,
          "standard_error": 0.018970943036939216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02107013167009885,
            "upper_bound": 0.1090754351423818
          },
          "point_estimate": 0.07854005560648245,
          "standard_error": 0.02415766482014592
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.734969073326155,
            "upper_bound": 60.967147918715774
          },
          "point_estimate": 60.843742920133465,
          "standard_error": 0.06003232179850338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.694126404249495,
            "upper_bound": 61.008155627853824
          },
          "point_estimate": 60.75892425969701,
          "standard_error": 0.08924492070407634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014348562292429218,
            "upper_bound": 0.3289722887633566
          },
          "point_estimate": 0.1166065902325508,
          "standard_error": 0.08474997152618886
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.6968743917213,
            "upper_bound": 60.9118900310385
          },
          "point_estimate": 60.79064644989172,
          "standard_error": 0.05567607279839923
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07738165477031161,
            "upper_bound": 0.2413842418664405
          },
          "point_estimate": 0.20000369004240873,
          "standard_error": 0.03851752465044839
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.235150210765,
            "upper_bound": 105.44665946290914
          },
          "point_estimate": 105.34000391567976,
          "standard_error": 0.053733646135413024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.24768660229051,
            "upper_bound": 105.4302689206021
          },
          "point_estimate": 105.35557002206193,
          "standard_error": 0.04263320088866263
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006778877537029805,
            "upper_bound": 0.29982594847259336
          },
          "point_estimate": 0.11376028968721091,
          "standard_error": 0.07426305693431189
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.28830493116516,
            "upper_bound": 105.55080026498106
          },
          "point_estimate": 105.40314451790002,
          "standard_error": 0.07032836226569558
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07128723555970601,
            "upper_bound": 0.24609393037863983
          },
          "point_estimate": 0.17882004648659566,
          "standard_error": 0.04413804919066993
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1167952.8870337305,
            "upper_bound": 1170071.9816597225
          },
          "point_estimate": 1168905.544141865,
          "standard_error": 547.0824219237164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1167645.4381944444,
            "upper_bound": 1169976.05
          },
          "point_estimate": 1168242.7447916667,
          "standard_error": 550.8855789465961
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.8024379597372,
            "upper_bound": 2737.0545805741576
          },
          "point_estimate": 1347.6082164918307,
          "standard_error": 635.4318765391507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1167517.9723860575,
            "upper_bound": 1168449.5478778812
          },
          "point_estimate": 1167899.0090097403,
          "standard_error": 236.6249046249784
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641.8979291259558,
            "upper_bound": 2466.649198974521
          },
          "point_estimate": 1821.4351255585664,
          "standard_error": 495.29957916641104
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563817.9089236113,
            "upper_bound": 1565160.2284143516
          },
          "point_estimate": 1564517.912065145,
          "standard_error": 342.00773819790595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563980.263888889,
            "upper_bound": 1565353.095568783
          },
          "point_estimate": 1564578.8020833333,
          "standard_error": 313.917751465862
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.63311672209545,
            "upper_bound": 1915.5205387706385
          },
          "point_estimate": 721.9417613494926,
          "standard_error": 443.355344948602
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1564206.0356957426,
            "upper_bound": 1565495.1606989135
          },
          "point_estimate": 1564850.6590909092,
          "standard_error": 330.3973864409609
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 451.68856335508247,
            "upper_bound": 1559.2890928271045
          },
          "point_estimate": 1140.4974976851956,
          "standard_error": 283.58816227352537
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1414990.2864652015,
            "upper_bound": 1417174.478675214
          },
          "point_estimate": 1416028.7257173385,
          "standard_error": 558.744009742854
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1414754.7115384615,
            "upper_bound": 1416922.4615384615
          },
          "point_estimate": 1415836.0137362638,
          "standard_error": 502.9183633233065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.57109154580942,
            "upper_bound": 2964.928295759404
          },
          "point_estimate": 1606.9530464708805,
          "standard_error": 666.8123207459743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415138.987339777,
            "upper_bound": 1417056.7315065
          },
          "point_estimate": 1416034.5991008992,
          "standard_error": 484.75164260103895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 757.1619923949207,
            "upper_bound": 2596.6602390555304
          },
          "point_estimate": 1857.914280945648,
          "standard_error": 488.02977356073075
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388759.9325139311,
            "upper_bound": 389254.6870567376
          },
          "point_estimate": 389008.8083763931,
          "standard_error": 126.61827199059697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388556.0904255319,
            "upper_bound": 389284.83351063833
          },
          "point_estimate": 389161.2052304965,
          "standard_error": 216.3476791057856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.49084011417595,
            "upper_bound": 709.6738161241641
          },
          "point_estimate": 451.3189414555903,
          "standard_error": 201.27420797553543
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388706.4317728153,
            "upper_bound": 389216.9457557181
          },
          "point_estimate": 388996.8852998066,
          "standard_error": 131.6985363958077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.92094024117944,
            "upper_bound": 515.1108268725305
          },
          "point_estimate": 421.9161496972767,
          "standard_error": 65.3195134269034
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556191.4140650703,
            "upper_bound": 557288.9522919673
          },
          "point_estimate": 556745.4863017076,
          "standard_error": 280.7267466680583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556149.6732954546,
            "upper_bound": 557461.9040404041
          },
          "point_estimate": 556748.3221861472,
          "standard_error": 301.5450808992549
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162.52328302368036,
            "upper_bound": 1562.5592858953987
          },
          "point_estimate": 600.1360024767769,
          "standard_error": 383.31709797161193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556326.6252382481,
            "upper_bound": 556977.9241029931
          },
          "point_estimate": 556628.7415584415,
          "standard_error": 166.13002987220614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 460.0608211742988,
            "upper_bound": 1233.3226368073306
          },
          "point_estimate": 936.0361428926836,
          "standard_error": 196.34838403362903
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 421987.4837944719,
            "upper_bound": 422946.0569168719
          },
          "point_estimate": 422439.8463368911,
          "standard_error": 246.1299570186426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 421861.65900383145,
            "upper_bound": 423148.5689655172
          },
          "point_estimate": 422094.3058908046,
          "standard_error": 338.3819321609883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.721812596051077,
            "upper_bound": 1297.798999373245
          },
          "point_estimate": 624.9167003997688,
          "standard_error": 340.0984125577632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422005.1379310345,
            "upper_bound": 422605.3225129385
          },
          "point_estimate": 422252.2738319152,
          "standard_error": 154.71417935210636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403.47433432561706,
            "upper_bound": 1032.4385327694908
          },
          "point_estimate": 818.5761644624029,
          "standard_error": 159.3526476191389
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 472567.2558889919,
            "upper_bound": 474160.71273115085
          },
          "point_estimate": 473265.72529736144,
          "standard_error": 413.5554796935263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 472157.0,
            "upper_bound": 473690.7353535354
          },
          "point_estimate": 472933.7819805195,
          "standard_error": 344.8336861514023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.57352241032966,
            "upper_bound": 1688.6672500201853
          },
          "point_estimate": 1117.646116521454,
          "standard_error": 483.26519999034554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 472670.6687413741,
            "upper_bound": 473265.50768012495
          },
          "point_estimate": 472946.3147917018,
          "standard_error": 147.8478038337086
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452.2263497191263,
            "upper_bound": 1956.8513887988815
          },
          "point_estimate": 1377.4662031736293,
          "standard_error": 433.6838800846321
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 995913.3275536518,
            "upper_bound": 998646.8351481396
          },
          "point_estimate": 997176.3818715144,
          "standard_error": 703.299588453693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 995172.2027027028,
            "upper_bound": 999183.3862612612
          },
          "point_estimate": 996850.990427928,
          "standard_error": 780.3585382501343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267.0099108001515,
            "upper_bound": 3327.2712299832606
          },
          "point_estimate": 1917.6008721494816,
          "standard_error": 876.8681219616944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 995617.3336268442,
            "upper_bound": 997161.747551368
          },
          "point_estimate": 996250.8737802738,
          "standard_error": 397.9024546337011
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 867.7584136762952,
            "upper_bound": 2939.614319169924
          },
          "point_estimate": 2339.4672564902708,
          "standard_error": 545.9360672569708
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295743.80624729,
            "upper_bound": 296175.39459015673
          },
          "point_estimate": 295949.2029984514,
          "standard_error": 110.95227500922546
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295672.24322493223,
            "upper_bound": 296245.8427700348
          },
          "point_estimate": 295879.2386178862,
          "standard_error": 129.45076164450842
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.27996328835803,
            "upper_bound": 661.6626716678223
          },
          "point_estimate": 305.54818481931744,
          "standard_error": 142.95145178404252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295603.90382026404,
            "upper_bound": 296125.79542325775
          },
          "point_estimate": 295812.7004751346,
          "standard_error": 134.66083154098754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.04817845868882,
            "upper_bound": 465.5484908596532
          },
          "point_estimate": 369.8447336769318,
          "standard_error": 75.27491079340254
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802679.150354943,
            "upper_bound": 803578.9756521739
          },
          "point_estimate": 803088.7337767426,
          "standard_error": 230.9150830807976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802567.7445652173,
            "upper_bound": 803535.7639751553
          },
          "point_estimate": 802821.0507246377,
          "standard_error": 273.571252425887
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.27005543400822,
            "upper_bound": 1173.3609287340312
          },
          "point_estimate": 723.1075182492483,
          "standard_error": 280.23257466858104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 802803.659661106,
            "upper_bound": 804116.7793839655
          },
          "point_estimate": 803452.8810841333,
          "standard_error": 342.598486865559
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 344.82633912304726,
            "upper_bound": 1054.7978589781392
          },
          "point_estimate": 768.870525586743,
          "standard_error": 205.14122439536908
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393385.5211503669,
            "upper_bound": 393986.69768074766
          },
          "point_estimate": 393653.80549752514,
          "standard_error": 155.44208896193672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393358.3951015532,
            "upper_bound": 393824.8682795699
          },
          "point_estimate": 393525.4928315412,
          "standard_error": 123.44610114576504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.54550625367355,
            "upper_bound": 688.5362553745944
          },
          "point_estimate": 332.210887829676,
          "standard_error": 158.1187260675941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393166.94262905733,
            "upper_bound": 393584.3653511143
          },
          "point_estimate": 393336.89479123027,
          "standard_error": 109.14228847971756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.64441724376985,
            "upper_bound": 738.9895479786859
          },
          "point_estimate": 518.7018397517888,
          "standard_error": 162.48555484435997
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165286.6149705492,
            "upper_bound": 165686.9774287879
          },
          "point_estimate": 165489.78911489897,
          "standard_error": 102.6008564009575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165185.7897727273,
            "upper_bound": 165806.87545454546
          },
          "point_estimate": 165535.29954545456,
          "standard_error": 145.84168164858852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.30480967868223,
            "upper_bound": 591.3656623647971
          },
          "point_estimate": 429.8195475055396,
          "standard_error": 132.51236282439757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165455.8727057303,
            "upper_bound": 165835.74718693286
          },
          "point_estimate": 165706.7861983471,
          "standard_error": 96.95838718620531
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.9019526304044,
            "upper_bound": 417.9758925275552
          },
          "point_estimate": 342.7543242946797,
          "standard_error": 55.08995417819139
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231124.77913600064,
            "upper_bound": 231443.89849681035
          },
          "point_estimate": 231287.03392656217,
          "standard_error": 81.7345783445437
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231022.5379746835,
            "upper_bound": 231499.85748945147
          },
          "point_estimate": 231304.70437763713,
          "standard_error": 115.69447292167388
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.051942867945385,
            "upper_bound": 457.7558697213312
          },
          "point_estimate": 342.36249159063516,
          "standard_error": 105.36613114382558
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231045.59081490157,
            "upper_bound": 231476.56889179297
          },
          "point_estimate": 231300.12079566004,
          "standard_error": 110.57269427409548
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.61138723682666,
            "upper_bound": 334.5255910710955
          },
          "point_estimate": 271.8630240124588,
          "standard_error": 44.92206733831842
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172562.418118695,
            "upper_bound": 172889.7784192479
          },
          "point_estimate": 172717.71249661475,
          "standard_error": 83.31515478596593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172591.214849921,
            "upper_bound": 172830.5406793049
          },
          "point_estimate": 172667.46119668247,
          "standard_error": 79.54320664306988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.13664011056852,
            "upper_bound": 421.00686456354214
          },
          "point_estimate": 183.0312997837148,
          "standard_error": 98.94811014130968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172625.16193244743,
            "upper_bound": 172792.78057581474
          },
          "point_estimate": 172710.0276851111,
          "standard_error": 43.7053416495605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.05125172691258,
            "upper_bound": 394.73479942433255
          },
          "point_estimate": 278.2952652473964,
          "standard_error": 79.84547141129195
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648962.6775586013,
            "upper_bound": 650212.5379990776
          },
          "point_estimate": 649620.0804420774,
          "standard_error": 318.8386168494157
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648791.8201754387,
            "upper_bound": 650626.6591478697
          },
          "point_estimate": 649724.115131579,
          "standard_error": 373.90355127882106
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.9954365622401,
            "upper_bound": 1896.9288651532772
          },
          "point_estimate": 975.0177733332354,
          "standard_error": 486.2838599093273
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649536.2184986289,
            "upper_bound": 650297.5091021621
          },
          "point_estimate": 649878.0835725678,
          "standard_error": 191.96223615831988
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 533.7537270502761,
            "upper_bound": 1360.5487502756068
          },
          "point_estimate": 1066.171292542626,
          "standard_error": 215.1158663259571
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1406.2620080647391,
            "upper_bound": 1407.8619774144402
          },
          "point_estimate": 1407.059139018504,
          "standard_error": 0.4101310659393058
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1405.9349161145335,
            "upper_bound": 1408.048380686693
          },
          "point_estimate": 1407.0661469836105,
          "standard_error": 0.636294561909178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.28866479087674335,
            "upper_bound": 2.2366412784365135
          },
          "point_estimate": 1.352774101288685,
          "standard_error": 0.534558740242881
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1406.5411478739518,
            "upper_bound": 1407.8419246705264
          },
          "point_estimate": 1407.2838872778195,
          "standard_error": 0.33699228267161724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.805538792590611,
            "upper_bound": 1.7162819450028226
          },
          "point_estimate": 1.3650834626051218,
          "standard_error": 0.2334676436999824
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1326188.9340873014,
            "upper_bound": 1328624.7030431544
          },
          "point_estimate": 1327416.3772420634,
          "standard_error": 623.5532408120645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1325254.650793651,
            "upper_bound": 1329167.120833333
          },
          "point_estimate": 1327883.3080357143,
          "standard_error": 1160.2793375177894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 460.9165043171703,
            "upper_bound": 3370.8065005729104
          },
          "point_estimate": 3124.0690653700644,
          "standard_error": 833.4959751259765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1325979.1529743697,
            "upper_bound": 1328406.425118688
          },
          "point_estimate": 1327223.7803339518,
          "standard_error": 626.4799514545621
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1372.1862468184572,
            "upper_bound": 2406.073441691008
          },
          "point_estimate": 2074.923371367333,
          "standard_error": 268.91229357738706
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1517148.2835582008,
            "upper_bound": 1518856.380520999
          },
          "point_estimate": 1517994.655395172,
          "standard_error": 434.67072988891215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1516896.508101852,
            "upper_bound": 1518951.5923611112
          },
          "point_estimate": 1518055.5859375,
          "standard_error": 662.2642761484038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.62170732836148,
            "upper_bound": 2452.3809714616264
          },
          "point_estimate": 1290.9842229138585,
          "standard_error": 646.428661013681
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1516746.3825136612,
            "upper_bound": 1519682.7463274463
          },
          "point_estimate": 1518273.2391774892,
          "standard_error": 771.3423706993464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 817.8192077185432,
            "upper_bound": 1865.5321573353035
          },
          "point_estimate": 1451.1019935154163,
          "standard_error": 267.9914851569772
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425294.3338214285,
            "upper_bound": 1426804.4225183148
          },
          "point_estimate": 1426053.1830845545,
          "standard_error": 386.0709116843258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1425119.8296703296,
            "upper_bound": 1426982.9302884615
          },
          "point_estimate": 1426276.6805555555,
          "standard_error": 445.34887289221354
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.40086280033412,
            "upper_bound": 2164.4196878560715
          },
          "point_estimate": 1407.580912830961,
          "standard_error": 580.9821217712008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1424506.4616489266,
            "upper_bound": 1426349.220641784
          },
          "point_estimate": 1425413.5787212788,
          "standard_error": 501.0278285930433
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667.345602005561,
            "upper_bound": 1679.1616731028596
          },
          "point_estimate": 1292.911429074969,
          "standard_error": 256.42850683354436
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762480.0428207672,
            "upper_bound": 763319.0174528769
          },
          "point_estimate": 762872.824180721,
          "standard_error": 214.1343646976125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762521.8786168981,
            "upper_bound": 763141.3354166667
          },
          "point_estimate": 762776.9704861111,
          "standard_error": 191.95264638138408
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.60142877139151,
            "upper_bound": 1064.8487498451393
          },
          "point_estimate": 382.26819764042784,
          "standard_error": 247.46981819831055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762551.9521063961,
            "upper_bound": 763143.8805379746
          },
          "point_estimate": 762852.5688852813,
          "standard_error": 157.74742677408886
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252.43675770031703,
            "upper_bound": 1014.5717919873284
          },
          "point_estimate": 714.9462265501814,
          "standard_error": 204.16745313828957
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266029.76207314484,
            "upper_bound": 266570.30169012863
          },
          "point_estimate": 266299.47570472717,
          "standard_error": 138.86706110743407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265908.0321167883,
            "upper_bound": 266726.40206812654
          },
          "point_estimate": 266319.11496350367,
          "standard_error": 258.8735280271134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.29629978371172,
            "upper_bound": 765.4695678883359
          },
          "point_estimate": 531.3818670624145,
          "standard_error": 171.14431337828168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265932.5849364163,
            "upper_bound": 266500.9341394451
          },
          "point_estimate": 266157.30391506304,
          "standard_error": 143.9845103391314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302.20723948196314,
            "upper_bound": 543.1529973441709
          },
          "point_estimate": 461.423596929534,
          "standard_error": 61.5962081358993
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476361.47622464434,
            "upper_bound": 477889.41475231905
          },
          "point_estimate": 477081.6867285096,
          "standard_error": 391.3286757157533
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476085.45562770567,
            "upper_bound": 477945.66623376624
          },
          "point_estimate": 476964.97217068647,
          "standard_error": 467.52612698941294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 324.87520059593953,
            "upper_bound": 2172.1657484493444
          },
          "point_estimate": 1061.1810168745658,
          "standard_error": 469.452870102224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476565.2552782413,
            "upper_bound": 477674.2859876965
          },
          "point_estimate": 477122.1352335976,
          "standard_error": 280.7932806809635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632.9264226321001,
            "upper_bound": 1714.3105912033357
          },
          "point_estimate": 1303.5297623466831,
          "standard_error": 286.8940258228926
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245376.69845431147,
            "upper_bound": 245759.01975439728
          },
          "point_estimate": 245576.30706188333,
          "standard_error": 98.25806670696971
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245347.85923423423,
            "upper_bound": 245824.76182432432
          },
          "point_estimate": 245578.07466216217,
          "standard_error": 98.46922264832172
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.948512138951724,
            "upper_bound": 550.9536844753806
          },
          "point_estimate": 262.14295916359634,
          "standard_error": 148.89237185974142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245565.3757837838,
            "upper_bound": 245857.77426620168
          },
          "point_estimate": 245725.4240961741,
          "standard_error": 74.66476972413281
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.93880094678735,
            "upper_bound": 430.063639206382
          },
          "point_estimate": 328.0392112884479,
          "standard_error": 72.07737670137836
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 693287.5055315963,
            "upper_bound": 694490.0124317162
          },
          "point_estimate": 693842.3575351902,
          "standard_error": 308.7179619293192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 693336.0523584905,
            "upper_bound": 694472.0424528302
          },
          "point_estimate": 693476.7853773584,
          "standard_error": 276.10635916530026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.6377824971922,
            "upper_bound": 1603.6570588879626
          },
          "point_estimate": 449.1007453288177,
          "standard_error": 419.7851164658574
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 693056.2321484416,
            "upper_bound": 693732.9807747285
          },
          "point_estimate": 693380.2427836314,
          "standard_error": 168.99871942257096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 320.0843721877056,
            "upper_bound": 1384.2938560054426
          },
          "point_estimate": 1030.8510484099418,
          "standard_error": 272.3991836920659
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358.88263888559754,
            "upper_bound": 360.1421548163288
          },
          "point_estimate": 359.46000369174527,
          "standard_error": 0.32396935669428256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358.691021653485,
            "upper_bound": 360.2741429445975
          },
          "point_estimate": 359.11071725180557,
          "standard_error": 0.3683494046289855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1937497751378073,
            "upper_bound": 1.7142645455306258
          },
          "point_estimate": 0.6816820939369621,
          "standard_error": 0.3762686705186492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358.8071880821744,
            "upper_bound": 359.3713335333344
          },
          "point_estimate": 359.0400108536072,
          "standard_error": 0.14473737618669405
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.38310212538309807,
            "upper_bound": 1.38274894494861
          },
          "point_estimate": 1.0794004106450767,
          "standard_error": 0.25960898474272476
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.7325405852886,
            "upper_bound": 170.8898754695025
          },
          "point_estimate": 170.80294013661995,
          "standard_error": 0.04040455595200863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.72402270453574,
            "upper_bound": 170.83218387902815
          },
          "point_estimate": 170.78989599625504,
          "standard_error": 0.02252384237950307
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007981839705355032,
            "upper_bound": 0.17744896506730523
          },
          "point_estimate": 0.04241889088605526,
          "standard_error": 0.0463113460323863
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.74942386626154,
            "upper_bound": 170.83903932809582
          },
          "point_estimate": 170.78813373898598,
          "standard_error": 0.022279672120750035
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03600114908698447,
            "upper_bound": 0.19370632439514196
          },
          "point_estimate": 0.13447115299170312,
          "standard_error": 0.043695932510596905
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.356121726996196,
            "upper_bound": 28.03795308758076
          },
          "point_estimate": 27.709466631058575,
          "standard_error": 0.1743877783207039
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.29434458723253,
            "upper_bound": 28.203782308088876
          },
          "point_estimate": 27.78266724365761,
          "standard_error": 0.21272130914040027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09919626890433468,
            "upper_bound": 0.9670217570481604
          },
          "point_estimate": 0.674166170501967,
          "standard_error": 0.2380656761668689
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.53364418615128,
            "upper_bound": 28.0634955019134
          },
          "point_estimate": 27.766403154288508,
          "standard_error": 0.13238322903147448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3114660171373719,
            "upper_bound": 0.742677023510678
          },
          "point_estimate": 0.5808059293186396,
          "standard_error": 0.11166597530391152
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.050994386994542,
            "upper_bound": 18.07980693568868
          },
          "point_estimate": 18.06620305969151,
          "standard_error": 0.007398775810283755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.050956634653765,
            "upper_bound": 18.08911410734764
          },
          "point_estimate": 18.0668548761766,
          "standard_error": 0.011499398928617288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016188956442009402,
            "upper_bound": 0.04626310581572645
          },
          "point_estimate": 0.0278250701779384,
          "standard_error": 0.010761204592905915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.048762921067382,
            "upper_bound": 18.07965426743688
          },
          "point_estimate": 18.06272674832491,
          "standard_error": 0.008142861803178093
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01357891497364818,
            "upper_bound": 0.03210748486658424
          },
          "point_estimate": 0.024613786565739093,
          "standard_error": 0.005203864128703292
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.161814493727217,
            "upper_bound": 25.193004537851465
          },
          "point_estimate": 25.17665831242176,
          "standard_error": 0.008006987982681648
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.152904219000007,
            "upper_bound": 25.195722184488908
          },
          "point_estimate": 25.17356178951457,
          "standard_error": 0.01312302002439569
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004772896836849563,
            "upper_bound": 0.0498828923595817
          },
          "point_estimate": 0.03166384259673406,
          "standard_error": 0.011201905646918066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17525424298466,
            "upper_bound": 25.21252818732265
          },
          "point_estimate": 25.196403339381725,
          "standard_error": 0.00983431535227054
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015265281881138343,
            "upper_bound": 0.03415861049024657
          },
          "point_estimate": 0.026697419341298576,
          "standard_error": 0.005193248166960825
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288912.8250837743,
            "upper_bound": 289550.6667209468
          },
          "point_estimate": 289197.4354465861,
          "standard_error": 164.58563004681824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288731.98333333334,
            "upper_bound": 289445.3432539683
          },
          "point_estimate": 289103.10878054926,
          "standard_error": 159.49178557362484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.999575583699226,
            "upper_bound": 733.290416981496
          },
          "point_estimate": 429.8522107019137,
          "standard_error": 184.1181257618497
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288791.87755448767,
            "upper_bound": 289167.9109977324
          },
          "point_estimate": 288952.5935271078,
          "standard_error": 96.45744216639844
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215.01004950520672,
            "upper_bound": 770.2780338539025
          },
          "point_estimate": 548.2728543669373,
          "standard_error": 162.97231292240383
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.025105786566012,
            "upper_bound": 16.07774649044613
          },
          "point_estimate": 16.050661361812946,
          "standard_error": 0.013493367986473312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.005829730918798,
            "upper_bound": 16.101126154831633
          },
          "point_estimate": 16.04226646692014,
          "standard_error": 0.020607354605935727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005631182574899499,
            "upper_bound": 0.07362598319470667
          },
          "point_estimate": 0.05700384923881338,
          "standard_error": 0.01926634507620148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.017115838582395,
            "upper_bound": 16.073952153682303
          },
          "point_estimate": 16.04332749629103,
          "standard_error": 0.014885546559422126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023426969032076493,
            "upper_bound": 0.05402094001016593
          },
          "point_estimate": 0.04495284942371492,
          "standard_error": 0.007272481616796231
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.251414852721933,
            "upper_bound": 22.284937102550625
          },
          "point_estimate": 22.26762447047603,
          "standard_error": 0.008607942658445394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.247998607727112,
            "upper_bound": 22.295268273990153
          },
          "point_estimate": 22.257786256153153,
          "standard_error": 0.011331227981567264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015422020678956493,
            "upper_bound": 0.04811527711421143
          },
          "point_estimate": 0.020773665365594683,
          "standard_error": 0.013020529609765234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.2561646872778,
            "upper_bound": 22.29072455609372
          },
          "point_estimate": 22.269469312473262,
          "standard_error": 0.009243763915787849
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014148466578278404,
            "upper_bound": 0.035155089475845516
          },
          "point_estimate": 0.028687249235988248,
          "standard_error": 0.005110121325403147
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128213.24543137995,
            "upper_bound": 128404.15965009504
          },
          "point_estimate": 128308.54097697296,
          "standard_error": 49.08882181401262
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128158.96294014085,
            "upper_bound": 128435.44014084506
          },
          "point_estimate": 128330.36189358371,
          "standard_error": 78.12432005906024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.721330743293365,
            "upper_bound": 274.8093453852437
          },
          "point_estimate": 199.7912803379798,
          "standard_error": 57.705559189669245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128206.72913234796,
            "upper_bound": 128404.8195666892
          },
          "point_estimate": 128295.15235046644,
          "standard_error": 50.41217087290504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.0375267369348,
            "upper_bound": 199.83396279328224
          },
          "point_estimate": 163.86914879513202,
          "standard_error": 25.219604448174664
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.96864664844865,
            "upper_bound": 23.99910278339904
          },
          "point_estimate": 23.98399604808842,
          "standard_error": 0.007782197406275234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.96353545733219,
            "upper_bound": 24.001498380011537
          },
          "point_estimate": 23.985713335685126,
          "standard_error": 0.009231881530582178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004086792902848033,
            "upper_bound": 0.04241578709797304
          },
          "point_estimate": 0.020652059025399315,
          "standard_error": 0.01036318382003041
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.96361184407941,
            "upper_bound": 23.99394847303799
          },
          "point_estimate": 23.979169230558572,
          "standard_error": 0.0078082829864856125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012865625052619008,
            "upper_bound": 0.03366755271569215
          },
          "point_estimate": 0.025884032715329124,
          "standard_error": 0.005195440515802921
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.780762856647623,
            "upper_bound": 20.825930083321975
          },
          "point_estimate": 20.80163682290358,
          "standard_error": 0.01165399826983632
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.77763183624662,
            "upper_bound": 20.832077377288503
          },
          "point_estimate": 20.7852562331918,
          "standard_error": 0.013532421190543188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007468302091459527,
            "upper_bound": 0.06162107164621903
          },
          "point_estimate": 0.0191794238690887,
          "standard_error": 0.015728844031010082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.77991177176874,
            "upper_bound": 20.81152230729012
          },
          "point_estimate": 20.792640806556385,
          "standard_error": 0.008075158156751283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013871627426645598,
            "upper_bound": 0.048810452865396585
          },
          "point_estimate": 0.038847465855265745,
          "standard_error": 0.008692592215743956
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.359795570286046,
            "upper_bound": 40.41038125790862
          },
          "point_estimate": 40.38567642182947,
          "standard_error": 0.012937341653086954
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.349780391407606,
            "upper_bound": 40.41794836476576
          },
          "point_estimate": 40.39202740196005,
          "standard_error": 0.022921476141677947
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024744639311525805,
            "upper_bound": 0.07387091421309151
          },
          "point_estimate": 0.04091865782828694,
          "standard_error": 0.02017875466265528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.354304498451434,
            "upper_bound": 40.39897573488376
          },
          "point_estimate": 40.37109216688239,
          "standard_error": 0.011311525404987392
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026491765002812837,
            "upper_bound": 0.05275189857110973
          },
          "point_estimate": 0.043260065959162104,
          "standard_error": 0.006788953662484445
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.38529723708614,
            "upper_bound": 73.48209430295091
          },
          "point_estimate": 73.42923136805894,
          "standard_error": 0.024998312189471964
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.36761625907477,
            "upper_bound": 73.47660018557029
          },
          "point_estimate": 73.41523236077813,
          "standard_error": 0.02833554849377572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0029421823343838624,
            "upper_bound": 0.1221460717535142
          },
          "point_estimate": 0.07089937240121967,
          "standard_error": 0.03109061525158148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.38527317258193,
            "upper_bound": 73.47904163905089
          },
          "point_estimate": 73.4400311312714,
          "standard_error": 0.024050477372090463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.033272520241723814,
            "upper_bound": 0.11380625106299158
          },
          "point_estimate": 0.08323963206627041,
          "standard_error": 0.02211897729034633
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.007224379554597,
            "upper_bound": 11.031537530232033
          },
          "point_estimate": 11.018697703274777,
          "standard_error": 0.006268429918066439
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.00138954722421,
            "upper_bound": 11.039416817303572
          },
          "point_estimate": 11.0114491234921,
          "standard_error": 0.009730477555099657
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001992725965783236,
            "upper_bound": 0.03408838579899547
          },
          "point_estimate": 0.015910690492880953,
          "standard_error": 0.008795527624692758
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.002919794785951,
            "upper_bound": 11.027015042731056
          },
          "point_estimate": 11.01130845709894,
          "standard_error": 0.006277257396164226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008403777514355237,
            "upper_bound": 0.025519900153866508
          },
          "point_estimate": 0.02090888746193546,
          "standard_error": 0.003929619760427095
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32303.592434116406,
            "upper_bound": 32407.205206772487
          },
          "point_estimate": 32348.258360352735,
          "standard_error": 26.98242897575481
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32294.14161904762,
            "upper_bound": 32367.867822222222
          },
          "point_estimate": 32334.985308641975,
          "standard_error": 21.32349841365876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.333839647768752,
            "upper_bound": 103.26517478889907
          },
          "point_estimate": 51.32126336442083,
          "standard_error": 21.976995548484236
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32302.180676591972,
            "upper_bound": 32344.24089222586
          },
          "point_estimate": 32324.186876767675,
          "standard_error": 10.734807332621047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.95869975163361,
            "upper_bound": 131.5452048148748
          },
          "point_estimate": 90.11057441591647,
          "standard_error": 31.370756196375854
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.278163544013665,
            "upper_bound": 28.794154653239687
          },
          "point_estimate": 28.531452678996896,
          "standard_error": 0.13144670309009404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.210063553509737,
            "upper_bound": 28.748341518562448
          },
          "point_estimate": 28.63118980590221,
          "standard_error": 0.16914716018731268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04186926046630906,
            "upper_bound": 0.7524680375720458
          },
          "point_estimate": 0.3998190112670896,
          "standard_error": 0.19567507761634387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.29725890070277,
            "upper_bound": 28.63672446287768
          },
          "point_estimate": 28.4676648053872,
          "standard_error": 0.08708625546927912
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22274282601231335,
            "upper_bound": 0.5812423543331764
          },
          "point_estimate": 0.4367824322742908,
          "standard_error": 0.09515241731003718
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978868.4944337194,
            "upper_bound": 981108.9451322393
          },
          "point_estimate": 979894.5298166024,
          "standard_error": 574.9389212040378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978589.3068693696,
            "upper_bound": 981431.281853282
          },
          "point_estimate": 979008.0675675676,
          "standard_error": 693.9109844248493
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.1714771148212,
            "upper_bound": 2827.1228072408994
          },
          "point_estimate": 862.5530995514531,
          "standard_error": 673.5289115626516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978905.9605164244,
            "upper_bound": 980626.4269102262
          },
          "point_estimate": 979735.7742365742,
          "standard_error": 440.83567375212584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 517.5842602238225,
            "upper_bound": 2366.2053898262993
          },
          "point_estimate": 1911.7817861082376,
          "standard_error": 462.2747953387697
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.978931201252,
            "upper_bound": 1962.0326697583123
          },
          "point_estimate": 1959.89080224155,
          "standard_error": 1.0408555670697028
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.1610414100985,
            "upper_bound": 1961.9533674568968
          },
          "point_estimate": 1959.309710997366,
          "standard_error": 1.2800531581743195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6369703568666676,
            "upper_bound": 5.876423101850635
          },
          "point_estimate": 3.335986848730377,
          "standard_error": 1.2873582015867826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1956.5366399691893,
            "upper_bound": 1960.1290236291409
          },
          "point_estimate": 1957.948498376623,
          "standard_error": 0.9201812682226228
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6958614357256314,
            "upper_bound": 4.676071508639345
          },
          "point_estimate": 3.483430679455718,
          "standard_error": 0.821251401742482
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.837999320487084,
            "upper_bound": 7.866730116763392
          },
          "point_estimate": 7.8522633250522365,
          "standard_error": 0.007353137975681789
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.830581152120096,
            "upper_bound": 7.876080454156343
          },
          "point_estimate": 7.847951704810762,
          "standard_error": 0.011959729380353943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007800559467372752,
            "upper_bound": 0.040219365104870126
          },
          "point_estimate": 0.025761293797929283,
          "standard_error": 0.009023423163333242
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.837382655943979,
            "upper_bound": 7.874624462703545
          },
          "point_estimate": 7.856612744900196,
          "standard_error": 0.009725472549728458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015047230630056856,
            "upper_bound": 0.0296273752207954
          },
          "point_estimate": 0.0244460682029108,
          "standard_error": 0.0037201857013544433
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.090540031421841,
            "upper_bound": 7.110397269130948
          },
          "point_estimate": 7.098065418826022,
          "standard_error": 0.005577143365376449
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.088750891046747,
            "upper_bound": 7.097535644689292
          },
          "point_estimate": 7.092060041521021,
          "standard_error": 0.0023747775308058027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005327527355125374,
            "upper_bound": 0.009578639422735447
          },
          "point_estimate": 0.004987103291231194,
          "standard_error": 0.0027921882844844314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.0900378211191875,
            "upper_bound": 7.095571201989424
          },
          "point_estimate": 7.092717472315762,
          "standard_error": 0.0014085248917165797
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022453412138169937,
            "upper_bound": 0.028504570209363422
          },
          "point_estimate": 0.01861979767302747,
          "standard_error": 0.0088690758134365
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.340021577492262,
            "upper_bound": 8.372993660557489
          },
          "point_estimate": 8.357178775507709,
          "standard_error": 0.00838441580541461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.34678975166929,
            "upper_bound": 8.372228833546679
          },
          "point_estimate": 8.356429482514258,
          "standard_error": 0.007395386533112906
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004445206165237582,
            "upper_bound": 0.044204287480106914
          },
          "point_estimate": 0.018988735562550037,
          "standard_error": 0.009318615358761663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.355244347315416,
            "upper_bound": 8.38452288518763
          },
          "point_estimate": 8.370534129885751,
          "standard_error": 0.007417074323066869
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01014171268097219,
            "upper_bound": 0.03948828735184312
          },
          "point_estimate": 0.02795413295200381,
          "standard_error": 0.007683092946910893
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.502874027955805,
            "upper_bound": 22.529747084567127
          },
          "point_estimate": 22.516327074066066,
          "standard_error": 0.0068740532154374975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.500323122853622,
            "upper_bound": 22.541837080632895
          },
          "point_estimate": 22.51172956308121,
          "standard_error": 0.011130525158800217
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004283051480035469,
            "upper_bound": 0.040527987229464296
          },
          "point_estimate": 0.025185998537657617,
          "standard_error": 0.009412133077297444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.502577109130428,
            "upper_bound": 22.538160528277565
          },
          "point_estimate": 22.524311344164172,
          "standard_error": 0.00907717945160288
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014119987796218814,
            "upper_bound": 0.02797928206051453
          },
          "point_estimate": 0.02284296041534804,
          "standard_error": 0.003568468022444995
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.696403632973537,
            "upper_bound": 13.739967169582972
          },
          "point_estimate": 13.71524746203806,
          "standard_error": 0.011334453671294997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.692048597322389,
            "upper_bound": 13.728815032295676
          },
          "point_estimate": 13.702214879339936,
          "standard_error": 0.009897675531456703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002285762300070494,
            "upper_bound": 0.04684527283657207
          },
          "point_estimate": 0.016213376769024884,
          "standard_error": 0.011621051246799562
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.693778844526966,
            "upper_bound": 13.71817492710596
          },
          "point_estimate": 13.702908318767436,
          "standard_error": 0.006250829045484237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010424199412047294,
            "upper_bound": 0.05319785805123855
          },
          "point_estimate": 0.03762124333197759,
          "standard_error": 0.012062493541357428
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.334392697819837,
            "upper_bound": 18.358988894302534
          },
          "point_estimate": 18.346424183786983,
          "standard_error": 0.0062984312922271
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.329011355202237,
            "upper_bound": 18.36237127152347
          },
          "point_estimate": 18.34806976077835,
          "standard_error": 0.006983225536976305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001089741933807698,
            "upper_bound": 0.03647537979462466
          },
          "point_estimate": 0.020413315537143036,
          "standard_error": 0.010312807946308716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.33613705260075,
            "upper_bound": 18.365629503294993
          },
          "point_estimate": 18.349379983388133,
          "standard_error": 0.007556197717378815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010768207653640557,
            "upper_bound": 0.026825412491723953
          },
          "point_estimate": 0.020950419991218727,
          "standard_error": 0.004105198495885386
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.760814815904505,
            "upper_bound": 10.77989905276158
          },
          "point_estimate": 10.769808208157652,
          "standard_error": 0.004902463996266093
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.759370578652794,
            "upper_bound": 10.783392305741708
          },
          "point_estimate": 10.766728664440038,
          "standard_error": 0.0048993220785858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003300897186676642,
            "upper_bound": 0.02830770498363389
          },
          "point_estimate": 0.009047704801433548,
          "standard_error": 0.006515467887770584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.763218748264226,
            "upper_bound": 10.785226957356802
          },
          "point_estimate": 10.772363135589169,
          "standard_error": 0.0057176029139041065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006332095658696828,
            "upper_bound": 0.02096414620034612
          },
          "point_estimate": 0.01636365622315783,
          "standard_error": 0.003634813457774618
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.04601221830328,
            "upper_bound": 16.08267095641891
          },
          "point_estimate": 16.062941356499003,
          "standard_error": 0.009433450135768838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.034951316065147,
            "upper_bound": 16.07936122518742
          },
          "point_estimate": 16.060186967873626,
          "standard_error": 0.010869471691354871
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0041968848770297445,
            "upper_bound": 0.05018296058005029
          },
          "point_estimate": 0.02902835174902696,
          "standard_error": 0.011321382098553883
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.037958630761032,
            "upper_bound": 16.06539313586286
          },
          "point_estimate": 16.051207663751114,
          "standard_error": 0.007224216859726997
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014690273380617676,
            "upper_bound": 0.04298699854021484
          },
          "point_estimate": 0.031473029361754334,
          "standard_error": 0.008105415345858366
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.25992364010758,
            "upper_bound": 22.307849988543587
          },
          "point_estimate": 22.282835293386007,
          "standard_error": 0.012279444234891057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.24590271742493,
            "upper_bound": 22.30935611814794
          },
          "point_estimate": 22.287750274229083,
          "standard_error": 0.016184685891913936
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002246770147873486,
            "upper_bound": 0.07030628592177188
          },
          "point_estimate": 0.05318827031008195,
          "standard_error": 0.01893495447450893
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.265740626755083,
            "upper_bound": 22.31125854847579
          },
          "point_estimate": 22.290384354048864,
          "standard_error": 0.011549184332699507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021937573926626753,
            "upper_bound": 0.052858511496279695
          },
          "point_estimate": 0.04100420477196198,
          "standard_error": 0.00832678898354029
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.998081347557209,
            "upper_bound": 11.029373945553878
          },
          "point_estimate": 11.011743302292745,
          "standard_error": 0.00799130427998328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.99678657367798,
            "upper_bound": 11.031583031625043
          },
          "point_estimate": 11.00062705085686,
          "standard_error": 0.006908675852097279
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016678019971170196,
            "upper_bound": 0.037585983775803064
          },
          "point_estimate": 0.008298685003048895,
          "standard_error": 0.007405545545857339
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.998784580658286,
            "upper_bound": 11.00875015875505
          },
          "point_estimate": 11.002941824075826,
          "standard_error": 0.002551943320345072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004340928716429502,
            "upper_bound": 0.03393674896510011
          },
          "point_estimate": 0.026761947276863,
          "standard_error": 0.007929298869837835
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.94910561711245,
            "upper_bound": 23.98373847233136
          },
          "point_estimate": 23.96546983868867,
          "standard_error": 0.008866105355948718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.942921102082877,
            "upper_bound": 23.97768015981994
          },
          "point_estimate": 23.966586366488883,
          "standard_error": 0.008989116001292187
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005736303005653312,
            "upper_bound": 0.049859036044194255
          },
          "point_estimate": 0.019584554432717005,
          "standard_error": 0.011394418127760956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.960116100339626,
            "upper_bound": 23.975505043061357
          },
          "point_estimate": 23.966432326064876,
          "standard_error": 0.003924815948316782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013623328903018844,
            "upper_bound": 0.040330248336811515
          },
          "point_estimate": 0.029568079117926947,
          "standard_error": 0.007425713049224971
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.781901939815413,
            "upper_bound": 20.812340279421814
          },
          "point_estimate": 20.79687881593876,
          "standard_error": 0.0078088305876526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.774831593735374,
            "upper_bound": 20.819108301335323
          },
          "point_estimate": 20.79674253457633,
          "standard_error": 0.013159121338975956
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00372433765740987,
            "upper_bound": 0.04315746043058855
          },
          "point_estimate": 0.03256164623411218,
          "standard_error": 0.010226954095430852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.782542456564823,
            "upper_bound": 20.819509173951197
          },
          "point_estimate": 20.801428593195713,
          "standard_error": 0.009866098777942922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016058258177891086,
            "upper_bound": 0.031205041622006737
          },
          "point_estimate": 0.02597285202331768,
          "standard_error": 0.0038328401686404385
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151804.1500394344,
            "upper_bound": 1153857.7533659786
          },
          "point_estimate": 1152837.8831832835,
          "standard_error": 526.1290456608975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151329.2588541666,
            "upper_bound": 1154576.525
          },
          "point_estimate": 1152802.4140625,
          "standard_error": 870.2689161764664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 441.5670027074197,
            "upper_bound": 2863.7158504610406
          },
          "point_estimate": 2330.1895088392075,
          "standard_error": 639.6470721771818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151994.6737583706,
            "upper_bound": 1154163.731583794
          },
          "point_estimate": 1152893.0435876625,
          "standard_error": 553.6102822238955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1097.5768920168423,
            "upper_bound": 2106.9219312215173
          },
          "point_estimate": 1754.3664240885903,
          "standard_error": 257.48209919010986
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1528835.075813492,
            "upper_bound": 1530663.8084231154
          },
          "point_estimate": 1529725.51890377,
          "standard_error": 468.11400479855706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1528676.1791666667,
            "upper_bound": 1531000.441666667
          },
          "point_estimate": 1529453.1875,
          "standard_error": 604.1721754154534
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.94845158179066,
            "upper_bound": 2668.642630226615
          },
          "point_estimate": 1162.615517796983,
          "standard_error": 645.1132444741395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1528532.398950247,
            "upper_bound": 1531223.4021049596
          },
          "point_estimate": 1529657.1360389611,
          "standard_error": 719.818309833919
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785.8006961521024,
            "upper_bound": 1993.9117325241496
          },
          "point_estimate": 1553.737355635069,
          "standard_error": 303.7696990352325
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1370883.3621706348,
            "upper_bound": 1372715.4256782776
          },
          "point_estimate": 1371912.6611405057,
          "standard_error": 474.2803246553672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1371263.1697530863,
            "upper_bound": 1372968.9365079366
          },
          "point_estimate": 1372582.3555555556,
          "standard_error": 472.4469052959216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.72870133773392,
            "upper_bound": 2182.4082105140287
          },
          "point_estimate": 636.3824617884994,
          "standard_error": 544.0437450504877
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1372237.3285714283,
            "upper_bound": 1372865.1028510523
          },
          "point_estimate": 1372664.816835017,
          "standard_error": 162.55512797846208
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 339.717947680881,
            "upper_bound": 2184.7335374138115
          },
          "point_estimate": 1576.7397865194582,
          "standard_error": 480.0091177528998
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374940.06693258055,
            "upper_bound": 375530.0019634675
          },
          "point_estimate": 375226.4103906889,
          "standard_error": 151.1134875291309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374709.9278350515,
            "upper_bound": 375598.0025773196
          },
          "point_estimate": 375202.90538373427,
          "standard_error": 184.27649071888428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.60133863598855,
            "upper_bound": 913.703258048285
          },
          "point_estimate": 592.5241363877853,
          "standard_error": 251.284925389756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 374843.7659863167,
            "upper_bound": 375666.00762036897
          },
          "point_estimate": 375262.2847770786,
          "standard_error": 216.8200873623435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 276.349718320752,
            "upper_bound": 629.5201231732706
          },
          "point_estimate": 505.2038442446754,
          "standard_error": 90.93308551807796
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529283.9430210489,
            "upper_bound": 530040.2252796468
          },
          "point_estimate": 529668.3655682079,
          "standard_error": 193.29906211890008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529012.8132045089,
            "upper_bound": 530190.8949275361
          },
          "point_estimate": 529930.672705314,
          "standard_error": 367.56987560092045
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.31255627890546,
            "upper_bound": 966.9618835577072
          },
          "point_estimate": 714.0455735550256,
          "standard_error": 250.29305342109592
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 529042.883792536,
            "upper_bound": 529983.9048589545
          },
          "point_estimate": 529488.9942028986,
          "standard_error": 244.55041236856576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.2047528323158,
            "upper_bound": 748.725049757798
          },
          "point_estimate": 643.9399435235565,
          "standard_error": 86.26109544017365
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 404787.8844271164,
            "upper_bound": 405283.5152160494
          },
          "point_estimate": 405035.11151410936,
          "standard_error": 124.34543924594396
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 404910.3674074074,
            "upper_bound": 405150.15
          },
          "point_estimate": 405032.8714285714,
          "standard_error": 67.3184748549275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.67698384653225,
            "upper_bound": 680.4457884197492
          },
          "point_estimate": 157.22377850503236,
          "standard_error": 133.77123317245545
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 404882.76196145127,
            "upper_bound": 405244.75999985065
          },
          "point_estimate": 405020.9229437229,
          "standard_error": 92.63849938592332
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.29261004559058,
            "upper_bound": 586.824488186572
          },
          "point_estimate": 414.4002459184118,
          "standard_error": 130.63205715945853
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474106.1778100649,
            "upper_bound": 474839.06522572663
          },
          "point_estimate": 474442.8819490828,
          "standard_error": 188.40916348070556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473989.487012987,
            "upper_bound": 474859.06493506493
          },
          "point_estimate": 474133.98322510824,
          "standard_error": 240.054678781594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.593328860286555,
            "upper_bound": 991.8719131267466
          },
          "point_estimate": 294.21346068579163,
          "standard_error": 254.62002651332307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474167.700983891,
            "upper_bound": 475129.49033109745
          },
          "point_estimate": 474639.7914994097,
          "standard_error": 244.446980944207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.22814115443455,
            "upper_bound": 806.0125488520343
          },
          "point_estimate": 628.3573738913877,
          "standard_error": 149.89934226261963
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 936954.7688539888,
            "upper_bound": 938619.1234493284
          },
          "point_estimate": 937815.780962556,
          "standard_error": 428.02404439125246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 936521.0752136752,
            "upper_bound": 938875.543091168
          },
          "point_estimate": 938230.7645299146,
          "standard_error": 641.6109035389638
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.46081008211348,
            "upper_bound": 2381.7018192547707
          },
          "point_estimate": 1487.5239162834048,
          "standard_error": 570.4009001461716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 937240.2156698634,
            "upper_bound": 938849.941530103
          },
          "point_estimate": 937971.982017982,
          "standard_error": 412.8619419586178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 773.5190610106853,
            "upper_bound": 1764.1156211007856
          },
          "point_estimate": 1421.5363352571148,
          "standard_error": 250.0286531644016
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282676.60412580747,
            "upper_bound": 283267.60632798076
          },
          "point_estimate": 282944.2717155778,
          "standard_error": 152.63257318837384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282526.2248062015,
            "upper_bound": 283212.5762273902
          },
          "point_estimate": 282840.0400516796,
          "standard_error": 176.3591280980442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.22535109656165,
            "upper_bound": 770.2288836125729
          },
          "point_estimate": 499.87515023399953,
          "standard_error": 172.26502491175978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282649.4389556537,
            "upper_bound": 282983.30067243037
          },
          "point_estimate": 282831.22164502164,
          "standard_error": 84.49361389872463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.6757132132703,
            "upper_bound": 703.2933937459188
          },
          "point_estimate": 509.1744504442438,
          "standard_error": 140.7868858311457
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765866.1061689815,
            "upper_bound": 767267.4250810186
          },
          "point_estimate": 766583.2212334656,
          "standard_error": 357.39086306910554
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765847.4594907407,
            "upper_bound": 767357.2760416666
          },
          "point_estimate": 766645.5390625,
          "standard_error": 409.23986920719864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.02379706664658,
            "upper_bound": 2005.7850517860552
          },
          "point_estimate": 951.110276551877,
          "standard_error": 443.1092725785404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 766196.4105457246,
            "upper_bound": 767203.3176912569
          },
          "point_estimate": 766719.6181818182,
          "standard_error": 261.3668528046934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 545.7835859391885,
            "upper_bound": 1613.056160835983
          },
          "point_estimate": 1195.5180985293075,
          "standard_error": 279.31873565489735
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386228.0762968592,
            "upper_bound": 386779.15518563194
          },
          "point_estimate": 386519.98121707194,
          "standard_error": 141.17357384552895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386191.63776595745,
            "upper_bound": 386793.3546099291
          },
          "point_estimate": 386670.0653495441,
          "standard_error": 155.65081811437963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.83312111839733,
            "upper_bound": 760.2977705445071
          },
          "point_estimate": 283.19019861598673,
          "standard_error": 194.61465145721553
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386474.08578984905,
            "upper_bound": 386969.5166533
          },
          "point_estimate": 386712.4870958828,
          "standard_error": 130.7542426750125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.93907630794263,
            "upper_bound": 615.2905733204201
          },
          "point_estimate": 470.52097889889734,
          "standard_error": 105.59158931554002
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159631.7905952381,
            "upper_bound": 160083.2871411863
          },
          "point_estimate": 159839.60208159286,
          "standard_error": 116.13409440992828
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159537.71392543858,
            "upper_bound": 160179.27138157893
          },
          "point_estimate": 159696.35867794487,
          "standard_error": 159.5537195833833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.88298784875597,
            "upper_bound": 583.5425530245423
          },
          "point_estimate": 250.56085864375876,
          "standard_error": 163.35572528922611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159563.00037976415,
            "upper_bound": 159783.25820951458
          },
          "point_estimate": 159656.9036226931,
          "standard_error": 57.98144579266452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.0732138369986,
            "upper_bound": 479.2210585078708
          },
          "point_estimate": 387.5010379105728,
          "standard_error": 86.89402496359494
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229365.100406902,
            "upper_bound": 229786.01981137693
          },
          "point_estimate": 229568.6113025357,
          "standard_error": 107.37486579672232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229356.09939353095,
            "upper_bound": 229921.39412997905
          },
          "point_estimate": 229460.064011181,
          "standard_error": 132.12796317811018
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.06683639342221,
            "upper_bound": 644.6297803899272
          },
          "point_estimate": 211.57327699854724,
          "standard_error": 168.62444248746675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 229393.42474343485,
            "upper_bound": 229715.70675768377
          },
          "point_estimate": 229526.29863595523,
          "standard_error": 84.96093945843899
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.95506672590648,
            "upper_bound": 460.17444591940335
          },
          "point_estimate": 357.56405481808224,
          "standard_error": 70.84449197515849
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167551.80161630458,
            "upper_bound": 167878.03360796574
          },
          "point_estimate": 167690.4387425938,
          "standard_error": 85.2387330130982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167482.814680711,
            "upper_bound": 167759.6331797235
          },
          "point_estimate": 167653.31413210445,
          "standard_error": 82.93188450877982
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.664054974987597,
            "upper_bound": 347.48385641155676
          },
          "point_estimate": 213.2245220209521,
          "standard_error": 80.35993531696562
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167513.43561315662,
            "upper_bound": 167717.26651921874
          },
          "point_estimate": 167618.4999940152,
          "standard_error": 53.161863641119766
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.05172259481463,
            "upper_bound": 416.93676777237033
          },
          "point_estimate": 284.5676422581903,
          "standard_error": 100.79970037045
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660351.2449121847,
            "upper_bound": 661930.1506817603
          },
          "point_estimate": 661113.3227118764,
          "standard_error": 402.2090682957463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660191.8670634921,
            "upper_bound": 661636.7729591837
          },
          "point_estimate": 661316.2984374999,
          "standard_error": 413.6965136184134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.71070187333632,
            "upper_bound": 2143.6485637283745
          },
          "point_estimate": 767.4383756609772,
          "standard_error": 550.5365388783222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660488.1487305998,
            "upper_bound": 661456.5726560025
          },
          "point_estimate": 661016.5532003711,
          "standard_error": 252.2659600643731
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 543.7823697420455,
            "upper_bound": 1859.1895428164096
          },
          "point_estimate": 1337.0286746352515,
          "standard_error": 344.2155551731063
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1328.870276132372,
            "upper_bound": 1332.576444026233
          },
          "point_estimate": 1330.5381481762174,
          "standard_error": 0.9567304709014433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1328.4122091321528,
            "upper_bound": 1332.170483926045
          },
          "point_estimate": 1329.694929804938,
          "standard_error": 0.9299082832867002
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19269638359791824,
            "upper_bound": 4.517134740527679
          },
          "point_estimate": 2.65758277039474,
          "standard_error": 1.1185333950976262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1328.8843891311376,
            "upper_bound": 1331.302232513855
          },
          "point_estimate": 1329.8733277771548,
          "standard_error": 0.6304583363736312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3045383754441198,
            "upper_bound": 4.415471502693914
          },
          "point_estimate": 3.1870230129053336,
          "standard_error": 0.8940957896249302
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331678.1060147395,
            "upper_bound": 1333440.2066780045
          },
          "point_estimate": 1332489.4266780044,
          "standard_error": 453.8447040604216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331413.3392857143,
            "upper_bound": 1333610.607142857
          },
          "point_estimate": 1331903.697420635,
          "standard_error": 615.3285750406923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.50853043156503,
            "upper_bound": 2481.779693439603
          },
          "point_estimate": 834.7624083942975,
          "standard_error": 627.2573696218701
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331556.417135358,
            "upper_bound": 1332876.387828366
          },
          "point_estimate": 1332066.7645640075,
          "standard_error": 337.01249622174174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 612.3233753522991,
            "upper_bound": 1952.396494248763
          },
          "point_estimate": 1513.67223289368,
          "standard_error": 338.51975171026766
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537255.7096408731,
            "upper_bound": 1539617.9612430558
          },
          "point_estimate": 1538570.8410383598,
          "standard_error": 608.462961932194
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537993.47255291,
            "upper_bound": 1539706.9756944445
          },
          "point_estimate": 1538966.1,
          "standard_error": 502.89899270735503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.1549775929218,
            "upper_bound": 2677.6061457965693
          },
          "point_estimate": 1169.349250073196,
          "standard_error": 571.7133358160895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537814.637084592,
            "upper_bound": 1539399.8516129032
          },
          "point_estimate": 1538642.28495671,
          "standard_error": 394.0147376844188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 693.0254633280538,
            "upper_bound": 2924.6384437973143
          },
          "point_estimate": 2037.7443216430152,
          "standard_error": 656.391561893723
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438939.9887809828,
            "upper_bound": 1441068.6600822648
          },
          "point_estimate": 1439977.4666239317,
          "standard_error": 545.0024567330051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438182.4188034188,
            "upper_bound": 1441185.746794872
          },
          "point_estimate": 1440135.2932692308,
          "standard_error": 797.5122540991448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.6028735981051,
            "upper_bound": 3394.401551916784
          },
          "point_estimate": 1754.267411163317,
          "standard_error": 735.2829773643906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438811.9379379577,
            "upper_bound": 1440678.493918649
          },
          "point_estimate": 1439821.146153846,
          "standard_error": 490.605664847902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1036.1261174702663,
            "upper_bound": 2342.0712631499423
          },
          "point_estimate": 1818.960730737624,
          "standard_error": 344.47440305927876
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765968.8336838626,
            "upper_bound": 766823.9903968254
          },
          "point_estimate": 766344.7812185847,
          "standard_error": 220.7239103039893
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765895.1150462963,
            "upper_bound": 766540.125
          },
          "point_estimate": 766262.3552827381,
          "standard_error": 164.82570907022165
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.03332040092141,
            "upper_bound": 949.4078850543344
          },
          "point_estimate": 418.1687566385041,
          "standard_error": 216.0081964293385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765847.6124497992,
            "upper_bound": 766393.9577178031
          },
          "point_estimate": 766138.4771645021,
          "standard_error": 144.0371083902991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230.37345196953427,
            "upper_bound": 1060.6281956203763
          },
          "point_estimate": 732.6368680484069,
          "standard_error": 244.0983483282889
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269451.86176645354,
            "upper_bound": 269905.62748111406
          },
          "point_estimate": 269658.1466457966,
          "standard_error": 117.20494869492208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269368.07037037035,
            "upper_bound": 269887.69300411525
          },
          "point_estimate": 269593.8971296296,
          "standard_error": 122.14473982074294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.87623136597514,
            "upper_bound": 590.734638030914
          },
          "point_estimate": 273.5889778650465,
          "standard_error": 140.7250452137016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269542.8475789312,
            "upper_bound": 269865.6509373571
          },
          "point_estimate": 269664.79567099566,
          "standard_error": 82.39608248163607
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.65451556291515,
            "upper_bound": 525.4726540895476
          },
          "point_estimate": 390.7040505971256,
          "standard_error": 102.20357629807911
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483865.5171530388,
            "upper_bound": 484666.45562092727
          },
          "point_estimate": 484268.9594538429,
          "standard_error": 204.9452467164501
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483746.8918859649,
            "upper_bound": 484764.6405075188
          },
          "point_estimate": 484401.1668494152,
          "standard_error": 275.96968542175154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.21765805690491,
            "upper_bound": 1160.5733025206669
          },
          "point_estimate": 828.6768212091037,
          "standard_error": 291.4914187870859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484060.3101864035,
            "upper_bound": 484516.0746779851
          },
          "point_estimate": 484353.6461038961,
          "standard_error": 117.24342946949882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 375.2705973404472,
            "upper_bound": 887.7255643458778
          },
          "point_estimate": 686.7114574374884,
          "standard_error": 131.52842581769775
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246305.53602551023,
            "upper_bound": 246633.131797538
          },
          "point_estimate": 246454.98467957025,
          "standard_error": 84.41777601189244
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246272.4455782313,
            "upper_bound": 246636.32078069323
          },
          "point_estimate": 246344.5535147392,
          "standard_error": 93.5766409769323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.88196677212707,
            "upper_bound": 436.7919464404134
          },
          "point_estimate": 145.67373210764544,
          "standard_error": 107.23058241124134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246301.13260191295,
            "upper_bound": 246810.44992254328
          },
          "point_estimate": 246569.7065641841,
          "standard_error": 129.23566544285256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.03644959180096,
            "upper_bound": 365.7916538005945
          },
          "point_estimate": 280.54799749809376,
          "standard_error": 68.7072669584898
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695292.1984209719,
            "upper_bound": 696284.6058355795
          },
          "point_estimate": 695801.8024198862,
          "standard_error": 253.89667108250524
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695127.579245283,
            "upper_bound": 696679.2938005391
          },
          "point_estimate": 695803.9772012578,
          "standard_error": 460.6937168355876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.30632826591975,
            "upper_bound": 1482.1738427427367
          },
          "point_estimate": 1069.1875277163374,
          "standard_error": 385.9279050904658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695323.113679068,
            "upper_bound": 696601.6819954332
          },
          "point_estimate": 696032.8226905171,
          "standard_error": 355.51159872246114
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 528.8518547942916,
            "upper_bound": 1023.8471920474776
          },
          "point_estimate": 844.8285456362028,
          "standard_error": 129.96363549161998
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2285.1675113493066,
            "upper_bound": 2288.211967012701
          },
          "point_estimate": 2286.7069592766065,
          "standard_error": 0.7759998436482382
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2285.326923076923,
            "upper_bound": 2288.780282331512
          },
          "point_estimate": 2286.4045649432537,
          "standard_error": 0.7859301146193254
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.31350071889868536,
            "upper_bound": 4.627894686438883
          },
          "point_estimate": 1.976668201982141,
          "standard_error": 1.0906172326725132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2285.8570972976,
            "upper_bound": 2287.858116655658
          },
          "point_estimate": 2286.7822646206255,
          "standard_error": 0.4991960996999942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2198968291893015,
            "upper_bound": 3.445288226226485
          },
          "point_estimate": 2.589833086297242,
          "standard_error": 0.5648319295488741
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.7104750513306,
            "upper_bound": 704.4379442302975
          },
          "point_estimate": 704.0508671894745,
          "standard_error": 0.18714713924132387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.6122536770878,
            "upper_bound": 704.6502020945518
          },
          "point_estimate": 703.7983787238988,
          "standard_error": 0.25788623690718093
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0502896785322742,
            "upper_bound": 0.9783466689904586
          },
          "point_estimate": 0.28726056544595857,
          "standard_error": 0.25048617560993386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.5973669956974,
            "upper_bound": 704.4248104026601
          },
          "point_estimate": 703.9132848091745,
          "standard_error": 0.21223437267186177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21416612172932517,
            "upper_bound": 0.7487048634712593
          },
          "point_estimate": 0.6241168915826796,
          "standard_error": 0.11925508224518833
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.23181579239204,
            "upper_bound": 140.4641503898809
          },
          "point_estimate": 140.33968238195695,
          "standard_error": 0.059860350282103666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.1974642774452,
            "upper_bound": 140.43436276401428
          },
          "point_estimate": 140.33056890098015,
          "standard_error": 0.0562390590602655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.033093606381685514,
            "upper_bound": 0.29976015299027126
          },
          "point_estimate": 0.14119483249238818,
          "standard_error": 0.07142400252512593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.15361756967448,
            "upper_bound": 140.34891084133994
          },
          "point_estimate": 140.2290527609305,
          "standard_error": 0.04887838397609999
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08617759569644269,
            "upper_bound": 0.2751908343963134
          },
          "point_estimate": 0.1994051316869998,
          "standard_error": 0.0532856750641271
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.633847910481414,
            "upper_bound": 52.73410155687211
          },
          "point_estimate": 52.67820535825865,
          "standard_error": 0.02594437956337451
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.61985794009557,
            "upper_bound": 52.72507676106284
          },
          "point_estimate": 52.63942564037485,
          "standard_error": 0.02969087139153804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007372772147002541,
            "upper_bound": 0.12698634927754207
          },
          "point_estimate": 0.04190458325621116,
          "standard_error": 0.032276602242954075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.635677477495456,
            "upper_bound": 52.70871188402003
          },
          "point_estimate": 52.67649761409958,
          "standard_error": 0.0186766833120608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027234950765052248,
            "upper_bound": 0.11930767680600568
          },
          "point_estimate": 0.08618045794072665,
          "standard_error": 0.025090855569648705
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.16929404002477,
            "upper_bound": 80.24663147437565
          },
          "point_estimate": 80.2063114733075,
          "standard_error": 0.01977856959110694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.15886149740443,
            "upper_bound": 80.24688133098675
          },
          "point_estimate": 80.19693136933287,
          "standard_error": 0.02453911824307641
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01913508640450156,
            "upper_bound": 0.11184359210861929
          },
          "point_estimate": 0.06372688807941743,
          "standard_error": 0.023114520899400037
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.14456643318933,
            "upper_bound": 80.22835636260402
          },
          "point_estimate": 80.19126168741644,
          "standard_error": 0.02223566159255807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03315191362031621,
            "upper_bound": 0.08747580658223028
          },
          "point_estimate": 0.06579505923207758,
          "standard_error": 0.01442837447837519
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292279.23290816665,
            "upper_bound": 292603.4877066111
          },
          "point_estimate": 292435.44966444443,
          "standard_error": 83.31954199145167
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292209.958,
            "upper_bound": 292714.82622222224
          },
          "point_estimate": 292342.766,
          "standard_error": 141.98686925113182
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.220426058823076,
            "upper_bound": 450.46856346923767
          },
          "point_estimate": 290.13058188918416,
          "standard_error": 114.09853369901376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292211.17784,
            "upper_bound": 292747.30922169465
          },
          "point_estimate": 292492.16982857144,
          "standard_error": 141.5066397049317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157.92395000894513,
            "upper_bound": 331.8322010872027
          },
          "point_estimate": 278.5083101908122,
          "standard_error": 43.529738848946295
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.44695261157695,
            "upper_bound": 62.515439214013426
          },
          "point_estimate": 62.48147624348155,
          "standard_error": 0.017547371394157754
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.415281495333545,
            "upper_bound": 62.51743223146499
          },
          "point_estimate": 62.50051590960666,
          "standard_error": 0.027011087910742797
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002857340578881697,
            "upper_bound": 0.10988776854401636
          },
          "point_estimate": 0.04316834218997679,
          "standard_error": 0.02809989384714307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.42084096331527,
            "upper_bound": 62.53082458725109
          },
          "point_estimate": 62.46453933378003,
          "standard_error": 0.02851275360178121
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.033499795970692546,
            "upper_bound": 0.07255268213311854
          },
          "point_estimate": 0.05829335258056408,
          "standard_error": 0.009942478529831777
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.35526907092805,
            "upper_bound": 105.46086612093518
          },
          "point_estimate": 105.40349604278909,
          "standard_error": 0.02725131840718371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.3599434700769,
            "upper_bound": 105.44320534314905
          },
          "point_estimate": 105.37563368551788,
          "standard_error": 0.022350247846583772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008897915165393042,
            "upper_bound": 0.1329194745649155
          },
          "point_estimate": 0.04241712004885333,
          "standard_error": 0.03250822901469884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.37350307989011,
            "upper_bound": 105.42996923591588
          },
          "point_estimate": 105.39722808857282,
          "standard_error": 0.014437464877227956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030073094653546725,
            "upper_bound": 0.12765072715643452
          },
          "point_estimate": 0.09104091038035345,
          "standard_error": 0.02693785927990662
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128883.53464444164,
            "upper_bound": 128972.97217861348
          },
          "point_estimate": 128929.59547268494,
          "standard_error": 22.850788058499393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128869.73272477424,
            "upper_bound": 128986.91405855629
          },
          "point_estimate": 128943.49858657244,
          "standard_error": 29.879804929173527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.359412951779658,
            "upper_bound": 132.52266723499983
          },
          "point_estimate": 76.97778247611623,
          "standard_error": 30.271446676093305
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128881.77335210687,
            "upper_bound": 128954.05383879774
          },
          "point_estimate": 128923.47226836768,
          "standard_error": 18.07041223642521
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.01451969425773,
            "upper_bound": 93.99701631391564
          },
          "point_estimate": 76.0910985783797,
          "standard_error": 13.423433562321904
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.717741002762466,
            "upper_bound": 60.83480723232141
          },
          "point_estimate": 60.772276228248494,
          "standard_error": 0.03005601429025381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.69716563120092,
            "upper_bound": 60.830380902279686
          },
          "point_estimate": 60.749018209646685,
          "standard_error": 0.029386059888149737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00932615546584076,
            "upper_bound": 0.16725393661126098
          },
          "point_estimate": 0.06507572142982236,
          "standard_error": 0.041346288429652794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.68419568934166,
            "upper_bound": 60.8148824834107
          },
          "point_estimate": 60.74485735918727,
          "standard_error": 0.0332967696897132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04359720430977949,
            "upper_bound": 0.13226752771915304
          },
          "point_estimate": 0.10002982797773302,
          "standard_error": 0.023756523536174055
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.00359298418812,
            "upper_bound": 102.10699717085733
          },
          "point_estimate": 102.05206135790368,
          "standard_error": 0.02647908472024261
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.9895779128464,
            "upper_bound": 102.08162509490734
          },
          "point_estimate": 102.0565018559658,
          "standard_error": 0.020963064339413303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009026991361341163,
            "upper_bound": 0.1360756830752947
          },
          "point_estimate": 0.050754453287975336,
          "standard_error": 0.03296012814454334
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.00749813414484,
            "upper_bound": 102.07277575989126
          },
          "point_estimate": 102.0400514645221,
          "standard_error": 0.016520530493938323
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02955976152492142,
            "upper_bound": 0.12409288436109316
          },
          "point_estimate": 0.08818161837335332,
          "standard_error": 0.02473828055610332
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.76843932038693,
            "upper_bound": 254.17730024643168
          },
          "point_estimate": 253.9519705394096,
          "standard_error": 0.10434262611372232
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.78524145642604,
            "upper_bound": 254.04085715284089
          },
          "point_estimate": 253.9096961605361,
          "standard_error": 0.055486351361325216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014235016467526928,
            "upper_bound": 0.4524686028413486
          },
          "point_estimate": 0.12976783279993728,
          "standard_error": 0.11037852460605098
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.82853347986443,
            "upper_bound": 253.9426347379353
          },
          "point_estimate": 253.89110243031132,
          "standard_error": 0.02866649861143002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07640533793614133,
            "upper_bound": 0.5057980766834438
          },
          "point_estimate": 0.3475754744392862,
          "standard_error": 0.1164358947852704
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 519.082961903945,
            "upper_bound": 519.880238136086
          },
          "point_estimate": 519.4734580644752,
          "standard_error": 0.20401062050526184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 518.7564168549071,
            "upper_bound": 519.9764532639465
          },
          "point_estimate": 519.5442555685815,
          "standard_error": 0.2695475495607522
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06261084168142346,
            "upper_bound": 1.2380548449174729
          },
          "point_estimate": 0.6846562712634842,
          "standard_error": 0.29614664238729754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 518.8455643466182,
            "upper_bound": 519.6435093939173
          },
          "point_estimate": 519.2232828218874,
          "standard_error": 0.20505485618389016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3869250720892472,
            "upper_bound": 0.8699468485044561
          },
          "point_estimate": 0.6786944989434334,
          "standard_error": 0.12839864959016525
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.06998077962246,
            "upper_bound": 32.096355290159806
          },
          "point_estimate": 32.084474830859406,
          "standard_error": 0.006788950626111915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.075753835478096,
            "upper_bound": 32.10239124362342
          },
          "point_estimate": 32.08575313728429,
          "standard_error": 0.006290972942418232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028341965662548037,
            "upper_bound": 0.03148680478376791
          },
          "point_estimate": 0.016261996768818607,
          "standard_error": 0.007497533513452386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.08234937195059,
            "upper_bound": 32.0935480947183
          },
          "point_estimate": 32.08761321435057,
          "standard_error": 0.002830277532884944
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009075479649302371,
            "upper_bound": 0.031662963693122576
          },
          "point_estimate": 0.022601753389948825,
          "standard_error": 0.0065789982553490015
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33009.35176300755,
            "upper_bound": 33058.484454059
          },
          "point_estimate": 33033.689714192085,
          "standard_error": 12.547811698862995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32999.85390199637,
            "upper_bound": 33071.077268602545
          },
          "point_estimate": 33032.05351381327,
          "standard_error": 18.40752483209644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.133417244942532,
            "upper_bound": 72.15357243347651
          },
          "point_estimate": 48.047540272210036,
          "standard_error": 15.088203052002788
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33011.4012889634,
            "upper_bound": 33060.31641212159
          },
          "point_estimate": 33038.271155160626,
          "standard_error": 12.229171546641105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.92669960201652,
            "upper_bound": 51.434178788157226
          },
          "point_estimate": 41.913013874877585,
          "standard_error": 6.761759824051677
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.4574231002249,
            "upper_bound": 141.71889189064467
          },
          "point_estimate": 141.58105984508416,
          "standard_error": 0.06727635861754311
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.41122497756845,
            "upper_bound": 141.74763048495225
          },
          "point_estimate": 141.53129273257295,
          "standard_error": 0.08208828494669425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.046388795406000466,
            "upper_bound": 0.36980709547557833
          },
          "point_estimate": 0.22076265406141493,
          "standard_error": 0.08635568715310114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.4760309831697,
            "upper_bound": 141.7474069428745
          },
          "point_estimate": 141.61896512709103,
          "standard_error": 0.07155024861794142
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10794470530281308,
            "upper_bound": 0.2868806810011488
          },
          "point_estimate": 0.2250747774902175,
          "standard_error": 0.04535859240544287
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978202.387770468,
            "upper_bound": 980150.0531484962
          },
          "point_estimate": 979063.5044058062,
          "standard_error": 500.1740221174638
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978135.5526315788,
            "upper_bound": 979322.4115497076
          },
          "point_estimate": 978985.666118421,
          "standard_error": 275.8591477587495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.83290107229813,
            "upper_bound": 2082.0287985628793
          },
          "point_estimate": 589.3930582861692,
          "standard_error": 490.94124390767234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978649.6614439058,
            "upper_bound": 979153.1034904156
          },
          "point_estimate": 978890.3809979494,
          "standard_error": 128.0127888074547
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.9176810164706,
            "upper_bound": 2426.472284187554
          },
          "point_estimate": 1668.2367781366938,
          "standard_error": 570.2485731395989
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1986.7162437591548,
            "upper_bound": 1988.5925329517063
          },
          "point_estimate": 1987.760185779242,
          "standard_error": 0.4836304931456292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1987.1535048899088,
            "upper_bound": 1988.943606694713
          },
          "point_estimate": 1987.932312249965,
          "standard_error": 0.5054709426148465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22363609393702147,
            "upper_bound": 2.215435135229673
          },
          "point_estimate": 1.3279830320248722,
          "standard_error": 0.4825924167121814
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1987.419313849955,
            "upper_bound": 1988.6013520087815
          },
          "point_estimate": 1987.9757449456515,
          "standard_error": 0.3054583795540807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.645438934151737,
            "upper_bound": 2.2860134800140965
          },
          "point_estimate": 1.6130802775837072,
          "standard_error": 0.4885937893413339
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.39637210941346,
            "upper_bound": 25.55628049840038
          },
          "point_estimate": 25.4791324896046,
          "standard_error": 0.040861417813500035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.399457933169703,
            "upper_bound": 25.575111130215255
          },
          "point_estimate": 25.48501503014805,
          "standard_error": 0.04846960635410318
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026769177705160657,
            "upper_bound": 0.2330409265887325
          },
          "point_estimate": 0.1092760687786195,
          "standard_error": 0.04878103533953888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.430410017812008,
            "upper_bound": 25.540657751814987
          },
          "point_estimate": 25.482261344753844,
          "standard_error": 0.02808630151728902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06714200236636494,
            "upper_bound": 0.1821084469540965
          },
          "point_estimate": 0.13640530947254634,
          "standard_error": 0.030118254846358824
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.295576491712307,
            "upper_bound": 25.397317548601222
          },
          "point_estimate": 25.339756358583664,
          "standard_error": 0.02646654719920519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.280770413692167,
            "upper_bound": 25.373902794494917
          },
          "point_estimate": 25.3180362972275,
          "standard_error": 0.022095651433360775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0053974108545272155,
            "upper_bound": 0.1082632453440667
          },
          "point_estimate": 0.05566628348381743,
          "standard_error": 0.027652255697395985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.28760091781062,
            "upper_bound": 25.37606247853939
          },
          "point_estimate": 25.33303350417324,
          "standard_error": 0.02328946404416434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028249531870290705,
            "upper_bound": 0.1257979283689442
          },
          "point_estimate": 0.08805495058858487,
          "standard_error": 0.028439634946329267
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.08303995375647,
            "upper_bound": 31.85853822088485
          },
          "point_estimate": 31.46363643488449,
          "standard_error": 0.19909916503569855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.9007861048684,
            "upper_bound": 32.09974566777858
          },
          "point_estimate": 31.32825501410946,
          "standard_error": 0.34366734530459225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1486837426010615,
            "upper_bound": 1.081468921564829
          },
          "point_estimate": 0.8033313483452192,
          "standard_error": 0.24321229576140604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.83781003788373,
            "upper_bound": 31.387941562348654
          },
          "point_estimate": 31.055892097926957,
          "standard_error": 0.1399634134405489
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.412411789364364,
            "upper_bound": 0.7865505701694385
          },
          "point_estimate": 0.6647707909532239,
          "standard_error": 0.09509886187807312
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.59622018398819,
            "upper_bound": 31.320943733090605
          },
          "point_estimate": 30.945618759714307,
          "standard_error": 0.1856296206407931
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.50216091318123,
            "upper_bound": 31.42977705332237
          },
          "point_estimate": 30.817321442197866,
          "standard_error": 0.24421998294385644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0980782115206776,
            "upper_bound": 1.0177586759931323
          },
          "point_estimate": 0.4692024412155348,
          "standard_error": 0.24683532542360956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.650423639550596,
            "upper_bound": 31.235306786276812
          },
          "point_estimate": 30.96064871529952,
          "standard_error": 0.14876069926071325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30848905855192077,
            "upper_bound": 0.7727838085011902
          },
          "point_estimate": 0.6157623026938686,
          "standard_error": 0.11391604072169484
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.797123795298432,
            "upper_bound": 21.1126766771046
          },
          "point_estimate": 20.944719417130425,
          "standard_error": 0.08112834780027098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.741031484659253,
            "upper_bound": 21.13478243592139
          },
          "point_estimate": 20.863596771013516,
          "standard_error": 0.11131993367115646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03689344047935385,
            "upper_bound": 0.43415934143778473
          },
          "point_estimate": 0.1913379221721224,
          "standard_error": 0.10646714227515527
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.758315337971116,
            "upper_bound": 21.065965386612543
          },
          "point_estimate": 20.901949081376717,
          "standard_error": 0.08098408352202696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12505969495005373,
            "upper_bound": 0.34308734205216396
          },
          "point_estimate": 0.2714132549236253,
          "standard_error": 0.05530201146936944
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.51253414309466,
            "upper_bound": 21.600277978756825
          },
          "point_estimate": 21.55148283624025,
          "standard_error": 0.02254228691096004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.50934399975812,
            "upper_bound": 21.57850144882098
          },
          "point_estimate": 21.536549420126537,
          "standard_error": 0.01651178842427324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009243486333120011,
            "upper_bound": 0.10006016826063896
          },
          "point_estimate": 0.04390369751480101,
          "standard_error": 0.023087860194849603
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.51426514619987,
            "upper_bound": 21.555910655742064
          },
          "point_estimate": 21.533581521335588,
          "standard_error": 0.010400372218158298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02466401444239289,
            "upper_bound": 0.10792186918938876
          },
          "point_estimate": 0.07510460628792788,
          "standard_error": 0.024086555614164888
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.700520032176456,
            "upper_bound": 34.80494434422199
          },
          "point_estimate": 34.744330401463785,
          "standard_error": 0.027313440538498625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.68774361159004,
            "upper_bound": 34.76026067019164
          },
          "point_estimate": 34.728360395506996,
          "standard_error": 0.01670340564638339
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01127233786256275,
            "upper_bound": 0.09187611135098311
          },
          "point_estimate": 0.03469358492178326,
          "standard_error": 0.02286787909302986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.710743349802485,
            "upper_bound": 34.739323416912306
          },
          "point_estimate": 34.72494543972894,
          "standard_error": 0.007341395049894271
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02504673434443213,
            "upper_bound": 0.13510228710693473
          },
          "point_estimate": 0.0911181928879552,
          "standard_error": 0.03464685116419916
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.70266349263064,
            "upper_bound": 25.763071928399153
          },
          "point_estimate": 25.73339637026672,
          "standard_error": 0.015523064486133406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.681790095484097,
            "upper_bound": 25.776030367227637
          },
          "point_estimate": 25.73631896670141,
          "standard_error": 0.02410317989015515
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013298348818505315,
            "upper_bound": 0.08517935532015526
          },
          "point_estimate": 0.06462438861747925,
          "standard_error": 0.019304064886886685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.718473501934568,
            "upper_bound": 25.779199168390083
          },
          "point_estimate": 25.7522326989592,
          "standard_error": 0.015437904137453091
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03022538712614962,
            "upper_bound": 0.06287965901259078
          },
          "point_estimate": 0.05165214211872543,
          "standard_error": 0.008181965563397993
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.68909130590863,
            "upper_bound": 27.948793195245972
          },
          "point_estimate": 27.81440769800375,
          "standard_error": 0.06655143686425523
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.658930979550156,
            "upper_bound": 27.97457290470548
          },
          "point_estimate": 27.765048899119,
          "standard_error": 0.08483267124522578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016057447116210814,
            "upper_bound": 0.3668315826036955
          },
          "point_estimate": 0.16585434212013056,
          "standard_error": 0.08874782218503866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.756463376158504,
            "upper_bound": 28.088370968009727
          },
          "point_estimate": 27.96848329622303,
          "standard_error": 0.08327117958090495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10999536091222224,
            "upper_bound": 0.28073405069579016
          },
          "point_estimate": 0.2219356642491062,
          "standard_error": 0.04271699139991032
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.582614410334788,
            "upper_bound": 28.859920504444784
          },
          "point_estimate": 28.713565840385193,
          "standard_error": 0.07103487047559749
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.522310943051743,
            "upper_bound": 28.866160142686823
          },
          "point_estimate": 28.656680441719818,
          "standard_error": 0.0849253652288463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03679535168120083,
            "upper_bound": 0.3831364362490614
          },
          "point_estimate": 0.22682286539109667,
          "standard_error": 0.08858504705758438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.538458417530578,
            "upper_bound": 28.79182508357115
          },
          "point_estimate": 28.64284793339837,
          "standard_error": 0.06522073981851749
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11486424482212491,
            "upper_bound": 0.308422961928988
          },
          "point_estimate": 0.2362808051123988,
          "standard_error": 0.05111462114602258
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.1621555897007,
            "upper_bound": 26.260588222020896
          },
          "point_estimate": 26.214646228775244,
          "standard_error": 0.02527984116084596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.158457249563597,
            "upper_bound": 26.280900276307033
          },
          "point_estimate": 26.240246853112488,
          "standard_error": 0.030225061223297833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008892331437091413,
            "upper_bound": 0.14446515040950433
          },
          "point_estimate": 0.06304060153634708,
          "standard_error": 0.03367914447283104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.197386935163244,
            "upper_bound": 26.257867306419335
          },
          "point_estimate": 26.22898318560761,
          "standard_error": 0.015309971991062104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030259889429106054,
            "upper_bound": 0.1044362420884148
          },
          "point_estimate": 0.0841978067908742,
          "standard_error": 0.017748179245972544
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.93853340576615,
            "upper_bound": 26.297919002769948
          },
          "point_estimate": 26.10222837938113,
          "standard_error": 0.09284980427364928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.866634008896263,
            "upper_bound": 26.394123818241628
          },
          "point_estimate": 25.965207317247593,
          "standard_error": 0.12309958700727608
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0129708324713524,
            "upper_bound": 0.48294188489082096
          },
          "point_estimate": 0.15263020060276988,
          "standard_error": 0.11654515892745552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.96884826979227,
            "upper_bound": 26.50900374211364
          },
          "point_estimate": 26.305799368044745,
          "standard_error": 0.12916561168300006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06760155069729232,
            "upper_bound": 0.39737800504713766
          },
          "point_estimate": 0.3087787720335431,
          "standard_error": 0.0727613949152003
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169910.9387630208,
            "upper_bound": 1171287.2819727494
          },
          "point_estimate": 1170653.2991654263,
          "standard_error": 353.46098770641646
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1170185.6440972222,
            "upper_bound": 1171476.2447916667
          },
          "point_estimate": 1170807.5544642855,
          "standard_error": 324.0483029241576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.58780692065503,
            "upper_bound": 1689.1374018867327
          },
          "point_estimate": 943.4219918447192,
          "standard_error": 372.90133542445494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1170524.3405572756,
            "upper_bound": 1171584.81717431
          },
          "point_estimate": 1171053.3131493507,
          "standard_error": 270.40861971220096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462.7987478506189,
            "upper_bound": 1640.1310977737603
          },
          "point_estimate": 1174.8644408757968,
          "standard_error": 330.43476766665066
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551415.0728422618,
            "upper_bound": 1554045.0859126984
          },
          "point_estimate": 1552799.008690476,
          "standard_error": 671.8285067880446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551625.3333333333,
            "upper_bound": 1554524.725
          },
          "point_estimate": 1553015.09375,
          "standard_error": 697.8075641503859
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323.91352966604103,
            "upper_bound": 3665.063324098905
          },
          "point_estimate": 1743.6714148771118,
          "standard_error": 868.6779985879347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551939.6113231552,
            "upper_bound": 1553420.9894894897
          },
          "point_estimate": 1552885.1864718616,
          "standard_error": 379.33013633852624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 937.4169062405034,
            "upper_bound": 3072.100871972139
          },
          "point_estimate": 2239.689010538045,
          "standard_error": 564.4620909710256
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1445015.7721842185,
            "upper_bound": 1453340.8352029915
          },
          "point_estimate": 1449437.4098916363,
          "standard_error": 2142.922341000807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443937.1923076925,
            "upper_bound": 1453790.111111111
          },
          "point_estimate": 1452322.7819368131,
          "standard_error": 2619.5344635351935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 651.2819336297022,
            "upper_bound": 11205.588807471302
          },
          "point_estimate": 2962.3880761250557,
          "standard_error": 3024.06170484689
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1450629.3792818682,
            "upper_bound": 1454151.9528974022
          },
          "point_estimate": 1452450.8922077925,
          "standard_error": 902.9160640007096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2806.3154293676803,
            "upper_bound": 8976.705080245982
          },
          "point_estimate": 7158.088364272126,
          "standard_error": 1507.3235962431131
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386330.4408535968,
            "upper_bound": 387714.1617814716
          },
          "point_estimate": 386924.245108916,
          "standard_error": 361.2394223032882
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386216.2805851064,
            "upper_bound": 387291.5606382979
          },
          "point_estimate": 386520.734929078,
          "standard_error": 272.57346355207767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.47099209890736,
            "upper_bound": 1410.8113788892515
          },
          "point_estimate": 473.7843868014098,
          "standard_error": 347.5806693685833
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386088.1322482791,
            "upper_bound": 386700.0834548893
          },
          "point_estimate": 386394.4315833103,
          "standard_error": 160.1510838785964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303.1337643584777,
            "upper_bound": 1736.675919838785
          },
          "point_estimate": 1208.7066414940111,
          "standard_error": 413.83350736494657
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560083.6961794872,
            "upper_bound": 560857.6887521368
          },
          "point_estimate": 560466.621435287,
          "standard_error": 196.43863321174155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560077.7756410257,
            "upper_bound": 560704.5295482296
          },
          "point_estimate": 560532.3821153846,
          "standard_error": 152.18606639882933
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.95054026413183,
            "upper_bound": 1086.0159803730753
          },
          "point_estimate": 306.36543609938474,
          "standard_error": 254.45745002561137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560259.855581817,
            "upper_bound": 560630.9447190545
          },
          "point_estimate": 560489.5966433566,
          "standard_error": 94.7742538158874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192.33081967948448,
            "upper_bound": 917.6422372812024
          },
          "point_estimate": 655.4864224406621,
          "standard_error": 173.98460752019542
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413745.06752164505,
            "upper_bound": 414162.8468560606
          },
          "point_estimate": 413951.69408279227,
          "standard_error": 107.05855911633458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413647.1590909091,
            "upper_bound": 414259.3986742424
          },
          "point_estimate": 413904.5988636364,
          "standard_error": 177.66316140570422
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.32973165265109,
            "upper_bound": 587.8657717223474
          },
          "point_estimate": 482.9948488114988,
          "standard_error": 132.89705770416302
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413672.13217915624,
            "upper_bound": 414026.4728354978
          },
          "point_estimate": 413829.2243211334,
          "standard_error": 90.050441223189
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.68767959625745,
            "upper_bound": 436.5918544044953
          },
          "point_estimate": 358.2351957289488,
          "standard_error": 55.00411769756142
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486292.771157672,
            "upper_bound": 487081.53389391536
          },
          "point_estimate": 486674.70763068774,
          "standard_error": 202.48125514161495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486142.7927407407,
            "upper_bound": 487307.90222222224
          },
          "point_estimate": 486416.9719999999,
          "standard_error": 333.4444399928646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.12136350207892,
            "upper_bound": 1053.3225410997898
          },
          "point_estimate": 762.0224984713277,
          "standard_error": 264.22013946763974
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486156.22180090816,
            "upper_bound": 487147.2299723847
          },
          "point_estimate": 486568.2414199134,
          "standard_error": 259.5437226399699
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396.62538000680144,
            "upper_bound": 826.8827676126904
          },
          "point_estimate": 674.9762727286618,
          "standard_error": 110.90209395916598
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975395.1209216008,
            "upper_bound": 977505.1448261278
          },
          "point_estimate": 976362.6143327068,
          "standard_error": 543.1079144575518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975066.1890350876,
            "upper_bound": 977487.6789473684
          },
          "point_estimate": 975838.9868421052,
          "standard_error": 724.6774424923367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.7504612449702,
            "upper_bound": 2871.503530599546
          },
          "point_estimate": 1160.0413988788391,
          "standard_error": 739.9910688070888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 974922.6634043744,
            "upper_bound": 976247.6123627414
          },
          "point_estimate": 975473.2028024606,
          "standard_error": 343.7825904300642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 793.2686362534117,
            "upper_bound": 2412.094270609952
          },
          "point_estimate": 1807.703171167736,
          "standard_error": 438.5790400231916
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298187.72182734846,
            "upper_bound": 299175.75395264116
          },
          "point_estimate": 298588.3331228858,
          "standard_error": 264.39139582964276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298070.618852459,
            "upper_bound": 298823.1844262295
          },
          "point_estimate": 298312.5870673953,
          "standard_error": 193.63972011768064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.25249629375965,
            "upper_bound": 813.7139908269161
          },
          "point_estimate": 399.6055798455023,
          "standard_error": 199.3378371795675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298138.2634230711,
            "upper_bound": 298756.30043320014
          },
          "point_estimate": 298422.46306152863,
          "standard_error": 163.5889743947413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.16531119133057,
            "upper_bound": 1322.0311137526169
          },
          "point_estimate": 882.7566847942834,
          "standard_error": 357.1651768355054
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806448.790531401,
            "upper_bound": 808242.949636646
          },
          "point_estimate": 807265.8544547965,
          "standard_error": 461.5067555253827
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806275.2192546583,
            "upper_bound": 808391.3125
          },
          "point_estimate": 806643.9495772946,
          "standard_error": 491.3926575517925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.52558708125605,
            "upper_bound": 2353.713137452429
          },
          "point_estimate": 883.3741581213275,
          "standard_error": 554.7021214619583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806461.6186691601,
            "upper_bound": 807263.1093691222
          },
          "point_estimate": 806785.9892715979,
          "standard_error": 205.67288660776256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502.7940909204184,
            "upper_bound": 1995.8084730902533
          },
          "point_estimate": 1537.290434089149,
          "standard_error": 389.518853025524
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403375.79872428265,
            "upper_bound": 403738.3353261817
          },
          "point_estimate": 403558.812415402,
          "standard_error": 93.34222879016777
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403282.1208791209,
            "upper_bound": 403856.2197802198
          },
          "point_estimate": 403610.2214285714,
          "standard_error": 154.88432243764098
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.02070504848838,
            "upper_bound": 507.2803381093455
          },
          "point_estimate": 408.41895621063424,
          "standard_error": 113.70504488212858
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 403268.71242872434,
            "upper_bound": 403779.47318611987
          },
          "point_estimate": 403498.5330098473,
          "standard_error": 129.34007251519077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195.97583070008693,
            "upper_bound": 373.1672002914066
          },
          "point_estimate": 311.23966748668124,
          "standard_error": 45.28416721120756
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164770.35671191552,
            "upper_bound": 165109.17684012066
          },
          "point_estimate": 164933.55800653595,
          "standard_error": 87.08781543107087
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164712.36821266968,
            "upper_bound": 165222.7899698341
          },
          "point_estimate": 164835.01495726497,
          "standard_error": 135.1388953403341
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.31317588562784,
            "upper_bound": 468.1437998335712
          },
          "point_estimate": 312.36626029147925,
          "standard_error": 117.86677134956916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164735.51431082567,
            "upper_bound": 165049.7670936077
          },
          "point_estimate": 164859.42727860375,
          "standard_error": 80.1272265947696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.7108622064608,
            "upper_bound": 352.75165642610773
          },
          "point_estimate": 290.47854510718554,
          "standard_error": 48.433621915847056
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233347.10241300365,
            "upper_bound": 233690.3140384615
          },
          "point_estimate": 233528.93136141635,
          "standard_error": 88.38889029648819
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233301.4423076923,
            "upper_bound": 233733.02564102568
          },
          "point_estimate": 233625.5569139194,
          "standard_error": 118.20384795557938
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.37323036905171,
            "upper_bound": 469.1899416702196
          },
          "point_estimate": 175.12020814103613,
          "standard_error": 119.38285499176848
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233360.53120841095,
            "upper_bound": 233677.39818254663
          },
          "point_estimate": 233566.6758075258,
          "standard_error": 81.24990639053347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.27172847212265,
            "upper_bound": 364.8301539749401
          },
          "point_estimate": 294.19666478996373,
          "standard_error": 57.60921992967765
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172858.4421020932,
            "upper_bound": 173001.8870112559
          },
          "point_estimate": 172928.17701026856,
          "standard_error": 36.79246574609224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172813.98815165876,
            "upper_bound": 173047.9212085308
          },
          "point_estimate": 172894.044628752,
          "standard_error": 59.6694190678772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.733390230417847,
            "upper_bound": 202.43883792257571
          },
          "point_estimate": 133.56047535393287,
          "standard_error": 44.862276283571774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172883.48984145006,
            "upper_bound": 173057.37681445788
          },
          "point_estimate": 172984.83322459532,
          "standard_error": 44.907822455435166
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.0249476930209,
            "upper_bound": 147.07449034214724
          },
          "point_estimate": 122.99488530495827,
          "standard_error": 18.55662846819572
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649366.5088886054,
            "upper_bound": 650407.9158150865
          },
          "point_estimate": 649852.2171549036,
          "standard_error": 266.724536160058
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649026.7648809524,
            "upper_bound": 650398.7837301587
          },
          "point_estimate": 649805.649702381,
          "standard_error": 345.88368011661225
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.05007572745174,
            "upper_bound": 1543.6958888437782
          },
          "point_estimate": 888.5044932138891,
          "standard_error": 325.9093692947384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649457.8973214285,
            "upper_bound": 650202.7533996565
          },
          "point_estimate": 649897.3219851577,
          "standard_error": 188.89983866855977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 449.6242720433936,
            "upper_bound": 1195.0262739421355
          },
          "point_estimate": 888.9470916507478,
          "standard_error": 212.9636802956665
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1352.7474165588403,
            "upper_bound": 1354.1531917902523
          },
          "point_estimate": 1353.4574762159516,
          "standard_error": 0.36030338379372234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1352.323515415049,
            "upper_bound": 1354.6150156763213
          },
          "point_estimate": 1353.6253502289242,
          "standard_error": 0.6305157633101527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27057919887672294,
            "upper_bound": 1.9548825064130448
          },
          "point_estimate": 1.5856313198262728,
          "standard_error": 0.4438372785987557
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1352.549766033309,
            "upper_bound": 1354.3613565291394
          },
          "point_estimate": 1353.5094156813625,
          "standard_error": 0.4618999241508391
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7609794368395579,
            "upper_bound": 1.436799625846111
          },
          "point_estimate": 1.2025228592782995,
          "standard_error": 0.17340340345317054
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332522.5779062498,
            "upper_bound": 1334271.0321329364
          },
          "point_estimate": 1333272.6625439343,
          "standard_error": 454.1014172441373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332301.2954081632,
            "upper_bound": 1333960.3055555555
          },
          "point_estimate": 1332689.7276785714,
          "standard_error": 390.1305502486253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.71099620091712,
            "upper_bound": 1849.4669838321352
          },
          "point_estimate": 785.9027127140919,
          "standard_error": 447.3053683768426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332331.9827050858,
            "upper_bound": 1333270.6215061275
          },
          "point_estimate": 1332760.707328386,
          "standard_error": 244.29615924390168
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384.3410159207059,
            "upper_bound": 2145.216678895814
          },
          "point_estimate": 1511.9593563479891,
          "standard_error": 501.39142241670703
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536930.0030497685,
            "upper_bound": 1538837.1574275796
          },
          "point_estimate": 1537864.02916336,
          "standard_error": 487.0515576055026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536611.8154761903,
            "upper_bound": 1539333.7333333334
          },
          "point_estimate": 1537668.1099537038,
          "standard_error": 752.3735690584662
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 459.5197029530284,
            "upper_bound": 2622.256040945649
          },
          "point_estimate": 1866.687255054278,
          "standard_error": 568.4668566045159
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537065.773207348,
            "upper_bound": 1539020.417760237
          },
          "point_estimate": 1538137.5232683984,
          "standard_error": 501.050558596064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 958.3975244608448,
            "upper_bound": 2017.645404017872
          },
          "point_estimate": 1618.8877782373672,
          "standard_error": 271.60907792989946
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1437264.3012820515,
            "upper_bound": 1438776.926197917
          },
          "point_estimate": 1438030.633768315,
          "standard_error": 388.5187825814371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1437035.3923076922,
            "upper_bound": 1439040.945054945
          },
          "point_estimate": 1438076.28125,
          "standard_error": 630.653995637951
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.31577411848353,
            "upper_bound": 2212.4119490870935
          },
          "point_estimate": 1435.849027873958,
          "standard_error": 534.8009641688151
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1437456.11659687,
            "upper_bound": 1438956.7242129373
          },
          "point_estimate": 1438186.4577422575,
          "standard_error": 395.3206870686606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 767.7104138517377,
            "upper_bound": 1645.2571534206347
          },
          "point_estimate": 1296.818489598092,
          "standard_error": 230.43718395134695
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765963.4292187501,
            "upper_bound": 767390.6582238344
          },
          "point_estimate": 766654.5723503637,
          "standard_error": 364.89202025247056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765697.1328125,
            "upper_bound": 767765.3199404762
          },
          "point_estimate": 766277.4489583333,
          "standard_error": 565.1571451413756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.82266419430286,
            "upper_bound": 1871.09338136478
          },
          "point_estimate": 1420.341070617229,
          "standard_error": 487.9532902158199
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765998.8372224164,
            "upper_bound": 767405.6636424732
          },
          "point_estimate": 766563.3063852814,
          "standard_error": 356.8267841263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706.7749104110991,
            "upper_bound": 1494.0195526474
          },
          "point_estimate": 1213.412982288708,
          "standard_error": 202.30835082252943
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269328.12767746917,
            "upper_bound": 269633.3943786008
          },
          "point_estimate": 269484.92829835386,
          "standard_error": 77.94345060818793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269235.3345679012,
            "upper_bound": 269680.20574074076
          },
          "point_estimate": 269573.3065843622,
          "standard_error": 117.70376290081416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.519183635706497,
            "upper_bound": 420.9156236383455
          },
          "point_estimate": 309.47558478348947,
          "standard_error": 115.42667495754677
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269301.1413281123,
            "upper_bound": 269579.9467371156
          },
          "point_estimate": 269424.16003848,
          "standard_error": 72.43341282526102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.42973562314612,
            "upper_bound": 321.3513894704014
          },
          "point_estimate": 259.21696520282086,
          "standard_error": 43.382015690138566
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483919.9436270364,
            "upper_bound": 484922.93806599826
          },
          "point_estimate": 484391.52581819135,
          "standard_error": 256.0704362696097
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483855.2212171053,
            "upper_bound": 484795.9894736842
          },
          "point_estimate": 484253.9979323308,
          "standard_error": 231.98242953957435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.38370751767572,
            "upper_bound": 1345.8731828165394
          },
          "point_estimate": 588.812876476322,
          "standard_error": 308.67767282116193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483829.86928842287,
            "upper_bound": 484424.9410755149
          },
          "point_estimate": 484173.54600136704,
          "standard_error": 149.70870699509774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 345.23626709416175,
            "upper_bound": 1187.2287993963837
          },
          "point_estimate": 855.533676884561,
          "standard_error": 225.35123881081535
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246219.4286222249,
            "upper_bound": 246521.30892042044
          },
          "point_estimate": 246361.8624603175,
          "standard_error": 77.37306515926363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246191.30743243243,
            "upper_bound": 246575.73482410985
          },
          "point_estimate": 246317.22888513515,
          "standard_error": 80.31875288262859
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.74546825964582,
            "upper_bound": 425.23710945503655
          },
          "point_estimate": 140.87304317466266,
          "standard_error": 98.18515853896328
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246251.3557333484,
            "upper_bound": 246672.87005883435
          },
          "point_estimate": 246490.2584064584,
          "standard_error": 106.52844674098226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.140852018343,
            "upper_bound": 328.79282624641337
          },
          "point_estimate": 257.57608608600833,
          "standard_error": 57.49900578438114
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 694893.5578212041,
            "upper_bound": 695687.5314690026
          },
          "point_estimate": 695321.0880937406,
          "standard_error": 203.6806372149888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 694970.4056603773,
            "upper_bound": 695873.5560796645
          },
          "point_estimate": 695389.2367475291,
          "standard_error": 265.56829898668497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.9574451288794,
            "upper_bound": 1138.0804162728618
          },
          "point_estimate": 620.9960903901382,
          "standard_error": 240.3305571117516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695008.7639326769,
            "upper_bound": 695793.4061690714
          },
          "point_estimate": 695425.779122764,
          "standard_error": 201.180608769525
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330.53466975720045,
            "upper_bound": 921.2985541379372
          },
          "point_estimate": 677.1354171366843,
          "standard_error": 170.96473036674516
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.8219222937417,
            "upper_bound": 364.4642508881536
          },
          "point_estimate": 360.0496080943864,
          "standard_error": 2.112806502387478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.6948264497633,
            "upper_bound": 362.6432237558492
          },
          "point_estimate": 356.9519521861542,
          "standard_error": 1.354619995769211
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08286092822076069,
            "upper_bound": 8.388714332367798
          },
          "point_estimate": 0.3747128564230643,
          "standard_error": 1.6670063702243472
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.81384087720846,
            "upper_bound": 357.6275765226041
          },
          "point_estimate": 357.11699464489715,
          "standard_error": 0.2115456502971147
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.207393374665796,
            "upper_bound": 9.861178879534924
          },
          "point_estimate": 7.0282789683948,
          "standard_error": 2.69888551645728
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.91590022054555,
            "upper_bound": 170.13943957106918
          },
          "point_estimate": 170.02765778498735,
          "standard_error": 0.05740847916162528
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.85296814612892,
            "upper_bound": 170.20964030551414
          },
          "point_estimate": 170.0106749926357,
          "standard_error": 0.1023053313638482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03472410426781085,
            "upper_bound": 0.304048501257995
          },
          "point_estimate": 0.26401544586894704,
          "standard_error": 0.07357133525454036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.86679200065754,
            "upper_bound": 170.16658642158143
          },
          "point_estimate": 169.99362923522852,
          "standard_error": 0.0773912916075006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12268730661774632,
            "upper_bound": 0.2249385256061422
          },
          "point_estimate": 0.1908370228864843,
          "standard_error": 0.026117860142411065
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.53510644287547,
            "upper_bound": 27.15828134309591
          },
          "point_estimate": 26.852492305341173,
          "standard_error": 0.15917521226602932
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.449705091583787,
            "upper_bound": 27.226303059878465
          },
          "point_estimate": 26.823856998338137,
          "standard_error": 0.18339423721759696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06465243973567544,
            "upper_bound": 0.8803286943725969
          },
          "point_estimate": 0.49196113128109814,
          "standard_error": 0.22546371373954663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.810504152299075,
            "upper_bound": 27.389577635111767
          },
          "point_estimate": 27.115393290724413,
          "standard_error": 0.1471099301114606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.26278000479722674,
            "upper_bound": 0.6893935612341732
          },
          "point_estimate": 0.5304149672398234,
          "standard_error": 0.10776140431387736
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.015226913616324,
            "upper_bound": 18.08620991953576
          },
          "point_estimate": 18.05238470740806,
          "standard_error": 0.018207484356746063
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.014751058104505,
            "upper_bound": 18.08585446928306
          },
          "point_estimate": 18.066067378590738,
          "standard_error": 0.0187441364972066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007095447743390293,
            "upper_bound": 0.10180583255618326
          },
          "point_estimate": 0.030466653998297323,
          "standard_error": 0.02504947518810233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.046666840854247,
            "upper_bound": 18.08295308801604
          },
          "point_estimate": 18.066711021357907,
          "standard_error": 0.00915823180522066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026815216900998945,
            "upper_bound": 0.08168647846334304
          },
          "point_estimate": 0.06059485555170878,
          "standard_error": 0.01464242175307104
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17703666070104,
            "upper_bound": 25.218622464906524
          },
          "point_estimate": 25.194281768098556,
          "standard_error": 0.0109341158352525
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.1758319779894,
            "upper_bound": 25.196551261555054
          },
          "point_estimate": 25.18806988325725,
          "standard_error": 0.005131312996994789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019568293286150603,
            "upper_bound": 0.03383450466645853
          },
          "point_estimate": 0.014365048842129104,
          "standard_error": 0.00825655481026933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.17747721840842,
            "upper_bound": 25.191890012664217
          },
          "point_estimate": 25.184357901260817,
          "standard_error": 0.0037525884697117303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007401625955931367,
            "upper_bound": 0.054421605829141226
          },
          "point_estimate": 0.03647188598409006,
          "standard_error": 0.0145548806930484
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292355.8695040318,
            "upper_bound": 292690.2305838413
          },
          "point_estimate": 292515.3098746032,
          "standard_error": 85.92199331588313
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292289.11,
            "upper_bound": 292825.7908571429
          },
          "point_estimate": 292433.69299999997,
          "standard_error": 126.38987793123184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.91382832724926,
            "upper_bound": 457.0242910861456
          },
          "point_estimate": 238.45054031327317,
          "standard_error": 110.95194258482242
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292309.2385350746,
            "upper_bound": 292801.08976970817
          },
          "point_estimate": 292521.316425974,
          "standard_error": 125.66513109689949
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.94626962528423,
            "upper_bound": 342.59198951046307
          },
          "point_estimate": 285.54496223989736,
          "standard_error": 48.60006624682781
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.059512226138963,
            "upper_bound": 16.09056104132069
          },
          "point_estimate": 16.074465706093342,
          "standard_error": 0.007952083600007537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.05074025373593,
            "upper_bound": 16.09174410635753
          },
          "point_estimate": 16.073322451788258,
          "standard_error": 0.010674620328897547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0063543728793919625,
            "upper_bound": 0.04654578279733314
          },
          "point_estimate": 0.025195877791906976,
          "standard_error": 0.009697615013734407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.064730565221293,
            "upper_bound": 16.089943886321112
          },
          "point_estimate": 16.080708846902382,
          "standard_error": 0.006433818209486666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014323039586233072,
            "upper_bound": 0.03449064477183062
          },
          "point_estimate": 0.026544673610711306,
          "standard_error": 0.0053209335962789905
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.231195270498784,
            "upper_bound": 22.267943741475193
          },
          "point_estimate": 22.248874806643737,
          "standard_error": 0.009447322034621465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.23036114179857,
            "upper_bound": 22.277523100646395
          },
          "point_estimate": 22.240947668124235,
          "standard_error": 0.010120370822166004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00445344814683728,
            "upper_bound": 0.05594103297568002
          },
          "point_estimate": 0.023385903637316414,
          "standard_error": 0.013248924732583942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.237797394995265,
            "upper_bound": 22.252380044447623
          },
          "point_estimate": 22.24420702678985,
          "standard_error": 0.003704775393911464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012896034467886529,
            "upper_bound": 0.04065781719027088
          },
          "point_estimate": 0.031464711564483445,
          "standard_error": 0.006668162151395715
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128796.30262325422,
            "upper_bound": 129017.34612726458
          },
          "point_estimate": 128911.8890246228,
          "standard_error": 56.68476141308242
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128759.35247349823,
            "upper_bound": 129061.79240282686
          },
          "point_estimate": 128976.58870379718,
          "standard_error": 94.88240659634856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.807478605862546,
            "upper_bound": 317.72456715078056
          },
          "point_estimate": 192.11665006391428,
          "standard_error": 76.71661344184403
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128831.48103777648,
            "upper_bound": 129018.69354316294
          },
          "point_estimate": 128929.1486118122,
          "standard_error": 50.15852765099808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.09329478753268,
            "upper_bound": 235.82856836355825
          },
          "point_estimate": 188.2209705826025,
          "standard_error": 34.328359680568695
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.971764356499136,
            "upper_bound": 24.022953035139793
          },
          "point_estimate": 23.994272955769844,
          "standard_error": 0.01323357215087546
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.963101160315137,
            "upper_bound": 24.00718704752527
          },
          "point_estimate": 23.99028518199301,
          "standard_error": 0.012464953486587473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005377352624628038,
            "upper_bound": 0.05649152937961685
          },
          "point_estimate": 0.03586792690522894,
          "standard_error": 0.012734747969650266
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.973637937279857,
            "upper_bound": 24.00010655951295
          },
          "point_estimate": 23.989302603304388,
          "standard_error": 0.006665202682173015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016625640984139985,
            "upper_bound": 0.06315358479443978
          },
          "point_estimate": 0.04411508763311883,
          "standard_error": 0.014057639911828824
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.78619652297505,
            "upper_bound": 20.818018450483084
          },
          "point_estimate": 20.8011054452385,
          "standard_error": 0.00810083876860804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.77546733311654,
            "upper_bound": 20.818093131150455
          },
          "point_estimate": 20.79752381736158,
          "standard_error": 0.009830872565235172
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030851047390554444,
            "upper_bound": 0.04312349781621823
          },
          "point_estimate": 0.02606569848950318,
          "standard_error": 0.01069314905924258
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.781750163122982,
            "upper_bound": 20.812734390574104
          },
          "point_estimate": 20.7959080372264,
          "standard_error": 0.008008678610036896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013107431187067668,
            "upper_bound": 0.035690423301699054
          },
          "point_estimate": 0.02707849808860926,
          "standard_error": 0.006174311264973458
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.47369893950868,
            "upper_bound": 40.572658321805726
          },
          "point_estimate": 40.52201132151976,
          "standard_error": 0.025366706637073844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.44667792679949,
            "upper_bound": 40.58613783567925
          },
          "point_estimate": 40.52519670593486,
          "standard_error": 0.03402375619130919
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015895826666742434,
            "upper_bound": 0.1488746701413449
          },
          "point_estimate": 0.08168064468256757,
          "standard_error": 0.032961288794787415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.46085443610421,
            "upper_bound": 40.591713394555846
          },
          "point_estimate": 40.5178163962221,
          "standard_error": 0.03376599533299811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.047723241635569166,
            "upper_bound": 0.10656392454733728
          },
          "point_estimate": 0.08435193799094755,
          "standard_error": 0.01529010278839982
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.36171706155037,
            "upper_bound": 73.51500056399703
          },
          "point_estimate": 73.43166860643615,
          "standard_error": 0.03933036516850089
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.33905302372189,
            "upper_bound": 73.51486739018881
          },
          "point_estimate": 73.38020581502417,
          "standard_error": 0.04590857060116356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007240662536656463,
            "upper_bound": 0.19994399567450336
          },
          "point_estimate": 0.06316916248448257,
          "standard_error": 0.048235544873914446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.35472082502065,
            "upper_bound": 73.46047075350052
          },
          "point_estimate": 73.39478535428603,
          "standard_error": 0.027005974915933317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.032016721555342235,
            "upper_bound": 0.16586456479544692
          },
          "point_estimate": 0.13102382422228387,
          "standard_error": 0.032186060475338535
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.001440294324215,
            "upper_bound": 11.025138120518104
          },
          "point_estimate": 11.012830129426828,
          "standard_error": 0.006047335795038861
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.997561387836726,
            "upper_bound": 11.022694099710794
          },
          "point_estimate": 11.015148095965325,
          "standard_error": 0.006916813303235366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013389656041899634,
            "upper_bound": 0.034841428799860984
          },
          "point_estimate": 0.01982974641690387,
          "standard_error": 0.00787047070523501
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.000232446625205,
            "upper_bound": 11.018102346418608
          },
          "point_estimate": 11.008966278405644,
          "standard_error": 0.00455460738951927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009828935603197846,
            "upper_bound": 0.027148201932311163
          },
          "point_estimate": 0.02019338525230555,
          "standard_error": 0.0045792036184118595
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32810.31896991039,
            "upper_bound": 32857.609355624314
          },
          "point_estimate": 32833.55961832417,
          "standard_error": 12.12628628319266
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32806.64615924589,
            "upper_bound": 32867.391414711194
          },
          "point_estimate": 32827.38176895307,
          "standard_error": 14.338748279166662
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.918201905157003,
            "upper_bound": 70.75626416575744
          },
          "point_estimate": 35.54760911619377,
          "standard_error": 16.275917682899617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32815.18536249609,
            "upper_bound": 32870.78427491714
          },
          "point_estimate": 32841.163535561915,
          "standard_error": 14.154484220394217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.72475554623368,
            "upper_bound": 51.613269829613664
          },
          "point_estimate": 40.472900437774136,
          "standard_error": 7.655812488401156
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.918376207281597,
            "upper_bound": 30.169709280877377
          },
          "point_estimate": 30.03917477068048,
          "standard_error": 0.06366209975623098
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.949903158460796,
            "upper_bound": 30.09703257734852
          },
          "point_estimate": 30.029073031937084,
          "standard_error": 0.04085303052075245
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006804046564601338,
            "upper_bound": 0.3327459209510065
          },
          "point_estimate": 0.09267367471526144,
          "standard_error": 0.0711233368477249
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.02463153447155,
            "upper_bound": 30.172291601379023
          },
          "point_estimate": 30.082261909015585,
          "standard_error": 0.03789229438395071
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04924853016329043,
            "upper_bound": 0.30295482749230274
          },
          "point_estimate": 0.2120136747414247,
          "standard_error": 0.06552040835091069
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978928.2290237572,
            "upper_bound": 981216.1766359648
          },
          "point_estimate": 979993.7144026732,
          "standard_error": 585.9403124723922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978389.8526315788,
            "upper_bound": 981041.2218045112
          },
          "point_estimate": 979902.8519736844,
          "standard_error": 650.9554997916335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 385.98319841056815,
            "upper_bound": 3239.1532098618964
          },
          "point_estimate": 1965.45993300086,
          "standard_error": 722.230314221713
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978563.7214598998,
            "upper_bound": 982592.0528716864
          },
          "point_estimate": 980514.2539986328,
          "standard_error": 1096.9832786836091
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 955.2285359407596,
            "upper_bound": 2618.819418558255
          },
          "point_estimate": 1953.551299227989,
          "standard_error": 458.73020076259337
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.6960108329251,
            "upper_bound": 1963.163848236567
          },
          "point_estimate": 1960.630461304448,
          "standard_error": 1.164427165995927
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.6747335475725,
            "upper_bound": 1961.771431392614
          },
          "point_estimate": 1959.8197305703668,
          "standard_error": 0.9880561084299652
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4594668730477927,
            "upper_bound": 4.5723845641627
          },
          "point_estimate": 2.900677523411594,
          "standard_error": 1.142500289698352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.2942513936016,
            "upper_bound": 1960.423333745291
          },
          "point_estimate": 1959.2653698539305,
          "standard_error": 0.5365925374070236
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2805165847324158,
            "upper_bound": 5.582915506064248
          },
          "point_estimate": 3.8817740543195938,
          "standard_error": 1.2893870330496957
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.855429373828483,
            "upper_bound": 7.899140993200843
          },
          "point_estimate": 7.874092188618678,
          "standard_error": 0.011392277936347091
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.844780138409352,
            "upper_bound": 7.885167101703177
          },
          "point_estimate": 7.865428141196441,
          "standard_error": 0.012044374439890152
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002156711280274571,
            "upper_bound": 0.04897292514684465
          },
          "point_estimate": 0.02924401611393501,
          "standard_error": 0.011206420269010904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.860411664838999,
            "upper_bound": 7.885748613934075
          },
          "point_estimate": 7.872415509841641,
          "standard_error": 0.006489140145096213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013709212633369274,
            "upper_bound": 0.055407021446609674
          },
          "point_estimate": 0.03791932627040081,
          "standard_error": 0.013133799336887096
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.195382313707333,
            "upper_bound": 7.232351703454363
          },
          "point_estimate": 7.215523654283129,
          "standard_error": 0.009504131581620652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.1975844942914815,
            "upper_bound": 7.239008958233445
          },
          "point_estimate": 7.220857878669214,
          "standard_error": 0.009965869321335788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020399889546016876,
            "upper_bound": 0.04715227459432276
          },
          "point_estimate": 0.02825703113962275,
          "standard_error": 0.011411371765745802
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.217169212310561,
            "upper_bound": 7.240957883372219
          },
          "point_estimate": 7.229806365318885,
          "standard_error": 0.006125682928343085
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01266582334892806,
            "upper_bound": 0.043062814421530954
          },
          "point_estimate": 0.03165983166911092,
          "standard_error": 0.0083163317502855
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.356050598100301,
            "upper_bound": 8.411642771683482
          },
          "point_estimate": 8.382796837185913,
          "standard_error": 0.014222954360824176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.351132398387914,
            "upper_bound": 8.424634408433885
          },
          "point_estimate": 8.370760591816861,
          "standard_error": 0.01600728690026656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004683894545744716,
            "upper_bound": 0.08772132036380682
          },
          "point_estimate": 0.04060479187545437,
          "standard_error": 0.020894831343982817
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.352755291287641,
            "upper_bound": 8.398099743709347
          },
          "point_estimate": 8.371919182282408,
          "standard_error": 0.011820713474408792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022663704205769183,
            "upper_bound": 0.060227703101532575
          },
          "point_estimate": 0.04729283682756641,
          "standard_error": 0.009413629244945603
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.492707113253367,
            "upper_bound": 22.53951325226574
          },
          "point_estimate": 22.51402795756973,
          "standard_error": 0.011994304110530474
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.485333468678355,
            "upper_bound": 22.530771887518384
          },
          "point_estimate": 22.51146180808296,
          "standard_error": 0.01099588410172462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006581409697569981,
            "upper_bound": 0.05619485239025993
          },
          "point_estimate": 0.03114525306011278,
          "standard_error": 0.012522612200211246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.49321434344615,
            "upper_bound": 22.522420798043136
          },
          "point_estimate": 22.50600485287145,
          "standard_error": 0.007517240155375483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015961876269760637,
            "upper_bound": 0.056134706142198704
          },
          "point_estimate": 0.03999355353158571,
          "standard_error": 0.01145778432535105
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.68917184615058,
            "upper_bound": 13.72320290330507
          },
          "point_estimate": 13.706653232508694,
          "standard_error": 0.008693356820429094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.683370694579825,
            "upper_bound": 13.734960228654598
          },
          "point_estimate": 13.71031684473747,
          "standard_error": 0.012260458839510642
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006347963560479568,
            "upper_bound": 0.04804305909159989
          },
          "point_estimate": 0.03824332093067634,
          "standard_error": 0.011220633859657669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.70528370360483,
            "upper_bound": 13.731294721284616
          },
          "point_estimate": 13.71949096673838,
          "standard_error": 0.006596437840284644
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016576525409541805,
            "upper_bound": 0.0363172364632822
          },
          "point_estimate": 0.028913740719381537,
          "standard_error": 0.005092838140395877
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.343695522356256,
            "upper_bound": 18.369891734855152
          },
          "point_estimate": 18.355935778928696,
          "standard_error": 0.006701823765304411
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.339912446889667,
            "upper_bound": 18.368077248063777
          },
          "point_estimate": 18.354978979603175,
          "standard_error": 0.0066870025047047555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003882517076678314,
            "upper_bound": 0.0360853355016654
          },
          "point_estimate": 0.017217204327625843,
          "standard_error": 0.008081984430611174
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.34689153901684,
            "upper_bound": 18.385455509260748
          },
          "point_estimate": 18.366890260045356,
          "standard_error": 0.010601743168582324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009766903493292484,
            "upper_bound": 0.030346947028711636
          },
          "point_estimate": 0.02238637166475232,
          "standard_error": 0.005582154044315644
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.764095331905995,
            "upper_bound": 10.784340367018824
          },
          "point_estimate": 10.774289267900034,
          "standard_error": 0.00514879234346828
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.766751876986028,
            "upper_bound": 10.784568777210836
          },
          "point_estimate": 10.772544193126809,
          "standard_error": 0.004588020514893188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028004457395226475,
            "upper_bound": 0.029034377895343383
          },
          "point_estimate": 0.011474331449006303,
          "standard_error": 0.006295070683062361
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.770679827727804,
            "upper_bound": 10.7793284977298
          },
          "point_estimate": 10.774751575836698,
          "standard_error": 0.0021755584939807133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006783125789419724,
            "upper_bound": 0.023684663673647483
          },
          "point_estimate": 0.01720868570680089,
          "standard_error": 0.00429372185744615
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.08490688075223,
            "upper_bound": 16.11687901962593
          },
          "point_estimate": 16.099212672902503,
          "standard_error": 0.008268025989462566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.078711632110874,
            "upper_bound": 16.114981971245292
          },
          "point_estimate": 16.093923919539428,
          "standard_error": 0.008442682848501604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025501622467594656,
            "upper_bound": 0.03925488632845805
          },
          "point_estimate": 0.019077151330067123,
          "standard_error": 0.009819415167876456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.081883436029884,
            "upper_bound": 16.09901794712301
          },
          "point_estimate": 16.090576851060884,
          "standard_error": 0.004430510982732066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009642940461861102,
            "upper_bound": 0.03773783743721116
          },
          "point_estimate": 0.027551996305145675,
          "standard_error": 0.007796543431776334
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.25029331850473,
            "upper_bound": 22.277496047279055
          },
          "point_estimate": 22.264454685501953,
          "standard_error": 0.006964717229144957
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.24536601545318,
            "upper_bound": 22.281455315692092
          },
          "point_estimate": 22.266145662709988,
          "standard_error": 0.0071327629313069334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003465400012262702,
            "upper_bound": 0.045204306200077406
          },
          "point_estimate": 0.013060142722813537,
          "standard_error": 0.010787496542845464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.257052168919905,
            "upper_bound": 22.274903781189085
          },
          "point_estimate": 22.266309905952795,
          "standard_error": 0.0045472022068150176
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010888537094492468,
            "upper_bound": 0.02957492034743658
          },
          "point_estimate": 0.02319655563526299,
          "standard_error": 0.004793907517531032
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.99612144321268,
            "upper_bound": 11.013119267686834
          },
          "point_estimate": 11.00479536285796,
          "standard_error": 0.004349877982502384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.993907696644428,
            "upper_bound": 11.018342452908117
          },
          "point_estimate": 11.004635033223687,
          "standard_error": 0.006256601331904486
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003380604503870494,
            "upper_bound": 0.026019963096107244
          },
          "point_estimate": 0.014210017497113724,
          "standard_error": 0.005375591257320618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.996366284870744,
            "upper_bound": 11.00996619588847
          },
          "point_estimate": 11.002225785960931,
          "standard_error": 0.003447881612114104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008402750927218624,
            "upper_bound": 0.01842329694968843
          },
          "point_estimate": 0.0145337253602122,
          "standard_error": 0.002613617177372222
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.98073035930012,
            "upper_bound": 24.001313561799737
          },
          "point_estimate": 23.991957126071615,
          "standard_error": 0.005257738595573102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.983825932300405,
            "upper_bound": 24.00362183310766
          },
          "point_estimate": 23.99482787039109,
          "standard_error": 0.005177329557148858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038186672907487658,
            "upper_bound": 0.025819870501338917
          },
          "point_estimate": 0.013991470101586356,
          "standard_error": 0.005679410146263501
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.986308177453644,
            "upper_bound": 24.0026511081015
          },
          "point_estimate": 23.994241053367187,
          "standard_error": 0.004115259588364744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007462778172239876,
            "upper_bound": 0.024195124104554232
          },
          "point_estimate": 0.01748567150016841,
          "standard_error": 0.0047288825967358545
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.794704979338764,
            "upper_bound": 20.8565820559469
          },
          "point_estimate": 20.823345936467064,
          "standard_error": 0.015897231409219177
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.784962535914993,
            "upper_bound": 20.85871560320393
          },
          "point_estimate": 20.806004989288255,
          "standard_error": 0.017666814164205725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006349903555889575,
            "upper_bound": 0.08237401723182756
          },
          "point_estimate": 0.04211026699924227,
          "standard_error": 0.019342826088987988
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.778871403937924,
            "upper_bound": 20.831330390133417
          },
          "point_estimate": 20.80200208728903,
          "standard_error": 0.01360509994337243
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02170339506560272,
            "upper_bound": 0.07044021246869647
          },
          "point_estimate": 0.05325319947572208,
          "standard_error": 0.012941286218655815
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169253.2664484126,
            "upper_bound": 1172244.8088019902
          },
          "point_estimate": 1170573.89843626,
          "standard_error": 770.3022115727578
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169004.0703125,
            "upper_bound": 1171505.6888392856
          },
          "point_estimate": 1169949.865625,
          "standard_error": 602.4390423857117
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.2134176669297,
            "upper_bound": 3434.145657156663
          },
          "point_estimate": 1615.5378408497777,
          "standard_error": 771.6186357620535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168689.8370593772,
            "upper_bound": 1171200.03312631
          },
          "point_estimate": 1169890.7974025975,
          "standard_error": 640.9467730456814
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 895.6248707293365,
            "upper_bound": 3643.412955422194
          },
          "point_estimate": 2563.0681802036665,
          "standard_error": 806.4488374320999
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552825.5758990576,
            "upper_bound": 1556268.5940672124
          },
          "point_estimate": 1554259.5368419313,
          "standard_error": 904.9220867385368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552453.3680555555,
            "upper_bound": 1555015.988839286
          },
          "point_estimate": 1553274.4479166665,
          "standard_error": 752.9545305584154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 346.48567301546973,
            "upper_bound": 3169.7327768513014
          },
          "point_estimate": 1711.8701602332294,
          "standard_error": 710.66885150055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552915.582295989,
            "upper_bound": 1554853.3235224015
          },
          "point_estimate": 1553885.4332251083,
          "standard_error": 509.50117427312296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 886.6529062569481,
            "upper_bound": 4459.54398930894
          },
          "point_estimate": 3017.3071143526713,
          "standard_error": 1130.300820242735
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1434082.0274697803,
            "upper_bound": 1435309.0848717946
          },
          "point_estimate": 1434709.461584249,
          "standard_error": 313.4469285928024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1433889.5370879122,
            "upper_bound": 1435691.9134615385
          },
          "point_estimate": 1434626.987179487,
          "standard_error": 478.9619036579546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.7734983586392,
            "upper_bound": 1817.1914250458249
          },
          "point_estimate": 1367.9655337909862,
          "standard_error": 453.5075100939837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1433914.5847415267,
            "upper_bound": 1435507.9121157324
          },
          "point_estimate": 1434891.7568431569,
          "standard_error": 404.404254323216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.9375960253726,
            "upper_bound": 1270.2015812577315
          },
          "point_estimate": 1045.4768493570343,
          "standard_error": 168.8779135349409
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386427.44197135675,
            "upper_bound": 387559.64347207453
          },
          "point_estimate": 386974.0098193177,
          "standard_error": 289.6683570454793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386067.4053191489,
            "upper_bound": 388174.0478723404
          },
          "point_estimate": 386739.63785460993,
          "standard_error": 504.86067636744104
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.07005964054383,
            "upper_bound": 1593.307343521644
          },
          "point_estimate": 1039.4723783010002,
          "standard_error": 397.8306912495902
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386091.9661934372,
            "upper_bound": 386929.17851944594
          },
          "point_estimate": 386394.44531638577,
          "standard_error": 217.20648528735083
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 519.1970961314594,
            "upper_bound": 1119.7301453344992
          },
          "point_estimate": 964.0745102608169,
          "standard_error": 142.245446312029
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559698.8106320513,
            "upper_bound": 560354.2034761905
          },
          "point_estimate": 560036.4662527472,
          "standard_error": 167.8833809034941
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559655.658974359,
            "upper_bound": 560526.7408791209
          },
          "point_estimate": 560007.4166666667,
          "standard_error": 214.64595994367795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.600767598744845,
            "upper_bound": 994.0102928143464
          },
          "point_estimate": 619.1228874697462,
          "standard_error": 240.5381695928355
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559854.124342896,
            "upper_bound": 560569.5039943762
          },
          "point_estimate": 560279.8787612388,
          "standard_error": 184.6963024217817
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303.9576340045274,
            "upper_bound": 708.3298964727695
          },
          "point_estimate": 559.3487408687764,
          "standard_error": 106.18946929660734
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414055.16239407466,
            "upper_bound": 414416.388131223
          },
          "point_estimate": 414237.126801948,
          "standard_error": 92.31819766821702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414022.8436688312,
            "upper_bound": 414467.67850378784
          },
          "point_estimate": 414241.1681818182,
          "standard_error": 116.5198174802248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.98107239042615,
            "upper_bound": 523.8365469499847
          },
          "point_estimate": 295.56529020719563,
          "standard_error": 111.46944222483
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413946.9204635573,
            "upper_bound": 414313.55738338886
          },
          "point_estimate": 414151.4781877214,
          "standard_error": 94.06797326283905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.66663450623085,
            "upper_bound": 397.3154524557666
          },
          "point_estimate": 307.92317177360576,
          "standard_error": 59.05033852564996
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479181.66874269006,
            "upper_bound": 479697.2888038848
          },
          "point_estimate": 479444.5021214496,
          "standard_error": 132.0119022697321
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479122.9714912281,
            "upper_bound": 479829.2894736842
          },
          "point_estimate": 479419.9893274854,
          "standard_error": 181.9167982837836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.963889251370947,
            "upper_bound": 786.3190978821509
          },
          "point_estimate": 428.4462488847978,
          "standard_error": 180.7597089532131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479247.2011287603,
            "upper_bound": 479696.30048864713
          },
          "point_estimate": 479458.1914900889,
          "standard_error": 113.0185219509724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.65453867996095,
            "upper_bound": 555.5935645819291
          },
          "point_estimate": 439.9254655093693,
          "standard_error": 79.24311890294389
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 973427.7133114034,
            "upper_bound": 976605.9807977496
          },
          "point_estimate": 975069.252801796,
          "standard_error": 812.109901895382
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 973070.8730263158,
            "upper_bound": 977592.552631579
          },
          "point_estimate": 974923.9140350876,
          "standard_error": 943.597027537471
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 410.9799640193918,
            "upper_bound": 4905.3892842935875
          },
          "point_estimate": 2542.1751590779672,
          "standard_error": 1361.8985003490209
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 974261.670895052,
            "upper_bound": 977478.2020629464
          },
          "point_estimate": 976026.957621326,
          "standard_error": 823.3720441821539
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1477.7576607020533,
            "upper_bound": 3431.497134043908
          },
          "point_estimate": 2708.538173404485,
          "standard_error": 515.8996173066002
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297447.36848625634,
            "upper_bound": 299066.15
          },
          "point_estimate": 298169.0315911731,
          "standard_error": 418.8902598958285
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297250.28193960513,
            "upper_bound": 299201.01219512196
          },
          "point_estimate": 297574.47967479675,
          "standard_error": 473.46980170115546
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.9072657910775,
            "upper_bound": 2256.1133623850324
          },
          "point_estimate": 595.8794869819918,
          "standard_error": 516.3913210571459
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297313.0395106755,
            "upper_bound": 298727.77371273714
          },
          "point_estimate": 297897.0174849541,
          "standard_error": 383.6932931409593
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299.29372793099355,
            "upper_bound": 1896.0859963659664
          },
          "point_estimate": 1390.491474713525,
          "standard_error": 389.95120081996816
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806578.5967160494,
            "upper_bound": 807624.8053015873
          },
          "point_estimate": 807074.8457001763,
          "standard_error": 267.86975023715524
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806335.8802469135,
            "upper_bound": 807709.6698412698
          },
          "point_estimate": 806980.4333333333,
          "standard_error": 336.45184917682167
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.24338775868006,
            "upper_bound": 1472.1855325301342
          },
          "point_estimate": 936.8473927009676,
          "standard_error": 339.00332477664233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806366.5971014493,
            "upper_bound": 807559.205394525
          },
          "point_estimate": 806849.0877922078,
          "standard_error": 305.756077368989
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 434.2502890372415,
            "upper_bound": 1140.7125463824525
          },
          "point_estimate": 894.6110094114508,
          "standard_error": 179.48688742119938
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398308.8391519798,
            "upper_bound": 399117.3900287914
          },
          "point_estimate": 398698.8628515355,
          "standard_error": 206.83289485327023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398244.19720496895,
            "upper_bound": 399186.89628623193
          },
          "point_estimate": 398629.9998641304,
          "standard_error": 230.4357713658867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.95302257328495,
            "upper_bound": 1181.6148647557748
          },
          "point_estimate": 484.6672263954145,
          "standard_error": 273.59162649281495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398404.1745456637,
            "upper_bound": 398803.74789712054
          },
          "point_estimate": 398596.828458498,
          "standard_error": 101.75124912988512
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 304.2046057400089,
            "upper_bound": 916.5546643578084
          },
          "point_estimate": 690.8675474154172,
          "standard_error": 154.0172078276781
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165378.41466370397,
            "upper_bound": 165615.97034049773
          },
          "point_estimate": 165492.22474466712,
          "standard_error": 61.07052353899491
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165333.53141564317,
            "upper_bound": 165650.84909502263
          },
          "point_estimate": 165447.3116515837,
          "standard_error": 85.9795709511943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.9030410002094,
            "upper_bound": 329.31379797704284
          },
          "point_estimate": 210.8535569353316,
          "standard_error": 72.646055123499
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165357.7664472024,
            "upper_bound": 165608.69291263778
          },
          "point_estimate": 165501.77781042486,
          "standard_error": 63.7197991221875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.6077971014366,
            "upper_bound": 255.14062313152505
          },
          "point_estimate": 203.65589347262815,
          "standard_error": 36.689882519572706
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232291.03297239917,
            "upper_bound": 232596.78496107575
          },
          "point_estimate": 232442.33621524615,
          "standard_error": 78.36560345790714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232262.61279193207,
            "upper_bound": 232678.77239915077
          },
          "point_estimate": 232353.4542108988,
          "standard_error": 133.21203843618324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.56834195565353,
            "upper_bound": 449.7747171741472
          },
          "point_estimate": 320.4875138962077,
          "standard_error": 112.3472693565776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232284.56821211672,
            "upper_bound": 232617.22935243117
          },
          "point_estimate": 232445.52457606088,
          "standard_error": 88.6480699927532
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.674579621238,
            "upper_bound": 319.81151606666833
          },
          "point_estimate": 260.8032506289046,
          "standard_error": 40.93927205378638
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172370.31177048068,
            "upper_bound": 172813.5453405364
          },
          "point_estimate": 172576.11878733165,
          "standard_error": 113.73986387029304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172370.64170616114,
            "upper_bound": 172870.0758293839
          },
          "point_estimate": 172487.01311780635,
          "standard_error": 102.20745763844128
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.5595070762122,
            "upper_bound": 602.0314513971276
          },
          "point_estimate": 151.64103186550787,
          "standard_error": 145.3958504692977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172408.6284071499,
            "upper_bound": 172557.13381430245
          },
          "point_estimate": 172459.54086292855,
          "standard_error": 38.63358199682208
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.08560576385868,
            "upper_bound": 493.5833964312285
          },
          "point_estimate": 380.26685683602096,
          "standard_error": 92.86816087160032
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649189.6535565476,
            "upper_bound": 649940.2388505881
          },
          "point_estimate": 649577.5433411283,
          "standard_error": 193.06364967118267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649052.3194444445,
            "upper_bound": 649999.9832589286
          },
          "point_estimate": 649782.2842261905,
          "standard_error": 253.9520882579019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.65896531173182,
            "upper_bound": 1087.4503098605123
          },
          "point_estimate": 462.4520542897732,
          "standard_error": 273.5953164283244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649235.4786782797,
            "upper_bound": 650006.999310535
          },
          "point_estimate": 649583.0389146567,
          "standard_error": 193.6221558889263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323.0395321477205,
            "upper_bound": 812.627605209423
          },
          "point_estimate": 643.2871271550015,
          "standard_error": 121.304348374064
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1311.9097330132138,
            "upper_bound": 1313.800697450578
          },
          "point_estimate": 1312.869748741242,
          "standard_error": 0.48506293552953406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1311.7734493465232,
            "upper_bound": 1314.182544768575
          },
          "point_estimate": 1312.6602236623582,
          "standard_error": 0.7052775026926152
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3689246291642817,
            "upper_bound": 2.906815012658227
          },
          "point_estimate": 1.6002450149144711,
          "standard_error": 0.633703883832346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.9521398905904,
            "upper_bound": 1314.5775190196932
          },
          "point_estimate": 1313.9359998799669,
          "standard_error": 0.41124113067376433
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9167945681485156,
            "upper_bound": 2.0833999986698992
          },
          "point_estimate": 1.61734721899967,
          "standard_error": 0.3058816073134461
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331737.2459711593,
            "upper_bound": 1333948.1076287203
          },
          "point_estimate": 1332841.2144883785,
          "standard_error": 565.3783580770876
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331151.142857143,
            "upper_bound": 1334240.892857143
          },
          "point_estimate": 1333045.9873015871,
          "standard_error": 720.3252409636448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302.26139255050083,
            "upper_bound": 3370.863127655315
          },
          "point_estimate": 1943.4691171635009,
          "standard_error": 720.2239424818866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331839.733997473,
            "upper_bound": 1334222.566448781
          },
          "point_estimate": 1333020.7989795918,
          "standard_error": 593.2555877715496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1022.275029010775,
            "upper_bound": 2462.8027207719733
          },
          "point_estimate": 1891.3773747623045,
          "standard_error": 370.9567784576922
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537034.3887291667,
            "upper_bound": 1539950.1351190477
          },
          "point_estimate": 1538361.5952579365,
          "standard_error": 750.7430714734396
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536514.0555555555,
            "upper_bound": 1540032.1714285717
          },
          "point_estimate": 1537747.465277778,
          "standard_error": 793.1201446363762
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452.0437023915552,
            "upper_bound": 3936.651958860365
          },
          "point_estimate": 1792.7094385897158,
          "standard_error": 883.5981672437088
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537052.300170068,
            "upper_bound": 1539623.456284349
          },
          "point_estimate": 1538243.407142857,
          "standard_error": 654.0649586104074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 974.1365186589328,
            "upper_bound": 3322.9859075835207
          },
          "point_estimate": 2499.196186285727,
          "standard_error": 627.1193323747743
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1439179.1280217492,
            "upper_bound": 1445981.480525755
          },
          "point_estimate": 1441933.003475275,
          "standard_error": 1821.736625740824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438349.2740384615,
            "upper_bound": 1443103.776923077
          },
          "point_estimate": 1439454.7432692307,
          "standard_error": 1385.4892452361685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.03325339246553,
            "upper_bound": 5665.547665185632
          },
          "point_estimate": 1827.0019058333744,
          "standard_error": 1464.5957048204618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438703.921730769,
            "upper_bound": 1441010.6482510248
          },
          "point_estimate": 1439734.7694305694,
          "standard_error": 589.8580403387379
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1365.857721713023,
            "upper_bound": 9111.15283618711
          },
          "point_estimate": 6080.252241680128,
          "standard_error": 2476.3774170783136
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765479.306574074,
            "upper_bound": 767106.9928761574
          },
          "point_estimate": 766240.7272511574,
          "standard_error": 416.34955162922176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765224.1527777778,
            "upper_bound": 766903.20703125
          },
          "point_estimate": 766215.1166666667,
          "standard_error": 370.8039663830357
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.65552918579107,
            "upper_bound": 2152.726367423892
          },
          "point_estimate": 1047.722513899253,
          "standard_error": 522.457382875406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765623.8186111111,
            "upper_bound": 766734.821225785
          },
          "point_estimate": 766279.2130411256,
          "standard_error": 277.5198089018602
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552.1869510151695,
            "upper_bound": 1918.4740142247072
          },
          "point_estimate": 1389.1478267647033,
          "standard_error": 366.20621463079016
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269368.82319459144,
            "upper_bound": 269736.8113632496
          },
          "point_estimate": 269533.6550443857,
          "standard_error": 94.40930287450136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269330.1417695473,
            "upper_bound": 269782.3425925926
          },
          "point_estimate": 269386.1053968254,
          "standard_error": 100.30430855730606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.822636021523905,
            "upper_bound": 415.6469026208442
          },
          "point_estimate": 131.10881064151545,
          "standard_error": 98.57007541566324
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269326.85078478814,
            "upper_bound": 269626.0759446315
          },
          "point_estimate": 269417.63172679173,
          "standard_error": 80.33421948938326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.7563272665844,
            "upper_bound": 395.8622115183192
          },
          "point_estimate": 314.1477291188861,
          "standard_error": 84.90264374084317
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483994.5439035088,
            "upper_bound": 484546.1505811403
          },
          "point_estimate": 484268.38338815793,
          "standard_error": 141.42578447766812
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483867.6820175439,
            "upper_bound": 484720.100877193
          },
          "point_estimate": 484254.322368421,
          "standard_error": 192.0784411994896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.76570716831554,
            "upper_bound": 825.73795691918
          },
          "point_estimate": 631.8980894394444,
          "standard_error": 201.92605621568876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483748.4412334268,
            "upper_bound": 484337.6324306896
          },
          "point_estimate": 483964.19548872183,
          "standard_error": 151.45386887088887
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277.9543827059169,
            "upper_bound": 576.4257324846013
          },
          "point_estimate": 470.3570157696926,
          "standard_error": 75.92438606869068
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246478.46931022097,
            "upper_bound": 246939.44925997424
          },
          "point_estimate": 246684.06239596737,
          "standard_error": 118.74764553451944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246446.0716216216,
            "upper_bound": 246782.46762387388
          },
          "point_estimate": 246647.86036036036,
          "standard_error": 98.333794123793
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.364709889771056,
            "upper_bound": 492.4131275191753
          },
          "point_estimate": 220.15232193612124,
          "standard_error": 113.86231026245036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246440.4575314919,
            "upper_bound": 246677.5446555231
          },
          "point_estimate": 246546.7600737101,
          "standard_error": 59.46011355824738
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126.94084559153264,
            "upper_bound": 570.0571901544006
          },
          "point_estimate": 396.1486449977108,
          "standard_error": 128.77813192409374
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 694898.768831368,
            "upper_bound": 695463.536522911
          },
          "point_estimate": 695181.6212331537,
          "standard_error": 144.59081897048216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 694733.0974842767,
            "upper_bound": 695606.060691824
          },
          "point_estimate": 695178.3026729559,
          "standard_error": 242.1344685653684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.17422400729782,
            "upper_bound": 817.3238471405825
          },
          "point_estimate": 481.7488322492558,
          "standard_error": 183.33494821135827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695056.237102982,
            "upper_bound": 695654.7615637903
          },
          "point_estimate": 695398.0394021074,
          "standard_error": 156.02313118468138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 302.58111236498553,
            "upper_bound": 585.5559715307173
          },
          "point_estimate": 484.1752160834804,
          "standard_error": 72.48041548023913
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2290.00397858082,
            "upper_bound": 2293.58350854095
          },
          "point_estimate": 2291.958532429738,
          "standard_error": 0.9184334451011896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2290.5897317765857,
            "upper_bound": 2293.8662227832124
          },
          "point_estimate": 2292.414239507731,
          "standard_error": 0.7967671451191654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.36270313072275034,
            "upper_bound": 4.305057521129567
          },
          "point_estimate": 2.4231543659421275,
          "standard_error": 1.0191004536408337
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2291.3235876818767,
            "upper_bound": 2293.9344899735293
          },
          "point_estimate": 2292.390282648875,
          "standard_error": 0.6675753562018794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2042777739194146,
            "upper_bound": 4.295357265516249
          },
          "point_estimate": 3.06476840062576,
          "standard_error": 0.8857975871385412
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706.5040600940604,
            "upper_bound": 708.8813024474498
          },
          "point_estimate": 707.6119557025479,
          "standard_error": 0.6055435523173508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706.2019073531658,
            "upper_bound": 709.7362938489943
          },
          "point_estimate": 706.6754639976679,
          "standard_error": 0.8913256016224448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09280762362360909,
            "upper_bound": 3.27292960422021
          },
          "point_estimate": 0.7066752758936912,
          "standard_error": 0.7905276371957991
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706.092558052729,
            "upper_bound": 709.4748682019888
          },
          "point_estimate": 707.4180695526416,
          "standard_error": 0.884417247789604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.451426830034712,
            "upper_bound": 2.372881652666582
          },
          "point_estimate": 2.0163585417207086,
          "standard_error": 0.39937857787261055
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.88980497486554,
            "upper_bound": 142.16427758402446
          },
          "point_estimate": 142.02243083959166,
          "standard_error": 0.07035444883114925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.82157798411714,
            "upper_bound": 142.20157327388716
          },
          "point_estimate": 141.98968679033626,
          "standard_error": 0.09294734788275986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05464856625007005,
            "upper_bound": 0.40393614068111294
          },
          "point_estimate": 0.2385490973819077,
          "standard_error": 0.09252533742128402
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.8583778653631,
            "upper_bound": 142.11635538360338
          },
          "point_estimate": 142.0073900232341,
          "standard_error": 0.06644130552274218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11993102173636884,
            "upper_bound": 0.28854262709292117
          },
          "point_estimate": 0.2344067404375677,
          "standard_error": 0.04160568841148057
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.972742086163166,
            "upper_bound": 53.03399686214356
          },
          "point_estimate": 52.999350908679766,
          "standard_error": 0.015870598708607155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.967516783953286,
            "upper_bound": 53.016783695299296
          },
          "point_estimate": 52.981319379332035,
          "standard_error": 0.01349025634848536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006282284962132632,
            "upper_bound": 0.06522620381210893
          },
          "point_estimate": 0.02360487002629589,
          "standard_error": 0.016140744216212824
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.9760600379406,
            "upper_bound": 53.0160110073808
          },
          "point_estimate": 52.99720824410276,
          "standard_error": 0.01012894681870553
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01568338712601362,
            "upper_bound": 0.07512653080856385
          },
          "point_estimate": 0.052893790384233294,
          "standard_error": 0.017049731692183278
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.93067601651029,
            "upper_bound": 81.06418646957783
          },
          "point_estimate": 80.99125014519602,
          "standard_error": 0.034342118473667436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.9099510206633,
            "upper_bound": 81.08130778252044
          },
          "point_estimate": 80.95076249964919,
          "standard_error": 0.03560221917127126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014178399131537149,
            "upper_bound": 0.1595622453128094
          },
          "point_estimate": 0.06724885611117085,
          "standard_error": 0.03568750087117705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.92256716298853,
            "upper_bound": 81.00518196522326
          },
          "point_estimate": 80.95552599803176,
          "standard_error": 0.021369881861878427
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03418003434219594,
            "upper_bound": 0.14706527313419496
          },
          "point_estimate": 0.11512829728942224,
          "standard_error": 0.02971936632006362
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291930.83239528566,
            "upper_bound": 292696.7781216667
          },
          "point_estimate": 292300.02407174604,
          "standard_error": 194.06973752110048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291951.284,
            "upper_bound": 292527.3061587302
          },
          "point_estimate": 292354.667,
          "standard_error": 170.59260920629882
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.41609078569022,
            "upper_bound": 1027.8343459923085
          },
          "point_estimate": 338.5500943095343,
          "standard_error": 236.2675216170875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292011.0508064516,
            "upper_bound": 292482.0708560062
          },
          "point_estimate": 292297.0234181818,
          "standard_error": 120.47648898557372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 235.5407016999228,
            "upper_bound": 913.0236278279737
          },
          "point_estimate": 647.587772027792,
          "standard_error": 175.89457996538744
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.00318606032215,
            "upper_bound": 63.08512355721059
          },
          "point_estimate": 63.042066496983566,
          "standard_error": 0.021038342422169648
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.982554911006346,
            "upper_bound": 63.11541032645946
          },
          "point_estimate": 63.02354991425038,
          "standard_error": 0.031026945973128432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013514175759734915,
            "upper_bound": 0.11298776728396344
          },
          "point_estimate": 0.06398861997469288,
          "standard_error": 0.026612152564344696
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.004546137370966,
            "upper_bound": 63.073841485513896
          },
          "point_estimate": 63.034473188549086,
          "standard_error": 0.01748318731900755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03473300599484448,
            "upper_bound": 0.08569961776999464
          },
          "point_estimate": 0.07022860072896817,
          "standard_error": 0.01233323146394972
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.98404851234322,
            "upper_bound": 107.26710496139889
          },
          "point_estimate": 107.11996968041352,
          "standard_error": 0.07216538844036123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.93056047948976,
            "upper_bound": 107.2514115005436
          },
          "point_estimate": 107.1242074440631,
          "standard_error": 0.07795041922280313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05503160253406441,
            "upper_bound": 0.4141693676206485
          },
          "point_estimate": 0.17841861424814742,
          "standard_error": 0.09097085913176528
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.96369819320398,
            "upper_bound": 107.19013339114665
          },
          "point_estimate": 107.09711672793551,
          "standard_error": 0.05758192732813651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1104492114278218,
            "upper_bound": 0.32565420541882445
          },
          "point_estimate": 0.2411326101918176,
          "standard_error": 0.05604594606748464
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128939.34395601146,
            "upper_bound": 129090.6523955167
          },
          "point_estimate": 129009.91500605088,
          "standard_error": 38.82927518596615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128920.5145390071,
            "upper_bound": 129107.62844759654
          },
          "point_estimate": 128976.7741578014,
          "standard_error": 41.41313645321567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.682925262200985,
            "upper_bound": 215.77883191382867
          },
          "point_estimate": 82.30194853884268,
          "standard_error": 47.76536097045496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128941.29811531476,
            "upper_bound": 129138.216045392
          },
          "point_estimate": 129030.44813484389,
          "standard_error": 51.70571103582896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.78975460528711,
            "upper_bound": 166.2196378502247
          },
          "point_estimate": 129.41266176191357,
          "standard_error": 29.3954282647066
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.12239335566596,
            "upper_bound": 61.20669214150439
          },
          "point_estimate": 61.16170321990302,
          "standard_error": 0.021729182216105335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.11507006015337,
            "upper_bound": 61.217596917034626
          },
          "point_estimate": 61.12694768587939,
          "standard_error": 0.028517568086438717
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00522570331771309,
            "upper_bound": 0.1127084204779773
          },
          "point_estimate": 0.032327633924291097,
          "standard_error": 0.02981479844827473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.12381880584174,
            "upper_bound": 61.18182177510964
          },
          "point_estimate": 61.143648025419054,
          "standard_error": 0.015045749050366677
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024012461765671615,
            "upper_bound": 0.08814516715644324
          },
          "point_estimate": 0.07231013733038011,
          "standard_error": 0.014930463485953102
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.12491251851938,
            "upper_bound": 104.33789120845506
          },
          "point_estimate": 104.2249600310125,
          "standard_error": 0.05473250763641464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.10891144725244,
            "upper_bound": 104.33748053910276
          },
          "point_estimate": 104.19126740709038,
          "standard_error": 0.054097162187812735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023169271104804563,
            "upper_bound": 0.3013392668844068
          },
          "point_estimate": 0.12747439437725117,
          "standard_error": 0.07045415181719226
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.12909324982812,
            "upper_bound": 104.3579720700474
          },
          "point_estimate": 104.22132863603274,
          "standard_error": 0.05905982122781409
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.071302635038039,
            "upper_bound": 0.2389277109737667
          },
          "point_estimate": 0.18212713037901404,
          "standard_error": 0.04250379244780258
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.9431376868258,
            "upper_bound": 255.30358789348776
          },
          "point_estimate": 255.11650572974003,
          "standard_error": 0.0923671715070053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.88229252230812,
            "upper_bound": 255.37379683649613
          },
          "point_estimate": 255.03062250523232,
          "standard_error": 0.14300837594565816
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07101190889956624,
            "upper_bound": 0.5322580779856747
          },
          "point_estimate": 0.3232163859822992,
          "standard_error": 0.1209311856305622
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.85208017625425,
            "upper_bound": 255.1372780559304
          },
          "point_estimate": 254.97055387166384,
          "standard_error": 0.07235449058716448
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17486825941660455,
            "upper_bound": 0.39388627140102866
          },
          "point_estimate": 0.30895493678146524,
          "standard_error": 0.05785616207923778
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.9829696407991,
            "upper_bound": 524.8710560197858
          },
          "point_estimate": 524.4190575862771,
          "standard_error": 0.22689094892129552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.6595375306152,
            "upper_bound": 524.8857389836787
          },
          "point_estimate": 524.4234116921994,
          "standard_error": 0.29163053129088967
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12473085169710237,
            "upper_bound": 1.4007253012618792
          },
          "point_estimate": 0.7444413189161723,
          "standard_error": 0.3126342331791635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 524.0904490826794,
            "upper_bound": 524.8300983535781
          },
          "point_estimate": 524.4488093148748,
          "standard_error": 0.18580910853806143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4134607913778955,
            "upper_bound": 0.9830892033171176
          },
          "point_estimate": 0.7579386972955717,
          "standard_error": 0.1509899280660709
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.551390990521746,
            "upper_bound": 32.58741930315734
          },
          "point_estimate": 32.56984959043591,
          "standard_error": 0.009246377578559872
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.5393459831664,
            "upper_bound": 32.59549722085108
          },
          "point_estimate": 32.57659534555873,
          "standard_error": 0.01540067201475486
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019198343708505452,
            "upper_bound": 0.05047504833283062
          },
          "point_estimate": 0.03521408530821689,
          "standard_error": 0.012853883986919942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.560795756357116,
            "upper_bound": 32.59469824296588
          },
          "point_estimate": 32.57853310300018,
          "standard_error": 0.009042734928800752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018002127793028216,
            "upper_bound": 0.03707805157997106
          },
          "point_estimate": 0.030822217035390446,
          "standard_error": 0.004815817230354409
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33007.071691025434,
            "upper_bound": 33065.22961977724
          },
          "point_estimate": 33034.279674422796,
          "standard_error": 14.93710005664999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32995.06803030303,
            "upper_bound": 33077.58525974026
          },
          "point_estimate": 33019.16604545455,
          "standard_error": 20.75014701506175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.070070001756599,
            "upper_bound": 84.32647704457878
          },
          "point_estimate": 39.05543794299269,
          "standard_error": 19.67940744627596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33001.024903260004,
            "upper_bound": 33081.56890305267
          },
          "point_estimate": 33038.512613931525,
          "standard_error": 21.057344929008288
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.048594826617837,
            "upper_bound": 61.114393074944765
          },
          "point_estimate": 49.77722196467869,
          "standard_error": 9.88990648449134
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.004299728396,
            "upper_bound": 150.92203092345693
          },
          "point_estimate": 150.42300277048497,
          "standard_error": 0.2353699258922708
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.02231945119644,
            "upper_bound": 150.75191376260895
          },
          "point_estimate": 150.22527867920024,
          "standard_error": 0.18322709702463008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045082135730187205,
            "upper_bound": 1.1107584701562254
          },
          "point_estimate": 0.3881712752533582,
          "standard_error": 0.2750963997305507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.14959408882453,
            "upper_bound": 150.59138957560862
          },
          "point_estimate": 150.35233258072344,
          "standard_error": 0.11130844771875834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2769909041628166,
            "upper_bound": 1.109002609673464
          },
          "point_estimate": 0.7846745660100868,
          "standard_error": 0.23436046898059928
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978394.7074300334,
            "upper_bound": 979319.6128289473
          },
          "point_estimate": 978835.7363043024,
          "standard_error": 235.10876579953495
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978269.8421052631,
            "upper_bound": 979111.5361842106
          },
          "point_estimate": 978873.1118421052,
          "standard_error": 199.06371554783033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.88460711665638,
            "upper_bound": 1266.2448840986458
          },
          "point_estimate": 470.7807640103721,
          "standard_error": 317.6980737768669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978222.9061784896,
            "upper_bound": 979172.243465475
          },
          "point_estimate": 978732.8604237868,
          "standard_error": 242.5682880804209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 313.02523135848355,
            "upper_bound": 1096.2984822838716
          },
          "point_estimate": 784.4163169758626,
          "standard_error": 205.5351778482085
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.1932307278855,
            "upper_bound": 1988.9131689897056
          },
          "point_estimate": 1986.9539866994623,
          "standard_error": 0.9559542583219246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.858977862804,
            "upper_bound": 1989.3819622847773
          },
          "point_estimate": 1985.797235127995,
          "standard_error": 1.2363795615495836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3445285727302369,
            "upper_bound": 5.359594177600447
          },
          "point_estimate": 2.9274494970844844,
          "standard_error": 1.2913341567407213
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.1530128689783,
            "upper_bound": 1988.1169969176976
          },
          "point_estimate": 1986.2180780356568,
          "standard_error": 0.7579245284477875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5503674432601802,
            "upper_bound": 4.114011088643664
          },
          "point_estimate": 3.1846193378718435,
          "standard_error": 0.6692663588357765
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.70049539433517,
            "upper_bound": 45.76177270648804
          },
          "point_estimate": 45.73163480240737,
          "standard_error": 0.01566636774525691
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.689363643730445,
            "upper_bound": 45.771866054447
          },
          "point_estimate": 45.737551309718185,
          "standard_error": 0.018128124673170334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009198432632590566,
            "upper_bound": 0.09138700014994408
          },
          "point_estimate": 0.04174217153466482,
          "standard_error": 0.02158423482192743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.68760228070891,
            "upper_bound": 45.74589663794052
          },
          "point_estimate": 45.71468768725061,
          "standard_error": 0.014569726144341346
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027554343394576672,
            "upper_bound": 0.06744274267466191
          },
          "point_estimate": 0.05229410647305653,
          "standard_error": 0.010194412835964414
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.56326087373258,
            "upper_bound": 49.65368027849395
          },
          "point_estimate": 49.60376735279192,
          "standard_error": 0.02323321775879724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.54125280137504,
            "upper_bound": 49.63911430002126
          },
          "point_estimate": 49.601432160966795,
          "standard_error": 0.02471127071163604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004116143904614529,
            "upper_bound": 0.11054590352708078
          },
          "point_estimate": 0.06045909731259251,
          "standard_error": 0.02549535659122505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.57940636702112,
            "upper_bound": 49.6655333353926
          },
          "point_estimate": 49.61336015966089,
          "standard_error": 0.02190519258010158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03259942245625272,
            "upper_bound": 0.10947278518377336
          },
          "point_estimate": 0.07756067851290233,
          "standard_error": 0.02319523226182846
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.9779612306748,
            "upper_bound": 36.03581413656955
          },
          "point_estimate": 36.00506577691363,
          "standard_error": 0.014908735670536893
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.96553272004949,
            "upper_bound": 36.03552034851988
          },
          "point_estimate": 35.99659202400209,
          "standard_error": 0.022336128320160232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008710502281620144,
            "upper_bound": 0.09153563881989958
          },
          "point_estimate": 0.051881828064015985,
          "standard_error": 0.019717980667485077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.96412395867559,
            "upper_bound": 36.023457596547665
          },
          "point_estimate": 35.99742587553661,
          "standard_error": 0.015077190782041843
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026743071001871777,
            "upper_bound": 0.06551219213951227
          },
          "point_estimate": 0.049722111664417895,
          "standard_error": 0.010892957983886249
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.82704693626256,
            "upper_bound": 40.88170928470708
          },
          "point_estimate": 40.854470283654265,
          "standard_error": 0.01394757097843717
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.81842013785376,
            "upper_bound": 40.89140866428998
          },
          "point_estimate": 40.848172774563494,
          "standard_error": 0.018800388521452327
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010214846007197667,
            "upper_bound": 0.08015479723555448
          },
          "point_estimate": 0.055015605228079925,
          "standard_error": 0.018525096121106005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.83312081593797,
            "upper_bound": 40.87589760839383
          },
          "point_estimate": 40.85340974087117,
          "standard_error": 0.010841526031535554
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026539490218732063,
            "upper_bound": 0.058773971673817024
          },
          "point_estimate": 0.04655713096986403,
          "standard_error": 0.008260797918725903
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.301732523346374,
            "upper_bound": 48.38794247772585
          },
          "point_estimate": 48.34223146541152,
          "standard_error": 0.022135691317297132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.2847556867784,
            "upper_bound": 48.391700539079615
          },
          "point_estimate": 48.321871951554016,
          "standard_error": 0.029701828175948426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019450141943185652,
            "upper_bound": 0.12193660968701252
          },
          "point_estimate": 0.06696857883721558,
          "standard_error": 0.02646741625802454
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.295762250012814,
            "upper_bound": 48.35246043819007
          },
          "point_estimate": 48.32054322363672,
          "standard_error": 0.014547175302809813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.035589156080746906,
            "upper_bound": 0.09507847463683138
          },
          "point_estimate": 0.07371585683259287,
          "standard_error": 0.01561318579822151
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.6236201294949,
            "upper_bound": 74.79861227080336
          },
          "point_estimate": 74.69624211160678,
          "standard_error": 0.04594741770120855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.5967289809499,
            "upper_bound": 74.72714061102617
          },
          "point_estimate": 74.67039151977613,
          "standard_error": 0.034756860938768216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019086493081468873,
            "upper_bound": 0.15343526160683738
          },
          "point_estimate": 0.08860853075962843,
          "standard_error": 0.03332453853848883
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.6068467287553,
            "upper_bound": 74.93378408914329
          },
          "point_estimate": 74.75145179209497,
          "standard_error": 0.09709650198760604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04434961296801846,
            "upper_bound": 0.2287409865774694
          },
          "point_estimate": 0.15358372174396567,
          "standard_error": 0.05907759514359783
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.69349614767262,
            "upper_bound": 88.9770686854211
          },
          "point_estimate": 88.79886729425282,
          "standard_error": 0.08112713089603345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.67731625819229,
            "upper_bound": 88.7675937956499
          },
          "point_estimate": 88.73377789359753,
          "standard_error": 0.029373018145974407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010304222364719704,
            "upper_bound": 0.11385668154520938
          },
          "point_estimate": 0.053322853023376166,
          "standard_error": 0.03138052994677872
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.68138849235385,
            "upper_bound": 88.73420863392843
          },
          "point_estimate": 88.70670149902901,
          "standard_error": 0.01343812414664294
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03234274660367007,
            "upper_bound": 0.4167943181982745
          },
          "point_estimate": 0.27095878120747574,
          "standard_error": 0.1350736448823731
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.02509544240396,
            "upper_bound": 63.21137035076764
          },
          "point_estimate": 63.1014140202014,
          "standard_error": 0.04946458978943576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.00416715995905,
            "upper_bound": 63.12278685822661
          },
          "point_estimate": 63.06728283238041,
          "standard_error": 0.036621732532528754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00949064325742466,
            "upper_bound": 0.16071158982105835
          },
          "point_estimate": 0.08337926960478173,
          "standard_error": 0.03537468838245311
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.01681097599085,
            "upper_bound": 63.120265864570555
          },
          "point_estimate": 63.0706562858353,
          "standard_error": 0.028444746457419495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0448725227569117,
            "upper_bound": 0.24714979677045704
          },
          "point_estimate": 0.16488029206965074,
          "standard_error": 0.06622234352597195
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.94275722862474,
            "upper_bound": 107.07259450162049
          },
          "point_estimate": 107.00839477223704,
          "standard_error": 0.03309902371498662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.92347358093434,
            "upper_bound": 107.0840650962789
          },
          "point_estimate": 107.01106811731408,
          "standard_error": 0.030042493859741143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012259422315365358,
            "upper_bound": 0.21602912079200248
          },
          "point_estimate": 0.056875593194347075,
          "standard_error": 0.05965383270385449
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106.96387172622106,
            "upper_bound": 107.0399783237417
          },
          "point_estimate": 107.00902707846998,
          "standard_error": 0.01916563374002015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.056348993078956855,
            "upper_bound": 0.1423995296106507
          },
          "point_estimate": 0.11035348225547904,
          "standard_error": 0.022557763382719018
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.27780609087739,
            "upper_bound": 65.3890909124856
          },
          "point_estimate": 65.33205512592993,
          "standard_error": 0.028594179545738178
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.25627604686991,
            "upper_bound": 65.45313860910231
          },
          "point_estimate": 65.30485265577397,
          "standard_error": 0.044532299501413075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007612618450383883,
            "upper_bound": 0.16340776407676996
          },
          "point_estimate": 0.09532824319191471,
          "standard_error": 0.04235588301179053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.25353675052152,
            "upper_bound": 65.32074231141529
          },
          "point_estimate": 65.28584469607206,
          "standard_error": 0.017054169752127205
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05164582333561665,
            "upper_bound": 0.11289455253193108
          },
          "point_estimate": 0.09475009447475262,
          "standard_error": 0.014636479657309936
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.216267688063056,
            "upper_bound": 61.93151341222574
          },
          "point_estimate": 61.523746137167585,
          "standard_error": 0.18430216532625565
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.15971449669432,
            "upper_bound": 61.92995883999761
          },
          "point_estimate": 61.26899605915919,
          "standard_error": 0.1615673185866583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028645117126989705,
            "upper_bound": 0.8672589205563406
          },
          "point_estimate": 0.16716859884580368,
          "standard_error": 0.16304474346240092
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.17502109311171,
            "upper_bound": 61.31209599132716
          },
          "point_estimate": 61.218425292185835,
          "standard_error": 0.03595011043881325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08119720901113696,
            "upper_bound": 0.789171350957689
          },
          "point_estimate": 0.6123057976963645,
          "standard_error": 0.1927048039187164
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.09608837180062,
            "upper_bound": 104.32797559395934
          },
          "point_estimate": 104.20439750536454,
          "standard_error": 0.059551132939797864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.07182760602056,
            "upper_bound": 104.33907737728175
          },
          "point_estimate": 104.14389048790912,
          "standard_error": 0.08302168671302518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01791517669275712,
            "upper_bound": 0.34182056206406547
          },
          "point_estimate": 0.1709102810320327,
          "standard_error": 0.08261591418245343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104.09850047428556,
            "upper_bound": 104.29961692206764
          },
          "point_estimate": 104.17117887433074,
          "standard_error": 0.05146372833509947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09926131232494588,
            "upper_bound": 0.26627955884485627
          },
          "point_estimate": 0.19889250486418536,
          "standard_error": 0.04657989667198351
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1215938.9177037035,
            "upper_bound": 1218109.2761251323
          },
          "point_estimate": 1217041.3891719575,
          "standard_error": 555.0199111634796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1215011.1166666667,
            "upper_bound": 1218408.7272486773
          },
          "point_estimate": 1217456.3616666668,
          "standard_error": 884.1119100803697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.03842912618947,
            "upper_bound": 3262.548962578058
          },
          "point_estimate": 1520.7096568908778,
          "standard_error": 775.3914809060327
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1216517.6466738577,
            "upper_bound": 1218228.9130925024
          },
          "point_estimate": 1217370.2856277055,
          "standard_error": 434.6582300833904
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1076.0416056597926,
            "upper_bound": 2271.075128732017
          },
          "point_estimate": 1854.445000487514,
          "standard_error": 300.45841268422726
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1543567.3271726193,
            "upper_bound": 1545757.3707467755
          },
          "point_estimate": 1544710.715737434,
          "standard_error": 562.0238092883438
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1543376.5,
            "upper_bound": 1546159.357142857
          },
          "point_estimate": 1545085.4114583335,
          "standard_error": 682.7446164831578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303.7229596079316,
            "upper_bound": 3035.5506888860596
          },
          "point_estimate": 1948.9339247742596,
          "standard_error": 752.1357896673344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1543417.0652268762,
            "upper_bound": 1545623.811891127
          },
          "point_estimate": 1544641.866125541,
          "standard_error": 562.7525932553209
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 895.0546146109956,
            "upper_bound": 2345.9707832406134
          },
          "point_estimate": 1870.0409276975463,
          "standard_error": 359.74235600117646
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1516293.5613888889,
            "upper_bound": 1519667.64077877
          },
          "point_estimate": 1518190.0910565476,
          "standard_error": 875.0938424898235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1517157.9791666667,
            "upper_bound": 1519951.1909722222
          },
          "point_estimate": 1518853.902604167,
          "standard_error": 653.7202891436133
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.19240003276946,
            "upper_bound": 3702.714891763864
          },
          "point_estimate": 1669.522883693408,
          "standard_error": 921.2863500636238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1517598.308333333,
            "upper_bound": 1519775.7006231986
          },
          "point_estimate": 1518820.8914502163,
          "standard_error": 539.7995286041629
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 911.5244545122572,
            "upper_bound": 4134.644250683663
          },
          "point_estimate": 2925.153322586292,
          "standard_error": 929.2503421517235
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 678949.7340901875,
            "upper_bound": 679795.8426755411
          },
          "point_estimate": 679345.2367568542,
          "standard_error": 216.53009801173897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 678872.3206060606,
            "upper_bound": 679789.3036363636
          },
          "point_estimate": 679085.9150974026,
          "standard_error": 245.4248412374071
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.694182673260652,
            "upper_bound": 1161.9329180989012
          },
          "point_estimate": 551.0936420343207,
          "standard_error": 288.57323731605766
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 678979.3406695783,
            "upper_bound": 679520.2167059751
          },
          "point_estimate": 679155.9142384888,
          "standard_error": 140.73901058635877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318.1167732166294,
            "upper_bound": 964.863519727426
          },
          "point_estimate": 723.7175076287878,
          "standard_error": 173.74474498016482
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1474115.7040887135,
            "upper_bound": 1475604.8214316238
          },
          "point_estimate": 1474939.119302503,
          "standard_error": 385.1862646500271
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1474337.4763736264,
            "upper_bound": 1475828.0721153845
          },
          "point_estimate": 1475174.811965812,
          "standard_error": 381.2101864763264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.68096449040928,
            "upper_bound": 1720.9729043184984
          },
          "point_estimate": 889.1829983164428,
          "standard_error": 403.6111260024608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1474429.690995892,
            "upper_bound": 1475855.4299868364
          },
          "point_estimate": 1475215.513986014,
          "standard_error": 371.0437451078224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 480.64760510111313,
            "upper_bound": 1803.5638422814643
          },
          "point_estimate": 1285.8137875792058,
          "standard_error": 382.550885812505
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 438587.1468938253,
            "upper_bound": 439084.7582801205
          },
          "point_estimate": 438826.61485609104,
          "standard_error": 127.59470309539924
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 438563.7083333334,
            "upper_bound": 439185.7740963856
          },
          "point_estimate": 438679.6905120482,
          "standard_error": 170.63105973138832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.93565910345945,
            "upper_bound": 683.525229230423
          },
          "point_estimate": 338.3074322468498,
          "standard_error": 191.30790590075975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 438516.88255775394,
            "upper_bound": 439118.80600781506
          },
          "point_estimate": 438821.2674385855,
          "standard_error": 152.6375781936369
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.9260895741952,
            "upper_bound": 543.9440206149062
          },
          "point_estimate": 426.3139066075453,
          "standard_error": 84.21327316425759
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 595127.0849078341,
            "upper_bound": 595841.0657834101
          },
          "point_estimate": 595490.041359447,
          "standard_error": 182.48232188986032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 594998.2338709678,
            "upper_bound": 595982.5927419355
          },
          "point_estimate": 595542.5927419355,
          "standard_error": 259.08245530075425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195.32486024197513,
            "upper_bound": 1041.9808266624411
          },
          "point_estimate": 648.7882251752872,
          "standard_error": 218.42695737326252
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 595202.2718859172,
            "upper_bound": 595790.7004354161
          },
          "point_estimate": 595508.4537075828,
          "standard_error": 148.3286487729647
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 353.29107972068863,
            "upper_bound": 754.7544831097695
          },
          "point_estimate": 608.0829311163269,
          "standard_error": 102.6712569166422
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1053170.7372789115,
            "upper_bound": 1054780.846560034
          },
          "point_estimate": 1053988.351743764,
          "standard_error": 412.7194171733484
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1052830.075,
            "upper_bound": 1055313.234285714
          },
          "point_estimate": 1053832.2388888889,
          "standard_error": 619.5940919561436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.0815517732575,
            "upper_bound": 2478.115847671362
          },
          "point_estimate": 1803.8217313091843,
          "standard_error": 611.4432811938158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1053622.5759716865,
            "upper_bound": 1055266.486903706
          },
          "point_estimate": 1054560.1775881262,
          "standard_error": 424.4288131076819
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 820.6774813579568,
            "upper_bound": 1675.2296101053462
          },
          "point_estimate": 1374.145878866383,
          "standard_error": 220.67642193998975
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 739917.421427696,
            "upper_bound": 740979.9297222224
          },
          "point_estimate": 740415.333611111,
          "standard_error": 273.53358960281804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 739814.5355392157,
            "upper_bound": 740960.3045751634
          },
          "point_estimate": 740188.4431372549,
          "standard_error": 301.6420519477665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.84668710658374,
            "upper_bound": 1410.6744945633975
          },
          "point_estimate": 674.2225127361239,
          "standard_error": 333.0944781408214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 740007.0650452488,
            "upper_bound": 741043.6750404622
          },
          "point_estimate": 740490.8669722434,
          "standard_error": 270.7263274086918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.2981018061267,
            "upper_bound": 1191.984181549068
          },
          "point_estimate": 912.2409538454508,
          "standard_error": 210.47930862606492
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 818578.3759407408,
            "upper_bound": 821264.4849907408
          },
          "point_estimate": 819823.010074074,
          "standard_error": 693.0227050934352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 818383.487037037,
            "upper_bound": 821819.3666666667
          },
          "point_estimate": 818941.2555555556,
          "standard_error": 750.8232676495481
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 216.1246383853017,
            "upper_bound": 3658.028799057143
          },
          "point_estimate": 1404.9034983913352,
          "standard_error": 879.2664659294006
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 817780.0994808007,
            "upper_bound": 819704.9887301435
          },
          "point_estimate": 818649.0398845599,
          "standard_error": 510.76649870845023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 793.119230720745,
            "upper_bound": 2921.2526624057423
          },
          "point_estimate": 2308.348774122968,
          "standard_error": 532.7925746724056
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455870.59986359125,
            "upper_bound": 456558.7222938368
          },
          "point_estimate": 456179.21412748017,
          "standard_error": 176.8556607361336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455827.85875,
            "upper_bound": 456355.6007638889
          },
          "point_estimate": 456079.59661458334,
          "standard_error": 116.64209341918396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.360257823994694,
            "upper_bound": 777.9952628128024
          },
          "point_estimate": 229.6848512347874,
          "standard_error": 193.37622642196516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 455797.748319114,
            "upper_bound": 456377.07345
          },
          "point_estimate": 456135.2636038961,
          "standard_error": 148.59455024580555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.53698090459352,
            "upper_bound": 841.6911751986596
          },
          "point_estimate": 589.9346871267197,
          "standard_error": 186.7053790301865
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332494.9945004252,
            "upper_bound": 332912.0136177721
          },
          "point_estimate": 332697.67701211735,
          "standard_error": 106.55839815068563
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332406.8191964286,
            "upper_bound": 332872.9056122449
          },
          "point_estimate": 332718.29776785715,
          "standard_error": 116.78327597308544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.47092908621985,
            "upper_bound": 632.5936387692302
          },
          "point_estimate": 242.41239995973325,
          "standard_error": 146.19216434257876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332603.08227686695,
            "upper_bound": 332860.62500561355
          },
          "point_estimate": 332745.2184833024,
          "standard_error": 64.9608874021594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.14484054424574,
            "upper_bound": 477.9061978262928
          },
          "point_estimate": 354.6141108975411,
          "standard_error": 81.5828592992067
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252593.26673661737,
            "upper_bound": 253341.86438450328
          },
          "point_estimate": 252897.34397783253,
          "standard_error": 200.58164727571773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252503.01025862063,
            "upper_bound": 252975.69706896553
          },
          "point_estimate": 252706.70853858785,
          "standard_error": 131.9946342706991
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.93225636242842,
            "upper_bound": 581.4842364524461
          },
          "point_estimate": 329.3174432568935,
          "standard_error": 129.3605449368981
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 252545.01493069495,
            "upper_bound": 252873.03058006792
          },
          "point_estimate": 252694.26769368563,
          "standard_error": 83.943549569328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.07808567816198,
            "upper_bound": 1008.2334149740864
          },
          "point_estimate": 670.3307650389019,
          "standard_error": 276.2651120811388
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179220.65867816092,
            "upper_bound": 179588.1558045977
          },
          "point_estimate": 179394.59673176167,
          "standard_error": 94.21762119156004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179142.9328817734,
            "upper_bound": 179632.77339901478
          },
          "point_estimate": 179326.78560872626,
          "standard_error": 114.25723322635912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.98097644775221,
            "upper_bound": 534.6864734384494
          },
          "point_estimate": 255.60635210001365,
          "standard_error": 116.86024626375644
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179213.6322042824,
            "upper_bound": 179406.98010408322
          },
          "point_estimate": 179311.36650246306,
          "standard_error": 48.46504413336526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.58384097402822,
            "upper_bound": 397.01829163405114
          },
          "point_estimate": 313.0176878409654,
          "standard_error": 64.40772986935004
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3199204.815373263,
            "upper_bound": 3203180.6350694443
          },
          "point_estimate": 3201324.1065972215,
          "standard_error": 1018.648635496751
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3199838.645833333,
            "upper_bound": 3204154.697916667
          },
          "point_estimate": 3200931.8541666665,
          "standard_error": 1315.8790359111945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 381.4714288526894,
            "upper_bound": 5877.094248160812
          },
          "point_estimate": 2938.547124080406,
          "standard_error": 1381.1768565891316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3200301.6666666665,
            "upper_bound": 3202841.09595163
          },
          "point_estimate": 3201159.9404761903,
          "standard_error": 666.1666780619122
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1702.365857572203,
            "upper_bound": 4626.308341994459
          },
          "point_estimate": 3390.069401188103,
          "standard_error": 837.3801720357673
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6443.783977555353,
            "upper_bound": 6454.392581771307
          },
          "point_estimate": 6449.201795976141,
          "standard_error": 2.709168885411974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6444.0100500454955,
            "upper_bound": 6456.315397836417
          },
          "point_estimate": 6448.207834394904,
          "standard_error": 2.4339411099609873
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.44490140611437035,
            "upper_bound": 15.585751655355788
          },
          "point_estimate": 6.159589075358994,
          "standard_error": 4.370893486419655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6447.30666872448,
            "upper_bound": 6458.328942364296
          },
          "point_estimate": 6453.129534523711,
          "standard_error": 2.85438096878439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.118587113812074,
            "upper_bound": 12.0361576068394
          },
          "point_estimate": 9.028910640067126,
          "standard_error": 2.020689196444864
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331059.8756264877,
            "upper_bound": 1333098.3570238096
          },
          "point_estimate": 1332076.38,
          "standard_error": 522.6713909719404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1330510.6011904762,
            "upper_bound": 1333706.0
          },
          "point_estimate": 1332189.7848214284,
          "standard_error": 823.3391765488146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 637.8851086751407,
            "upper_bound": 2903.483193452734
          },
          "point_estimate": 2202.5787608964183,
          "standard_error": 608.2234505354223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331217.76435247,
            "upper_bound": 1333036.3323218797
          },
          "point_estimate": 1332029.8344155843,
          "standard_error": 458.3124348200882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1067.8498819979568,
            "upper_bound": 2148.4090371243187
          },
          "point_estimate": 1750.5345962855904,
          "standard_error": 275.53879760581054
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536847.1858559027,
            "upper_bound": 1538848.0299193948
          },
          "point_estimate": 1537900.6995205027,
          "standard_error": 511.4152720122203
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536625.0416666667,
            "upper_bound": 1539213.7239583335
          },
          "point_estimate": 1538213.5523148146,
          "standard_error": 569.7436597927788
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316.1459118871845,
            "upper_bound": 2945.040706048371
          },
          "point_estimate": 1313.1518871035742,
          "standard_error": 710.1995119292783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1537753.7274305555,
            "upper_bound": 1539516.6480269658
          },
          "point_estimate": 1538799.8404761904,
          "standard_error": 456.8177437239568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 760.5258526177782,
            "upper_bound": 2141.471967858909
          },
          "point_estimate": 1703.1702078189076,
          "standard_error": 345.19947800585385
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1437288.606153846,
            "upper_bound": 1439505.285213675
          },
          "point_estimate": 1438540.3336874235,
          "standard_error": 575.8834082071023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1437768.641025641,
            "upper_bound": 1439780.2606837607
          },
          "point_estimate": 1439030.7846153846,
          "standard_error": 446.26439557319424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.29843615426373,
            "upper_bound": 2283.3892844617367
          },
          "point_estimate": 1342.7730556481451,
          "standard_error": 628.3096907747157
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1438933.3559641372,
            "upper_bound": 1440023.5508895393
          },
          "point_estimate": 1439643.8105894106,
          "standard_error": 263.6959924215876
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642.527182859429,
            "upper_bound": 2768.148685761721
          },
          "point_estimate": 1924.9751983084516,
          "standard_error": 637.4997186054258
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765547.1938035714,
            "upper_bound": 766567.2486433531
          },
          "point_estimate": 766050.5107341269,
          "standard_error": 261.7108877472279
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765401.9809027778,
            "upper_bound": 766661.7375
          },
          "point_estimate": 765938.4040178572,
          "standard_error": 315.57605201275203
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.14391557545382,
            "upper_bound": 1478.1290081329644
          },
          "point_estimate": 865.4123577607944,
          "standard_error": 359.46089641651344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765778.2218019853,
            "upper_bound": 766513.6634631284
          },
          "point_estimate": 766179.5862554113,
          "standard_error": 185.606548692366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474.8880357077016,
            "upper_bound": 1107.19235443168
          },
          "point_estimate": 868.2495313201888,
          "standard_error": 162.79473233102314
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269380.5208888889,
            "upper_bound": 269888.49451490305
          },
          "point_estimate": 269607.4477272193,
          "standard_error": 131.8602239849849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269260.0444444445,
            "upper_bound": 269802.5523809524
          },
          "point_estimate": 269483.9203703704,
          "standard_error": 139.15006797991657
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.53895054261772,
            "upper_bound": 612.1975606127302
          },
          "point_estimate": 341.81671859818306,
          "standard_error": 146.42696213688674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269297.54113260435,
            "upper_bound": 270130.68142674695
          },
          "point_estimate": 269634.85537277535,
          "standard_error": 235.72185098342743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.5320730419695,
            "upper_bound": 610.2156553962819
          },
          "point_estimate": 439.850605387184,
          "standard_error": 126.86985436457095
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484086.8541834273,
            "upper_bound": 484440.50670687127
          },
          "point_estimate": 484268.7250777986,
          "standard_error": 90.24223980448996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484069.91318139096,
            "upper_bound": 484520.14473684214
          },
          "point_estimate": 484231.2218567252,
          "standard_error": 127.51965021156208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.695671823429876,
            "upper_bound": 534.2213655157406
          },
          "point_estimate": 383.0967267513249,
          "standard_error": 129.06540662321663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484128.9884014843,
            "upper_bound": 484444.805271969
          },
          "point_estimate": 484290.61213260423,
          "standard_error": 81.48113281343385
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.62432945960677,
            "upper_bound": 387.79243704619745
          },
          "point_estimate": 299.21076031265807,
          "standard_error": 58.7722774490934
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246257.09500536253,
            "upper_bound": 246590.6068484556
          },
          "point_estimate": 246418.67896369583,
          "standard_error": 85.36896739570676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246190.58783783784,
            "upper_bound": 246630.62711148648
          },
          "point_estimate": 246406.42173423423,
          "standard_error": 123.5585485844196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.56921770846568,
            "upper_bound": 499.4889328890712
          },
          "point_estimate": 280.0290752987684,
          "standard_error": 104.63329521640506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246155.5335346176,
            "upper_bound": 246600.26209153887
          },
          "point_estimate": 246387.28322218324,
          "standard_error": 114.79498253812596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162.28200866824037,
            "upper_bound": 362.4473834857219
          },
          "point_estimate": 285.5984951902383,
          "standard_error": 52.34025054920379
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695139.4217389937,
            "upper_bound": 696948.555211141
          },
          "point_estimate": 695942.8716793952,
          "standard_error": 466.4886679368602
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695039.9386792453,
            "upper_bound": 696522.7379454926
          },
          "point_estimate": 695540.9096698114,
          "standard_error": 413.01648213560486
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.8616924184435,
            "upper_bound": 2076.350292382605
          },
          "point_estimate": 1056.5560667769462,
          "standard_error": 455.5776280987736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695269.3825610398,
            "upper_bound": 696179.3479022371
          },
          "point_estimate": 695682.9013967165,
          "standard_error": 236.3245672310507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 567.3379931685342,
            "upper_bound": 2215.719699969777
          },
          "point_estimate": 1556.0115392962373,
          "standard_error": 487.4321335958036
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.7852243212814,
            "upper_bound": 357.37959830797706
          },
          "point_estimate": 357.0645763992612,
          "standard_error": 0.1527682139864961
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.56530139861513,
            "upper_bound": 357.4058102355872
          },
          "point_estimate": 357.00151967336694,
          "standard_error": 0.20938824741192583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04295494539172008,
            "upper_bound": 0.8745247839012967
          },
          "point_estimate": 0.6230691897856905,
          "standard_error": 0.19674417149054316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.8753169152376,
            "upper_bound": 357.2223085248412
          },
          "point_estimate": 357.03829119349626,
          "standard_error": 0.08788987699672962
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2615632587906835,
            "upper_bound": 0.6644996599450388
          },
          "point_estimate": 0.5087544878958197,
          "standard_error": 0.1098053004180725
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.92436681318014,
            "upper_bound": 170.4491923619831
          },
          "point_estimate": 170.14274893483514,
          "standard_error": 0.13811380711382204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.9052860499212,
            "upper_bound": 170.20548029268747
          },
          "point_estimate": 170.05820306817947,
          "standard_error": 0.07278484531049428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03669469154352558,
            "upper_bound": 0.4410996684360019
          },
          "point_estimate": 0.1624073617136666,
          "standard_error": 0.11024192399302596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.91397338601226,
            "upper_bound": 170.362592513042
          },
          "point_estimate": 170.0604487648129,
          "standard_error": 0.11737697536675576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11146049033040066,
            "upper_bound": 0.6843822617832888
          },
          "point_estimate": 0.4609431846385579,
          "standard_error": 0.17908540274790846
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.93237932488955,
            "upper_bound": 27.360353461118876
          },
          "point_estimate": 27.154406787784353,
          "standard_error": 0.10949430231314804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.908670339688616,
            "upper_bound": 27.383783308944547
          },
          "point_estimate": 27.200603122619157,
          "standard_error": 0.11566040692037892
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0529022768389141,
            "upper_bound": 0.605729903038634
          },
          "point_estimate": 0.3356988580247674,
          "standard_error": 0.14329469055223087
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.13562189735322,
            "upper_bound": 27.43771155460714
          },
          "point_estimate": 27.31567842734595,
          "standard_error": 0.07644630230288356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.17277951282043624,
            "upper_bound": 0.4809963070626159
          },
          "point_estimate": 0.36298088484301866,
          "standard_error": 0.08021240628846148
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.011423355411353,
            "upper_bound": 18.05638273566928
          },
          "point_estimate": 18.03510600467809,
          "standard_error": 0.0115535110458519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.002197097419874,
            "upper_bound": 18.068267660168072
          },
          "point_estimate": 18.041751558742035,
          "standard_error": 0.01600603934909126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008276897517573729,
            "upper_bound": 0.06701019243679389
          },
          "point_estimate": 0.036158641868940626,
          "standard_error": 0.015467362301331736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.018576659588454,
            "upper_bound": 18.059781458751484
          },
          "point_estimate": 18.04168228782963,
          "standard_error": 0.010465288500815568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019128894353928946,
            "upper_bound": 0.048311125707654855
          },
          "point_estimate": 0.03864924028095418,
          "standard_error": 0.007484542509316085
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.177139606693103,
            "upper_bound": 25.21418726354522
          },
          "point_estimate": 25.19532025545237,
          "standard_error": 0.00947389696050431
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.173724561035844,
            "upper_bound": 25.216662160540068
          },
          "point_estimate": 25.19313254756957,
          "standard_error": 0.01103146772214146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008052665512913398,
            "upper_bound": 0.0525217432053359
          },
          "point_estimate": 0.029966779940812326,
          "standard_error": 0.011373171090549168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.185941336508968,
            "upper_bound": 25.21820893124582
          },
          "point_estimate": 25.20224750936768,
          "standard_error": 0.008220359558193327
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016504303801685997,
            "upper_bound": 0.04097936897651262
          },
          "point_estimate": 0.03150613204587172,
          "standard_error": 0.006295060743803773
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292102.02568,
            "upper_bound": 292543.00491428573
          },
          "point_estimate": 292309.0238704762,
          "standard_error": 112.94783735709338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292038.7138,
            "upper_bound": 292518.52599999995
          },
          "point_estimate": 292232.2102857143,
          "standard_error": 109.78853848427296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.78948202731642,
            "upper_bound": 601.6865125179215
          },
          "point_estimate": 275.5121179086571,
          "standard_error": 149.98695548614748
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291987.21932369197,
            "upper_bound": 292369.19936798577
          },
          "point_estimate": 292181.0932155844,
          "standard_error": 96.92683138741516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.9529512593234,
            "upper_bound": 504.75340260195
          },
          "point_estimate": 377.009699541621,
          "standard_error": 89.31359656006478
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.069272596780774,
            "upper_bound": 16.087451277472574
          },
          "point_estimate": 16.078352823288935,
          "standard_error": 0.004645325901470293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.066035418006052,
            "upper_bound": 16.091437857916578
          },
          "point_estimate": 16.075133813351997,
          "standard_error": 0.006207206799558167
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001332013855686237,
            "upper_bound": 0.027263876188438216
          },
          "point_estimate": 0.01949128524147112,
          "standard_error": 0.007067353472089536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.06429087594645,
            "upper_bound": 16.08712202818119
          },
          "point_estimate": 16.075559816553273,
          "standard_error": 0.005836423944719312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008999550540415777,
            "upper_bound": 0.019297646553485485
          },
          "point_estimate": 0.015507771181122322,
          "standard_error": 0.0026402845997726063
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.2482135398474,
            "upper_bound": 22.302396077669016
          },
          "point_estimate": 22.27131434202171,
          "standard_error": 0.01410110521873014
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.245524905722423,
            "upper_bound": 22.282336093309972
          },
          "point_estimate": 22.261807283052597,
          "standard_error": 0.010708980131054875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007143657264178249,
            "upper_bound": 0.05302028338513318
          },
          "point_estimate": 0.02423908484491679,
          "standard_error": 0.011296502361756493
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.248772683689257,
            "upper_bound": 22.272907565478747
          },
          "point_estimate": 22.26201611854021,
          "standard_error": 0.006162362964239947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014238051455373005,
            "upper_bound": 0.06867796040072012
          },
          "point_estimate": 0.04693924703583115,
          "standard_error": 0.016707062077928166
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128764.05788133094,
            "upper_bound": 128945.3244787986
          },
          "point_estimate": 128856.27930506476,
          "standard_error": 46.56872265240536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128715.70892226147,
            "upper_bound": 128996.97173144876
          },
          "point_estimate": 128858.69567137808,
          "standard_error": 73.97962242420036
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.069206127109748,
            "upper_bound": 269.0045807365938
          },
          "point_estimate": 166.58032283765976,
          "standard_error": 59.95017282056559
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128774.42781439028,
            "upper_bound": 128955.09891181267
          },
          "point_estimate": 128882.203542747,
          "standard_error": 45.31703520657812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.86401199786508,
            "upper_bound": 188.2481510975291
          },
          "point_estimate": 155.30784363856318,
          "standard_error": 24.110924159432155
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.98469501439902,
            "upper_bound": 24.045349479490664
          },
          "point_estimate": 24.01181648554411,
          "standard_error": 0.015616740796897132
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.97979116240521,
            "upper_bound": 24.037863354879992
          },
          "point_estimate": 24.0037981890434,
          "standard_error": 0.011753850853146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004635028075033274,
            "upper_bound": 0.081329712967471
          },
          "point_estimate": 0.02623568088306893,
          "standard_error": 0.019354262482535432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.979935811152114,
            "upper_bound": 24.006070943535622
          },
          "point_estimate": 23.994692328665536,
          "standard_error": 0.006632461495530129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016782765090497052,
            "upper_bound": 0.07192636549441547
          },
          "point_estimate": 0.052102205915673175,
          "standard_error": 0.015135812580320703
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.787804865473607,
            "upper_bound": 20.838668052879715
          },
          "point_estimate": 20.811182590451313,
          "standard_error": 0.013083833996684497
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.77903866163174,
            "upper_bound": 20.833930171891794
          },
          "point_estimate": 20.800664067552542,
          "standard_error": 0.01208029608524912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037024970053988098,
            "upper_bound": 0.06343531279351997
          },
          "point_estimate": 0.032244950619749066,
          "standard_error": 0.017974658483888108
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.78759371704548,
            "upper_bound": 20.808239530501464
          },
          "point_estimate": 20.797538220355065,
          "standard_error": 0.005117947628562151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01823051962367178,
            "upper_bound": 0.06020753958463669
          },
          "point_estimate": 0.04380553536627989,
          "standard_error": 0.01178705297615845
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.46997593763916,
            "upper_bound": 40.54251992069867
          },
          "point_estimate": 40.50394737212099,
          "standard_error": 0.018671892501933973
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.46030861769741,
            "upper_bound": 40.55273153662259
          },
          "point_estimate": 40.49622749953325,
          "standard_error": 0.018258810189101183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008410921614767851,
            "upper_bound": 0.10601294856851316
          },
          "point_estimate": 0.037554682023187846,
          "standard_error": 0.026100866001200887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.48050060559663,
            "upper_bound": 40.56023573485997
          },
          "point_estimate": 40.51989864871855,
          "standard_error": 0.022372791257663845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025257489507492344,
            "upper_bound": 0.08014039529016244
          },
          "point_estimate": 0.0624063648415256,
          "standard_error": 0.013989101314058503
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.33006325760275,
            "upper_bound": 73.44398853363576
          },
          "point_estimate": 73.38188303066966,
          "standard_error": 0.0292630105183461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.29213564825473,
            "upper_bound": 73.43307085845674
          },
          "point_estimate": 73.3601875774905,
          "standard_error": 0.03137335031495562
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008346938477330618,
            "upper_bound": 0.1474805717251363
          },
          "point_estimate": 0.09567314731342091,
          "standard_error": 0.03695398975476889
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.34397496965867,
            "upper_bound": 73.40370255904051
          },
          "point_estimate": 73.37324313215005,
          "standard_error": 0.014913232731503728
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04080740828487146,
            "upper_bound": 0.1316010277475785
          },
          "point_estimate": 0.09727885938210004,
          "standard_error": 0.024916535698609067
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.004131967719532,
            "upper_bound": 11.019134506937238
          },
          "point_estimate": 11.011091486818088,
          "standard_error": 0.0038472942540650826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.00075212346474,
            "upper_bound": 11.018794171489985
          },
          "point_estimate": 11.008163528771968,
          "standard_error": 0.004593251927457259
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024598497215011597,
            "upper_bound": 0.02016956211378022
          },
          "point_estimate": 0.01201380865703293,
          "standard_error": 0.004656894912379601
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.002655593976073,
            "upper_bound": 11.01892714354122
          },
          "point_estimate": 11.010619932906208,
          "standard_error": 0.004206746771323343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005343654079188769,
            "upper_bound": 0.016790571206535675
          },
          "point_estimate": 0.012872931156940966,
          "standard_error": 0.0029637232675330956
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32811.38949510606,
            "upper_bound": 32868.32044906569
          },
          "point_estimate": 32839.5303206177,
          "standard_error": 14.595657836232371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32801.40584187262,
            "upper_bound": 32875.02682338758
          },
          "point_estimate": 32841.12337251356,
          "standard_error": 18.394885541142983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.753661717848331,
            "upper_bound": 82.98849522647897
          },
          "point_estimate": 53.796171323408046,
          "standard_error": 18.28778528728004
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32792.095132029994,
            "upper_bound": 32851.27641092233
          },
          "point_estimate": 32815.8166858458,
          "standard_error": 14.881992581731666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.90472475957646,
            "upper_bound": 62.585982371755144
          },
          "point_estimate": 48.64291926744403,
          "standard_error": 9.27177644252346
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.5661505260849,
            "upper_bound": 30.60695745762308
          },
          "point_estimate": 30.585322868633188,
          "standard_error": 0.010510160617812457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.55441429168103,
            "upper_bound": 30.61019640409375
          },
          "point_estimate": 30.580851565617067,
          "standard_error": 0.015197719720144146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003948942462110672,
            "upper_bound": 0.05943034072418378
          },
          "point_estimate": 0.03322916186216565,
          "standard_error": 0.01470753422526513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.563322672327047,
            "upper_bound": 30.61637623497474
          },
          "point_estimate": 30.586796840776643,
          "standard_error": 0.013910387754152096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017670350553689818,
            "upper_bound": 0.04512193951087981
          },
          "point_estimate": 0.03504794496136972,
          "standard_error": 0.007349810162757661
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978163.9275992066,
            "upper_bound": 979309.3742379908
          },
          "point_estimate": 978732.9847702588,
          "standard_error": 292.8734061225811
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977762.6666666667,
            "upper_bound": 979536.9520676692
          },
          "point_estimate": 978905.9206140352,
          "standard_error": 467.3664805039914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.90320731466304,
            "upper_bound": 1689.759723070952
          },
          "point_estimate": 1422.2960867667473,
          "standard_error": 434.5767320180865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978295.8514390616,
            "upper_bound": 979732.8543451652
          },
          "point_estimate": 979065.6909774436,
          "standard_error": 379.925446866791
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 600.7470127698487,
            "upper_bound": 1178.872467857806
          },
          "point_estimate": 974.6298557194644,
          "standard_error": 149.1625617838688
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.2727381273005,
            "upper_bound": 1960.245520244187
          },
          "point_estimate": 1958.6146476344377,
          "standard_error": 0.7630262269208249
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1956.6279558308645,
            "upper_bound": 1960.2040560193911
          },
          "point_estimate": 1958.2965526528417,
          "standard_error": 0.8683165323579604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09215438942233402,
            "upper_bound": 3.5830763813106823
          },
          "point_estimate": 2.3682468203308735,
          "standard_error": 1.0265613408228726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.037436775452,
            "upper_bound": 1960.1182874525923
          },
          "point_estimate": 1958.3545653915169,
          "standard_error": 0.7787310365651869
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9436122757846104,
            "upper_bound": 3.3864775566124097
          },
          "point_estimate": 2.539772444688078,
          "standard_error": 0.6655017237224804
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.880824661997235,
            "upper_bound": 7.924763356345035
          },
          "point_estimate": 7.901242189516786,
          "standard_error": 0.011293055425911571
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.883161753327898,
            "upper_bound": 7.924064577972136
          },
          "point_estimate": 7.889294344525616,
          "standard_error": 0.011152095972202148
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008904421846802564,
            "upper_bound": 0.05886945030844837
          },
          "point_estimate": 0.013164703720877089,
          "standard_error": 0.016215042821320785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.887370782512113,
            "upper_bound": 7.9088097833495725
          },
          "point_estimate": 7.896236711027831,
          "standard_error": 0.005459669601695229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014314799087560996,
            "upper_bound": 0.05259794987311151
          },
          "point_estimate": 0.03776604196861323,
          "standard_error": 0.01037924876276414
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.199120883983142,
            "upper_bound": 7.227625107984172
          },
          "point_estimate": 7.212522486053073,
          "standard_error": 0.007317082012407655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.194645072323818,
            "upper_bound": 7.224790809648673
          },
          "point_estimate": 7.213818080571276,
          "standard_error": 0.007096346138887232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001957774451930119,
            "upper_bound": 0.03824031889950792
          },
          "point_estimate": 0.016705139848825504,
          "standard_error": 0.008775797031934969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.1990215971348706,
            "upper_bound": 7.219955816704629
          },
          "point_estimate": 7.20957089865779,
          "standard_error": 0.005530939801489567
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00987749445758872,
            "upper_bound": 0.03394120225337726
          },
          "point_estimate": 0.02430927437499741,
          "standard_error": 0.006375473694902523
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.354316412144781,
            "upper_bound": 8.37971427536841
          },
          "point_estimate": 8.366731090897558,
          "standard_error": 0.006487041067538656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.351437101345057,
            "upper_bound": 8.381530723070053
          },
          "point_estimate": 8.364209509178284,
          "standard_error": 0.007284177302005551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004745887593723604,
            "upper_bound": 0.036077084182539944
          },
          "point_estimate": 0.020233109098376487,
          "standard_error": 0.008076582613133399
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.356437309781306,
            "upper_bound": 8.37380880019755
          },
          "point_estimate": 8.365261146423098,
          "standard_error": 0.004423981268382912
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010984066103283311,
            "upper_bound": 0.028154362664523477
          },
          "point_estimate": 0.021594723264026654,
          "standard_error": 0.0044338487284600455
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.489710680970287,
            "upper_bound": 22.54244673826479
          },
          "point_estimate": 22.510999207362392,
          "standard_error": 0.014122259688321736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.484664183923712,
            "upper_bound": 22.52130448584864
          },
          "point_estimate": 22.496970717076124,
          "standard_error": 0.008571376695799337
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017411974866549257,
            "upper_bound": 0.04033561797624554
          },
          "point_estimate": 0.020919357367158137,
          "standard_error": 0.011122161237566772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.488599280547284,
            "upper_bound": 22.514420738229575
          },
          "point_estimate": 22.5011968524584,
          "standard_error": 0.006613295315712356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009951315251055193,
            "upper_bound": 0.0703996364839542
          },
          "point_estimate": 0.04693910146115451,
          "standard_error": 0.01921510483016281
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.709374243637344,
            "upper_bound": 13.782743048808538
          },
          "point_estimate": 13.737150005886916,
          "standard_error": 0.02054727208471598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.705100724470626,
            "upper_bound": 13.73773189870466
          },
          "point_estimate": 13.713374504550874,
          "standard_error": 0.008963763331092361
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001828094071377195,
            "upper_bound": 0.03439804332616696
          },
          "point_estimate": 0.01251272937686224,
          "standard_error": 0.01033675984653193
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.706922953558395,
            "upper_bound": 13.735155107118445
          },
          "point_estimate": 13.717099334425807,
          "standard_error": 0.007708304022841058
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006945320847297061,
            "upper_bound": 0.10490044680338168
          },
          "point_estimate": 0.0686131754774168,
          "standard_error": 0.032495916528441804
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.32033565966127,
            "upper_bound": 18.351459033262888
          },
          "point_estimate": 18.336078147626957,
          "standard_error": 0.007969900954599712
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.31848494936045,
            "upper_bound": 18.35947546579308
          },
          "point_estimate": 18.331906321873863,
          "standard_error": 0.012225185253360462
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006261579619648015,
            "upper_bound": 0.0477539004235674
          },
          "point_estimate": 0.0306996793262542,
          "standard_error": 0.01072336816624346
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.325399077015312,
            "upper_bound": 18.35665656291544
          },
          "point_estimate": 18.34178255570036,
          "standard_error": 0.008095386494376492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015806932725147962,
            "upper_bound": 0.033478761455597396
          },
          "point_estimate": 0.026464550569190105,
          "standard_error": 0.004656844360200851
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.768869211599595,
            "upper_bound": 10.784009853323106
          },
          "point_estimate": 10.776128285051085,
          "standard_error": 0.003888700407720218
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.764877233889209,
            "upper_bound": 10.78587499229489
          },
          "point_estimate": 10.775488592880953,
          "standard_error": 0.005038981874932273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032092395909136756,
            "upper_bound": 0.02179445439726495
          },
          "point_estimate": 0.014577073304853527,
          "standard_error": 0.00478555935437914
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.76514186374855,
            "upper_bound": 10.785530260363204
          },
          "point_estimate": 10.774314743230018,
          "standard_error": 0.0052471145696527475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006961417145783306,
            "upper_bound": 0.01682384446210473
          },
          "point_estimate": 0.013013968209476772,
          "standard_error": 0.002617931050769665
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.05940374621181,
            "upper_bound": 16.094358665770677
          },
          "point_estimate": 16.078105742477916,
          "standard_error": 0.008943114873422949
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.05710309832057,
            "upper_bound": 16.10083241694276
          },
          "point_estimate": 16.085600651810736,
          "standard_error": 0.01028583215061645
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003511832222497178,
            "upper_bound": 0.049208249875582984
          },
          "point_estimate": 0.02911227751155909,
          "standard_error": 0.011391401046101549
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.063919874459305,
            "upper_bound": 16.10068001725595
          },
          "point_estimate": 16.082606306302953,
          "standard_error": 0.009873164973335909
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013395733751072,
            "upper_bound": 0.039656170179110946
          },
          "point_estimate": 0.02991986383711335,
          "standard_error": 0.007034612066662337
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.255448946594704,
            "upper_bound": 22.28640876061925
          },
          "point_estimate": 22.270029272938658,
          "standard_error": 0.007936884457864802
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.252945650370087,
            "upper_bound": 22.29372486605948
          },
          "point_estimate": 22.26211217874331,
          "standard_error": 0.009457407477534516
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031981261048121853,
            "upper_bound": 0.043941328006577525
          },
          "point_estimate": 0.017574685141332543,
          "standard_error": 0.0109533564692324
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.25266110703903,
            "upper_bound": 22.29447867671173
          },
          "point_estimate": 22.26803596250412,
          "standard_error": 0.01133656133726411
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011856573075689708,
            "upper_bound": 0.03465681100377132
          },
          "point_estimate": 0.026500184238626,
          "standard_error": 0.005915358149008214
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.003069072991416,
            "upper_bound": 11.01636268679567
          },
          "point_estimate": 11.009470958085544,
          "standard_error": 0.0034027392663587936
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.9994235218781,
            "upper_bound": 11.018354686917538
          },
          "point_estimate": 11.009887139675238,
          "standard_error": 0.004645710673421924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016834208348368582,
            "upper_bound": 0.019104899570687856
          },
          "point_estimate": 0.012593655879417407,
          "standard_error": 0.004228571452771534
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.000627573889282,
            "upper_bound": 11.01159048687396
          },
          "point_estimate": 11.005404799932911,
          "standard_error": 0.0027852775962277786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006317051516310517,
            "upper_bound": 0.014535307353644895
          },
          "point_estimate": 0.011334118035461402,
          "standard_error": 0.002166269102895261
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.97735120861302,
            "upper_bound": 24.049009035638885
          },
          "point_estimate": 24.006906290212022,
          "standard_error": 0.018872844258337546
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.97088239407097,
            "upper_bound": 24.02792991193955
          },
          "point_estimate": 23.98600927567419,
          "standard_error": 0.01221785475920951
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00270104518612299,
            "upper_bound": 0.061648377992009375
          },
          "point_estimate": 0.017992952315242162,
          "standard_error": 0.01488339221554047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.97378691289108,
            "upper_bound": 23.990268867419136
          },
          "point_estimate": 23.981937441856257,
          "standard_error": 0.00423926866000837
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008950096447640329,
            "upper_bound": 0.09078578728358572
          },
          "point_estimate": 0.06284135718490805,
          "standard_error": 0.02358561383616263
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.793935670377213,
            "upper_bound": 20.828287790580124
          },
          "point_estimate": 20.810752476982323,
          "standard_error": 0.008782924290838755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.79039414557378,
            "upper_bound": 20.83195287454523
          },
          "point_estimate": 20.807810394772435,
          "standard_error": 0.01190169328162706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007293155888667868,
            "upper_bound": 0.05000560895114701
          },
          "point_estimate": 0.030315055926794444,
          "standard_error": 0.010968092803826034
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.785044044398923,
            "upper_bound": 20.818809552703502
          },
          "point_estimate": 20.79865132151535,
          "standard_error": 0.008457993960459317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015593790396182158,
            "upper_bound": 0.03838127046244275
          },
          "point_estimate": 0.02927279962466981,
          "standard_error": 0.005948867988287938
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168734.4959720983,
            "upper_bound": 1170228.2142103177
          },
          "point_estimate": 1169452.8946453372,
          "standard_error": 383.1106073855643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168194.3348214286,
            "upper_bound": 1170297.49375
          },
          "point_estimate": 1169454.0581597222,
          "standard_error": 567.1225457497488
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.52951813021247,
            "upper_bound": 2251.4753616884636
          },
          "point_estimate": 1634.734605613065,
          "standard_error": 552.2618204359298
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168420.5364426114,
            "upper_bound": 1169853.5467171718
          },
          "point_estimate": 1169054.4287337663,
          "standard_error": 370.7536642885972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702.3459321135584,
            "upper_bound": 1622.0946339505335
          },
          "point_estimate": 1275.9699571313092,
          "standard_error": 244.03801404631923
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552441.049589947,
            "upper_bound": 1554292.351778935
          },
          "point_estimate": 1553391.648617725,
          "standard_error": 473.87855698355344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552194.3928571427,
            "upper_bound": 1554996.3541666667
          },
          "point_estimate": 1553316.8993055555,
          "standard_error": 596.2681639989944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325.1424108941583,
            "upper_bound": 2813.9098862931824
          },
          "point_estimate": 1672.0000611495211,
          "standard_error": 736.2979033013776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1551944.277888889,
            "upper_bound": 1554508.440023956
          },
          "point_estimate": 1553317.918831169,
          "standard_error": 676.9334244786372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 879.9354079087209,
            "upper_bound": 1995.8032093441875
          },
          "point_estimate": 1577.9636684668476,
          "standard_error": 293.479344832159
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1434078.6637637364,
            "upper_bound": 1437101.5110790597
          },
          "point_estimate": 1435488.089137668,
          "standard_error": 779.3710699943183
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1433733.7876602565,
            "upper_bound": 1437587.5807692308
          },
          "point_estimate": 1434855.0787545787,
          "standard_error": 881.7102182115896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549.4331761430722,
            "upper_bound": 4187.796870523412
          },
          "point_estimate": 1961.1795454705089,
          "standard_error": 1026.4931014845426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1434105.971233858,
            "upper_bound": 1436742.6249916232
          },
          "point_estimate": 1435523.103096903,
          "standard_error": 687.5499689022787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1151.0745075751747,
            "upper_bound": 3495.147144774455
          },
          "point_estimate": 2610.7700415546187,
          "standard_error": 629.9196830357257
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388483.02297872346,
            "upper_bound": 389044.436555851
          },
          "point_estimate": 388764.69305851066,
          "standard_error": 143.31648105980068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388426.09840425535,
            "upper_bound": 389123.5726950355
          },
          "point_estimate": 388806.8515957447,
          "standard_error": 187.67364956660597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.16446238886395,
            "upper_bound": 807.265170774549
          },
          "point_estimate": 489.1907617141217,
          "standard_error": 189.0273802407286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388462.62008281215,
            "upper_bound": 389101.6886010802
          },
          "point_estimate": 388788.1673943078,
          "standard_error": 162.10875333430144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.49519049109372,
            "upper_bound": 622.1930381492565
          },
          "point_estimate": 480.1168652621872,
          "standard_error": 92.86322452652313
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560195.7192564103,
            "upper_bound": 561326.339965812
          },
          "point_estimate": 560720.4799035408,
          "standard_error": 289.9704738846372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560033.4,
            "upper_bound": 561161.6446153845
          },
          "point_estimate": 560600.7145604396,
          "standard_error": 291.0526147533094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.3981480574882,
            "upper_bound": 1491.56563044246
          },
          "point_estimate": 724.3467729351305,
          "standard_error": 323.37158708446935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560137.8580032734,
            "upper_bound": 560955.4775346977
          },
          "point_estimate": 560532.8357642358,
          "standard_error": 209.75436248867248
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 421.94209453842944,
            "upper_bound": 1324.8926549045584
          },
          "point_estimate": 967.8455271396876,
          "standard_error": 251.679849357017
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413972.64283549786,
            "upper_bound": 414909.56339498784
          },
          "point_estimate": 414443.7427944624,
          "standard_error": 238.93572647983763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413923.87381628784,
            "upper_bound": 414932.0511363637
          },
          "point_estimate": 414466.0247474747,
          "standard_error": 227.25542880160407
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 150.8056421143553,
            "upper_bound": 1310.6737335490632
          },
          "point_estimate": 484.45067293338343,
          "standard_error": 315.08660209945486
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414060.0457572466,
            "upper_bound": 414563.8245940353
          },
          "point_estimate": 414331.3701593861,
          "standard_error": 126.11032539717756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359.2137503871289,
            "upper_bound": 1065.9252916596593
          },
          "point_estimate": 795.5264539786771,
          "standard_error": 177.62391383180537
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 480000.83904605266,
            "upper_bound": 480556.78020942985
          },
          "point_estimate": 480274.3376174813,
          "standard_error": 142.09566190813925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479865.425,
            "upper_bound": 480629.875
          },
          "point_estimate": 480251.79276315786,
          "standard_error": 176.08039200849578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.29799293844702,
            "upper_bound": 792.9340506923967
          },
          "point_estimate": 566.6867749393073,
          "standard_error": 188.63930575684307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479872.1253273108,
            "upper_bound": 480427.63146250584
          },
          "point_estimate": 480113.6644224197,
          "standard_error": 141.67972153125712
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.20291218560567,
            "upper_bound": 600.8912481802703
          },
          "point_estimate": 472.8118330285512,
          "standard_error": 86.753555913366
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 974084.8698084536,
            "upper_bound": 976030.741914578
          },
          "point_estimate": 974961.66396721,
          "standard_error": 500.5402282558363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 973768.4315789476,
            "upper_bound": 975974.1466165412
          },
          "point_estimate": 974392.7362938595,
          "standard_error": 568.0955791840789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.6122015335303,
            "upper_bound": 2573.211104316309
          },
          "point_estimate": 1165.4818100102868,
          "standard_error": 581.4225137703517
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 974219.2355214176,
            "upper_bound": 975359.6597307222
          },
          "point_estimate": 974709.7821599452,
          "standard_error": 291.1704699163092
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 642.7610026043574,
            "upper_bound": 2276.551715754033
          },
          "point_estimate": 1665.404432819584,
          "standard_error": 446.63729192330834
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297343.9518631437,
            "upper_bound": 298198.76477151085
          },
          "point_estimate": 297724.5519263776,
          "standard_error": 221.57702315752857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297254.55261969287,
            "upper_bound": 297945.0548780488
          },
          "point_estimate": 297624.4951219512,
          "standard_error": 184.2357589727167
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.96373474271604,
            "upper_bound": 958.2244021954634
          },
          "point_estimate": 405.73442602847786,
          "standard_error": 204.8901382268404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297351.83440300316,
            "upper_bound": 297915.2089797114
          },
          "point_estimate": 297634.25750184775,
          "standard_error": 150.64085514785825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.0938327072708,
            "upper_bound": 1055.0204127378754
          },
          "point_estimate": 737.0861664747558,
          "standard_error": 234.21937356947055
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806697.7898309178,
            "upper_bound": 808078.0234851622
          },
          "point_estimate": 807386.7591494133,
          "standard_error": 352.90996023639724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806293.7934782609,
            "upper_bound": 808138.6135265701
          },
          "point_estimate": 807661.1381987578,
          "standard_error": 505.1952433800523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.80488012658284,
            "upper_bound": 2249.993950272018
          },
          "point_estimate": 1137.7731832063143,
          "standard_error": 508.4544293380704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806731.7881536921,
            "upper_bound": 808176.1405279504
          },
          "point_estimate": 807415.194240542,
          "standard_error": 370.0260111412466
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 681.4521026779169,
            "upper_bound": 1494.037248055531
          },
          "point_estimate": 1176.8831096913652,
          "standard_error": 211.9954125041692
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397985.5401667098,
            "upper_bound": 398414.4806274154
          },
          "point_estimate": 398182.1536766735,
          "standard_error": 110.60024756332092
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397897.88858695654,
            "upper_bound": 398447.6491545894
          },
          "point_estimate": 398113.0798913044,
          "standard_error": 119.60677856715397
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.4808186903666,
            "upper_bound": 588.850033024053
          },
          "point_estimate": 256.71567707286823,
          "standard_error": 131.57060875988842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398000.5873164411,
            "upper_bound": 398295.40439784713
          },
          "point_estimate": 398152.4188594015,
          "standard_error": 74.27536320357532
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.6657655951803,
            "upper_bound": 476.027660031529
          },
          "point_estimate": 367.4237929763262,
          "standard_error": 88.55820240859111
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165169.97886363635,
            "upper_bound": 165411.59984343435
          },
          "point_estimate": 165292.03378174602,
          "standard_error": 61.867648722776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165149.69696969696,
            "upper_bound": 165492.61616161617
          },
          "point_estimate": 165247.50787337663,
          "standard_error": 97.09011806286972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.051498347889066,
            "upper_bound": 380.0893068505363
          },
          "point_estimate": 248.65728717635352,
          "standard_error": 90.1897654386737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165069.25686520376,
            "upper_bound": 165432.55319634703
          },
          "point_estimate": 165242.14232585597,
          "standard_error": 96.83999876382656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.27318195284118,
            "upper_bound": 255.98990139340165
          },
          "point_estimate": 206.32476008248057,
          "standard_error": 33.945135949656866
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232354.18189611763,
            "upper_bound": 232589.2556569103
          },
          "point_estimate": 232471.04873192805,
          "standard_error": 59.88273386393709
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232314.68378450105,
            "upper_bound": 232631.62415327068
          },
          "point_estimate": 232464.70987261145,
          "standard_error": 73.77802164113292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.18896028989276,
            "upper_bound": 337.85894524915875
          },
          "point_estimate": 163.72145330354482,
          "standard_error": 75.80089078689295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232353.76886857065,
            "upper_bound": 232604.6298997113
          },
          "point_estimate": 232484.50359831253,
          "standard_error": 63.66158279232207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 109.63296310384004,
            "upper_bound": 255.4678116982468
          },
          "point_estimate": 200.0035433886692,
          "standard_error": 37.35806389995817
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172523.24888926037,
            "upper_bound": 172975.0049500395
          },
          "point_estimate": 172719.394697021,
          "standard_error": 116.58655867742569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172517.99897314375,
            "upper_bound": 172813.1202606635
          },
          "point_estimate": 172664.25,
          "standard_error": 71.43849696169943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.82504456520764,
            "upper_bound": 459.5696880495302
          },
          "point_estimate": 165.93908609428263,
          "standard_error": 102.48214590570964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172556.77151058006,
            "upper_bound": 172762.30811165986
          },
          "point_estimate": 172670.7271250077,
          "standard_error": 52.159882184949296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.64026490331884,
            "upper_bound": 567.9018885346904
          },
          "point_estimate": 388.5204080622587,
          "standard_error": 136.75801782551235
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648932.0728080356,
            "upper_bound": 649419.7542332057
          },
          "point_estimate": 649174.2771513606,
          "standard_error": 125.0007361131627
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648838.1287202381,
            "upper_bound": 649564.0511904762
          },
          "point_estimate": 649161.0459821429,
          "standard_error": 212.1898898471936
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.13854641100557,
            "upper_bound": 666.0691954964359
          },
          "point_estimate": 463.1468024025384,
          "standard_error": 163.4041221021163
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648803.9557939955,
            "upper_bound": 649513.6373751628
          },
          "point_estimate": 649134.6378942486,
          "standard_error": 179.93345510099618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.5760518807814,
            "upper_bound": 510.825579813882
          },
          "point_estimate": 416.9674069983007,
          "standard_error": 65.15275591153019
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1311.905005153366,
            "upper_bound": 1314.30995503271
          },
          "point_estimate": 1313.004424329591,
          "standard_error": 0.6180243146739303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1311.1199345461546,
            "upper_bound": 1314.1515085495648
          },
          "point_estimate": 1312.6344819340168,
          "standard_error": 0.7545048323669306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18120904192681508,
            "upper_bound": 3.2593390014767567
          },
          "point_estimate": 2.0356226789918583,
          "standard_error": 0.743411735525977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1311.3838996044008,
            "upper_bound": 1313.3589183946196
          },
          "point_estimate": 1312.110089311497,
          "standard_error": 0.5012749435571056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9403069995072122,
            "upper_bound": 2.7918392064964217
          },
          "point_estimate": 2.0609679542554957,
          "standard_error": 0.5203833721575307
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228499.376803968,
            "upper_bound": 1231481.6255564815
          },
          "point_estimate": 1229940.1000211642,
          "standard_error": 766.3742296540206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227934.0,
            "upper_bound": 1232227.0266666666
          },
          "point_estimate": 1228962.933888889,
          "standard_error": 1117.871060756316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 222.385877718601,
            "upper_bound": 4328.425913155171
          },
          "point_estimate": 3069.7134791684916,
          "standard_error": 1137.4736668747985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228058.7594491928,
            "upper_bound": 1231811.3629443182
          },
          "point_estimate": 1230086.7958441558,
          "standard_error": 990.6188653165516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1389.8867065584,
            "upper_bound": 3165.057550797477
          },
          "point_estimate": 2546.797419852231,
          "standard_error": 452.4950072298761
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1370912.2869883156,
            "upper_bound": 1372670.2678806584
          },
          "point_estimate": 1371689.1009273955,
          "standard_error": 455.25997209744247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1370706.956018519,
            "upper_bound": 1372124.324955908
          },
          "point_estimate": 1371546.9049382715,
          "standard_error": 357.7242747128355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.07341744214938,
            "upper_bound": 1938.412512098664
          },
          "point_estimate": 909.3277550599992,
          "standard_error": 434.2554465772074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1370538.3034335691,
            "upper_bound": 1371944.2210118617
          },
          "point_estimate": 1371294.1443001444,
          "standard_error": 370.41102198021326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 526.4067364269699,
            "upper_bound": 2165.493259116985
          },
          "point_estimate": 1512.1225362552238,
          "standard_error": 488.07838422073337
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1271102.3791327996,
            "upper_bound": 1273622.5584729062
          },
          "point_estimate": 1272363.575235359,
          "standard_error": 647.7539971275934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1271050.5061576357,
            "upper_bound": 1274400.2793103447
          },
          "point_estimate": 1271797.528735632,
          "standard_error": 985.2372485772868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153.28436394513344,
            "upper_bound": 3994.002199207224
          },
          "point_estimate": 1875.9363029022568,
          "standard_error": 1008.8953401132842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1270954.0613224315,
            "upper_bound": 1273408.6543728136
          },
          "point_estimate": 1272084.3718763995,
          "standard_error": 686.2012494867117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1261.218855801029,
            "upper_bound": 2723.379422477536
          },
          "point_estimate": 2154.888130263297,
          "standard_error": 380.9191366519071
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333041.59341414145,
            "upper_bound": 334273.4810219878
          },
          "point_estimate": 333566.06637590186,
          "standard_error": 321.0050522045697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332953.38474747475,
            "upper_bound": 333726.8035714285
          },
          "point_estimate": 333395.8761363636,
          "standard_error": 217.0562060656247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.80288851936058,
            "upper_bound": 1113.0206131944317
          },
          "point_estimate": 457.6572714204164,
          "standard_error": 256.83670222873417
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333116.8956638756,
            "upper_bound": 333496.5343138434
          },
          "point_estimate": 333326.6461865407,
          "standard_error": 96.87582006381602
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.7147343571774,
            "upper_bound": 1577.2807245550189
          },
          "point_estimate": 1069.3634960196814,
          "standard_error": 394.68120708497133
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256071.9234467919,
            "upper_bound": 256437.57976286887
          },
          "point_estimate": 256254.25010116253,
          "standard_error": 93.93436927048364
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255941.86776212833,
            "upper_bound": 256575.18309859151
          },
          "point_estimate": 256271.18485915492,
          "standard_error": 179.4470481772958
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.54601984719915,
            "upper_bound": 506.8351536779075
          },
          "point_estimate": 467.74254225931014,
          "standard_error": 125.40918648328388
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255968.65587802292,
            "upper_bound": 256479.79230103252
          },
          "point_estimate": 256222.11412109016,
          "standard_error": 134.34880848537136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.74404329430504,
            "upper_bound": 360.5726721142794
          },
          "point_estimate": 313.9188235136436,
          "standard_error": 39.051500694274154
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447466.31878048787,
            "upper_bound": 447992.94435540074
          },
          "point_estimate": 447731.1939184088,
          "standard_error": 134.5823528425526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447395.88902439026,
            "upper_bound": 448028.60942944256
          },
          "point_estimate": 447752.8871951219,
          "standard_error": 162.98701486945447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.93804743231902,
            "upper_bound": 768.3483961152215
          },
          "point_estimate": 413.09484181244704,
          "standard_error": 166.68814112402407
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 447514.1556059451,
            "upper_bound": 448033.3213018198
          },
          "point_estimate": 447774.08574596135,
          "standard_error": 135.74673011063197
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.9396699849102,
            "upper_bound": 592.7349312499447
          },
          "point_estimate": 447.7867887013299,
          "standard_error": 94.60492743728253
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236142.62858633525,
            "upper_bound": 236674.28214324365
          },
          "point_estimate": 236385.48837688105,
          "standard_error": 136.06149729168845
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236022.64204545453,
            "upper_bound": 236555.20231910943
          },
          "point_estimate": 236400.43217893215,
          "standard_error": 146.45429191328827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.12057915640375,
            "upper_bound": 680.4622430330212
          },
          "point_estimate": 369.0216090329664,
          "standard_error": 157.07635006356327
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236220.95289349693,
            "upper_bound": 236503.18954087223
          },
          "point_estimate": 236386.15464665205,
          "standard_error": 72.50076744692748
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.65282357982423,
            "upper_bound": 634.8271013538653
          },
          "point_estimate": 453.957758658196,
          "standard_error": 127.17601894857825
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639883.5541315266,
            "upper_bound": 640978.6937455619
          },
          "point_estimate": 640357.0217606516,
          "standard_error": 284.0138355523998
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639703.1359649124,
            "upper_bound": 640570.1032894737
          },
          "point_estimate": 640258.5950292398,
          "standard_error": 236.76342127583723
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.89063326835335,
            "upper_bound": 1157.5925078695957
          },
          "point_estimate": 605.7708413506737,
          "standard_error": 247.64809536579256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 639893.1179932795,
            "upper_bound": 640526.9742515492
          },
          "point_estimate": 640250.9389838232,
          "standard_error": 164.50094496387828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327.9796390597893,
            "upper_bound": 1386.9536998244098
          },
          "point_estimate": 951.223193882423,
          "standard_error": 327.3014938261938
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117895.5106321506,
            "upper_bound": 118115.5257842455
          },
          "point_estimate": 118003.36858907956,
          "standard_error": 56.2658801885276
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117818.05194805194,
            "upper_bound": 118146.648989899
          },
          "point_estimate": 117997.71040120591,
          "standard_error": 79.34492464741186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.63676701531,
            "upper_bound": 318.08988170668965
          },
          "point_estimate": 243.58898279665172,
          "standard_error": 71.00454910213715
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117928.78536037424,
            "upper_bound": 118143.6193425974
          },
          "point_estimate": 118058.2725839096,
          "standard_error": 55.641228364797136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.0408710278504,
            "upper_bound": 231.65863213176664
          },
          "point_estimate": 186.6568227898506,
          "standard_error": 31.785624162179477
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124287.33266211604,
            "upper_bound": 124407.8127986348
          },
          "point_estimate": 124344.55487864996,
          "standard_error": 30.711891734319487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124281.51552901024,
            "upper_bound": 124393.29607508532
          },
          "point_estimate": 124334.54351535835,
          "standard_error": 22.78675026979035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.88394360152587,
            "upper_bound": 166.39983574888822
          },
          "point_estimate": 41.81506805740263,
          "standard_error": 42.43458608140115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124293.07407665347,
            "upper_bound": 124386.6510227849
          },
          "point_estimate": 124332.26301139132,
          "standard_error": 23.516264909026027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.81521087580339,
            "upper_bound": 142.07925693992755
          },
          "point_estimate": 102.2991403541821,
          "standard_error": 26.369400880355204
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200475.04965136052,
            "upper_bound": 200817.61787393165
          },
          "point_estimate": 200644.0450499302,
          "standard_error": 87.58629467900228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200390.4478021978,
            "upper_bound": 200914.08351648352
          },
          "point_estimate": 200601.15689865692,
          "standard_error": 139.3255778270387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.584383505204183,
            "upper_bound": 480.3899978816664
          },
          "point_estimate": 344.1892044022104,
          "standard_error": 110.40941388118084
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200498.39342299063,
            "upper_bound": 200800.8497167332
          },
          "point_estimate": 200678.41634080207,
          "standard_error": 76.42810647050244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.66810040888177,
            "upper_bound": 357.44014601820663
          },
          "point_estimate": 291.2400973666616,
          "standard_error": 46.93537171133906
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272306.28942193795,
            "upper_bound": 272835.40541814733
          },
          "point_estimate": 272554.85923833214,
          "standard_error": 135.41951773236212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272170.67661691544,
            "upper_bound": 272799.9398839137
          },
          "point_estimate": 272537.8382729211,
          "standard_error": 188.38551614039267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.75357605762626,
            "upper_bound": 799.4420625981181
          },
          "point_estimate": 465.2883170379542,
          "standard_error": 169.29867936928196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272264.7167643946,
            "upper_bound": 272754.6796210206
          },
          "point_estimate": 272554.66741616593,
          "standard_error": 124.85653956111318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.85929754646563,
            "upper_bound": 601.959340520532
          },
          "point_estimate": 452.0341911836182,
          "standard_error": 104.38687327103942
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 300126.543628025,
            "upper_bound": 302338.61768540205
          },
          "point_estimate": 301235.36424733285,
          "standard_error": 567.5121504939202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 299603.868852459,
            "upper_bound": 303181.98913934425
          },
          "point_estimate": 301032.2862021858,
          "standard_error": 1205.9832093759835
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 218.98214279263692,
            "upper_bound": 2995.9228822625614
          },
          "point_estimate": 2776.940739469924,
          "standard_error": 822.5612803500221
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 301242.6989439943,
            "upper_bound": 303058.081359226
          },
          "point_estimate": 302515.0902703854,
          "standard_error": 474.0074334509879
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1294.3741952337691,
            "upper_bound": 2158.494689352364
          },
          "point_estimate": 1892.9195641343044,
          "standard_error": 221.3177993999452
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323445.84336283186,
            "upper_bound": 324006.5738864306
          },
          "point_estimate": 323750.22704031464,
          "standard_error": 144.23790226525452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323481.54867256636,
            "upper_bound": 324057.51720747293
          },
          "point_estimate": 323864.2867256637,
          "standard_error": 131.3823441216181
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.24237563764795,
            "upper_bound": 731.8152831138271
          },
          "point_estimate": 298.3686525790208,
          "standard_error": 167.9724088892013
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323615.9051859354,
            "upper_bound": 324011.08322139696
          },
          "point_estimate": 323851.201677968,
          "standard_error": 100.27488625094637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.58848735298955,
            "upper_bound": 652.7992740974344
          },
          "point_estimate": 479.8084825198703,
          "standard_error": 127.33652021922924
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406415.1667659832,
            "upper_bound": 406738.1950937169
          },
          "point_estimate": 406586.3698553792,
          "standard_error": 82.43265552637655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406439.8037037037,
            "upper_bound": 406771.6619753086
          },
          "point_estimate": 406616.6775793651,
          "standard_error": 81.45154714644048
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.22321129506466,
            "upper_bound": 442.29251881440865
          },
          "point_estimate": 237.79530800053013,
          "standard_error": 100.00826417879404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 406587.18015995313,
            "upper_bound": 406857.87167385145
          },
          "point_estimate": 406725.08008658007,
          "standard_error": 72.94948200310388
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.8464307957419,
            "upper_bound": 371.6749379614743
          },
          "point_estimate": 274.14544703849504,
          "standard_error": 66.76041658128939
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277390.699142827,
            "upper_bound": 278194.2338816739
          },
          "point_estimate": 277761.90410684224,
          "standard_error": 206.62499307672832
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277190.2414772727,
            "upper_bound": 278295.40277777775
          },
          "point_estimate": 277578.44772727275,
          "standard_error": 242.70308332699096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.13332969717558,
            "upper_bound": 1147.639737087438
          },
          "point_estimate": 630.8955372021993,
          "standard_error": 269.88116479282434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277200.34360959753,
            "upper_bound": 277765.4081477425
          },
          "point_estimate": 277442.9352420307,
          "standard_error": 144.18313126653618
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263.58517872774667,
            "upper_bound": 906.3810166695572
          },
          "point_estimate": 690.7065653856282,
          "standard_error": 161.3417735923228
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122046.51674083174,
            "upper_bound": 122271.39507337273
          },
          "point_estimate": 122149.97398383405,
          "standard_error": 57.99033107390976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121991.93330536912,
            "upper_bound": 122298.8433072334
          },
          "point_estimate": 122110.25570469798,
          "standard_error": 71.6108757151208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.55277251784412,
            "upper_bound": 318.6512128606936
          },
          "point_estimate": 149.59775777195776,
          "standard_error": 76.47535003373358
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122023.66322272082,
            "upper_bound": 122150.66447044634
          },
          "point_estimate": 122089.03976292165,
          "standard_error": 32.16279875964055
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.94617703656976,
            "upper_bound": 249.22362486233055
          },
          "point_estimate": 193.2513209296191,
          "standard_error": 45.04880960463617
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206107.834640391,
            "upper_bound": 206428.73924655304
          },
          "point_estimate": 206267.70987332976,
          "standard_error": 82.3416690931465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206011.25968523003,
            "upper_bound": 206468.16172316385
          },
          "point_estimate": 206320.43079096044,
          "standard_error": 132.4970931116427
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72.37569341847612,
            "upper_bound": 459.03012320652454
          },
          "point_estimate": 349.21218413923543,
          "standard_error": 104.76826571991846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205990.32346295932,
            "upper_bound": 206375.18747326345
          },
          "point_estimate": 206141.60562036833,
          "standard_error": 97.50395872840735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170.11923905768617,
            "upper_bound": 332.312689317281
          },
          "point_estimate": 273.3672168014206,
          "standard_error": 41.91987093806253
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152607.84078581387,
            "upper_bound": 152798.92934885767
          },
          "point_estimate": 152706.23286644087,
          "standard_error": 48.73778621199742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152577.05041841004,
            "upper_bound": 152819.63687985652
          },
          "point_estimate": 152735.7129707113,
          "standard_error": 56.5266006728815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.70524607841662,
            "upper_bound": 271.7688610747357
          },
          "point_estimate": 138.24462443548595,
          "standard_error": 63.81927351368498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152630.70150237568,
            "upper_bound": 152799.6193275193
          },
          "point_estimate": 152704.4419605499,
          "standard_error": 43.04349552499656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.45300299241923,
            "upper_bound": 209.84978734705737
          },
          "point_estimate": 162.3541329337366,
          "standard_error": 32.88525201791665
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47668.45978244999,
            "upper_bound": 47735.37564451536
          },
          "point_estimate": 47700.16772109588,
          "standard_error": 17.09212278276692
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47660.43817528106,
            "upper_bound": 47726.16999874852
          },
          "point_estimate": 47699.13020148927,
          "standard_error": 18.670271408507237
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.361170188601166,
            "upper_bound": 88.59345443897892
          },
          "point_estimate": 40.77037092861368,
          "standard_error": 20.371685500616064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47661.61505647295,
            "upper_bound": 47718.37053640162
          },
          "point_estimate": 47691.87089441439,
          "standard_error": 14.703095446291291
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.580156415970325,
            "upper_bound": 78.70799735331549
          },
          "point_estimate": 56.904944448620505,
          "standard_error": 14.441654551408112
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1146085.8342821798,
            "upper_bound": 1148022.141495722
          },
          "point_estimate": 1146943.7955580358,
          "standard_error": 498.5345463694106
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1145997.7864583333,
            "upper_bound": 1147570.1551339286
          },
          "point_estimate": 1146677.6192708332,
          "standard_error": 392.3479085182501
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.27471849706353,
            "upper_bound": 2269.0111534672296
          },
          "point_estimate": 912.7827506697404,
          "standard_error": 518.9170833653427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1146260.1708427144,
            "upper_bound": 1147437.544607356
          },
          "point_estimate": 1146795.2506493507,
          "standard_error": 302.20640383433573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 527.4650470499354,
            "upper_bound": 2354.5586058681397
          },
          "point_estimate": 1660.430950554099,
          "standard_error": 520.2023704247126
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104456.77656380566,
            "upper_bound": 104721.62318289033
          },
          "point_estimate": 104568.84394419358,
          "standard_error": 69.25619381658008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104417.8920817369,
            "upper_bound": 104605.77183908048
          },
          "point_estimate": 104549.62944718116,
          "standard_error": 50.926124997032126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.495659295519815,
            "upper_bound": 255.14007089182505
          },
          "point_estimate": 114.97586464459276,
          "standard_error": 58.623387609574706
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104490.22885372964,
            "upper_bound": 104862.1556326616
          },
          "point_estimate": 104641.86680101509,
          "standard_error": 100.39157612778898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.49549532575847,
            "upper_bound": 339.967640166503
          },
          "point_estimate": 230.5647584678741,
          "standard_error": 84.56034522968551
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26536.774498444047,
            "upper_bound": 26591.75684884784
          },
          "point_estimate": 26562.133759477943,
          "standard_error": 14.151008849163272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26523.633656425263,
            "upper_bound": 26599.70674835406
          },
          "point_estimate": 26551.48204502967,
          "standard_error": 17.009198439358528
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.67656295861637,
            "upper_bound": 74.56117771797386
          },
          "point_estimate": 33.20193474339455,
          "standard_error": 17.139893024656406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26524.94676410916,
            "upper_bound": 26557.99680735134
          },
          "point_estimate": 26540.31322357233,
          "standard_error": 8.657323308742232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.162927503284344,
            "upper_bound": 59.635924518773095
          },
          "point_estimate": 47.20492632195618,
          "standard_error": 10.798519794241852
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25827.56581893072,
            "upper_bound": 25885.471162624825
          },
          "point_estimate": 25857.5471478953,
          "standard_error": 14.58584160015399
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25845.102812308945,
            "upper_bound": 25877.918544935805
          },
          "point_estimate": 25856.198787446505,
          "standard_error": 10.330997599473047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.722188910305635,
            "upper_bound": 76.88898822125132
          },
          "point_estimate": 21.021032127992736,
          "standard_error": 16.056221372638284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25846.24812360101,
            "upper_bound": 25865.019607949725
          },
          "point_estimate": 25854.8531133631,
          "standard_error": 4.828279273759663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.553457312074546,
            "upper_bound": 69.59222632869894
          },
          "point_estimate": 48.64340711212126,
          "standard_error": 15.128313619921649
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550593.4706582491,
            "upper_bound": 551175.5733189032
          },
          "point_estimate": 550849.3082515632,
          "standard_error": 150.13408182169283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550517.7704545455,
            "upper_bound": 551144.5064935065
          },
          "point_estimate": 550675.8068181819,
          "standard_error": 134.86576303694028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.736055739948355,
            "upper_bound": 680.7854475096361
          },
          "point_estimate": 268.9365217404596,
          "standard_error": 153.071577644986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550547.4304985338,
            "upper_bound": 551063.0695051707
          },
          "point_estimate": 550752.1783549783,
          "standard_error": 131.46518096679526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.79749241613447,
            "upper_bound": 685.2915716621562
          },
          "point_estimate": 500.1198983849041,
          "standard_error": 148.9121293341238
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.7325871315325,
            "upper_bound": 1129.975769982782
          },
          "point_estimate": 1129.3223670142277,
          "standard_error": 0.3194211487458502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.5618180564177,
            "upper_bound": 1130.2328057107386
          },
          "point_estimate": 1129.0909228961605,
          "standard_error": 0.4431865675856447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12774789849237805,
            "upper_bound": 1.7541645560702068
          },
          "point_estimate": 0.8778568621306098,
          "standard_error": 0.4535841092781804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.6123955316702,
            "upper_bound": 1129.479904718099
          },
          "point_estimate": 1129.0373002168533,
          "standard_error": 0.23014727292407708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5157977945733282,
            "upper_bound": 1.3391911228372937
          },
          "point_estimate": 1.0637873067329922,
          "standard_error": 0.20627099684655997
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.463273213543445,
            "upper_bound": 34.489210681013795
          },
          "point_estimate": 34.47566290506411,
          "standard_error": 0.006661831052949757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.462244584490335,
            "upper_bound": 34.49754818893657
          },
          "point_estimate": 34.4681146930338,
          "standard_error": 0.008462217584803259
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00184602878952636,
            "upper_bound": 0.035830706699354106
          },
          "point_estimate": 0.00986517194924544,
          "standard_error": 0.010447727466217327
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.46626222583738,
            "upper_bound": 34.498194305645335
          },
          "point_estimate": 34.480558685862505,
          "standard_error": 0.007985681953360541
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009963742695715525,
            "upper_bound": 0.02749218073236245
          },
          "point_estimate": 0.022211594367458447,
          "standard_error": 0.004206664497397567
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.90877555289887,
            "upper_bound": 37.977298381663154
          },
          "point_estimate": 37.94064597694091,
          "standard_error": 0.017684657451393884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.896253506803305,
            "upper_bound": 37.990575568520654
          },
          "point_estimate": 37.929980599842175,
          "standard_error": 0.01746565794527003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017302945616050288,
            "upper_bound": 0.09463221606136428
          },
          "point_estimate": 0.022455510127428028,
          "standard_error": 0.023827806428739752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.90171293880661,
            "upper_bound": 37.92960651209595
          },
          "point_estimate": 37.91587064666647,
          "standard_error": 0.006955836472364078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02104226176697484,
            "upper_bound": 0.07502170246999224
          },
          "point_estimate": 0.05910648890625166,
          "standard_error": 0.0137850355991939
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.567574660462277,
            "upper_bound": 26.61386001796004
          },
          "point_estimate": 26.588626607624224,
          "standard_error": 0.011943034533764848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.5564785983261,
            "upper_bound": 26.61716487469865
          },
          "point_estimate": 26.57751999254198,
          "standard_error": 0.012861093967800752
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030532801254030016,
            "upper_bound": 0.0552164491838083
          },
          "point_estimate": 0.026312567594882645,
          "standard_error": 0.013606945980054224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.56200885750652,
            "upper_bound": 26.592065823240453
          },
          "point_estimate": 26.575319689959848,
          "standard_error": 0.007748097439277819
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013183396700976516,
            "upper_bound": 0.051858667376053824
          },
          "point_estimate": 0.039926685960483745,
          "standard_error": 0.010252372193018009
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.804105476867385,
            "upper_bound": 32.869626405772195
          },
          "point_estimate": 32.8348103221641,
          "standard_error": 0.016795620736636013
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.78497203627385,
            "upper_bound": 32.86571019739367
          },
          "point_estimate": 32.83022303272139,
          "standard_error": 0.02001315825115229
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010555032357953732,
            "upper_bound": 0.09481983783175268
          },
          "point_estimate": 0.058035014982798255,
          "standard_error": 0.021146844923036933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.80245026227408,
            "upper_bound": 32.84552650354463
          },
          "point_estimate": 32.82406552709175,
          "standard_error": 0.010781291509821943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028258499341706284,
            "upper_bound": 0.07456017722273489
          },
          "point_estimate": 0.05598282223647029,
          "standard_error": 0.01275380692902718
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.562335729411835,
            "upper_bound": 40.62000330452891
          },
          "point_estimate": 40.59013663648189,
          "standard_error": 0.014701737559614335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.551348701257545,
            "upper_bound": 40.6196112534067
          },
          "point_estimate": 40.59250292513761,
          "standard_error": 0.01731745479986101
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012752350316619766,
            "upper_bound": 0.08459082008936492
          },
          "point_estimate": 0.042405757665567416,
          "standard_error": 0.017821193945318448
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.56029092961053,
            "upper_bound": 40.603504233522216
          },
          "point_estimate": 40.58229871596968,
          "standard_error": 0.011721225720163871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02427740246750899,
            "upper_bound": 0.06560359151583897
          },
          "point_estimate": 0.0488840056848002,
          "standard_error": 0.011009423710205244
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.37351893048935,
            "upper_bound": 73.48325008974686
          },
          "point_estimate": 73.42683475413844,
          "standard_error": 0.02807610825739838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.33064900448285,
            "upper_bound": 73.50815516336174
          },
          "point_estimate": 73.42491534346127,
          "standard_error": 0.041254857605856794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0176010471512714,
            "upper_bound": 0.16022619961088988
          },
          "point_estimate": 0.12666024920691338,
          "standard_error": 0.03614169825320843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.39931475223887,
            "upper_bound": 73.47680959384213
          },
          "point_estimate": 73.43192728195838,
          "standard_error": 0.019826502130038
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05359243256200475,
            "upper_bound": 0.11785311699012298
          },
          "point_estimate": 0.09362985323653948,
          "standard_error": 0.016758440015119957
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.7171215474638,
            "upper_bound": 65.83179207566401
          },
          "point_estimate": 65.77427743569238,
          "standard_error": 0.029479906324943374
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.71164426570769,
            "upper_bound": 65.88063688866106
          },
          "point_estimate": 65.75685446145941,
          "standard_error": 0.03862409097072363
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01326175590520578,
            "upper_bound": 0.1886304867114967
          },
          "point_estimate": 0.07832217841550528,
          "standard_error": 0.04694338301444146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.73381136551163,
            "upper_bound": 65.82365597407396
          },
          "point_estimate": 65.77228209598525,
          "standard_error": 0.023321785742471984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05527006547054182,
            "upper_bound": 0.12384866671318936
          },
          "point_estimate": 0.09813018160564198,
          "standard_error": 0.017618942267623547
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.72937903061747,
            "upper_bound": 56.81685984360158
          },
          "point_estimate": 56.772858516714905,
          "standard_error": 0.02238071896082234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.73047873587471,
            "upper_bound": 56.839671747788216
          },
          "point_estimate": 56.74596175036339,
          "standard_error": 0.032297853070845296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008623106644242822,
            "upper_bound": 0.1324798697819234
          },
          "point_estimate": 0.06481823832659565,
          "standard_error": 0.03603613685819097
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.72970187733786,
            "upper_bound": 56.77542399995587
          },
          "point_estimate": 56.749324769102515,
          "standard_error": 0.01153292172307852
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04104055998656792,
            "upper_bound": 0.09439514353022072
          },
          "point_estimate": 0.07433738674672896,
          "standard_error": 0.013528075339526251
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.69092332062084,
            "upper_bound": 108.9214034693016
          },
          "point_estimate": 108.79158506308508,
          "standard_error": 0.06008400801931975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.67727882542697,
            "upper_bound": 108.88700507996803
          },
          "point_estimate": 108.73161231201354,
          "standard_error": 0.04286871089709829
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01374193180204436,
            "upper_bound": 0.24512201115946344
          },
          "point_estimate": 0.07040422287163325,
          "standard_error": 0.058134797983084845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.71965410239466,
            "upper_bound": 108.8378889446913
          },
          "point_estimate": 108.76409953362334,
          "standard_error": 0.03012309437144303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03949122590206169,
            "upper_bound": 0.27599093886573467
          },
          "point_estimate": 0.1997791614673835,
          "standard_error": 0.0637673206603594
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.75983670324838,
            "upper_bound": 45.82227780328506
          },
          "point_estimate": 45.79326418666126,
          "standard_error": 0.015995403611330547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.77070246419218,
            "upper_bound": 45.84120490092469
          },
          "point_estimate": 45.79216069964265,
          "standard_error": 0.016932363871895575
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009173212784513768,
            "upper_bound": 0.08964079593617422
          },
          "point_estimate": 0.037318840740126966,
          "standard_error": 0.020904570698131304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.77737062131395,
            "upper_bound": 45.840036414810896
          },
          "point_estimate": 45.81701823144389,
          "standard_error": 0.015987692575411865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02520243291674556,
            "upper_bound": 0.07310853101706838
          },
          "point_estimate": 0.053348874362860114,
          "standard_error": 0.013628800971593613
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.12982368182484,
            "upper_bound": 50.209113195145775
          },
          "point_estimate": 50.16684467253455,
          "standard_error": 0.020316987225076244
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.11911497327997,
            "upper_bound": 50.21065718918949
          },
          "point_estimate": 50.1469124453739,
          "standard_error": 0.029503186014159497
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009060703094284313,
            "upper_bound": 0.12155645735225366
          },
          "point_estimate": 0.06281038175029265,
          "standard_error": 0.02785610750236359
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.11294534201985,
            "upper_bound": 50.17103466632102
          },
          "point_estimate": 50.13994221623959,
          "standard_error": 0.014603859673183608
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.034403710557189104,
            "upper_bound": 0.08999735404054746
          },
          "point_estimate": 0.0675156339738333,
          "standard_error": 0.015629003744953268
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.41951623890964,
            "upper_bound": 93.68656927151002
          },
          "point_estimate": 93.56687565262072,
          "standard_error": 0.06874662303716868
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.48419523617432,
            "upper_bound": 93.73723618401765
          },
          "point_estimate": 93.58422598324242,
          "standard_error": 0.06523259899993981
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03214699218616559,
            "upper_bound": 0.3192699018272512
          },
          "point_estimate": 0.18146743075132815,
          "standard_error": 0.06853110150497883
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.5244940856279,
            "upper_bound": 93.70858013562297
          },
          "point_estimate": 93.62310346688182,
          "standard_error": 0.04664915404007981
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09196213776993584,
            "upper_bound": 0.32280643369358225
          },
          "point_estimate": 0.22887926699268996,
          "standard_error": 0.06828271505761513
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975760.0733709274,
            "upper_bound": 977432.6025438596
          },
          "point_estimate": 976552.056265664,
          "standard_error": 428.2887872817913
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975412.8186090224,
            "upper_bound": 977803.4276315788
          },
          "point_estimate": 975960.8210526316,
          "standard_error": 592.4705803378517
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.77875526606695,
            "upper_bound": 2367.393529023037
          },
          "point_estimate": 1414.9901635631238,
          "standard_error": 595.9684312314026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975901.8429873568,
            "upper_bound": 977414.3913636096
          },
          "point_estimate": 976557.9707450444,
          "standard_error": 385.8990968693596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 678.44746195988,
            "upper_bound": 1766.2052019054786
          },
          "point_estimate": 1427.5985271529023,
          "standard_error": 272.7284450263442
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1376508.9357242065,
            "upper_bound": 1380443.084350235
          },
          "point_estimate": 1378176.8144415051,
          "standard_error": 1024.803060846784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1376196.6255144032,
            "upper_bound": 1379132.3086419753
          },
          "point_estimate": 1377243.2266313932,
          "standard_error": 696.9359761320726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.54480843049973,
            "upper_bound": 3535.652251674026
          },
          "point_estimate": 2020.9991594905885,
          "standard_error": 950.8848942704368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1376373.2709284627,
            "upper_bound": 1378159.3237962965
          },
          "point_estimate": 1377118.6618566618,
          "standard_error": 445.0134575982715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1006.6296595495512,
            "upper_bound": 4997.604816770637
          },
          "point_estimate": 3401.0650419359526,
          "standard_error": 1235.4457146082311
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1266766.0948946702,
            "upper_bound": 1268590.858173235
          },
          "point_estimate": 1267706.569694855,
          "standard_error": 467.1423400932912
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1266479.6862068966,
            "upper_bound": 1269155.921182266
          },
          "point_estimate": 1267852.5,
          "standard_error": 626.6264860740399
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240.39421297359831,
            "upper_bound": 2715.776643739375
          },
          "point_estimate": 1983.8929520202064,
          "standard_error": 669.9236429621267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1266960.1682702317,
            "upper_bound": 1269039.12148273
          },
          "point_estimate": 1268037.9073891626,
          "standard_error": 548.5878927887719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 843.0190975462637,
            "upper_bound": 1912.1719051749756
          },
          "point_estimate": 1557.696092287494,
          "standard_error": 265.64851586339915
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1361978.136413874,
            "upper_bound": 1364881.2607278803
          },
          "point_estimate": 1363384.501072898,
          "standard_error": 741.7721012582086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1362075.5802469137,
            "upper_bound": 1364659.7345679011
          },
          "point_estimate": 1363163.2474279837,
          "standard_error": 624.7350152636336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.51893651280028,
            "upper_bound": 4107.236640044896
          },
          "point_estimate": 1915.6335641388432,
          "standard_error": 986.0210691079374
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1362718.7182135202,
            "upper_bound": 1364275.7499815887
          },
          "point_estimate": 1363386.236075036,
          "standard_error": 397.4293765905234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1027.6724767978612,
            "upper_bound": 3371.5148060516153
          },
          "point_estimate": 2474.123534631263,
          "standard_error": 607.0750415612354
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1307899.8843253967,
            "upper_bound": 1310048.020672619
          },
          "point_estimate": 1308959.301240079,
          "standard_error": 549.5241493636705
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1307385.2660714283,
            "upper_bound": 1310341.9389880951
          },
          "point_estimate": 1308773.3459821427,
          "standard_error": 760.6238546536994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 276.91261008385294,
            "upper_bound": 3303.902411760283
          },
          "point_estimate": 1703.7253472529535,
          "standard_error": 716.5278535527099
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1307226.5904873875,
            "upper_bound": 1309311.9910695024
          },
          "point_estimate": 1308147.5599257883,
          "standard_error": 530.2065346312258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1057.8862053157816,
            "upper_bound": 2305.1105345112987
          },
          "point_estimate": 1835.123878930669,
          "standard_error": 320.5772374996973
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350172.58005444903,
            "upper_bound": 350792.0565140415
          },
          "point_estimate": 350465.6792384005,
          "standard_error": 158.30472736336867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350149.5629120879,
            "upper_bound": 350726.8522435897
          },
          "point_estimate": 350412.59054487175,
          "standard_error": 160.59814041318202
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.73240419437504,
            "upper_bound": 855.8891476894786
          },
          "point_estimate": 384.31256401042623,
          "standard_error": 183.53763870714775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 350215.10283771105,
            "upper_bound": 350713.5774200948
          },
          "point_estimate": 350450.21700799203,
          "standard_error": 137.79908619285683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228.68553768866036,
            "upper_bound": 724.7913083704246
          },
          "point_estimate": 529.9638287365904,
          "standard_error": 131.64188008761286
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399063.0428340127,
            "upper_bound": 399430.70404530066
          },
          "point_estimate": 399245.8100444272,
          "standard_error": 93.90879009454537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398940.1811594203,
            "upper_bound": 399497.5797101449
          },
          "point_estimate": 399264.2578502415,
          "standard_error": 145.70229324802116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.23997666930404,
            "upper_bound": 533.6227235697598
          },
          "point_estimate": 363.0296220423043,
          "standard_error": 117.57702986769137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 399230.8858055118,
            "upper_bound": 399541.5901781712
          },
          "point_estimate": 399376.4929418408,
          "standard_error": 79.92166145860857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189.3638000928629,
            "upper_bound": 385.1537258499788
          },
          "point_estimate": 312.1724020966258,
          "standard_error": 50.41003459231297
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 917254.1970008432,
            "upper_bound": 919410.7000358136
          },
          "point_estimate": 918389.9050248016,
          "standard_error": 555.045655499308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 916811.095,
            "upper_bound": 919884.3069642856
          },
          "point_estimate": 918869.7890625,
          "standard_error": 720.8592577307425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.2135945658426,
            "upper_bound": 2870.111235920403
          },
          "point_estimate": 1565.7165358905418,
          "standard_error": 778.0994067483696
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 918053.4806629834,
            "upper_bound": 919777.9084517044
          },
          "point_estimate": 919061.4123376624,
          "standard_error": 446.5940078204303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 768.3683392163432,
            "upper_bound": 2250.7581254262395
          },
          "point_estimate": 1846.2638282029243,
          "standard_error": 344.8204616224135
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 823140.4313871033,
            "upper_bound": 824006.5663333334
          },
          "point_estimate": 823589.184584656,
          "standard_error": 222.31863678853128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 822890.4814814815,
            "upper_bound": 824194.2985185186
          },
          "point_estimate": 823839.1814814815,
          "standard_error": 361.3461234567428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.13982120955242,
            "upper_bound": 1198.9731076027167
          },
          "point_estimate": 752.307467977255,
          "standard_error": 287.81929209201854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 823240.319047619,
            "upper_bound": 824248.4732172227
          },
          "point_estimate": 823835.4371717172,
          "standard_error": 267.58728092603343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.76704812649984,
            "upper_bound": 908.0523515836596
          },
          "point_estimate": 741.1593452080355,
          "standard_error": 126.681675935987
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 764976.6782539682,
            "upper_bound": 765898.3863125
          },
          "point_estimate": 765459.1685193452,
          "standard_error": 236.63513709073672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 764841.8114583334,
            "upper_bound": 766141.4875
          },
          "point_estimate": 765605.5230654762,
          "standard_error": 316.81397550906894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 213.60559120767363,
            "upper_bound": 1354.9155553726005
          },
          "point_estimate": 807.8275423249338,
          "standard_error": 294.9577246050087
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 764825.8776371308,
            "upper_bound": 765865.3193501825
          },
          "point_estimate": 765420.0751623376,
          "standard_error": 267.5462469022755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 410.3342362456114,
            "upper_bound": 993.7709248731312
          },
          "point_estimate": 788.0438145238912,
          "standard_error": 149.11032113444088
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326340.17205252615,
            "upper_bound": 326754.49067284935
          },
          "point_estimate": 326523.18724135484,
          "standard_error": 107.48503972874596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326268.4388392857,
            "upper_bound": 326669.5693452381
          },
          "point_estimate": 326442.0764597506,
          "standard_error": 104.5119828069406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.96757011434615,
            "upper_bound": 468.8219024059373
          },
          "point_estimate": 252.4851628537759,
          "standard_error": 101.024188965485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326289.7356937995,
            "upper_bound": 326508.562068668
          },
          "point_estimate": 326385.10929962894,
          "standard_error": 55.041077174185105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.48475896240117,
            "upper_bound": 510.2930600421324
          },
          "point_estimate": 358.72337636321515,
          "standard_error": 111.56097382064146
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575870.6496529983,
            "upper_bound": 576511.152843254
          },
          "point_estimate": 576190.3284586797,
          "standard_error": 164.1494224598805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575742.0746031746,
            "upper_bound": 576621.9070294785
          },
          "point_estimate": 576189.0753968253,
          "standard_error": 213.0625662946055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.34254262748968,
            "upper_bound": 1027.1087850986794
          },
          "point_estimate": 652.2197660398516,
          "standard_error": 236.44925972503583
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 575730.652272233,
            "upper_bound": 576672.6150638099
          },
          "point_estimate": 576125.5162646876,
          "standard_error": 241.87971359829768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 320.129040936536,
            "upper_bound": 672.7111111624221
          },
          "point_estimate": 546.38376338344,
          "standard_error": 90.6224365327176
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189186.22148494943,
            "upper_bound": 189478.78206489023
          },
          "point_estimate": 189331.930455424,
          "standard_error": 74.78358309239403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189174.6933290155,
            "upper_bound": 189553.97668393783
          },
          "point_estimate": 189292.17530224525,
          "standard_error": 95.65226879002094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.572492280072595,
            "upper_bound": 452.8642162778343
          },
          "point_estimate": 185.09797015945685,
          "standard_error": 102.07596925942175
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189169.81660226363,
            "upper_bound": 189376.12682142123
          },
          "point_estimate": 189265.7725321311,
          "standard_error": 51.47283534931941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.8865242828874,
            "upper_bound": 324.20278575223733
          },
          "point_estimate": 249.87852289822953,
          "standard_error": 49.355988141579
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147284.39134109314,
            "upper_bound": 147462.7069883041
          },
          "point_estimate": 147379.68100427353,
          "standard_error": 45.804243060906835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147283.1052631579,
            "upper_bound": 147488.13157894736
          },
          "point_estimate": 147393.50708502025,
          "standard_error": 49.056843622848845
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.52890289216614,
            "upper_bound": 249.7070506275548
          },
          "point_estimate": 125.53750210324928,
          "standard_error": 59.0362369683268
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147369.67371749168,
            "upper_bound": 147514.83601046266
          },
          "point_estimate": 147434.52088963668,
          "standard_error": 38.507666206760085
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.014875200606745,
            "upper_bound": 200.85385764192887
          },
          "point_estimate": 152.91188266317508,
          "standard_error": 35.74385446312485
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501966.22599282453,
            "upper_bound": 503158.32965427265
          },
          "point_estimate": 502479.977277669,
          "standard_error": 307.7283803502551
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501851.2993150685,
            "upper_bound": 502633.5905631659
          },
          "point_estimate": 502451.007762557,
          "standard_error": 221.59187186339935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.08850523216138,
            "upper_bound": 1296.6959119105663
          },
          "point_estimate": 352.55211894644344,
          "standard_error": 311.18486458437127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501876.924695586,
            "upper_bound": 502522.56007479865
          },
          "point_estimate": 502241.88325920654,
          "standard_error": 167.68015909572222
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 315.296067090672,
            "upper_bound": 1495.1601553302949
          },
          "point_estimate": 1023.4993561541736,
          "standard_error": 356.75439885398595
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1048.937183063509,
            "upper_bound": 1051.41197989169
          },
          "point_estimate": 1050.030118245697,
          "standard_error": 0.639069432176739
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1048.7852035759308,
            "upper_bound": 1050.638115836036
          },
          "point_estimate": 1049.6180865498427,
          "standard_error": 0.5008913986310055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.34776785423369794,
            "upper_bound": 2.7057902501812823
          },
          "point_estimate": 1.3523087450091935,
          "standard_error": 0.5817127377771775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1048.9168941682192,
            "upper_bound": 1050.104999723555
          },
          "point_estimate": 1049.5320404183708,
          "standard_error": 0.2985212401487137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7399588204411608,
            "upper_bound": 3.064996738596951
          },
          "point_estimate": 2.1376786001655605,
          "standard_error": 0.6929334017540576
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1115026.8780411256,
            "upper_bound": 1116461.8490631313
          },
          "point_estimate": 1115805.830523088,
          "standard_error": 367.6540246587144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1115118.9757575758,
            "upper_bound": 1116747.616161616
          },
          "point_estimate": 1116172.0473484849,
          "standard_error": 418.69571187883616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.49773358534108,
            "upper_bound": 1921.7381701249665
          },
          "point_estimate": 1054.1257733308971,
          "standard_error": 430.84915329885587
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1115956.152720627,
            "upper_bound": 1116856.64993565
          },
          "point_estimate": 1116544.895080677,
          "standard_error": 231.24845985196177
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523.3457420821977,
            "upper_bound": 1651.3231353777376
          },
          "point_estimate": 1227.940558962816,
          "standard_error": 309.08560202390936
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234861.2239252643,
            "upper_bound": 1237026.9916481485
          },
          "point_estimate": 1235927.1288121694,
          "standard_error": 555.6281406530569
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234116.8523809523,
            "upper_bound": 1237722.111111111
          },
          "point_estimate": 1235687.3458333332,
          "standard_error": 994.6114508303216
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 275.0693617828203,
            "upper_bound": 3024.6807600790416
          },
          "point_estimate": 2573.8968068040836,
          "standard_error": 717.0324515822201
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1235201.1766899766,
            "upper_bound": 1237774.071018134
          },
          "point_estimate": 1236656.0918614718,
          "standard_error": 649.7808949455198
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1187.1353322217217,
            "upper_bound": 2185.5044995728094
          },
          "point_estimate": 1855.265239283832,
          "standard_error": 255.48137832722503
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1150591.1190735365,
            "upper_bound": 1152460.378279018
          },
          "point_estimate": 1151435.3205195933,
          "standard_error": 484.8088796237848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1150246.6875,
            "upper_bound": 1152107.5200892857
          },
          "point_estimate": 1151350.0555555555,
          "standard_error": 483.4974515513547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.1870095023191,
            "upper_bound": 2193.82576753632
          },
          "point_estimate": 1449.900664988173,
          "standard_error": 537.6898664295063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1150654.172457627,
            "upper_bound": 1151682.402375402
          },
          "point_estimate": 1151159.052922078,
          "standard_error": 264.43274665633385
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 645.2263589825491,
            "upper_bound": 2263.795506566762
          },
          "point_estimate": 1613.71709187477,
          "standard_error": 475.09175268716314
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 683503.6723633157,
            "upper_bound": 684423.5329692828
          },
          "point_estimate": 683968.9048838917,
          "standard_error": 234.78007190701013
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 683479.0351851853,
            "upper_bound": 684583.6280864198
          },
          "point_estimate": 683876.6402116402,
          "standard_error": 274.20329882106614
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.13143595718472,
            "upper_bound": 1426.6451166474103
          },
          "point_estimate": 666.5984550173512,
          "standard_error": 323.4652384619611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 683624.6025072262,
            "upper_bound": 684674.7498587912
          },
          "point_estimate": 684235.911111111,
          "standard_error": 265.9587026428834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 422.86487927660886,
            "upper_bound": 1018.0024691174908
          },
          "point_estimate": 782.794895272721,
          "standard_error": 155.06253701623456
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263514.9167891362,
            "upper_bound": 263964.0563681562
          },
          "point_estimate": 263718.12395157584,
          "standard_error": 115.59313167699952
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263457.26318438,
            "upper_bound": 263959.97463768115
          },
          "point_estimate": 263581.1728778468,
          "standard_error": 126.59117978161028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.48462359361257,
            "upper_bound": 594.9856333499492
          },
          "point_estimate": 206.02244363709272,
          "standard_error": 140.9983328784415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263416.38649014593,
            "upper_bound": 263862.24681853474
          },
          "point_estimate": 263591.19267833617,
          "standard_error": 115.25407094699116
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.90601444311017,
            "upper_bound": 514.9586425046964
          },
          "point_estimate": 384.41278252435984,
          "standard_error": 100.58622621231298
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414388.0810322871,
            "upper_bound": 414865.3686900253
          },
          "point_estimate": 414619.5953350469,
          "standard_error": 121.95227787595503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414274.6431818182,
            "upper_bound": 414844.0488816739
          },
          "point_estimate": 414654.63044507575,
          "standard_error": 146.07332723181312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.51352879671964,
            "upper_bound": 740.3402279926677
          },
          "point_estimate": 360.5847902745238,
          "standard_error": 162.00543420939076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414533.8090448403,
            "upper_bound": 414803.0152338603
          },
          "point_estimate": 414701.1483175915,
          "standard_error": 68.5919328597591
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.5455286469689,
            "upper_bound": 532.4009227136394
          },
          "point_estimate": 406.3956480478624,
          "standard_error": 83.67513003354239
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231551.98119477092,
            "upper_bound": 232003.26447805535
          },
          "point_estimate": 231770.53573613623,
          "standard_error": 115.83807107143858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231439.0825949367,
            "upper_bound": 232110.9905063291
          },
          "point_estimate": 231708.8384757384,
          "standard_error": 181.10315988752075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.08413546617528,
            "upper_bound": 632.9184881305506
          },
          "point_estimate": 420.04496975789743,
          "standard_error": 143.7161552035386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231496.6320722787,
            "upper_bound": 231999.3970464135
          },
          "point_estimate": 231713.88947887556,
          "standard_error": 136.12545095555777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.9676828118645,
            "upper_bound": 472.5275167756477
          },
          "point_estimate": 385.22689111311297,
          "standard_error": 63.964378222563525
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 545437.5121884327,
            "upper_bound": 546228.9673424542
          },
          "point_estimate": 545807.3651533995,
          "standard_error": 203.18536585032209
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 545350.7388059702,
            "upper_bound": 546312.6298507462
          },
          "point_estimate": 545595.2039800994,
          "standard_error": 233.34793152896955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.97935742937383,
            "upper_bound": 1103.1594901164656
          },
          "point_estimate": 402.5475602039392,
          "standard_error": 259.00632139545144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 545431.8121949495,
            "upper_bound": 546553.580834147
          },
          "point_estimate": 545968.4648575305,
          "standard_error": 316.218061919037
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.09234680034353,
            "upper_bound": 856.6921181207306
          },
          "point_estimate": 676.823786400137,
          "standard_error": 147.16888012110414
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1970.9013138784223,
            "upper_bound": 1973.7022567771085
          },
          "point_estimate": 1972.2803386992848,
          "standard_error": 0.7177029245266293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1970.0036307391729,
            "upper_bound": 1974.5893574297188
          },
          "point_estimate": 1971.8711169000328,
          "standard_error": 1.319144529568255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4134605820441832,
            "upper_bound": 3.889894472168628
          },
          "point_estimate": 3.0748213309532395,
          "standard_error": 0.8973707704630256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1970.0295509004436,
            "upper_bound": 1972.7122109169816
          },
          "point_estimate": 1971.1010384817616,
          "standard_error": 0.713566406960894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5405852538626374,
            "upper_bound": 2.8055155619431535
          },
          "point_estimate": 2.396664694615415,
          "standard_error": 0.32422393831978985
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 574.1344997740921,
            "upper_bound": 577.8113872079207
          },
          "point_estimate": 575.601971607392,
          "standard_error": 0.9954099179496838
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573.9180207799693,
            "upper_bound": 575.9908171106558
          },
          "point_estimate": 574.7467118838229,
          "standard_error": 0.5013472162914466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.21031349011866035,
            "upper_bound": 2.6802646109924364
          },
          "point_estimate": 0.8886408028683466,
          "standard_error": 0.6645640990635978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 574.3822851744264,
            "upper_bound": 577.2833472666337
          },
          "point_estimate": 575.3168009500745,
          "standard_error": 0.7755644025244678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6103030790421675,
            "upper_bound": 4.99769379724531
          },
          "point_estimate": 3.315435009089126,
          "standard_error": 1.4067996559064044
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.15326577908844,
            "upper_bound": 113.33547649974084
          },
          "point_estimate": 113.24329081597337,
          "standard_error": 0.04648914042546176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.13324865585535,
            "upper_bound": 113.3601631187963
          },
          "point_estimate": 113.23482645530004,
          "standard_error": 0.06259988201441177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02848744543253474,
            "upper_bound": 0.2515762785921144
          },
          "point_estimate": 0.13594764058287656,
          "standard_error": 0.0595620210902985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.1446403504938,
            "upper_bound": 113.38396265532448
          },
          "point_estimate": 113.25643878272857,
          "standard_error": 0.060638051109328885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08519095409588237,
            "upper_bound": 0.19645886902424797
          },
          "point_estimate": 0.15427822419172016,
          "standard_error": 0.02852766086742035
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.90165154300962,
            "upper_bound": 36.94045197243132
          },
          "point_estimate": 36.92137764673078,
          "standard_error": 0.009902037218410011
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.903877332688694,
            "upper_bound": 36.946811345641066
          },
          "point_estimate": 36.916123957377835,
          "standard_error": 0.01188375530278003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004456322568841653,
            "upper_bound": 0.06415405799432
          },
          "point_estimate": 0.02092408541590999,
          "standard_error": 0.01440363067249442
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.91342261706688,
            "upper_bound": 36.94854230073335
          },
          "point_estimate": 36.92931708913132,
          "standard_error": 0.008970901761847266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017436174600300225,
            "upper_bound": 0.04336007022883026
          },
          "point_estimate": 0.032942563944495726,
          "standard_error": 0.006795280974566117
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.59797565766457,
            "upper_bound": 61.688303996811555
          },
          "point_estimate": 61.64155519353837,
          "standard_error": 0.023024864121190168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.60171518714395,
            "upper_bound": 61.68178010719778
          },
          "point_estimate": 61.63173913180685,
          "standard_error": 0.019825176035851415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010183421523168774,
            "upper_bound": 0.12842914078199863
          },
          "point_estimate": 0.05438725577498743,
          "standard_error": 0.027846763358985123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.60509050560191,
            "upper_bound": 61.67748643137748
          },
          "point_estimate": 61.634261468085576,
          "standard_error": 0.018613890966438288
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029955573381006455,
            "upper_bound": 0.10543556258887483
          },
          "point_estimate": 0.07664629207856635,
          "standard_error": 0.019682592614724173
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272774.69997672754,
            "upper_bound": 273429.9880623433
          },
          "point_estimate": 273056.23398585746,
          "standard_error": 168.74128513262323
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272701.8709273183,
            "upper_bound": 273140.7932330827
          },
          "point_estimate": 273074.52763157897,
          "standard_error": 136.83696131817376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.696436929941285,
            "upper_bound": 656.7307100511712
          },
          "point_estimate": 259.49401118252274,
          "standard_error": 179.85708362540237
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 272681.2287824825,
            "upper_bound": 273106.08589158487
          },
          "point_estimate": 272931.33637340105,
          "standard_error": 108.80822143374932
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 177.15400309971324,
            "upper_bound": 824.8442099299205
          },
          "point_estimate": 563.0854795550065,
          "standard_error": 197.9379701228462
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.697452127975744,
            "upper_bound": 47.76694455236055
          },
          "point_estimate": 47.731163470475074,
          "standard_error": 0.0178128240785522
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.67806855028511,
            "upper_bound": 47.78843579425736
          },
          "point_estimate": 47.72187532564479,
          "standard_error": 0.029699460435654485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002325914594048498,
            "upper_bound": 0.09718828245453869
          },
          "point_estimate": 0.06498898955792554,
          "standard_error": 0.025023452702935035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.691373313555,
            "upper_bound": 47.79650895106238
          },
          "point_estimate": 47.754687858893945,
          "standard_error": 0.027243061213512425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03394393545233719,
            "upper_bound": 0.07069079275122006
          },
          "point_estimate": 0.05947348929270827,
          "standard_error": 0.00908925392234144
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.91496966765173,
            "upper_bound": 84.02098814260562
          },
          "point_estimate": 83.96886441081003,
          "standard_error": 0.026995765412737644
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.90383777047624,
            "upper_bound": 84.03379447989948
          },
          "point_estimate": 83.97204986166042,
          "standard_error": 0.031406833314313896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026837849667716355,
            "upper_bound": 0.15632994117110866
          },
          "point_estimate": 0.09633690698512593,
          "standard_error": 0.033310644749032635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.95490256986379,
            "upper_bound": 84.03646218323685
          },
          "point_estimate": 83.99296920147872,
          "standard_error": 0.020843705257419392
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04629463978193991,
            "upper_bound": 0.11827837110197384
          },
          "point_estimate": 0.0898790310741255,
          "standard_error": 0.0185952158827629
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116369.92432258598,
            "upper_bound": 116579.96262464386
          },
          "point_estimate": 116471.98336106022,
          "standard_error": 53.820571204592426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116326.61627492878,
            "upper_bound": 116613.01963141024
          },
          "point_estimate": 116447.33955280828,
          "standard_error": 77.24551259491399
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.01081916837028,
            "upper_bound": 308.29961584069616
          },
          "point_estimate": 182.64871368042063,
          "standard_error": 67.00817889038768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116392.96876195943,
            "upper_bound": 116597.8543014309
          },
          "point_estimate": 116490.93762071262,
          "standard_error": 51.959936405856894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98.86206245167948,
            "upper_bound": 221.794035789047
          },
          "point_estimate": 179.46643240153122,
          "standard_error": 30.82783464798877
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.1851423599296,
            "upper_bound": 47.22709641862817
          },
          "point_estimate": 47.20396271621852,
          "standard_error": 0.01083049082793387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.1813250965552,
            "upper_bound": 47.226565589623306
          },
          "point_estimate": 47.1914664130536,
          "standard_error": 0.00994003582649636
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002938897137076246,
            "upper_bound": 0.05375479312852736
          },
          "point_estimate": 0.02411602292656106,
          "standard_error": 0.012686181693588703
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.17313588865991,
            "upper_bound": 47.1961418764464
          },
          "point_estimate": 47.18355303728557,
          "standard_error": 0.006133789141746555
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011820556119728725,
            "upper_bound": 0.04876039240243285
          },
          "point_estimate": 0.03606219919874373,
          "standard_error": 0.009944860923116274
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.11204232519496,
            "upper_bound": 79.20546204984615
          },
          "point_estimate": 79.15848898133375,
          "standard_error": 0.023897423743870452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.08483449460904,
            "upper_bound": 79.22882591867148
          },
          "point_estimate": 79.1625829802542,
          "standard_error": 0.04299701631531144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010675300696354865,
            "upper_bound": 0.12942264567235054
          },
          "point_estimate": 0.10843964076647408,
          "standard_error": 0.030897219580419844
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.08214012045869,
            "upper_bound": 79.1868108848822
          },
          "point_estimate": 79.13158526173311,
          "standard_error": 0.02763373146938605
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05159167209806384,
            "upper_bound": 0.09439134019488732
          },
          "point_estimate": 0.08003933578733705,
          "standard_error": 0.010909329745110484
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.61292955258767,
            "upper_bound": 214.95838436814665
          },
          "point_estimate": 214.77298007501273,
          "standard_error": 0.08847476934784716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.524070243864,
            "upper_bound": 214.98051289952215
          },
          "point_estimate": 214.73458778184877,
          "standard_error": 0.11782770578264738
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01645240308811624,
            "upper_bound": 0.4637382789818531
          },
          "point_estimate": 0.2982834649403948,
          "standard_error": 0.13577378477648433
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.5671554799342,
            "upper_bound": 214.89868152933937
          },
          "point_estimate": 214.7234842217232,
          "standard_error": 0.08513928478867229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1280901338095342,
            "upper_bound": 0.3852748754152344
          },
          "point_estimate": 0.29478627599485807,
          "standard_error": 0.06908673625850836
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.3188115509517,
            "upper_bound": 488.2489746552932
          },
          "point_estimate": 487.72404539375896,
          "standard_error": 0.2400204791041793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.1343672301225,
            "upper_bound": 487.92704722296753
          },
          "point_estimate": 487.6709086098441,
          "standard_error": 0.195180723871773
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09431668265610532,
            "upper_bound": 0.9775597949603596
          },
          "point_estimate": 0.5242571624571855,
          "standard_error": 0.2306397820060146
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 487.4150503933397,
            "upper_bound": 487.7708566193612
          },
          "point_estimate": 487.6287603274107,
          "standard_error": 0.0904036818392007
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2782011544826484,
            "upper_bound": 1.158511553178244
          },
          "point_estimate": 0.8003899454451987,
          "standard_error": 0.26825537202323296
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.057957058592265,
            "upper_bound": 19.084870491102787
          },
          "point_estimate": 19.07083025819254,
          "standard_error": 0.006910088356859781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.054311856600595,
            "upper_bound": 19.091397932339905
          },
          "point_estimate": 19.063575848310812,
          "standard_error": 0.009732664302585932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033684705434511115,
            "upper_bound": 0.03819823079296683
          },
          "point_estimate": 0.01807193495344236,
          "standard_error": 0.009653364505873206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.05106090613391,
            "upper_bound": 19.0727009888687
          },
          "point_estimate": 19.06154420108724,
          "standard_error": 0.005504455886435714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011512508521734433,
            "upper_bound": 0.028610702309038186
          },
          "point_estimate": 0.023054638523512635,
          "standard_error": 0.004259741666156789
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30373.12052347745,
            "upper_bound": 30429.981222819955
          },
          "point_estimate": 30403.970043302117,
          "standard_error": 14.40873308328179
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30391.44546722455,
            "upper_bound": 30424.371757322177
          },
          "point_estimate": 30411.45405857741,
          "standard_error": 9.015819357325476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.893076180913302,
            "upper_bound": 66.93196941144123
          },
          "point_estimate": 24.775327699623062,
          "standard_error": 14.050318219831144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30399.54361981742,
            "upper_bound": 30442.586592405776
          },
          "point_estimate": 30419.44821387817,
          "standard_error": 10.936812650505994
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.265911416412877,
            "upper_bound": 70.39086008238507
          },
          "point_estimate": 48.179248954480805,
          "standard_error": 15.620574721451264
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.01416732413072,
            "upper_bound": 119.24237224486689
          },
          "point_estimate": 119.1315981119873,
          "standard_error": 0.05852856384438541
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.94283600471658,
            "upper_bound": 119.27304318035031
          },
          "point_estimate": 119.1620585637336,
          "standard_error": 0.07443065283906244
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028041159758953136,
            "upper_bound": 0.3430236304200257
          },
          "point_estimate": 0.18048126169222703,
          "standard_error": 0.0855866319851761
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.08781538927605,
            "upper_bound": 119.3301775776974
          },
          "point_estimate": 119.2426278691098,
          "standard_error": 0.0613933735764551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10233782337779522,
            "upper_bound": 0.24397309720827556
          },
          "point_estimate": 0.19488782776532873,
          "standard_error": 0.03514308338413939
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964304.1426090226,
            "upper_bound": 966985.5228292084
          },
          "point_estimate": 965569.4279960316,
          "standard_error": 688.1306641172645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964035.954887218,
            "upper_bound": 967417.6900584796
          },
          "point_estimate": 965227.4654605264,
          "standard_error": 701.2692503610301
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 138.55400707973766,
            "upper_bound": 3982.8455148166945
          },
          "point_estimate": 1294.6130783318165,
          "standard_error": 951.0641590930164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964021.1768164882,
            "upper_bound": 965972.0248091604
          },
          "point_estimate": 964923.4723171564,
          "standard_error": 514.5705390690924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 950.712252357176,
            "upper_bound": 2931.3715598051726
          },
          "point_estimate": 2293.451044748825,
          "standard_error": 507.1378823774646
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1971.1804512874537,
            "upper_bound": 1974.1684503181648
          },
          "point_estimate": 1972.6691341362875,
          "standard_error": 0.7631559869773499
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1970.5220883534137,
            "upper_bound": 1974.542055383697
          },
          "point_estimate": 1972.8117659828504,
          "standard_error": 0.8529584394959363
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1441252363698766,
            "upper_bound": 4.854315449474799
          },
          "point_estimate": 2.5262977327460994,
          "standard_error": 1.3913814945352263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1971.3370887484484,
            "upper_bound": 1974.826995359444
          },
          "point_estimate": 1973.281442371804,
          "standard_error": 0.9082530994815812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4284635006671051,
            "upper_bound": 3.192386489087149
          },
          "point_estimate": 2.538577910077396,
          "standard_error": 0.4567445347642243
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.614304075693735,
            "upper_bound": 30.6700186047025
          },
          "point_estimate": 30.639945555023168,
          "standard_error": 0.014297185499366509
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.60394978329904,
            "upper_bound": 30.668080377104197
          },
          "point_estimate": 30.62846151158601,
          "standard_error": 0.01870613435808159
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005194040193624739,
            "upper_bound": 0.07800131864732367
          },
          "point_estimate": 0.040064088551164466,
          "standard_error": 0.017194912639495692
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.603144627671146,
            "upper_bound": 30.65312408048568
          },
          "point_estimate": 30.625719529886688,
          "standard_error": 0.013093205590446896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022437169118155954,
            "upper_bound": 0.06441544333321855
          },
          "point_estimate": 0.04766944907186625,
          "standard_error": 0.01190366323012773
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.714472720597115,
            "upper_bound": 33.76481498211773
          },
          "point_estimate": 33.738689528722844,
          "standard_error": 0.01289797257123769
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.69967574291418,
            "upper_bound": 33.7817706374566
          },
          "point_estimate": 33.72970392027405,
          "standard_error": 0.02214914761457123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005392035194502194,
            "upper_bound": 0.0724058929153333
          },
          "point_estimate": 0.04760398643519637,
          "standard_error": 0.018685241789164052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.70617465240761,
            "upper_bound": 33.772594363133635
          },
          "point_estimate": 33.730732457527694,
          "standard_error": 0.016956102715901162
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023511264876076816,
            "upper_bound": 0.05076298241903788
          },
          "point_estimate": 0.04296922985769983,
          "standard_error": 0.006613435103188757
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.70528603175722,
            "upper_bound": 23.73324452507736
          },
          "point_estimate": 23.717991811044097,
          "standard_error": 0.0071651155346574465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.697904198512504,
            "upper_bound": 23.728939862119024
          },
          "point_estimate": 23.71679070558804,
          "standard_error": 0.009418084727476177
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004351958323462052,
            "upper_bound": 0.038176803034387446
          },
          "point_estimate": 0.02276086908025137,
          "standard_error": 0.008589053094322226
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.701874520229175,
            "upper_bound": 23.745676453218906
          },
          "point_estimate": 23.72077424992037,
          "standard_error": 0.01139086786547392
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011445498246355617,
            "upper_bound": 0.032783436233984706
          },
          "point_estimate": 0.023900396217262868,
          "standard_error": 0.006316047268970848
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.375969558600836,
            "upper_bound": 27.43196204944637
          },
          "point_estimate": 27.40015359907772,
          "standard_error": 0.014605177862736726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.371397828162088,
            "upper_bound": 27.41378377258024
          },
          "point_estimate": 27.390472698539902,
          "standard_error": 0.009264293588725029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003689770219278588,
            "upper_bound": 0.05877740756086176
          },
          "point_estimate": 0.019296126954885393,
          "standard_error": 0.014204636874719803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.382528849753296,
            "upper_bound": 27.41517773108362
          },
          "point_estimate": 27.394968254613858,
          "standard_error": 0.00836427381406959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014275259333605713,
            "upper_bound": 0.07049994353211304
          },
          "point_estimate": 0.048753421455931936,
          "standard_error": 0.016862829890982287
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.70257506351467,
            "upper_bound": 34.762487320179915
          },
          "point_estimate": 34.73265985707168,
          "standard_error": 0.0152390571704852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.700359966933156,
            "upper_bound": 34.75486763539887
          },
          "point_estimate": 34.742887237831624,
          "standard_error": 0.014105055894650288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004104606257333799,
            "upper_bound": 0.08431448161693418
          },
          "point_estimate": 0.02931610424263176,
          "standard_error": 0.020945161713435287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.72297858568314,
            "upper_bound": 34.74600853082242
          },
          "point_estimate": 34.73843279732933,
          "standard_error": 0.005972796084599745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019954967558083847,
            "upper_bound": 0.0698828763568774
          },
          "point_estimate": 0.050863568751585715,
          "standard_error": 0.01306592199746515
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.79754681906467,
            "upper_bound": 56.97904033110119
          },
          "point_estimate": 56.87197283847966,
          "standard_error": 0.04822634923378151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.778216796875,
            "upper_bound": 56.89964475446429
          },
          "point_estimate": 56.83936283637153,
          "standard_error": 0.02887885626901807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012797994089195913,
            "upper_bound": 0.14274289703612075
          },
          "point_estimate": 0.07085204028248926,
          "standard_error": 0.03670538791043347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.806226923119056,
            "upper_bound": 56.88273516502809
          },
          "point_estimate": 56.83763788149351,
          "standard_error": 0.01954189299058485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03752250477677294,
            "upper_bound": 0.23979133866198668
          },
          "point_estimate": 0.16051811735361582,
          "standard_error": 0.06419996856506058
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.26253505931149,
            "upper_bound": 66.37223796071734
          },
          "point_estimate": 66.32272268001275,
          "standard_error": 0.02820232578649267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.2860556107974,
            "upper_bound": 66.38455833386547
          },
          "point_estimate": 66.33064207239013,
          "standard_error": 0.02900594501037576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013979738775164,
            "upper_bound": 0.12851184945340824
          },
          "point_estimate": 0.06791375442058208,
          "standard_error": 0.02960982782131504
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.29963763003865,
            "upper_bound": 66.40704124085708
          },
          "point_estimate": 66.35371554580487,
          "standard_error": 0.028320086245415304
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03704244782134119,
            "upper_bound": 0.13222914448750953
          },
          "point_estimate": 0.09378219851901544,
          "standard_error": 0.027723575542683208
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.66444623857982,
            "upper_bound": 47.79260223332665
          },
          "point_estimate": 47.72307704017254,
          "standard_error": 0.03298611781971977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.646431323391795,
            "upper_bound": 47.7826086756991
          },
          "point_estimate": 47.69970185452039,
          "standard_error": 0.03035936054904944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0201839418097897,
            "upper_bound": 0.1668364786110874
          },
          "point_estimate": 0.06539756275678064,
          "standard_error": 0.03918607985192933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.688357991396416,
            "upper_bound": 47.74988347384833
          },
          "point_estimate": 47.71718052474065,
          "standard_error": 0.01536586513838922
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04307786171124731,
            "upper_bound": 0.1499060690360999
          },
          "point_estimate": 0.10991404502156324,
          "standard_error": 0.02926594958160509
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.96352444721751,
            "upper_bound": 84.06581033157322
          },
          "point_estimate": 84.01665429573912,
          "standard_error": 0.026168831650980228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.94620389306775,
            "upper_bound": 84.06934266930108
          },
          "point_estimate": 84.0321859798722,
          "standard_error": 0.02630157873317119
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0154986032581874,
            "upper_bound": 0.1514894535828444
          },
          "point_estimate": 0.055104780816566334,
          "standard_error": 0.03578638230975376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.94174175575593,
            "upper_bound": 84.07093274954096
          },
          "point_estimate": 84.01913053473385,
          "standard_error": 0.03403447172831074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.035828077146627516,
            "upper_bound": 0.1135124263854222
          },
          "point_estimate": 0.08695851772694932,
          "standard_error": 0.018892196707510207
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.20537809166092,
            "upper_bound": 45.29420133210196
          },
          "point_estimate": 45.24563746504474,
          "standard_error": 0.022820721770542785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.20665206698181,
            "upper_bound": 45.27051781584731
          },
          "point_estimate": 45.24191441323705,
          "standard_error": 0.013882095477597708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001077661342859624,
            "upper_bound": 0.10283476160466463
          },
          "point_estimate": 0.0331795794179686,
          "standard_error": 0.02986474309611904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.20206247825768,
            "upper_bound": 45.28078184668191
          },
          "point_estimate": 45.23617845583016,
          "standard_error": 0.02029468264764933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02452136241937037,
            "upper_bound": 0.10825874813199006
          },
          "point_estimate": 0.076047066600004,
          "standard_error": 0.02352340012699122
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.156527566687835,
            "upper_bound": 47.2427299202557
          },
          "point_estimate": 47.19671029793308,
          "standard_error": 0.02214710574771641
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.14261062566091,
            "upper_bound": 47.259995956470846
          },
          "point_estimate": 47.178670631056406,
          "standard_error": 0.02760362531497112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010746133983907906,
            "upper_bound": 0.12402722085342188
          },
          "point_estimate": 0.05208113893047404,
          "standard_error": 0.02792665673597251
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.16601365674627,
            "upper_bound": 47.27884290712082
          },
          "point_estimate": 47.2212535142703,
          "standard_error": 0.03123024772880182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030772996776191507,
            "upper_bound": 0.092300653621168
          },
          "point_estimate": 0.07370179461075953,
          "standard_error": 0.015859130632029865
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.08486789485957,
            "upper_bound": 81.28977628045389
          },
          "point_estimate": 81.20455399698798,
          "standard_error": 0.054001233856046445
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.1807841779644,
            "upper_bound": 81.30307514201266
          },
          "point_estimate": 81.25003271130666,
          "standard_error": 0.031037847239136343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027425047533036163,
            "upper_bound": 0.17157299864584905
          },
          "point_estimate": 0.07669052588144676,
          "standard_error": 0.0457646202063194
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.23361647024777,
            "upper_bound": 81.29979969984062
          },
          "point_estimate": 81.26477090059785,
          "standard_error": 0.016596952996682755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04623099544460307,
            "upper_bound": 0.26735920584809625
          },
          "point_estimate": 0.17942178822072694,
          "standard_error": 0.06926831329376432
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963599.9378871396,
            "upper_bound": 966238.6965720552
          },
          "point_estimate": 964811.3839275272,
          "standard_error": 677.226656999865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963192.6013157894,
            "upper_bound": 966534.178362573
          },
          "point_estimate": 964065.2342575188,
          "standard_error": 765.9000233618983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 349.9482158923641,
            "upper_bound": 3383.4640662473353
          },
          "point_estimate": 1464.553290249021,
          "standard_error": 756.624066946665
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963487.2860934574,
            "upper_bound": 964349.11513928
          },
          "point_estimate": 963890.6045796308,
          "standard_error": 222.07717664873417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 718.2166401009098,
            "upper_bound": 2832.610607357728
          },
          "point_estimate": 2267.051870233764,
          "standard_error": 536.222053248517
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1234366.4319470895,
            "upper_bound": 1237314.790276984
          },
          "point_estimate": 1235817.618132275,
          "standard_error": 756.0574501721026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1233701.5,
            "upper_bound": 1237618.8238095238
          },
          "point_estimate": 1235813.665740741,
          "standard_error": 1213.0624904043232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442.1109199289244,
            "upper_bound": 4363.971247523908
          },
          "point_estimate": 2842.568337867573,
          "standard_error": 988.0379063359198
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1235366.264478065,
            "upper_bound": 1238788.5344741032
          },
          "point_estimate": 1237157.885108225,
          "standard_error": 920.967042666461
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1499.8461283411284,
            "upper_bound": 3188.996434497559
          },
          "point_estimate": 2519.1790957133617,
          "standard_error": 441.7114913917589
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1327925.0743598712,
            "upper_bound": 1329852.3353854166
          },
          "point_estimate": 1328868.0741680835,
          "standard_error": 492.1543843741394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1327700.2517857142,
            "upper_bound": 1329860.4523809524
          },
          "point_estimate": 1328767.9889455782,
          "standard_error": 440.1622428648727
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.71621779966425,
            "upper_bound": 2748.588119952905
          },
          "point_estimate": 1128.660117462335,
          "standard_error": 787.933801499177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1327160.4033266848,
            "upper_bound": 1329987.8208106968
          },
          "point_estimate": 1328273.4582560298,
          "standard_error": 723.3499306532599
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 767.0301647422014,
            "upper_bound": 2200.500026354947
          },
          "point_estimate": 1650.2325568676424,
          "standard_error": 367.7958534369859
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369619.49944642856,
            "upper_bound": 370981.62343537423
          },
          "point_estimate": 370308.4036394558,
          "standard_error": 349.0518280645311
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369400.8729591837,
            "upper_bound": 371444.7551020408
          },
          "point_estimate": 370186.60076530615,
          "standard_error": 676.1546722685365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.24955489319163,
            "upper_bound": 1954.9601074353516
          },
          "point_estimate": 1681.8228322845555,
          "standard_error": 530.4870404682398
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369593.10199519375,
            "upper_bound": 371119.53791904944
          },
          "point_estimate": 370583.4689901935,
          "standard_error": 382.75425698552743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 762.4981710809718,
            "upper_bound": 1399.8956611608123
          },
          "point_estimate": 1167.7866415514702,
          "standard_error": 167.05603659993312
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552097.3754241072,
            "upper_bound": 553182.7608802309
          },
          "point_estimate": 552624.4681655845,
          "standard_error": 278.2103835253366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 551909.0505050505,
            "upper_bound": 553488.4675324676
          },
          "point_estimate": 552265.0142045454,
          "standard_error": 447.3554422695804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.9323717048238,
            "upper_bound": 1509.2710486597807
          },
          "point_estimate": 1020.2127697663918,
          "standard_error": 380.4908787370077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 552131.5733147805,
            "upper_bound": 553126.5743801653
          },
          "point_estimate": 552494.8204250295,
          "standard_error": 254.69969801492397
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 532.6679119852633,
            "upper_bound": 1106.9871465682231
          },
          "point_estimate": 925.7538610629192,
          "standard_error": 143.16210862197474
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389710.81860815606,
            "upper_bound": 390590.61541666667
          },
          "point_estimate": 390171.4927976191,
          "standard_error": 224.6888334734572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389949.02056737593,
            "upper_bound": 390610.3953900709
          },
          "point_estimate": 390153.1909827761,
          "standard_error": 137.70328600049473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.46372671037289,
            "upper_bound": 1188.2786431591353
          },
          "point_estimate": 253.04452489587237,
          "standard_error": 287.7245780949448
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390054.9336205381,
            "upper_bound": 390352.3266583229
          },
          "point_estimate": 390171.47596021,
          "standard_error": 75.90802320145586
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.8819231389967,
            "upper_bound": 1068.5098136681495
          },
          "point_estimate": 750.5208272935715,
          "standard_error": 215.5291389882216
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483572.6547112573,
            "upper_bound": 485963.60595394735
          },
          "point_estimate": 484503.8208077485,
          "standard_error": 658.1806331344245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483234.24342105264,
            "upper_bound": 484647.8240131579
          },
          "point_estimate": 483839.5475146199,
          "standard_error": 347.76627915540365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.668131676645714,
            "upper_bound": 1444.4477343558776
          },
          "point_estimate": 947.0210290203396,
          "standard_error": 388.52963983182826
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 483782.2590885742,
            "upper_bound": 484394.5740769835
          },
          "point_estimate": 484056.73174982914,
          "standard_error": 156.82915127068276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 391.99796865681975,
            "upper_bound": 3338.5654535056924
          },
          "point_estimate": 2191.327719868703,
          "standard_error": 989.4371626964718
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1051138.602730159,
            "upper_bound": 1052951.450362585
          },
          "point_estimate": 1052061.4389433106,
          "standard_error": 462.0169117862723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1050285.4285714286,
            "upper_bound": 1053208.933333333
          },
          "point_estimate": 1052429.9842857143,
          "standard_error": 755.0208342538011
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.80915871559188,
            "upper_bound": 2643.149762617177
          },
          "point_estimate": 1218.332579798755,
          "standard_error": 663.1199174993345
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1050670.4328117112,
            "upper_bound": 1052790.9220536533
          },
          "point_estimate": 1051671.8163265309,
          "standard_error": 538.7629479718719
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 905.568912433887,
            "upper_bound": 1860.2593743254904
          },
          "point_estimate": 1536.705612987849,
          "standard_error": 240.91342170303508
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294871.0222796259,
            "upper_bound": 295168.99075940857
          },
          "point_estimate": 295006.3374439964,
          "standard_error": 76.61014623895458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294825.1747311828,
            "upper_bound": 295210.6471774194
          },
          "point_estimate": 294917.29653897847,
          "standard_error": 80.46578860038913
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.035626599551,
            "upper_bound": 372.5271562895245
          },
          "point_estimate": 157.20840487027718,
          "standard_error": 81.79245833446427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294896.7027921891,
            "upper_bound": 295197.3146233674
          },
          "point_estimate": 295018.2537704231,
          "standard_error": 78.10247598158875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.87616564085695,
            "upper_bound": 324.86967913693906
          },
          "point_estimate": 255.93487534300468,
          "standard_error": 64.2636842916115
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 778568.7656188786,
            "upper_bound": 779553.3828610268
          },
          "point_estimate": 779025.4208552854,
          "standard_error": 253.3739498293424
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 778388.0395981087,
            "upper_bound": 779556.1677811551
          },
          "point_estimate": 778745.7393617021,
          "standard_error": 300.0572109290624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.70846998448388,
            "upper_bound": 1327.791300682213
          },
          "point_estimate": 739.1245304949202,
          "standard_error": 302.05472493187347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 778483.4850890264,
            "upper_bound": 779707.1809231457
          },
          "point_estimate": 779090.1093672285,
          "standard_error": 319.1571832503299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 364.7694407905611,
            "upper_bound": 1120.1103725741457
          },
          "point_estimate": 846.3333728099988,
          "standard_error": 202.204750007718
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372036.4181186225,
            "upper_bound": 372552.31784715137
          },
          "point_estimate": 372289.4731264173,
          "standard_error": 132.69287156082095
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371878.7551020408,
            "upper_bound": 372770.0979591837
          },
          "point_estimate": 372217.0325255102,
          "standard_error": 202.3357101270912
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.48716132484242,
            "upper_bound": 759.7240020479478
          },
          "point_estimate": 510.12817040765594,
          "standard_error": 179.8159048583137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372145.49189804675,
            "upper_bound": 372664.20740905055
          },
          "point_estimate": 372394.4237211768,
          "standard_error": 141.95314976955808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 260.19467392913884,
            "upper_bound": 535.4777589199094
          },
          "point_estimate": 443.5102646950404,
          "standard_error": 68.64017057143214
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159419.95096693697,
            "upper_bound": 159837.60622595652
          },
          "point_estimate": 159600.18892233312,
          "standard_error": 108.43787519556297
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159409.9642077355,
            "upper_bound": 159716.19759825326
          },
          "point_estimate": 159534.61999636097,
          "standard_error": 66.82049431546224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.93837788012116,
            "upper_bound": 459.23134940160475
          },
          "point_estimate": 125.54427820782392,
          "standard_error": 114.93197056765813
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159397.63549844697,
            "upper_bound": 159566.2393367602
          },
          "point_estimate": 159505.71285657573,
          "standard_error": 43.39199153544579
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.37214549876036,
            "upper_bound": 518.1638936506196
          },
          "point_estimate": 359.5012317125302,
          "standard_error": 121.7545456266732
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191689.4094154135,
            "upper_bound": 191983.68882456137
          },
          "point_estimate": 191830.21577506265,
          "standard_error": 75.18210150340921
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191665.99780701753,
            "upper_bound": 191922.7823026316
          },
          "point_estimate": 191819.56701127815,
          "standard_error": 50.84089181068778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.75848806296971,
            "upper_bound": 417.93842994855993
          },
          "point_estimate": 97.08615028955435,
          "standard_error": 104.7985965031716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191782.23684559183,
            "upper_bound": 191877.95303385705
          },
          "point_estimate": 191837.81934381407,
          "standard_error": 24.58710664670176
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.41126633809908,
            "upper_bound": 349.88552380678266
          },
          "point_estimate": 250.0720214791305,
          "standard_error": 68.03551398856828
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152880.95688888055,
            "upper_bound": 153192.44015216085
          },
          "point_estimate": 153030.79691376555,
          "standard_error": 79.99176952502508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152794.7193277311,
            "upper_bound": 153256.3343837535
          },
          "point_estimate": 153046.32930672268,
          "standard_error": 113.18774354807525
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.5827582273257,
            "upper_bound": 478.2410543246579
          },
          "point_estimate": 332.4582492447207,
          "standard_error": 124.86277226861051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152871.90921687824,
            "upper_bound": 153092.1843337335
          },
          "point_estimate": 152967.43054676417,
          "standard_error": 56.1680230745835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.1122848383108,
            "upper_bound": 330.5589370642684
          },
          "point_estimate": 266.292492413486,
          "standard_error": 47.83737802047791
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2127943.2652314818,
            "upper_bound": 2132357.781668871
          },
          "point_estimate": 2130133.389817019,
          "standard_error": 1134.0813053349466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2128264.966666667,
            "upper_bound": 2132960.614197531
          },
          "point_estimate": 2129808.459656085,
          "standard_error": 976.0038459641016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.6115242515412,
            "upper_bound": 7521.993846087715
          },
          "point_estimate": 1747.941440078689,
          "standard_error": 1702.592527414094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2128579.736214857,
            "upper_bound": 2130679.810227273
          },
          "point_estimate": 2129439.453246753,
          "standard_error": 541.1981780285613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1065.6412915343983,
            "upper_bound": 5099.726975775845
          },
          "point_estimate": 3773.703506346798,
          "standard_error": 896.5899814982033
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-01",
      "fullname": "2021-05-01/memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4300.545808596837,
            "upper_bound": 4312.423907010847
          },
          "point_estimate": 4306.005475294465,
          "standard_error": 3.060191717632373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4298.6203451944675,
            "upper_bound": 4312.093155219293
          },
          "point_estimate": 4302.578471319173,
          "standard_error": 3.665769161843638
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.930660550920085,
            "upper_bound": 15.896260051946172
          },
          "point_estimate": 5.9608596892630645,
          "standard_error": 3.8455023894247815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4301.604319356935,
            "upper_bound": 4312.244798099827
          },
          "point_estimate": 4305.9115172804495,
          "standard_error": 2.7688369273202507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6824180168633056,
            "upper_bound": 13.398926318744982
          },
          "point_estimate": 10.229685635506325,
          "standard_error": 2.4689585337557993
        }
      }
    }
  }
}
