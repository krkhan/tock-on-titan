{
  "name": "2021-05-06",
  "benchmarks": {
    "memchr1/fallback/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/fallback/empty/never",
        "directory_name": "memchr1/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4263461641146495,
            "upper_bound": 2.444407754298862
          },
          "point_estimate": 2.436082818084952,
          "standard_error": 0.004638719848702386
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.427317477996126,
            "upper_bound": 2.4485765471215686
          },
          "point_estimate": 2.438636446379066,
          "standard_error": 0.004631149268480901
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017882626455568084,
            "upper_bound": 0.02451361710318657
          },
          "point_estimate": 0.01316897723030194,
          "standard_error": 0.006060807636779671
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4239947410918217,
            "upper_bound": 2.4410525245478465
          },
          "point_estimate": 2.4329361260741624,
          "standard_error": 0.004406017335747101
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0064519741845619665,
            "upper_bound": 0.02050965943202233
          },
          "point_estimate": 0.01546276009088622,
          "standard_error": 0.003751427843963692
        }
      }
    },
    "memchr1/fallback/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/common",
        "directory_name": "memchr1/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641403.2151264272,
            "upper_bound": 642734.8921367307
          },
          "point_estimate": 642144.795978836,
          "standard_error": 344.19008598590335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641605.6545808967,
            "upper_bound": 642885.7178362573
          },
          "point_estimate": 642432.5021929825,
          "standard_error": 307.5053619238049
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187.8757622784717,
            "upper_bound": 1588.963162494939
          },
          "point_estimate": 649.3545119803821,
          "standard_error": 362.8854224813588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 641425.1715684745,
            "upper_bound": 642781.537914545
          },
          "point_estimate": 642138.2576441102,
          "standard_error": 348.2370276849217
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 354.76529009532356,
            "upper_bound": 1588.2943979911413
          },
          "point_estimate": 1148.6705010769458,
          "standard_error": 343.05724303432413
        }
      }
    },
    "memchr1/fallback/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/never",
        "directory_name": "memchr1/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32023.592831527676,
            "upper_bound": 32075.8110978836
          },
          "point_estimate": 32047.910364176532,
          "standard_error": 13.400149262282165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32014.34651675485,
            "upper_bound": 32072.376377865963
          },
          "point_estimate": 32041.786963550858,
          "standard_error": 20.001933551300723
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9546634244139046,
            "upper_bound": 74.67575410633776
          },
          "point_estimate": 37.71695982421917,
          "standard_error": 19.25432819252137
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32015.54679071494,
            "upper_bound": 32058.92584164513
          },
          "point_estimate": 32039.25290776243,
          "standard_error": 11.106453410922263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.726579565790544,
            "upper_bound": 60.09926506695163
          },
          "point_estimate": 44.613999563977025,
          "standard_error": 10.80990210230856
        }
      }
    },
    "memchr1/fallback/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/rare",
        "directory_name": "memchr1/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32566.790736422525,
            "upper_bound": 32614.516750423525
          },
          "point_estimate": 32590.825771798707,
          "standard_error": 12.178184256621757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32560.222720478327,
            "upper_bound": 32619.05650224215
          },
          "point_estimate": 32592.963139013453,
          "standard_error": 15.28148164588283
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.766833106677035,
            "upper_bound": 70.45053569992308
          },
          "point_estimate": 43.6134816472313,
          "standard_error": 15.055558911898093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32573.121948385786,
            "upper_bound": 32610.53165321375
          },
          "point_estimate": 32592.084456350825,
          "standard_error": 9.465912282958616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.460114450577603,
            "upper_bound": 51.82812764005365
          },
          "point_estimate": 40.64662929503463,
          "standard_error": 7.52894524420722
        }
      }
    },
    "memchr1/fallback/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/uncommon",
        "directory_name": "memchr1/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151761.62992207112,
            "upper_bound": 152070.24392782428
          },
          "point_estimate": 151920.95696652718,
          "standard_error": 79.02173204673576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151763.30523012552,
            "upper_bound": 152149.4490934449
          },
          "point_estimate": 151903.0829846583,
          "standard_error": 97.10927571945868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.783053588195973,
            "upper_bound": 464.6937704111159
          },
          "point_estimate": 245.20078918238113,
          "standard_error": 101.86447584485676
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151762.86006273862,
            "upper_bound": 152067.77677957134
          },
          "point_estimate": 151902.41263924362,
          "standard_error": 79.87513111136313
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.00727449885036,
            "upper_bound": 349.0530925612879
          },
          "point_estimate": 263.3246774609031,
          "standard_error": 56.41964539713599
        }
      }
    },
    "memchr1/fallback/huge/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/fallback/huge/verycommon",
        "directory_name": "memchr1/fallback_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128153.0596380474,
            "upper_bound": 1133876.2538660415
          },
          "point_estimate": 1130445.8572667146,
          "standard_error": 1546.566236048673
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1127852.68013468,
            "upper_bound": 1131170.9875541124
          },
          "point_estimate": 1128559.626262626,
          "standard_error": 992.7493454016128
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167.37455480634168,
            "upper_bound": 4143.042635789687
          },
          "point_estimate": 1708.803171935307,
          "standard_error": 1098.8516321323027
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1127818.5257857756,
            "upper_bound": 1130538.0385454546
          },
          "point_estimate": 1128956.612514758,
          "standard_error": 697.7503400455137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1052.7256069335133,
            "upper_bound": 7767.777252625505
          },
          "point_estimate": 5159.231179562733,
          "standard_error": 2167.6338336807735
        }
      }
    },
    "memchr1/fallback/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/common",
        "directory_name": "memchr1/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.18327839100544,
            "upper_bound": 289.5504023060083
          },
          "point_estimate": 289.3612778219499,
          "standard_error": 0.09412069828090072
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.05275855753996,
            "upper_bound": 289.6214445656492
          },
          "point_estimate": 289.29057071499284,
          "standard_error": 0.13681255982140184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06113886475688825,
            "upper_bound": 0.5570158947538243
          },
          "point_estimate": 0.3719522801049574,
          "standard_error": 0.12696411727833248
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.172084561066,
            "upper_bound": 289.60317914206297
          },
          "point_estimate": 289.37671209999644,
          "standard_error": 0.11189921629251905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16817466958942895,
            "upper_bound": 0.3797898573905606
          },
          "point_estimate": 0.3137702530802078,
          "standard_error": 0.05222216912603458
        }
      }
    },
    "memchr1/fallback/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/never",
        "directory_name": "memchr1/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.26868229621729,
            "upper_bound": 39.30992487504989
          },
          "point_estimate": 39.28909442568538,
          "standard_error": 0.010593609067288855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.2571753566247,
            "upper_bound": 39.32112546140414
          },
          "point_estimate": 39.289465837878424,
          "standard_error": 0.01854606247753783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005190026163051398,
            "upper_bound": 0.058208801090673445
          },
          "point_estimate": 0.045677047845053304,
          "standard_error": 0.013974353485867223
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.25325939371539,
            "upper_bound": 39.296138710280935
          },
          "point_estimate": 39.27009018367643,
          "standard_error": 0.011089930803604127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02223221268012062,
            "upper_bound": 0.042184067480066506
          },
          "point_estimate": 0.03538065512685832,
          "standard_error": 0.005104870965997078
        }
      }
    },
    "memchr1/fallback/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/rare",
        "directory_name": "memchr1/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.33210400310117,
            "upper_bound": 49.6741122724895
          },
          "point_estimate": 49.503855853779314,
          "standard_error": 0.08728088681208258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.26428130966219,
            "upper_bound": 49.69732679305208
          },
          "point_estimate": 49.550114188890525,
          "standard_error": 0.10182700736689498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.037201832132617414,
            "upper_bound": 0.5572136151136198
          },
          "point_estimate": 0.22140693386290053,
          "standard_error": 0.13238032073128148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.30749735549838,
            "upper_bound": 49.59373812929367
          },
          "point_estimate": 49.471949812380046,
          "standard_error": 0.07234716035148736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15330363960760732,
            "upper_bound": 0.37715779365122304
          },
          "point_estimate": 0.2911344818073847,
          "standard_error": 0.05729367909738105
        }
      }
    },
    "memchr1/fallback/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/uncommon",
        "directory_name": "memchr1/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.29002451530592,
            "upper_bound": 83.50059328868456
          },
          "point_estimate": 83.39536909800862,
          "standard_error": 0.05361371619426228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.27824131809277,
            "upper_bound": 83.49078928934792
          },
          "point_estimate": 83.43417443180084,
          "standard_error": 0.06268615298237308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012011421940684951,
            "upper_bound": 0.30582225966355653
          },
          "point_estimate": 0.16649620427188838,
          "standard_error": 0.07771142776939854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.3075130383799,
            "upper_bound": 83.46111947425032
          },
          "point_estimate": 83.40338834463776,
          "standard_error": 0.03976025748631819
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08164607885427858,
            "upper_bound": 0.2403128607003703
          },
          "point_estimate": 0.1786882703262299,
          "standard_error": 0.04195282295544506
        }
      }
    },
    "memchr1/fallback/small/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/fallback/small/verycommon",
        "directory_name": "memchr1/fallback_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 554.9461887924465,
            "upper_bound": 555.9867306992647
          },
          "point_estimate": 555.4289676690823,
          "standard_error": 0.268040097255614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 554.8525018328446,
            "upper_bound": 555.9851103197877
          },
          "point_estimate": 555.1056010584678,
          "standard_error": 0.29334882432937653
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07824738841092463,
            "upper_bound": 1.4572695067528878
          },
          "point_estimate": 0.6009226423740032,
          "standard_error": 0.3640295490271859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 555.0121844832794,
            "upper_bound": 555.6171094919258
          },
          "point_estimate": 555.3039911975854,
          "standard_error": 0.15342046284818378
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3803394465146944,
            "upper_bound": 1.1868380737484725
          },
          "point_estimate": 0.8900370709819244,
          "standard_error": 0.2149293337781442
        }
      }
    },
    "memchr1/fallback/tiny/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/common",
        "directory_name": "memchr1/fallback_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.23920803086869,
            "upper_bound": 55.36616229483669
          },
          "point_estimate": 55.29716184208619,
          "standard_error": 0.0325932953574531
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.2452984523085,
            "upper_bound": 55.34114500912964
          },
          "point_estimate": 55.27228144654088,
          "standard_error": 0.02151366531947532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003913820289605248,
            "upper_bound": 0.17285720021784298
          },
          "point_estimate": 0.04915025384438598,
          "standard_error": 0.041182031104053066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.253324971522595,
            "upper_bound": 55.29370936746241
          },
          "point_estimate": 55.27593842827896,
          "standard_error": 0.010256367512239717
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029163778148102216,
            "upper_bound": 0.15091604682179896
          },
          "point_estimate": 0.10838298011811848,
          "standard_error": 0.03192084610497858
        }
      }
    },
    "memchr1/fallback/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/never",
        "directory_name": "memchr1/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.116728974741873,
            "upper_bound": 6.123526471714293
          },
          "point_estimate": 6.120307202946599,
          "standard_error": 0.0017438467953623407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.115330344702363,
            "upper_bound": 6.124576724341617
          },
          "point_estimate": 6.12156784679231,
          "standard_error": 0.001960816997733124
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008174689867396928,
            "upper_bound": 0.010344646408796416
          },
          "point_estimate": 0.004871705073388745,
          "standard_error": 0.0022875104046694624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.117941563304547,
            "upper_bound": 6.124581091827615
          },
          "point_estimate": 6.121580875251031,
          "standard_error": 0.0016768794089124263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025303584487315713,
            "upper_bound": 0.007336890341025477
          },
          "point_estimate": 0.005804582804664198,
          "standard_error": 0.0011960182710108677
        }
      }
    },
    "memchr1/fallback/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/rare",
        "directory_name": "memchr1/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.291037109139074,
            "upper_bound": 9.31590310840668
          },
          "point_estimate": 9.303406876117794,
          "standard_error": 0.006372108025027834
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.28945486449685,
            "upper_bound": 9.320466715813328
          },
          "point_estimate": 9.301678032808642,
          "standard_error": 0.0069010416671569964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004432594202873586,
            "upper_bound": 0.035573609154729825
          },
          "point_estimate": 0.015517341742324643,
          "standard_error": 0.00861344059808981
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.297909613014628,
            "upper_bound": 9.317916493598345
          },
          "point_estimate": 9.306568813745336,
          "standard_error": 0.005143593863188613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01084539013864688,
            "upper_bound": 0.02756252364656974
          },
          "point_estimate": 0.02121885794598086,
          "standard_error": 0.00425513790611048
        }
      }
    },
    "memchr1/fallback/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/fallback/tiny/uncommon",
        "directory_name": "memchr1/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.74241491858139,
            "upper_bound": 37.84055345748858
          },
          "point_estimate": 37.79064304040715,
          "standard_error": 0.0251966241999556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.73332698595864,
            "upper_bound": 37.85647510047704
          },
          "point_estimate": 37.782007223702784,
          "standard_error": 0.031195219967146554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02088397452735909,
            "upper_bound": 0.14496515297508386
          },
          "point_estimate": 0.08977761841302051,
          "standard_error": 0.03194761736286275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.753540425110565,
            "upper_bound": 37.84892012215176
          },
          "point_estimate": 37.811933545999025,
          "standard_error": 0.02443277847075318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04544204409872915,
            "upper_bound": 0.10811784132745852
          },
          "point_estimate": 0.08428879341344442,
          "standard_error": 0.016055051315596213
        }
      }
    },
    "memchr1/krate/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/krate/empty/never",
        "directory_name": "memchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2446407177997808,
            "upper_bound": 0.24692397756668724
          },
          "point_estimate": 0.245532779744598,
          "standard_error": 0.0006268890123920629
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24449540270724424,
            "upper_bound": 0.24568569536562268
          },
          "point_estimate": 0.24466286561010273,
          "standard_error": 0.0003747539012581626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00009306231804249116,
            "upper_bound": 0.001516447368611563
          },
          "point_estimate": 0.0004302776079664959,
          "standard_error": 0.0004047653431345875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24477080577818833,
            "upper_bound": 0.24670900466703283
          },
          "point_estimate": 0.2454713134564182,
          "standard_error": 0.0005002420762303894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000325146581204691,
            "upper_bound": 0.0031787517311315165
          },
          "point_estimate": 0.002092927251485379,
          "standard_error": 0.0009338449428225524
        }
      }
    },
    "memchr1/krate/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/common",
        "directory_name": "memchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219441.61440905288,
            "upper_bound": 219729.50646779977
          },
          "point_estimate": 219580.288327118,
          "standard_error": 73.91694708397334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219401.45883534136,
            "upper_bound": 219738.66957831325
          },
          "point_estimate": 219552.88382099828,
          "standard_error": 73.43794407766065
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.81659479890666,
            "upper_bound": 400.7437957769274
          },
          "point_estimate": 197.29222812387368,
          "standard_error": 96.5737352221555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219493.02759479845,
            "upper_bound": 219660.8676962264
          },
          "point_estimate": 219581.80583633235,
          "standard_error": 42.42751884434048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.31513513113325,
            "upper_bound": 328.2183083426251
          },
          "point_estimate": 246.34578298135892,
          "standard_error": 55.36086538153076
        }
      }
    },
    "memchr1/krate/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/never",
        "directory_name": "memchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9025.71523784981,
            "upper_bound": 9038.210836583045
          },
          "point_estimate": 9031.147505026847,
          "standard_error": 3.2391096790552516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9023.740416459676,
            "upper_bound": 9035.951884181392
          },
          "point_estimate": 9028.073161947344,
          "standard_error": 3.3367587315102565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9695575337068056,
            "upper_bound": 14.105816619912034
          },
          "point_estimate": 6.639311737570086,
          "standard_error": 3.4808142877500043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9025.348423697742,
            "upper_bound": 9032.1292277281
          },
          "point_estimate": 9028.206886407184,
          "standard_error": 1.7409036338708297
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3334145708551017,
            "upper_bound": 15.286262157592487
          },
          "point_estimate": 10.765112059659335,
          "standard_error": 3.3631528379932756
        }
      }
    },
    "memchr1/krate/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/rare",
        "directory_name": "memchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9410.298432971464,
            "upper_bound": 9420.801563255614
          },
          "point_estimate": 9415.721227598897,
          "standard_error": 2.6983227110599817
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9408.393898963732,
            "upper_bound": 9422.708664363849
          },
          "point_estimate": 9417.926230569949,
          "standard_error": 3.649567408095761
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.151095083572799,
            "upper_bound": 15.688482518104552
          },
          "point_estimate": 9.03411027225459,
          "standard_error": 3.4761999282857436
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9410.539824839532,
            "upper_bound": 9422.77923708295
          },
          "point_estimate": 9418.229495996233,
          "standard_error": 3.138149555765361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.739999146459591,
            "upper_bound": 11.096603040609148
          },
          "point_estimate": 9.016062134457176,
          "standard_error": 1.579175184142671
        }
      }
    },
    "memchr1/krate/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/uncommon",
        "directory_name": "memchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76397.18377601968,
            "upper_bound": 76645.98300231062
          },
          "point_estimate": 76502.12663803497,
          "standard_error": 65.61742060791724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76366.78143459915,
            "upper_bound": 76555.04825949366
          },
          "point_estimate": 76433.82107695399,
          "standard_error": 48.02247622538284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.208929276918026,
            "upper_bound": 221.6861520453123
          },
          "point_estimate": 118.71557352529688,
          "standard_error": 54.461454741259026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76365.65102166595,
            "upper_bound": 76458.27893586595
          },
          "point_estimate": 76408.95459477231,
          "standard_error": 23.49342018274232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.212594462533545,
            "upper_bound": 321.62893787192644
          },
          "point_estimate": 219.0658771792629,
          "standard_error": 80.33327933028735
        }
      }
    },
    "memchr1/krate/huge/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/krate/huge/verycommon",
        "directory_name": "memchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458809.2629806547,
            "upper_bound": 459646.1888071801
          },
          "point_estimate": 459201.4025788691,
          "standard_error": 215.2432367952417
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458612.13125,
            "upper_bound": 459682.4630208333
          },
          "point_estimate": 459129.66216517857,
          "standard_error": 284.1760373987798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.57694162058612,
            "upper_bound": 1136.8681615666198
          },
          "point_estimate": 740.8296915663958,
          "standard_error": 301.72297035861675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 458912.0898811235,
            "upper_bound": 459369.63191657217
          },
          "point_estimate": 459196.3773051948,
          "standard_error": 116.4406516489669
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.222618279496,
            "upper_bound": 940.6582729062604
          },
          "point_estimate": 717.1985784262353,
          "standard_error": 163.1393649671558
        }
      }
    },
    "memchr1/krate/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/common",
        "directory_name": "memchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.65632914312724,
            "upper_bound": 205.10186869394144
          },
          "point_estimate": 204.8847858146331,
          "standard_error": 0.11382713176225548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.59744198609016,
            "upper_bound": 205.25586216210132
          },
          "point_estimate": 204.86033662322657,
          "standard_error": 0.17411825154750904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09740334611223311,
            "upper_bound": 0.6797936867259683
          },
          "point_estimate": 0.450730227739236,
          "standard_error": 0.14413092166553515
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.7163065688997,
            "upper_bound": 205.16665261841752
          },
          "point_estimate": 204.89409071762827,
          "standard_error": 0.11625530928801732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22611576130782615,
            "upper_bound": 0.4767417764342814
          },
          "point_estimate": 0.3809662640835049,
          "standard_error": 0.06541483790339288
        }
      }
    },
    "memchr1/krate/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/never",
        "directory_name": "memchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.485926988212196,
            "upper_bound": 7.498824428345698
          },
          "point_estimate": 7.491844390055059,
          "standard_error": 0.003325556009910796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.483871203995035,
            "upper_bound": 7.500641762298404
          },
          "point_estimate": 7.488108360965766,
          "standard_error": 0.003886080170941928
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018075389735617412,
            "upper_bound": 0.018582020485872496
          },
          "point_estimate": 0.006680442568806365,
          "standard_error": 0.004167380773231763
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.4833565504755395,
            "upper_bound": 7.494209133947693
          },
          "point_estimate": 7.487718155938918,
          "standard_error": 0.002754457241704121
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003907381962831312,
            "upper_bound": 0.014686133412818192
          },
          "point_estimate": 0.011067544676610412,
          "standard_error": 0.0027314221390116623
        }
      }
    },
    "memchr1/krate/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/rare",
        "directory_name": "memchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.123474329995368,
            "upper_bound": 12.139009415438448
          },
          "point_estimate": 12.13164635349714,
          "standard_error": 0.003961897298695584
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.125051563356491,
            "upper_bound": 12.140247987778237
          },
          "point_estimate": 12.133249497708974,
          "standard_error": 0.003346314449840447
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009832715899538234,
            "upper_bound": 0.022002872655106205
          },
          "point_estimate": 0.008883020911276903,
          "standard_error": 0.005858656046928566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.129715184272166,
            "upper_bound": 12.14115798003389
          },
          "point_estimate": 12.134501062876245,
          "standard_error": 0.0029123577217109444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005847091250079399,
            "upper_bound": 0.01778440583778872
          },
          "point_estimate": 0.01322917589969867,
          "standard_error": 0.00319074548824946
        }
      }
    },
    "memchr1/krate/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/uncommon",
        "directory_name": "memchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.02065954944817,
            "upper_bound": 46.08429869566535
          },
          "point_estimate": 46.05385819544713,
          "standard_error": 0.016284182945738218
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.01989133457129,
            "upper_bound": 46.10552506363852
          },
          "point_estimate": 46.05114003422947,
          "standard_error": 0.02612712704223715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002636667668358625,
            "upper_bound": 0.09977346258160276
          },
          "point_estimate": 0.07122904650577505,
          "standard_error": 0.02509717968826513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.05276746116914,
            "upper_bound": 46.1000819147688
          },
          "point_estimate": 46.07993269779122,
          "standard_error": 0.012300580040595546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03100288861363071,
            "upper_bound": 0.0698340891193006
          },
          "point_estimate": 0.05438821141559133,
          "standard_error": 0.010603402284827423
        }
      }
    },
    "memchr1/krate/small/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/krate/small/verycommon",
        "directory_name": "memchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.4147092483839,
            "upper_bound": 496.3084273109765
          },
          "point_estimate": 495.80646458256496,
          "standard_error": 0.2314753513081275
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.27868835072667,
            "upper_bound": 496.1710005999782
          },
          "point_estimate": 495.4742074314181,
          "standard_error": 0.25770244517637153
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08512574118899116,
            "upper_bound": 1.0889418280548044
          },
          "point_estimate": 0.4234778509005942,
          "standard_error": 0.2637977429626698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.3704363333812,
            "upper_bound": 495.91150400691663
          },
          "point_estimate": 495.6258567575111,
          "standard_error": 0.13776625231215975
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27515949976787984,
            "upper_bound": 1.0874911447163615
          },
          "point_estimate": 0.7709756871666753,
          "standard_error": 0.23445792982651825
        }
      }
    },
    "memchr1/krate/tiny/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/common",
        "directory_name": "memchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.84860740629426,
            "upper_bound": 51.92877190317445
          },
          "point_estimate": 51.883923237879614,
          "standard_error": 0.020683441578232466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.84903916052096,
            "upper_bound": 51.91155825562508
          },
          "point_estimate": 51.87105706413814,
          "standard_error": 0.01277659485686116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003271724458724064,
            "upper_bound": 0.10453363562771369
          },
          "point_estimate": 0.021863445315952516,
          "standard_error": 0.025297578245283153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.85658137950101,
            "upper_bound": 51.89557424546187
          },
          "point_estimate": 51.87086937591343,
          "standard_error": 0.0100235357851113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01704686074714506,
            "upper_bound": 0.09690592943003476
          },
          "point_estimate": 0.06888614808806978,
          "standard_error": 0.021634363571084007
        }
      }
    },
    "memchr1/krate/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/never",
        "directory_name": "memchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6862880887724545,
            "upper_bound": 3.7023559302457465
          },
          "point_estimate": 3.6943465981572943,
          "standard_error": 0.004108211128464133
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6828478224676418,
            "upper_bound": 3.703790515360746
          },
          "point_estimate": 3.695210339582919,
          "standard_error": 0.004834391469619707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003836827497752666,
            "upper_bound": 0.02301720807225103
          },
          "point_estimate": 0.010583680019270342,
          "standard_error": 0.0053914649016464315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.689481133656103,
            "upper_bound": 3.710137563330229
          },
          "point_estimate": 3.7012889590628126,
          "standard_error": 0.0053520263200037485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007192557921126743,
            "upper_bound": 0.017631311630110353
          },
          "point_estimate": 0.0136631816745059,
          "standard_error": 0.0026740894807649345
        }
      }
    },
    "memchr1/krate/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/rare",
        "directory_name": "memchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.337027633639196,
            "upper_bound": 6.35919491909377
          },
          "point_estimate": 6.3475957499737214,
          "standard_error": 0.005683291839998976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.3362853576748055,
            "upper_bound": 6.360361863931439
          },
          "point_estimate": 6.342126966596663,
          "standard_error": 0.00674737493832207
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015506902564137288,
            "upper_bound": 0.031532075758999766
          },
          "point_estimate": 0.012093983687567356,
          "standard_error": 0.008045862988571365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.337575903881265,
            "upper_bound": 6.35078371717057
          },
          "point_estimate": 6.34224364189077,
          "standard_error": 0.0034088551754567317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008662261194797826,
            "upper_bound": 0.024964168372485017
          },
          "point_estimate": 0.019010927345399393,
          "standard_error": 0.004211443571636322
        }
      }
    },
    "memchr1/krate/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/krate/tiny/uncommon",
        "directory_name": "memchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.942410241100134,
            "upper_bound": 17.9668291938189
          },
          "point_estimate": 17.953345463069986,
          "standard_error": 0.0063167118901838355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.937912799479314,
            "upper_bound": 17.96713542512209
          },
          "point_estimate": 17.946207635701697,
          "standard_error": 0.007420399067673473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024063382146423656,
            "upper_bound": 0.032475955354151584
          },
          "point_estimate": 0.012899149584787752,
          "standard_error": 0.007695779879030986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.939198762643418,
            "upper_bound": 17.95826183092088
          },
          "point_estimate": 17.9455388365012,
          "standard_error": 0.004908757910907032
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006701532868419379,
            "upper_bound": 0.028821140356046833
          },
          "point_estimate": 0.02104702171652053,
          "standard_error": 0.005810556002701828
        }
      }
    },
    "memchr1/libc/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/libc/empty/never",
        "directory_name": "memchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9554111935310512,
            "upper_bound": 1.9591241185825936
          },
          "point_estimate": 1.9573482059362028,
          "standard_error": 0.000950558705032863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9551286260078975,
            "upper_bound": 1.9595792441501283
          },
          "point_estimate": 1.9578910630336213,
          "standard_error": 0.000849077570958715
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003175020141580373,
            "upper_bound": 0.005862011348977638
          },
          "point_estimate": 0.001548263638753823,
          "standard_error": 0.0015349976702458625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.957202468038943,
            "upper_bound": 1.9585140700833104
          },
          "point_estimate": 1.957777641319835,
          "standard_error": 0.0003324088863945556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00145630979420348,
            "upper_bound": 0.004107028652842265
          },
          "point_estimate": 0.0031621967970686597,
          "standard_error": 0.0006926537667047341
        }
      }
    },
    "memchr1/libc/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/common",
        "directory_name": "memchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280552.2508371795,
            "upper_bound": 280818.29235042736
          },
          "point_estimate": 280688.3726752136,
          "standard_error": 68.22213049757279
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280483.58653846156,
            "upper_bound": 280913.7290598291
          },
          "point_estimate": 280706.4865384615,
          "standard_error": 126.53170980447156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.550096942834095,
            "upper_bound": 366.23165542122155
          },
          "point_estimate": 304.25390780357196,
          "standard_error": 97.20306940539704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280557.17996555683,
            "upper_bound": 280859.2878636907
          },
          "point_estimate": 280738.6565034965,
          "standard_error": 77.53824579496786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143.66877811859774,
            "upper_bound": 265.8255243852878
          },
          "point_estimate": 227.58244722545956,
          "standard_error": 31.22992650431716
        }
      }
    },
    "memchr1/libc/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/never",
        "directory_name": "memchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8422.982467411854,
            "upper_bound": 8432.189528029307
          },
          "point_estimate": 8427.308923771197,
          "standard_error": 2.3653853569393153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8421.427360774818,
            "upper_bound": 8437.088654128938
          },
          "point_estimate": 8424.14633735779,
          "standard_error": 3.710957630277476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9548518695117272,
            "upper_bound": 12.618627612441944
          },
          "point_estimate": 4.215145714579089,
          "standard_error": 2.8996998869934596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8420.760479585264,
            "upper_bound": 8434.844909525558
          },
          "point_estimate": 8426.906498934082,
          "standard_error": 3.752543855155999
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7982147442759113,
            "upper_bound": 9.300862292962185
          },
          "point_estimate": 7.8960080353390945,
          "standard_error": 1.4010560246024026
        }
      }
    },
    "memchr1/libc/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/rare",
        "directory_name": "memchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9478.740089110092,
            "upper_bound": 9490.731665299018
          },
          "point_estimate": 9484.325239856602,
          "standard_error": 3.0833500720671867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9476.61592689295,
            "upper_bound": 9493.72819843342
          },
          "point_estimate": 9480.028766318535,
          "standard_error": 4.219191158878131
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.46962225145364395,
            "upper_bound": 17.011477691022414
          },
          "point_estimate": 6.034684049955363,
          "standard_error": 4.0098869877602175
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9477.515751542547,
            "upper_bound": 9491.43226099094
          },
          "point_estimate": 9483.898710793124,
          "standard_error": 3.5298837170143686
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9072138648114394,
            "upper_bound": 12.235012023807249
          },
          "point_estimate": 10.24302735413078,
          "standard_error": 2.0107539751816748
        }
      }
    },
    "memchr1/libc/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/uncommon",
        "directory_name": "memchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72729.90448388888,
            "upper_bound": 72930.78279214285
          },
          "point_estimate": 72812.86286436507,
          "standard_error": 53.28022498088821
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72712.95766666667,
            "upper_bound": 72849.0115357143
          },
          "point_estimate": 72766.140875,
          "standard_error": 29.492806905360393
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.079737431556303,
            "upper_bound": 174.22741820683677
          },
          "point_estimate": 56.45885665098539,
          "standard_error": 46.2319098819552
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72754.65320168067,
            "upper_bound": 72828.15315917828
          },
          "point_estimate": 72786.69057662338,
          "standard_error": 18.937728983058737
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.389431768735825,
            "upper_bound": 263.98328837691554
          },
          "point_estimate": 177.75593133671964,
          "standard_error": 69.31190236942626
        }
      }
    },
    "memchr1/libc/huge/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/libc/huge/verycommon",
        "directory_name": "memchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 588697.2066628265,
            "upper_bound": 589507.8016299444
          },
          "point_estimate": 589091.1012525602,
          "standard_error": 206.92190515193525
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 588663.2530241936,
            "upper_bound": 589667.3113799284
          },
          "point_estimate": 589059.6574500769,
          "standard_error": 223.5377401982655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.93791480311768,
            "upper_bound": 1208.5683559630904
          },
          "point_estimate": 476.07180465303026,
          "standard_error": 297.3777513189416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 588628.8700565269,
            "upper_bound": 589626.1441203445
          },
          "point_estimate": 589156.5059069962,
          "standard_error": 255.28125441264885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 310.62473204998383,
            "upper_bound": 895.3128764891842
          },
          "point_estimate": 689.7328672299667,
          "standard_error": 142.66737426141373
        }
      }
    },
    "memchr1/libc/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/common",
        "directory_name": "memchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.34457428818064,
            "upper_bound": 219.59775345710983
          },
          "point_estimate": 219.46387145709664,
          "standard_error": 0.06487270610413111
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.2772260065764,
            "upper_bound": 219.5845642904717
          },
          "point_estimate": 219.4301174850578,
          "standard_error": 0.11096549826366744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008483788038208546,
            "upper_bound": 0.4118273594766169
          },
          "point_estimate": 0.21878060554778073,
          "standard_error": 0.11253593433467846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.34006910531824,
            "upper_bound": 219.53812117874253
          },
          "point_estimate": 219.43639157705692,
          "standard_error": 0.0507863709795426
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1175439199209095,
            "upper_bound": 0.28930111376080747
          },
          "point_estimate": 0.21690848495071752,
          "standard_error": 0.0503397301508178
        }
      }
    },
    "memchr1/libc/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/never",
        "directory_name": "memchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.401473495459347,
            "upper_bound": 7.4374254081472095
          },
          "point_estimate": 7.418733314629044,
          "standard_error": 0.00922559186311238
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.396810814605818,
            "upper_bound": 7.449468593907021
          },
          "point_estimate": 7.408199012938875,
          "standard_error": 0.01284913656798028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004117933880750356,
            "upper_bound": 0.04998294332215228
          },
          "point_estimate": 0.02467168759963153,
          "standard_error": 0.012648198069024608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.389203550212368,
            "upper_bound": 7.420228509618077
          },
          "point_estimate": 7.403774384222561,
          "standard_error": 0.008254700167245893
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015796984274264948,
            "upper_bound": 0.03797912132660668
          },
          "point_estimate": 0.03088986567359572,
          "standard_error": 0.005477968381019744
        }
      }
    },
    "memchr1/libc/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/rare",
        "directory_name": "memchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.232392528626644,
            "upper_bound": 11.256841887678522
          },
          "point_estimate": 11.2425380706896,
          "standard_error": 0.006444385150893385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.230265802908306,
            "upper_bound": 11.24589845915943
          },
          "point_estimate": 11.237921436777224,
          "standard_error": 0.004195308718016326
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018634520447398488,
            "upper_bound": 0.02112140425956836
          },
          "point_estimate": 0.01126600341600999,
          "standard_error": 0.004877050135876294
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.227746971753582,
            "upper_bound": 11.242013507607052
          },
          "point_estimate": 11.23507904231776,
          "standard_error": 0.003713531535887416
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005772476447431186,
            "upper_bound": 0.03224932476195942
          },
          "point_estimate": 0.02162252876553171,
          "standard_error": 0.008449601576737947
        }
      }
    },
    "memchr1/libc/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/uncommon",
        "directory_name": "memchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.275640106818,
            "upper_bound": 47.33174584986267
          },
          "point_estimate": 47.30016217045002,
          "standard_error": 0.01458494937893898
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.2738155814241,
            "upper_bound": 47.313200580167646
          },
          "point_estimate": 47.291181324183185,
          "standard_error": 0.010155988473915556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0028965241683430506,
            "upper_bound": 0.061783547332901166
          },
          "point_estimate": 0.02607561521348805,
          "standard_error": 0.014392081737995851
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.28410108851472,
            "upper_bound": 47.310467210361466
          },
          "point_estimate": 47.29847806975921,
          "standard_error": 0.00663608932796865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014592539179315642,
            "upper_bound": 0.06987399838387816
          },
          "point_estimate": 0.048667793980777856,
          "standard_error": 0.016230200809549546
        }
      }
    },
    "memchr1/libc/small/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/libc/small/verycommon",
        "directory_name": "memchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 536.1433771149897,
            "upper_bound": 536.96340696161
          },
          "point_estimate": 536.5489453582595,
          "standard_error": 0.20836048680405475
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 536.2395445749092,
            "upper_bound": 537.2004159046664
          },
          "point_estimate": 536.3329975615991,
          "standard_error": 0.24815317962690644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03068396374943377,
            "upper_bound": 1.3582737340711832
          },
          "point_estimate": 0.14533867824971716,
          "standard_error": 0.3846724334728357
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 536.2396238635254,
            "upper_bound": 536.6253479145773
          },
          "point_estimate": 536.394243338515,
          "standard_error": 0.09808308844991576
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3050914781814018,
            "upper_bound": 0.8959773763490119
          },
          "point_estimate": 0.6941735316649054,
          "standard_error": 0.1402826981287386
        }
      }
    },
    "memchr1/libc/tiny/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/common",
        "directory_name": "memchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.36638914242686,
            "upper_bound": 49.43941699915783
          },
          "point_estimate": 49.39577883791388,
          "standard_error": 0.01963402048578507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.36473867908007,
            "upper_bound": 49.39490395132019
          },
          "point_estimate": 49.38675003661605,
          "standard_error": 0.009056016217774993
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031859898369513886,
            "upper_bound": 0.05030354019918489
          },
          "point_estimate": 0.01460715903382489,
          "standard_error": 0.012867653542057478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.38171202185009,
            "upper_bound": 49.3967160398471
          },
          "point_estimate": 49.38968267808176,
          "standard_error": 0.0037778410419979903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011196614439253676,
            "upper_bound": 0.09853468030933205
          },
          "point_estimate": 0.06510102501476452,
          "standard_error": 0.02795150955874702
        }
      }
    },
    "memchr1/libc/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/never",
        "directory_name": "memchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4209446965658987,
            "upper_bound": 3.4260598249842062
          },
          "point_estimate": 3.4233841691231093,
          "standard_error": 0.0013073991109264504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4198491142477607,
            "upper_bound": 3.4263765529403747
          },
          "point_estimate": 3.4223485712937523,
          "standard_error": 0.001660855577634457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004448650628730603,
            "upper_bound": 0.007327914654894735
          },
          "point_estimate": 0.004708406992015882,
          "standard_error": 0.0016981725771332869
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.420747055488791,
            "upper_bound": 3.4263588727637804
          },
          "point_estimate": 3.423808226389036,
          "standard_error": 0.001438072961371247
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022412844491925613,
            "upper_bound": 0.005672474717064968
          },
          "point_estimate": 0.004357210911302715,
          "standard_error": 0.0009076806784205568
        }
      }
    },
    "memchr1/libc/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/rare",
        "directory_name": "memchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.123662577224042,
            "upper_bound": 6.1414870042910215
          },
          "point_estimate": 6.131012141794601,
          "standard_error": 0.004671649826574683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.122473720966807,
            "upper_bound": 6.133074463567894
          },
          "point_estimate": 6.127727851576685,
          "standard_error": 0.002698434821478251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005491156368766272,
            "upper_bound": 0.014549671262837249
          },
          "point_estimate": 0.006259690301215891,
          "standard_error": 0.0036265155049021464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.1265999122004215,
            "upper_bound": 6.132423792061126
          },
          "point_estimate": 6.130183638292865,
          "standard_error": 0.0014734086036986462
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033747659576125176,
            "upper_bound": 0.02315758273830325
          },
          "point_estimate": 0.01549991725493134,
          "standard_error": 0.006125079245846746
        }
      }
    },
    "memchr1/libc/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/libc/tiny/uncommon",
        "directory_name": "memchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.690808043032465,
            "upper_bound": 18.726114480122742
          },
          "point_estimate": 18.709321075902853,
          "standard_error": 0.008907590640559104
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.703489960735595,
            "upper_bound": 18.72005585988105
          },
          "point_estimate": 18.70762688943234,
          "standard_error": 0.004758150075300151
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002130285852908656,
            "upper_bound": 0.04590445306735921
          },
          "point_estimate": 0.00786842620920997,
          "standard_error": 0.010210585503197868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.705842868803092,
            "upper_bound": 18.72478453943584
          },
          "point_estimate": 18.714158213755287,
          "standard_error": 0.004911734631101654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0067975502550106075,
            "upper_bound": 0.0425006250074504
          },
          "point_estimate": 0.029724245658409938,
          "standard_error": 0.00934491030163238
        }
      }
    },
    "memchr1/naive/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr1/naive/empty/never",
        "directory_name": "memchr1/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5029226469610583,
            "upper_bound": 0.5221591287859981
          },
          "point_estimate": 0.5119494706594859,
          "standard_error": 0.004935362465557124
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4957233886286379,
            "upper_bound": 0.521613097081004
          },
          "point_estimate": 0.5135171980565278,
          "standard_error": 0.00645223587127553
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005184150328580049,
            "upper_bound": 0.028144697953478112
          },
          "point_estimate": 0.014517238743714153,
          "standard_error": 0.007012500737883391
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5010689722027131,
            "upper_bound": 0.5166804808796273
          },
          "point_estimate": 0.509368656802812,
          "standard_error": 0.003921064972094395
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00837032915538283,
            "upper_bound": 0.022233386468068637
          },
          "point_estimate": 0.01653926419775529,
          "standard_error": 0.003990819730646
        }
      }
    },
    "memchr1/naive/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/common",
        "directory_name": "memchr1/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461007.67043964734,
            "upper_bound": 464130.15940758743
          },
          "point_estimate": 462527.6489044606,
          "standard_error": 800.641441583711
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 460222.84810126584,
            "upper_bound": 464609.864978903
          },
          "point_estimate": 462519.4137658228,
          "standard_error": 1133.3281660727714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 860.2614315627197,
            "upper_bound": 4590.023953637388
          },
          "point_estimate": 2866.5751450574853,
          "standard_error": 988.981895990544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 460958.550491003,
            "upper_bound": 463849.2038967011
          },
          "point_estimate": 462674.6213381555,
          "standard_error": 739.7367601848782
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1538.666699114966,
            "upper_bound": 3317.522983971616
          },
          "point_estimate": 2664.3792717466367,
          "standard_error": 460.081266154118
        }
      }
    },
    "memchr1/naive/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/never",
        "directory_name": "memchr1/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145431.61378871428,
            "upper_bound": 145664.16796682542
          },
          "point_estimate": 145549.9392763492,
          "standard_error": 59.183198639450154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145424.8308,
            "upper_bound": 145706.0895
          },
          "point_estimate": 145549.89905,
          "standard_error": 50.6923290765853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.816370800436165,
            "upper_bound": 365.5513321101688
          },
          "point_estimate": 79.30987352530693,
          "standard_error": 103.27599541788834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145466.62429411765,
            "upper_bound": 145660.73263003663
          },
          "point_estimate": 145549.15382857143,
          "standard_error": 49.23837646024613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.19448363290608,
            "upper_bound": 259.8316687023914
          },
          "point_estimate": 197.2375570409604,
          "standard_error": 42.77353188483725
        }
      }
    },
    "memchr1/naive/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/rare",
        "directory_name": "memchr1/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146983.7358471942,
            "upper_bound": 147181.49703293008
          },
          "point_estimate": 147082.706609543,
          "standard_error": 50.53159409832414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146970.6945564516,
            "upper_bound": 147210.74798387097
          },
          "point_estimate": 147054.86491935485,
          "standard_error": 66.85170709997466
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.218384742928905,
            "upper_bound": 293.2262912861062
          },
          "point_estimate": 164.68118699568234,
          "standard_error": 63.589696224754846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147034.94377045348,
            "upper_bound": 147196.97147354274
          },
          "point_estimate": 147131.53250942606,
          "standard_error": 41.773248792189506
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.1796606267024,
            "upper_bound": 217.65796764036463
          },
          "point_estimate": 167.94374664824215,
          "standard_error": 32.515985287034994
        }
      }
    },
    "memchr1/naive/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/uncommon",
        "directory_name": "memchr1/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200630.70316522763,
            "upper_bound": 201084.15190091904
          },
          "point_estimate": 200851.10337585036,
          "standard_error": 115.59714985171637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200552.28178963895,
            "upper_bound": 201116.2252747253
          },
          "point_estimate": 200823.42912087913,
          "standard_error": 128.3266105355483
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.96154207745607,
            "upper_bound": 650.7093269091408
          },
          "point_estimate": 418.05129807259345,
          "standard_error": 146.91407548963383
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200604.26929081848,
            "upper_bound": 200999.7572165444
          },
          "point_estimate": 200767.8650206936,
          "standard_error": 101.47745643881473
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 194.18210750928492,
            "upper_bound": 510.6159067793873
          },
          "point_estimate": 385.5023412528249,
          "standard_error": 82.45597730317242
        }
      }
    },
    "memchr1/naive/huge/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr1/naive/huge/verycommon",
        "directory_name": "memchr1/naive_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 949769.0756951566,
            "upper_bound": 956594.3462179486
          },
          "point_estimate": 953005.124693732,
          "standard_error": 1744.1233435098616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 949981.3455840456,
            "upper_bound": 956448.9273504274
          },
          "point_estimate": 951361.0288461538,
          "standard_error": 1725.5329746804273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 666.5321744914625,
            "upper_bound": 9672.1429409773
          },
          "point_estimate": 3628.4530837872335,
          "standard_error": 2307.39059934498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 947527.1022071508,
            "upper_bound": 952672.0935356914
          },
          "point_estimate": 949942.5104895104,
          "standard_error": 1292.0912430151504
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2427.659295948566,
            "upper_bound": 7830.963740420786
          },
          "point_estimate": 5800.485331464554,
          "standard_error": 1416.3054962208182
        }
      }
    },
    "memchr1/naive/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/common",
        "directory_name": "memchr1/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.52958947288056,
            "upper_bound": 242.6357108904803
          },
          "point_estimate": 240.92751732346483,
          "standard_error": 0.7950503343115555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.59552842129204,
            "upper_bound": 241.47698028835683
          },
          "point_estimate": 240.68461745769463,
          "standard_error": 0.5034321481038655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4258526601342556,
            "upper_bound": 3.3705002400285196
          },
          "point_estimate": 1.1482304236564005,
          "standard_error": 0.7207483118016458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240.02240940779663,
            "upper_bound": 242.5039953859373
          },
          "point_estimate": 241.0652835734872,
          "standard_error": 0.621381081680988
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6927801076400014,
            "upper_bound": 3.8425392709355464
          },
          "point_estimate": 2.644474168104175,
          "standard_error": 0.8814299651404401
        }
      }
    },
    "memchr1/naive/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/never",
        "directory_name": "memchr1/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.15453225130872,
            "upper_bound": 172.59515071233233
          },
          "point_estimate": 172.3462238254112,
          "standard_error": 0.1142882620139477
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.10673394686808,
            "upper_bound": 172.53378539017842
          },
          "point_estimate": 172.16329910055995,
          "standard_error": 0.11314970019917016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0235447699173826,
            "upper_bound": 0.4639861883686954
          },
          "point_estimate": 0.1594368409029644,
          "standard_error": 0.13389209570946492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.1185030443153,
            "upper_bound": 172.42410024825523
          },
          "point_estimate": 172.21791336979024,
          "standard_error": 0.07894780788142036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11225120734445762,
            "upper_bound": 0.5389156774445725
          },
          "point_estimate": 0.38169240266792065,
          "standard_error": 0.1206658870472599
        }
      }
    },
    "memchr1/naive/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/rare",
        "directory_name": "memchr1/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.96178261816118,
            "upper_bound": 180.26163101150735
          },
          "point_estimate": 180.11048296553437,
          "standard_error": 0.07670476886138008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.88350907271052,
            "upper_bound": 180.3023491377649
          },
          "point_estimate": 180.09862505409274,
          "standard_error": 0.1032223576821346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08675553722321311,
            "upper_bound": 0.427330163461993
          },
          "point_estimate": 0.30518834202054623,
          "standard_error": 0.09327714602683786
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.94226698241852,
            "upper_bound": 180.40094675381337
          },
          "point_estimate": 180.21016470225445,
          "standard_error": 0.11937909568378188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14746425959725754,
            "upper_bound": 0.31908913510966713
          },
          "point_estimate": 0.2559061849956724,
          "standard_error": 0.043935627912889075
        }
      }
    },
    "memchr1/naive/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/uncommon",
        "directory_name": "memchr1/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.7408867396532,
            "upper_bound": 191.23081891895023
          },
          "point_estimate": 190.96758234926656,
          "standard_error": 0.1267760012817608
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.6520573204784,
            "upper_bound": 191.2523225606559
          },
          "point_estimate": 190.8569983141036,
          "standard_error": 0.14443437802561002
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09511611890563626,
            "upper_bound": 0.6729034995650816
          },
          "point_estimate": 0.32421749456008747,
          "standard_error": 0.15128162632430112
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.84861534551936,
            "upper_bound": 191.27131240402196
          },
          "point_estimate": 191.02059620796464,
          "standard_error": 0.1072046368732372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1683185069217958,
            "upper_bound": 0.5473315909066746
          },
          "point_estimate": 0.42275803781518223,
          "standard_error": 0.09755994952006064
        }
      }
    },
    "memchr1/naive/small/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr1/naive/small/verycommon",
        "directory_name": "memchr1/naive_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.03984253973414,
            "upper_bound": 299.6112131698846
          },
          "point_estimate": 297.8198546552854,
          "standard_error": 0.9129646141046572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.39238006281136,
            "upper_bound": 300.2500329398599
          },
          "point_estimate": 297.8754477339181,
          "standard_error": 1.2835729168756398
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0214771025604,
            "upper_bound": 5.021505763676943
          },
          "point_estimate": 3.1062218966079365,
          "standard_error": 1.0749606182418998
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297.7293779689357,
            "upper_bound": 300.5525527130459
          },
          "point_estimate": 299.3395752217243,
          "standard_error": 0.7102384715346524
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7655928844936712,
            "upper_bound": 3.848725150475587
          },
          "point_estimate": 3.0450327289168535,
          "standard_error": 0.5330194656947939
        }
      }
    },
    "memchr1/naive/tiny/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/common",
        "directory_name": "memchr1/naive_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.184699048883857,
            "upper_bound": 30.49971292089456
          },
          "point_estimate": 30.315856119906776,
          "standard_error": 0.08268442830679816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.132646103689225,
            "upper_bound": 30.36310395376075
          },
          "point_estimate": 30.27576988328842,
          "standard_error": 0.06940915566913551
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03580802959576942,
            "upper_bound": 0.2937584769202495
          },
          "point_estimate": 0.17984275785942397,
          "standard_error": 0.06572662642717306
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.154189375242808,
            "upper_bound": 30.302986238838933
          },
          "point_estimate": 30.23014381562959,
          "standard_error": 0.03851280396960554
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08576895780381433,
            "upper_bound": 0.4103570017151203
          },
          "point_estimate": 0.27604968955305925,
          "standard_error": 0.1044798508057157
        }
      }
    },
    "memchr1/naive/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/never",
        "directory_name": "memchr1/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.629419052278926,
            "upper_bound": 26.660195016400458
          },
          "point_estimate": 26.645734386482495,
          "standard_error": 0.007870702694660888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.627860534020144,
            "upper_bound": 26.66530552062683
          },
          "point_estimate": 26.650630461844752,
          "standard_error": 0.007968860947518485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005371962330975308,
            "upper_bound": 0.04544952571402667
          },
          "point_estimate": 0.01689799772475577,
          "standard_error": 0.010461351070193964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.64308263064295,
            "upper_bound": 26.66058089523229
          },
          "point_estimate": 26.65156233430795,
          "standard_error": 0.0045861921346514985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011761558264139813,
            "upper_bound": 0.03432103921865959
          },
          "point_estimate": 0.026223265336406475,
          "standard_error": 0.0059499500405938285
        }
      }
    },
    "memchr1/naive/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/rare",
        "directory_name": "memchr1/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.767933887684233,
            "upper_bound": 26.79885992504695
          },
          "point_estimate": 26.782675793034212,
          "standard_error": 0.007946538756578293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.760396251454697,
            "upper_bound": 26.802341582051973
          },
          "point_estimate": 26.773455181191828,
          "standard_error": 0.01241535999963885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0020661467618483917,
            "upper_bound": 0.04686114881677523
          },
          "point_estimate": 0.028211317333164697,
          "standard_error": 0.010577141294803777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.761632480099316,
            "upper_bound": 26.8124731274962
          },
          "point_estimate": 26.78449933996083,
          "standard_error": 0.014380721120904933
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014328213046092449,
            "upper_bound": 0.03355724556875668
          },
          "point_estimate": 0.026488139877049767,
          "standard_error": 0.005104311629252289
        }
      }
    },
    "memchr1/naive/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr1/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr1",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr1/naive/tiny/uncommon",
        "directory_name": "memchr1/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.195650068628343,
            "upper_bound": 22.417197537801652
          },
          "point_estimate": 22.30027429175473,
          "standard_error": 0.056953729436124895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.134524412907584,
            "upper_bound": 22.458441129156657
          },
          "point_estimate": 22.266485430973223,
          "standard_error": 0.0834785594535607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010048074279696432,
            "upper_bound": 0.29856330207546655
          },
          "point_estimate": 0.19957836978862345,
          "standard_error": 0.08524776054695214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.17827348072211,
            "upper_bound": 22.35824962234704
          },
          "point_estimate": 22.244263298936165,
          "standard_error": 0.046038984870796135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08155712960493382,
            "upper_bound": 0.23797008084264343
          },
          "point_estimate": 0.1904527832027552,
          "standard_error": 0.03703075941412531
        }
      }
    },
    "memchr2/fallback/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/fallback/empty/never",
        "directory_name": "memchr2/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.26907360363994,
            "upper_bound": 2.294971129253195
          },
          "point_estimate": 2.2819429330427337,
          "standard_error": 0.006608515823533393
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2651147734277877,
            "upper_bound": 2.2925946948088174
          },
          "point_estimate": 2.285345609566613,
          "standard_error": 0.007056927055104829
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003025984185715948,
            "upper_bound": 0.04309202282650088
          },
          "point_estimate": 0.012479800233620808,
          "standard_error": 0.010220861726939576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.26087355397862,
            "upper_bound": 2.2907821337989747
          },
          "point_estimate": 2.276202391514484,
          "standard_error": 0.008344856498616438
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011057675309367365,
            "upper_bound": 0.02921961152211815
          },
          "point_estimate": 0.02203601140956241,
          "standard_error": 0.004746996090868484
        }
      }
    },
    "memchr2/fallback/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/common",
        "directory_name": "memchr2/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218471.1538062168,
            "upper_bound": 1219888.7331113426
          },
          "point_estimate": 1219208.792903439,
          "standard_error": 364.7160470517547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217846.2333333334,
            "upper_bound": 1220158.379166667
          },
          "point_estimate": 1219591.4811111111,
          "standard_error": 603.511663248489
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.04913975206772,
            "upper_bound": 1935.3767393902635
          },
          "point_estimate": 858.0994186823168,
          "standard_error": 507.61700548741135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218858.51522637,
            "upper_bound": 1220205.5105642898
          },
          "point_estimate": 1219769.7287445888,
          "standard_error": 350.720526838715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 592.6612853087223,
            "upper_bound": 1436.8393465624345
          },
          "point_estimate": 1215.7849582468784,
          "standard_error": 196.1987388664212
        }
      }
    },
    "memchr2/fallback/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/never",
        "directory_name": "memchr2/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83578.04403849664,
            "upper_bound": 83641.5251401204
          },
          "point_estimate": 83607.54189992702,
          "standard_error": 16.258014103034057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83576.36896551723,
            "upper_bound": 83644.6988505747
          },
          "point_estimate": 83587.46642720306,
          "standard_error": 18.61138910356586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8043719329876815,
            "upper_bound": 86.51482087784838
          },
          "point_estimate": 18.012504306650428,
          "standard_error": 23.147801342877653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83577.94714789902,
            "upper_bound": 83629.92289951214
          },
          "point_estimate": 83602.57932825795,
          "standard_error": 13.656388494358064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.89312000099668,
            "upper_bound": 68.2849127042101
          },
          "point_estimate": 54.13028139229422,
          "standard_error": 12.048728616073054
        }
      }
    },
    "memchr2/fallback/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/rare",
        "directory_name": "memchr2/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89515.9843076081,
            "upper_bound": 89641.97673938541
          },
          "point_estimate": 89575.5885669325,
          "standard_error": 32.25446020907796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89517.39581280788,
            "upper_bound": 89644.32019704433
          },
          "point_estimate": 89542.51156267105,
          "standard_error": 35.08311587535478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.8305875663492,
            "upper_bound": 178.83275298270652
          },
          "point_estimate": 58.41188275608612,
          "standard_error": 46.345374281463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89526.32911255686,
            "upper_bound": 89663.6735390013
          },
          "point_estimate": 89583.75441110613,
          "standard_error": 35.57151450255006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.58738121764516,
            "upper_bound": 141.8263686588415
          },
          "point_estimate": 107.29061309212716,
          "standard_error": 24.463404416560184
        }
      }
    },
    "memchr2/fallback/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/fallback/huge/uncommon",
        "directory_name": "memchr2/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323267.25577538984,
            "upper_bound": 323696.01381900546
          },
          "point_estimate": 323473.9511834528,
          "standard_error": 109.41154558899116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323264.4017699115,
            "upper_bound": 323699.601892822
          },
          "point_estimate": 323455.5306784661,
          "standard_error": 97.68690550627772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.29463689524783,
            "upper_bound": 637.8435192364526
          },
          "point_estimate": 238.11495869294205,
          "standard_error": 148.68361578230804
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323380.0917275875,
            "upper_bound": 323720.504113605
          },
          "point_estimate": 323530.39312722674,
          "standard_error": 90.75817396897482
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.8395380072392,
            "upper_bound": 496.9511677835997
          },
          "point_estimate": 365.2825695457232,
          "standard_error": 89.15450266732864
        }
      }
    },
    "memchr2/fallback/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/common",
        "directory_name": "memchr2/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 435.0887752020383,
            "upper_bound": 435.62193329810407
          },
          "point_estimate": 435.36009411060786,
          "standard_error": 0.13637513626042935
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 435.0294209215442,
            "upper_bound": 435.6620650325702
          },
          "point_estimate": 435.4386575182808,
          "standard_error": 0.1418465085476697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05106837788922441,
            "upper_bound": 0.7633397666592269
          },
          "point_estimate": 0.3765908834457541,
          "standard_error": 0.19409462474932856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 435.2048157550212,
            "upper_bound": 435.6723580932469
          },
          "point_estimate": 435.3897439422346,
          "standard_error": 0.12084773922183256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2273653254566748,
            "upper_bound": 0.5915206552798727
          },
          "point_estimate": 0.4540410344987611,
          "standard_error": 0.09276478683022664
        }
      }
    },
    "memchr2/fallback/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/never",
        "directory_name": "memchr2/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.1159433289188,
            "upper_bound": 103.25022727095374
          },
          "point_estimate": 103.17727339771866,
          "standard_error": 0.03446805313577953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.08122249274275,
            "upper_bound": 103.23329157290672
          },
          "point_estimate": 103.16356394331392,
          "standard_error": 0.04547515864249574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01670451139020149,
            "upper_bound": 0.18876614405311384
          },
          "point_estimate": 0.10005539662192206,
          "standard_error": 0.04078736433396563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.14069533854568,
            "upper_bound": 103.25681427732198
          },
          "point_estimate": 103.1885181091712,
          "standard_error": 0.02953767110150924
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05534113125359358,
            "upper_bound": 0.15822675499092087
          },
          "point_estimate": 0.11543391208015806,
          "standard_error": 0.030151184304248894
        }
      }
    },
    "memchr2/fallback/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/rare",
        "directory_name": "memchr2/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.88468353853884,
            "upper_bound": 118.05948456468612
          },
          "point_estimate": 117.47002286565726,
          "standard_error": 0.3011212406164136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.78483874004908,
            "upper_bound": 118.36525633200438
          },
          "point_estimate": 117.33867300624652,
          "standard_error": 0.4275016453487798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19275489865965056,
            "upper_bound": 1.778108743057926
          },
          "point_estimate": 0.845217344523682,
          "standard_error": 0.403915248196784
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.49402787984128,
            "upper_bound": 117.6309128879302
          },
          "point_estimate": 117.04764906275366,
          "standard_error": 0.2846574004905991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.570066371951775,
            "upper_bound": 1.2645489328061723
          },
          "point_estimate": 1.0018375822990149,
          "standard_error": 0.17746934302942963
        }
      }
    },
    "memchr2/fallback/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/fallback/small/uncommon",
        "directory_name": "memchr2/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.81609827308196,
            "upper_bound": 169.09542830441706
          },
          "point_estimate": 168.95746065620628,
          "standard_error": 0.07162606051124026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.73796604968408,
            "upper_bound": 169.16104059438624
          },
          "point_estimate": 168.96928066741367,
          "standard_error": 0.0905934236870086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04479311406024236,
            "upper_bound": 0.4579826931092144
          },
          "point_estimate": 0.1859479769757764,
          "standard_error": 0.10381415478023404
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.77253720109425,
            "upper_bound": 169.20944942111043
          },
          "point_estimate": 169.00003557738344,
          "standard_error": 0.11500644150276884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13513232482333054,
            "upper_bound": 0.29847892784117946
          },
          "point_estimate": 0.238202233616119,
          "standard_error": 0.041870562753608835
        }
      }
    },
    "memchr2/fallback/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/never",
        "directory_name": "memchr2/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.221515538794195,
            "upper_bound": 13.243844984532796
          },
          "point_estimate": 13.231042497505214,
          "standard_error": 0.005823776412564228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.219241452513169,
            "upper_bound": 13.235142862215246
          },
          "point_estimate": 13.229265681176413,
          "standard_error": 0.0043567531730067666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010459376132541708,
            "upper_bound": 0.021551062787745528
          },
          "point_estimate": 0.012937907420215626,
          "standard_error": 0.005186412852304051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.223451151663308,
            "upper_bound": 13.231722945687816
          },
          "point_estimate": 13.227548505780208,
          "standard_error": 0.0021824137635305324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005669442319218771,
            "upper_bound": 0.02869957321254227
          },
          "point_estimate": 0.01952310805777318,
          "standard_error": 0.0070843472041340495
        }
      }
    },
    "memchr2/fallback/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/rare",
        "directory_name": "memchr2/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.484031254065712,
            "upper_bound": 21.5213671882256
          },
          "point_estimate": 21.50365347853614,
          "standard_error": 0.00952897765947375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.49206709350991,
            "upper_bound": 21.52664151908578
          },
          "point_estimate": 21.500272831400075,
          "standard_error": 0.009436005557591625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002429601184622268,
            "upper_bound": 0.05556501992872465
          },
          "point_estimate": 0.015201448830780884,
          "standard_error": 0.01342921858678672
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.501276547805556,
            "upper_bound": 21.528366147166892
          },
          "point_estimate": 21.51459875096665,
          "standard_error": 0.007113455521855041
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013139478574730934,
            "upper_bound": 0.04387677611457995
          },
          "point_estimate": 0.03177684350491454,
          "standard_error": 0.008135455679209376
        }
      }
    },
    "memchr2/fallback/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/fallback/tiny/uncommon",
        "directory_name": "memchr2/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.35394349969139,
            "upper_bound": 54.531745845088906
          },
          "point_estimate": 54.43459084043777,
          "standard_error": 0.045672094411539334
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.33718188669451,
            "upper_bound": 54.4953893222733
          },
          "point_estimate": 54.39960298328184,
          "standard_error": 0.03785497711174629
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01656240008328294,
            "upper_bound": 0.2252943807956657
          },
          "point_estimate": 0.100723392510093,
          "standard_error": 0.0582921351627224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 54.389635216472854,
            "upper_bound": 54.57774289385619
          },
          "point_estimate": 54.45337076447143,
          "standard_error": 0.05079644020014833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05944271512662147,
            "upper_bound": 0.21058652973540395
          },
          "point_estimate": 0.1526700788554746,
          "standard_error": 0.04251381510646931
        }
      }
    },
    "memchr2/krate/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/krate/empty/never",
        "directory_name": "memchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48899329263709246,
            "upper_bound": 0.4895984244048246
          },
          "point_estimate": 0.4892868801875709,
          "standard_error": 0.00015541815688533057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4888741945112668,
            "upper_bound": 0.4897396435509766
          },
          "point_estimate": 0.48913112078646503,
          "standard_error": 0.0002454868881739617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001315633914554247,
            "upper_bound": 0.0008299012266158055
          },
          "point_estimate": 0.0005517302916442081,
          "standard_error": 0.00019493755414800624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4888032657197111,
            "upper_bound": 0.4893738283109481
          },
          "point_estimate": 0.4890415172642516,
          "standard_error": 0.0001470808257200127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002971358053109963,
            "upper_bound": 0.0006264672095076995
          },
          "point_estimate": 0.0005170844344316773,
          "standard_error": 0.00008350091852087202
        }
      }
    },
    "memchr2/krate/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/common",
        "directory_name": "memchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409435.5655024969,
            "upper_bound": 410553.0558596398
          },
          "point_estimate": 409923.0341836098,
          "standard_error": 290.0053172128247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409337.6462546816,
            "upper_bound": 410348.405664794
          },
          "point_estimate": 409690.7473693597,
          "standard_error": 198.77986907880955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.93872873257511,
            "upper_bound": 1187.7818454576034
          },
          "point_estimate": 316.3301957323462,
          "standard_error": 322.4609669389934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409509.2586382934,
            "upper_bound": 409835.2573697704
          },
          "point_estimate": 409707.7373704947,
          "standard_error": 82.84155074698387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.32681212722116,
            "upper_bound": 1356.034527854732
          },
          "point_estimate": 965.9616174098976,
          "standard_error": 308.54671588719594
        }
      }
    },
    "memchr2/krate/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/never",
        "directory_name": "memchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11418.058204998284,
            "upper_bound": 11544.117826179818
          },
          "point_estimate": 11473.354533257148,
          "standard_error": 32.43371526004484
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11410.405825235372,
            "upper_bound": 11544.420004238184
          },
          "point_estimate": 11422.33287198912,
          "standard_error": 33.40650254457494
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3217684190002768,
            "upper_bound": 172.9486309481428
          },
          "point_estimate": 17.78355219248987,
          "standard_error": 39.004011037225446
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11413.72357423934,
            "upper_bound": 11436.39171738022
          },
          "point_estimate": 11420.887351491485,
          "standard_error": 6.028604704380752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.308081812119449,
            "upper_bound": 153.05754996542808
          },
          "point_estimate": 107.93208678726978,
          "standard_error": 35.4656945248265
        }
      }
    },
    "memchr2/krate/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/rare",
        "directory_name": "memchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15691.013167883712,
            "upper_bound": 15737.053015770169
          },
          "point_estimate": 15710.267169632143,
          "standard_error": 12.075416588395534
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15684.000246837397,
            "upper_bound": 15720.115377969763
          },
          "point_estimate": 15703.624388048956,
          "standard_error": 10.471465025355068
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4413037363564123,
            "upper_bound": 42.50802899479284
          },
          "point_estimate": 26.653447507369485,
          "standard_error": 10.288429975683714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15682.558089201428,
            "upper_bound": 15710.139825318522
          },
          "point_estimate": 15695.562628818268,
          "standard_error": 7.249144936458883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.541837362513952,
            "upper_bound": 59.30351971921469
          },
          "point_estimate": 40.195684991039656,
          "standard_error": 14.77759471648473
        }
      }
    },
    "memchr2/krate/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/krate/huge/uncommon",
        "directory_name": "memchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160420.4394957157,
            "upper_bound": 161074.54335791545
          },
          "point_estimate": 160749.38100646157,
          "standard_error": 166.68538827779494
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160224.58650442478,
            "upper_bound": 161183.95243362832
          },
          "point_estimate": 160840.49841972184,
          "standard_error": 243.67350644915817
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125.93316428342423,
            "upper_bound": 947.8057797659704
          },
          "point_estimate": 652.8507620821591,
          "standard_error": 212.25055987970265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160166.2880510109,
            "upper_bound": 160902.26846725988
          },
          "point_estimate": 160491.49266750948,
          "standard_error": 191.2599828354417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 325.6741341583295,
            "upper_bound": 687.244583758112
          },
          "point_estimate": 554.3191942978611,
          "standard_error": 92.01071848616796
        }
      }
    },
    "memchr2/krate/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/common",
        "directory_name": "memchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.49686113148897,
            "upper_bound": 408.1869880849825
          },
          "point_estimate": 407.8130826647795,
          "standard_error": 0.17765334641372038
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.49325189487377,
            "upper_bound": 408.1203567183926
          },
          "point_estimate": 407.656778645216,
          "standard_error": 0.139444785882223
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06597436421422773,
            "upper_bound": 0.9241726986720434
          },
          "point_estimate": 0.2756147052415954,
          "standard_error": 0.22655716624239536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 407.54596898104546,
            "upper_bound": 407.8552071208954
          },
          "point_estimate": 407.68573004113887,
          "standard_error": 0.08015846839813977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18677585261914365,
            "upper_bound": 0.8057980731763995
          },
          "point_estimate": 0.5939267196107976,
          "standard_error": 0.16131543651031507
        }
      }
    },
    "memchr2/krate/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/never",
        "directory_name": "memchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.074392137962027,
            "upper_bound": 12.09286176214734
          },
          "point_estimate": 12.083160378802331,
          "standard_error": 0.0047483431860916615
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.069734723129889,
            "upper_bound": 12.096166837313188
          },
          "point_estimate": 12.078466161551068,
          "standard_error": 0.0071185356271066735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002398105817507788,
            "upper_bound": 0.0259418356783288
          },
          "point_estimate": 0.014915407321486876,
          "standard_error": 0.006330136446841787
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.069993641338211,
            "upper_bound": 12.083662618347708
          },
          "point_estimate": 12.075474801145557,
          "standard_error": 0.003532848191998078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007945254654742238,
            "upper_bound": 0.01959425074647875
          },
          "point_estimate": 0.01579813682151963,
          "standard_error": 0.002976184753343088
        }
      }
    },
    "memchr2/krate/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/rare",
        "directory_name": "memchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.597505452584095,
            "upper_bound": 23.680627695489292
          },
          "point_estimate": 23.6421594170562,
          "standard_error": 0.021459192120677296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.595041704949377,
            "upper_bound": 23.6876066547204
          },
          "point_estimate": 23.66947232508283,
          "standard_error": 0.023782361716189335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005374568041615972,
            "upper_bound": 0.11073736218817869
          },
          "point_estimate": 0.03492567731309896,
          "standard_error": 0.028163618183221033
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.61351223116247,
            "upper_bound": 23.67942031792079
          },
          "point_estimate": 23.648850091245727,
          "standard_error": 0.017268165235271065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022885584943709993,
            "upper_bound": 0.08922894706094658
          },
          "point_estimate": 0.07149770186139033,
          "standard_error": 0.01624553128402809
        }
      }
    },
    "memchr2/krate/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/krate/small/uncommon",
        "directory_name": "memchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.79828676390672,
            "upper_bound": 111.82748168632745
          },
          "point_estimate": 111.23490268512084,
          "standard_error": 0.26832566627322985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.60974443182371,
            "upper_bound": 111.51622206641908
          },
          "point_estimate": 110.91025713641312,
          "standard_error": 0.22454171738910875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06225144869065406,
            "upper_bound": 1.0466961831447956
          },
          "point_estimate": 0.4593268727344755,
          "standard_error": 0.26201441897167427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.63585188691522,
            "upper_bound": 111.05873616129496
          },
          "point_estimate": 110.77249157066622,
          "standard_error": 0.10917180832561624
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2127229060093948,
            "upper_bound": 1.2863221314971862
          },
          "point_estimate": 0.8918703522720492,
          "standard_error": 0.3057420648643337
        }
      }
    },
    "memchr2/krate/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/never",
        "directory_name": "memchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.6435822221664935,
            "upper_bound": 4.648592905482326
          },
          "point_estimate": 4.645809029161472,
          "standard_error": 0.0012910770213539122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.643059014779476,
            "upper_bound": 4.647150374287104
          },
          "point_estimate": 4.645721945977824,
          "standard_error": 0.0010911665556127615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00022727627263909869,
            "upper_bound": 0.005613866883766228
          },
          "point_estimate": 0.0029523639699103005,
          "standard_error": 0.0013278094138149171
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.643218877098801,
            "upper_bound": 4.646641379483807
          },
          "point_estimate": 4.644544231658995,
          "standard_error": 0.0008872624722469981
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015418012413854624,
            "upper_bound": 0.00616330099389913
          },
          "point_estimate": 0.004316383365086463,
          "standard_error": 0.0013652721822232118
        }
      }
    },
    "memchr2/krate/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/rare",
        "directory_name": "memchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.187189011573496,
            "upper_bound": 11.198841031595473
          },
          "point_estimate": 11.193049076322824,
          "standard_error": 0.0029845061126876292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.18473679979956,
            "upper_bound": 11.19985425904202
          },
          "point_estimate": 11.192096313341594,
          "standard_error": 0.003340631927170433
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009010210942343484,
            "upper_bound": 0.016766249204790588
          },
          "point_estimate": 0.009958235312713296,
          "standard_error": 0.004662267238453867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.189078519031732,
            "upper_bound": 11.202748503737835
          },
          "point_estimate": 11.19531818906445,
          "standard_error": 0.00347809103943365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004492697651174947,
            "upper_bound": 0.013031256713332716
          },
          "point_estimate": 0.009943861522269323,
          "standard_error": 0.0020525631973648422
        }
      }
    },
    "memchr2/krate/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/krate/tiny/uncommon",
        "directory_name": "memchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.566687749432376,
            "upper_bound": 55.6296317198839
          },
          "point_estimate": 55.599539412479295,
          "standard_error": 0.016096739472623765
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.57061988540551,
            "upper_bound": 55.63565682510072
          },
          "point_estimate": 55.601835912551635,
          "standard_error": 0.016561589670380083
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00824764047764191,
            "upper_bound": 0.08836236288827459
          },
          "point_estimate": 0.04093168444939858,
          "standard_error": 0.018979447332191604
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.58720445912839,
            "upper_bound": 55.61842900740314
          },
          "point_estimate": 55.60053765606406,
          "standard_error": 0.007980876170827617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02407554098497969,
            "upper_bound": 0.07279912077305145
          },
          "point_estimate": 0.05362580161539293,
          "standard_error": 0.012802638751486607
        }
      }
    },
    "memchr2/naive/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr2/naive/empty/never",
        "directory_name": "memchr2/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6517941961178771,
            "upper_bound": 0.6528533841572506
          },
          "point_estimate": 0.6522677739093976,
          "standard_error": 0.0002746879208208036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6515679022311212,
            "upper_bound": 0.6525791428417503
          },
          "point_estimate": 0.6522074600557288,
          "standard_error": 0.00027029488474939617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000052009777960031474,
            "upper_bound": 0.0012474958140815666
          },
          "point_estimate": 0.0007947412997260872,
          "standard_error": 0.0002963847347426119
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6516987504605111,
            "upper_bound": 0.6525279833791495
          },
          "point_estimate": 0.6521044565503142,
          "standard_error": 0.00021420906371014264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003695308073668711,
            "upper_bound": 0.0012911439259629357
          },
          "point_estimate": 0.0009158403855847296,
          "standard_error": 0.0002727797727052034
        }
      }
    },
    "memchr2/naive/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/common",
        "directory_name": "memchr2/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827978.7141073232,
            "upper_bound": 830112.1063825757
          },
          "point_estimate": 829066.8984839466,
          "standard_error": 545.0082708431892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827784.5909090909,
            "upper_bound": 830678.1045454545
          },
          "point_estimate": 828905.7671356422,
          "standard_error": 897.4704045561775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.28174869933486,
            "upper_bound": 3271.312195445273
          },
          "point_estimate": 2476.6102492131786,
          "standard_error": 748.0893442288748
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 827507.6435325388,
            "upper_bound": 830013.5559981256
          },
          "point_estimate": 828701.1515348288,
          "standard_error": 628.4922286478223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1095.0678086927733,
            "upper_bound": 2273.159465940542
          },
          "point_estimate": 1825.734576424624,
          "standard_error": 304.4053344931048
        }
      }
    },
    "memchr2/naive/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/never",
        "directory_name": "memchr2/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242392.72985714287,
            "upper_bound": 242786.88065714287
          },
          "point_estimate": 242593.79400529095,
          "standard_error": 100.31558383367688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242425.22166666668,
            "upper_bound": 242788.17428571428
          },
          "point_estimate": 242584.025,
          "standard_error": 77.90289138267353
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.577896839378884,
            "upper_bound": 543.4710949514863
          },
          "point_estimate": 154.68487180936395,
          "standard_error": 134.3229978430739
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 242467.39654320988,
            "upper_bound": 242715.34273333332
          },
          "point_estimate": 242604.82296103897,
          "standard_error": 61.93852902057363
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123.5587401624813,
            "upper_bound": 456.4049670490545
          },
          "point_estimate": 332.87877817291013,
          "standard_error": 83.19128173378799
        }
      }
    },
    "memchr2/naive/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/rare",
        "directory_name": "memchr2/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246003.9925128164,
            "upper_bound": 246409.5332969756
          },
          "point_estimate": 246219.07249973193,
          "standard_error": 103.76036314125588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 245973.29922779923,
            "upper_bound": 246496.35754504503
          },
          "point_estimate": 246289.9548141892,
          "standard_error": 137.64255548230355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.38314934406925,
            "upper_bound": 584.2653636812902
          },
          "point_estimate": 310.4377767690258,
          "standard_error": 127.97156638137965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246077.93030932808,
            "upper_bound": 246383.4384125868
          },
          "point_estimate": 246255.1952790453,
          "standard_error": 78.15964575811294
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.57234715336793,
            "upper_bound": 447.3463935056814
          },
          "point_estimate": 344.51574164090033,
          "standard_error": 72.18148841573131
        }
      }
    },
    "memchr2/naive/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr2/naive/huge/uncommon",
        "directory_name": "memchr2/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356036.41199460626,
            "upper_bound": 357481.71397518873
          },
          "point_estimate": 356702.85035059333,
          "standard_error": 371.6776851644937
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355754.6450916936,
            "upper_bound": 357595.42330097087
          },
          "point_estimate": 356196.4093851133,
          "standard_error": 482.91649019933254
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.75100861244501,
            "upper_bound": 1884.8505779450052
          },
          "point_estimate": 670.0048408558779,
          "standard_error": 498.32861596390745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355903.6542234178,
            "upper_bound": 357847.17883582524
          },
          "point_estimate": 356776.00678350776,
          "standard_error": 549.0724016999017
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 367.6657698664428,
            "upper_bound": 1539.9546676090192
          },
          "point_estimate": 1242.3618977526344,
          "standard_error": 281.4423287939111
        }
      }
    },
    "memchr2/naive/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/common",
        "directory_name": "memchr2/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.43760932116277,
            "upper_bound": 334.932861585307
          },
          "point_estimate": 334.6813642506602,
          "standard_error": 0.1266663690172985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.3135063872376,
            "upper_bound": 335.04464400639904
          },
          "point_estimate": 334.668007657111,
          "standard_error": 0.2411172477479924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08395771722059242,
            "upper_bound": 0.6613805769761044
          },
          "point_estimate": 0.5191992351081123,
          "standard_error": 0.1675461681788007
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 334.33771474970706,
            "upper_bound": 334.7069426911099
          },
          "point_estimate": 334.47933903619133,
          "standard_error": 0.09624236757841952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2764536100835396,
            "upper_bound": 0.5040563261529079
          },
          "point_estimate": 0.421804177403286,
          "standard_error": 0.05904235427422541
        }
      }
    },
    "memchr2/naive/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/never",
        "directory_name": "memchr2/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.56774469046957,
            "upper_bound": 278.8907884769534
          },
          "point_estimate": 278.72113292467174,
          "standard_error": 0.08299224065720316
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.5216010571138,
            "upper_bound": 278.89775264278444
          },
          "point_estimate": 278.6536453940407,
          "standard_error": 0.09376155041746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.049710642586945325,
            "upper_bound": 0.4513347222007757
          },
          "point_estimate": 0.2533484827380535,
          "standard_error": 0.10592186107924938
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.64336962376234,
            "upper_bound": 279.0037956469939
          },
          "point_estimate": 278.82513058411786,
          "standard_error": 0.09091180885195684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13383975547245133,
            "upper_bound": 0.36086758648636846
          },
          "point_estimate": 0.27662064197819847,
          "standard_error": 0.059617887013052595
        }
      }
    },
    "memchr2/naive/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/rare",
        "directory_name": "memchr2/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.422085301238,
            "upper_bound": 295.93053163633937
          },
          "point_estimate": 295.6843547993541,
          "standard_error": 0.13035974331758413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.3212632665028,
            "upper_bound": 296.0559563062587
          },
          "point_estimate": 295.71974774655473,
          "standard_error": 0.17897136033293348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10416338124784665,
            "upper_bound": 0.7666107244714484
          },
          "point_estimate": 0.4661502106197745,
          "standard_error": 0.17282423987421688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295.44480247274157,
            "upper_bound": 295.81421662223113
          },
          "point_estimate": 295.6163505565267,
          "standard_error": 0.09450320607294306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2312656205217848,
            "upper_bound": 0.5361561461284755
          },
          "point_estimate": 0.4360815768173396,
          "standard_error": 0.07670116960034397
        }
      }
    },
    "memchr2/naive/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr2/naive/small/uncommon",
        "directory_name": "memchr2/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.703561092922,
            "upper_bound": 372.5264071581079
          },
          "point_estimate": 372.0652619649517,
          "standard_error": 0.21325760755043136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.66924051096225,
            "upper_bound": 372.3711247758135
          },
          "point_estimate": 371.89640354997584,
          "standard_error": 0.14452723386538163
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07265075415471202,
            "upper_bound": 1.0031017885369804
          },
          "point_estimate": 0.27231539203965266,
          "standard_error": 0.23884325658947228
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371.7813621210571,
            "upper_bound": 373.0852411613199
          },
          "point_estimate": 372.347585680013,
          "standard_error": 0.389122848295592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16308686441607873,
            "upper_bound": 0.9933690413445572
          },
          "point_estimate": 0.7105127812492547,
          "standard_error": 0.22314984394393983
        }
      }
    },
    "memchr2/naive/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/never",
        "directory_name": "memchr2/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.93441931571302,
            "upper_bound": 35.98920053362517
          },
          "point_estimate": 35.96092536635474,
          "standard_error": 0.014040333867804108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.920913751806324,
            "upper_bound": 35.999407790116855
          },
          "point_estimate": 35.95238957925055,
          "standard_error": 0.021719931970895053
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012854291742810223,
            "upper_bound": 0.07800972834959796
          },
          "point_estimate": 0.05502268960008847,
          "standard_error": 0.016538862537789148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.93307168270593,
            "upper_bound": 35.97305223181661
          },
          "point_estimate": 35.95275165181632,
          "standard_error": 0.010213493865900456
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027087764687247516,
            "upper_bound": 0.05751187287949765
          },
          "point_estimate": 0.04678632129372888,
          "standard_error": 0.007756523414362926
        }
      }
    },
    "memchr2/naive/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/rare",
        "directory_name": "memchr2/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.482181128174666,
            "upper_bound": 38.548341442732394
          },
          "point_estimate": 38.5156697522331,
          "standard_error": 0.016944510563420345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.45182532065728,
            "upper_bound": 38.55920652931661
          },
          "point_estimate": 38.53189902278403,
          "standard_error": 0.02740260654259637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00874297189630049,
            "upper_bound": 0.09575211630828968
          },
          "point_estimate": 0.06694225888194809,
          "standard_error": 0.02489512569383847
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.49105843623953,
            "upper_bound": 38.54712350396238
          },
          "point_estimate": 38.52478104863052,
          "standard_error": 0.01430189881797454
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03485187324678202,
            "upper_bound": 0.06785921686960655
          },
          "point_estimate": 0.05650059830210079,
          "standard_error": 0.008455713993038542
        }
      }
    },
    "memchr2/naive/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr2/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr2",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr2/naive/tiny/uncommon",
        "directory_name": "memchr2/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.43256280679592,
            "upper_bound": 37.507784415692946
          },
          "point_estimate": 37.46952706357837,
          "standard_error": 0.01927958721296162
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.41811972496715,
            "upper_bound": 37.52315571346778
          },
          "point_estimate": 37.460448609778126,
          "standard_error": 0.02468278366251267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00420226316935929,
            "upper_bound": 0.10696216173983088
          },
          "point_estimate": 0.07786317689317046,
          "standard_error": 0.0267903876740244
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.44078508246128,
            "upper_bound": 37.51140041833419
          },
          "point_estimate": 37.470247045382465,
          "standard_error": 0.017810552015368367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03580293721103956,
            "upper_bound": 0.08180874570064686
          },
          "point_estimate": 0.0642296814326742,
          "standard_error": 0.011831910126142669
        }
      }
    },
    "memchr3/fallback/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/fallback/empty/never",
        "directory_name": "memchr3/fallback_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5707836532617945,
            "upper_bound": 2.574848382011356
          },
          "point_estimate": 2.57281787494629,
          "standard_error": 0.0010403988073846137
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5703393971713875,
            "upper_bound": 2.575822911991844
          },
          "point_estimate": 2.5728451444070015,
          "standard_error": 0.001288511814014454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007603910467460895,
            "upper_bound": 0.006211811246119716
          },
          "point_estimate": 0.003105537620752276,
          "standard_error": 0.0014155836436111355
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5712394532288507,
            "upper_bound": 2.575972056745347
          },
          "point_estimate": 2.573552678909374,
          "standard_error": 0.001290137466819697
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019292287216297703,
            "upper_bound": 0.004381848630636719
          },
          "point_estimate": 0.003458861376077658,
          "standard_error": 0.0006314972951737639
        }
      }
    },
    "memchr3/fallback/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/common",
        "directory_name": "memchr3/fallback_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1716469.2268344157,
            "upper_bound": 1718957.1162040944
          },
          "point_estimate": 1717721.8249007936,
          "standard_error": 634.4360563704454
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1716291.6590909092,
            "upper_bound": 1718966.2392045455
          },
          "point_estimate": 1717885.0137085137,
          "standard_error": 478.2585676234789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.24771255533784,
            "upper_bound": 3820.077682179916
          },
          "point_estimate": 693.8063514703317,
          "standard_error": 1112.4426777431934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1717547.421977151,
            "upper_bound": 1718544.4023569024
          },
          "point_estimate": 1717919.547225502,
          "standard_error": 254.347695122708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 914.6320976828596,
            "upper_bound": 2829.2559987975146
          },
          "point_estimate": 2113.9091010383263,
          "standard_error": 480.0579958487414
        }
      }
    },
    "memchr3/fallback/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/never",
        "directory_name": "memchr3/fallback_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124692.01527211758,
            "upper_bound": 124824.4813879376
          },
          "point_estimate": 124756.56008167536,
          "standard_error": 33.98999925808071
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124652.7133072407,
            "upper_bound": 124840.1683219178
          },
          "point_estimate": 124735.80270167429,
          "standard_error": 47.88037837405377
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.028534865130982,
            "upper_bound": 189.14935572583175
          },
          "point_estimate": 128.79355380650645,
          "standard_error": 42.193510664352075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124671.84641990912,
            "upper_bound": 124802.90342029266
          },
          "point_estimate": 124738.7623109767,
          "standard_error": 33.58827788785134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.35556231267788,
            "upper_bound": 145.98969505918544
          },
          "point_estimate": 113.93701740458135,
          "standard_error": 21.700122508465416
        }
      }
    },
    "memchr3/fallback/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/rare",
        "directory_name": "memchr3/fallback_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129201.55836738716,
            "upper_bound": 129320.47298139703
          },
          "point_estimate": 129262.43397430486,
          "standard_error": 30.520124389882433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129181.56028368794,
            "upper_bound": 129355.7695035461
          },
          "point_estimate": 129274.45035460991,
          "standard_error": 46.13466480999174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.008420726090364,
            "upper_bound": 173.07518237351897
          },
          "point_estimate": 129.14129238813197,
          "standard_error": 35.80872595491582
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129162.8036298448,
            "upper_bound": 129322.87555856188
          },
          "point_estimate": 129245.67294832828,
          "standard_error": 41.80305388964034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.39968076219875,
            "upper_bound": 127.23696448851456
          },
          "point_estimate": 101.95669986872548,
          "standard_error": 17.579415049828402
        }
      }
    },
    "memchr3/fallback/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/fallback/huge/uncommon",
        "directory_name": "memchr3/fallback_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 523050.0598915816,
            "upper_bound": 523973.50393656455
          },
          "point_estimate": 523490.3912307256,
          "standard_error": 237.31037370800271
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 522891.2193877551,
            "upper_bound": 524115.23968253966
          },
          "point_estimate": 523284.54285714286,
          "standard_error": 298.2569508270786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.53819337487508,
            "upper_bound": 1369.053848611091
          },
          "point_estimate": 607.5835892132392,
          "standard_error": 333.27748493802005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 522845.9136292658,
            "upper_bound": 523584.91259863944
          },
          "point_estimate": 523194.8047866419,
          "standard_error": 186.1577900078017
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369.08292524465986,
            "upper_bound": 982.297345860473
          },
          "point_estimate": 792.1093280738614,
          "standard_error": 148.88225909130858
        }
      }
    },
    "memchr3/fallback/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/common",
        "directory_name": "memchr3/fallback_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 742.8804724385049,
            "upper_bound": 746.9780124972945
          },
          "point_estimate": 745.0194269810584,
          "standard_error": 1.0520812001244506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 740.8568391230127,
            "upper_bound": 747.6548096501513
          },
          "point_estimate": 746.4311083002544,
          "standard_error": 1.7518275484632984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3631395684280734,
            "upper_bound": 5.476047343820712
          },
          "point_estimate": 2.447368973024924,
          "standard_error": 1.4197766945201695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 740.8302914668922,
            "upper_bound": 745.8068738141698
          },
          "point_estimate": 742.4560399720392,
          "standard_error": 1.2504864602821704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6272949079637888,
            "upper_bound": 4.102836679029054
          },
          "point_estimate": 3.5024196616701713,
          "standard_error": 0.5677881798086358
        }
      }
    },
    "memchr3/fallback/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/never",
        "directory_name": "memchr3/fallback_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.35997889506555,
            "upper_bound": 145.54385157103664
          },
          "point_estimate": 145.44363140925333,
          "standard_error": 0.04725607535479288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.32488250289163,
            "upper_bound": 145.51160512976318
          },
          "point_estimate": 145.42532243768832,
          "standard_error": 0.040047375744495885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018130007313622316,
            "upper_bound": 0.22939311188280845
          },
          "point_estimate": 0.09607243641101593,
          "standard_error": 0.057297956493527064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.37759393546455,
            "upper_bound": 145.46171857700296
          },
          "point_estimate": 145.4231435353907,
          "standard_error": 0.021273279623886837
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.059534787831700846,
            "upper_bound": 0.2217854158576042
          },
          "point_estimate": 0.15822193250926203,
          "standard_error": 0.04586176918258966
        }
      }
    },
    "memchr3/fallback/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/rare",
        "directory_name": "memchr3/fallback_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.034203810574,
            "upper_bound": 169.8132035454749
          },
          "point_estimate": 169.41496469610055,
          "standard_error": 0.2002344350224048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.87080307999963,
            "upper_bound": 169.92240979400367
          },
          "point_estimate": 169.36265780792988,
          "standard_error": 0.35413920657710396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.13694973414271366,
            "upper_bound": 1.1372250080778048
          },
          "point_estimate": 0.7281472627542487,
          "standard_error": 0.2800895512248341
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.35928468000154,
            "upper_bound": 170.17081464515667
          },
          "point_estimate": 169.8148291001421,
          "standard_error": 0.2057000989464539
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.41301326792869625,
            "upper_bound": 0.8229520660282796
          },
          "point_estimate": 0.6651061179139217,
          "standard_error": 0.1081009481725274
        }
      }
    },
    "memchr3/fallback/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/fallback/small/uncommon",
        "directory_name": "memchr3/fallback_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.62322525006425,
            "upper_bound": 278.960608072677
          },
          "point_estimate": 278.7864933584036,
          "standard_error": 0.08657460446918525
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.51172378351845,
            "upper_bound": 279.1182265140823
          },
          "point_estimate": 278.6998059458796,
          "standard_error": 0.1420601615073158
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05172883828523368,
            "upper_bound": 0.4710755891393121
          },
          "point_estimate": 0.29264545861682256,
          "standard_error": 0.11516956503101904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.57066194393144,
            "upper_bound": 278.96959931865706
          },
          "point_estimate": 278.7892736413124,
          "standard_error": 0.10501822500720982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16392610872104466,
            "upper_bound": 0.33906270138030803
          },
          "point_estimate": 0.2889567193628209,
          "standard_error": 0.04294323603276458
        }
      }
    },
    "memchr3/fallback/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/never",
        "directory_name": "memchr3/fallback_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.55855109738564,
            "upper_bound": 18.572826443505996
          },
          "point_estimate": 18.565844591463787,
          "standard_error": 0.003657729911233963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.55346627584542,
            "upper_bound": 18.575139757045015
          },
          "point_estimate": 18.56790300123565,
          "standard_error": 0.005575215162547531
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001879441163021516,
            "upper_bound": 0.020743326099493167
          },
          "point_estimate": 0.011609964084573534,
          "standard_error": 0.004762317366923821
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.559007069435378,
            "upper_bound": 18.573892815040782
          },
          "point_estimate": 18.567855599595724,
          "standard_error": 0.0038794583905586814
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006983479463897447,
            "upper_bound": 0.01493022493074572
          },
          "point_estimate": 0.012230223666591737,
          "standard_error": 0.001989821291712209
        }
      }
    },
    "memchr3/fallback/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/rare",
        "directory_name": "memchr3/fallback_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.64411325578405,
            "upper_bound": 32.68991144833679
          },
          "point_estimate": 32.66817503810845,
          "standard_error": 0.0117246760586163
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.64362094070881,
            "upper_bound": 32.69788646193733
          },
          "point_estimate": 32.6749129318285,
          "standard_error": 0.015398585745636902
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007621040164955684,
            "upper_bound": 0.06863656094088595
          },
          "point_estimate": 0.039100532045431,
          "standard_error": 0.014134557446766548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.64698508022548,
            "upper_bound": 32.692403786431754
          },
          "point_estimate": 32.66481789657458,
          "standard_error": 0.011621325818794102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02007218205329703,
            "upper_bound": 0.052164015121067074
          },
          "point_estimate": 0.03924208307208949,
          "standard_error": 0.008744712393619429
        }
      }
    },
    "memchr3/fallback/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/fallback/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "fallback/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/fallback/tiny/uncommon",
        "directory_name": "memchr3/fallback_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.19817468354069,
            "upper_bound": 93.31538272528812
          },
          "point_estimate": 93.25608659907472,
          "standard_error": 0.029922428852098382
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.16443924366976,
            "upper_bound": 93.3396202329488
          },
          "point_estimate": 93.2608457305785,
          "standard_error": 0.04840576764427778
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03014827000419857,
            "upper_bound": 0.17321929255096263
          },
          "point_estimate": 0.12107485423730478,
          "standard_error": 0.036569641155656585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.19124800415878,
            "upper_bound": 93.29370574264614
          },
          "point_estimate": 93.24616475763844,
          "standard_error": 0.025949647180225032
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0612893295330751,
            "upper_bound": 0.1196817315133176
          },
          "point_estimate": 0.09938885569347232,
          "standard_error": 0.014900474400678787
        }
      }
    },
    "memchr3/krate/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/krate/empty/never",
        "directory_name": "memchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48873629117942674,
            "upper_bound": 0.4895418289396964
          },
          "point_estimate": 0.48914438894782186,
          "standard_error": 0.00020659188075009176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4886737745319766,
            "upper_bound": 0.4896528353929611
          },
          "point_estimate": 0.4891784505050678,
          "standard_error": 0.00021807700994744384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00008438908414248723,
            "upper_bound": 0.0012029183073523262
          },
          "point_estimate": 0.0006355005285899695,
          "standard_error": 0.00030390970761688153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48915428299888414,
            "upper_bound": 0.4895879231765262
          },
          "point_estimate": 0.48932631826904727,
          "standard_error": 0.00011097178175988134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003639774585359276,
            "upper_bound": 0.0008858329083682837
          },
          "point_estimate": 0.000689143652127408,
          "standard_error": 0.00013503989725952289
        }
      }
    },
    "memchr3/krate/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/common",
        "directory_name": "memchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 628047.7717255746,
            "upper_bound": 629172.0662356322
          },
          "point_estimate": 628585.7490065682,
          "standard_error": 288.34128930736415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 627643.9396551724,
            "upper_bound": 629265.5620689655
          },
          "point_estimate": 628432.4875,
          "standard_error": 355.86641880957166
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.76515955500074,
            "upper_bound": 1667.9984453586744
          },
          "point_estimate": 1178.2093267550574,
          "standard_error": 464.7876924666312
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 628125.4048178507,
            "upper_bound": 628745.0443157328
          },
          "point_estimate": 628413.0715629199,
          "standard_error": 154.54177261189045
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 465.1191027760927,
            "upper_bound": 1181.2845063199172
          },
          "point_estimate": 958.5225627691768,
          "standard_error": 175.3408430207502
        }
      }
    },
    "memchr3/krate/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/never",
        "directory_name": "memchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15218.033136018636,
            "upper_bound": 15237.221486799075
          },
          "point_estimate": 15227.613314671908,
          "standard_error": 4.911997347739647
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15212.76996091569,
            "upper_bound": 15241.68137562814
          },
          "point_estimate": 15228.020128419876,
          "standard_error": 6.633286793413868
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.947315895468679,
            "upper_bound": 27.951634868080436
          },
          "point_estimate": 21.432031345845314,
          "standard_error": 6.590649179603624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15225.025801048252,
            "upper_bound": 15236.619528974372
          },
          "point_estimate": 15230.741955448237,
          "standard_error": 2.9171071311054253
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.565718100288173,
            "upper_bound": 20.298088358543072
          },
          "point_estimate": 16.330203650232946,
          "standard_error": 2.7701080222416725
        }
      }
    },
    "memchr3/krate/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/rare",
        "directory_name": "memchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20492.753450169395,
            "upper_bound": 20533.42489553252
          },
          "point_estimate": 20511.128524105297,
          "standard_error": 10.476666218700814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20488.03049124788,
            "upper_bound": 20537.501599849427
          },
          "point_estimate": 20498.96329757199,
          "standard_error": 12.087532782580492
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4353007650342258,
            "upper_bound": 55.43393304746748
          },
          "point_estimate": 16.396640420366555,
          "standard_error": 13.234096369267055
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20487.01933534851,
            "upper_bound": 20540.70901688362
          },
          "point_estimate": 20506.37526821005,
          "standard_error": 14.02235011737311
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.664841968463406,
            "upper_bound": 47.49426491311032
          },
          "point_estimate": 34.92858602909094,
          "standard_error": 9.437045243721148
        }
      }
    },
    "memchr3/krate/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/krate/huge/uncommon",
        "directory_name": "memchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217599.31254771352,
            "upper_bound": 218128.39326341645
          },
          "point_estimate": 217816.41836285905,
          "standard_error": 140.3331610483427
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217533.81547619047,
            "upper_bound": 217895.82142857145
          },
          "point_estimate": 217684.2534545068,
          "standard_error": 98.4721688097755
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.5618334378993,
            "upper_bound": 440.1297074638935
          },
          "point_estimate": 268.35500773574216,
          "standard_error": 100.48045664692874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217528.5759115573,
            "upper_bound": 217744.1777526996
          },
          "point_estimate": 217617.4153215832,
          "standard_error": 55.76268279751391
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.87021056246545,
            "upper_bound": 696.891166053251
          },
          "point_estimate": 466.804651084354,
          "standard_error": 182.66435000260975
        }
      }
    },
    "memchr3/krate/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/common",
        "directory_name": "memchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630.7448549304569,
            "upper_bound": 631.5330340027517
          },
          "point_estimate": 631.1273812620675,
          "standard_error": 0.202413896483807
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630.5263102216364,
            "upper_bound": 631.9013735215567
          },
          "point_estimate": 631.0628663591274,
          "standard_error": 0.32757480486106566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08283947860270652,
            "upper_bound": 1.1272528501618695
          },
          "point_estimate": 0.825950869280922,
          "standard_error": 0.29228335489796353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 630.5947634937622,
            "upper_bound": 631.3806229837722
          },
          "point_estimate": 630.8965860610896,
          "standard_error": 0.20188038831977248
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3460521698679637,
            "upper_bound": 0.791497574229119
          },
          "point_estimate": 0.6725263080910472,
          "standard_error": 0.10289093355937448
        }
      }
    },
    "memchr3/krate/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/never",
        "directory_name": "memchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.336857289853814,
            "upper_bound": 15.364075269575965
          },
          "point_estimate": 15.350631735967337,
          "standard_error": 0.007013042669282459
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.332667419999856,
            "upper_bound": 15.369788196682528
          },
          "point_estimate": 15.351372077017274,
          "standard_error": 0.008239026829107253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006105424664451168,
            "upper_bound": 0.046444798884350144
          },
          "point_estimate": 0.02019318739326389,
          "standard_error": 0.010752292639415472
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.32943201954241,
            "upper_bound": 15.356539429332576
          },
          "point_estimate": 15.34305845187283,
          "standard_error": 0.007354517928236212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012960623728200818,
            "upper_bound": 0.02947808486400905
          },
          "point_estimate": 0.023449252396401835,
          "standard_error": 0.0042741906030275174
        }
      }
    },
    "memchr3/krate/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/rare",
        "directory_name": "memchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.52586462159932,
            "upper_bound": 33.559357766148814
          },
          "point_estimate": 33.542665077883,
          "standard_error": 0.00854772881409347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.526440255274466,
            "upper_bound": 33.56117871842399
          },
          "point_estimate": 33.541841145241705,
          "standard_error": 0.008391754438713981
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006875699549996648,
            "upper_bound": 0.048031674199139326
          },
          "point_estimate": 0.01899462679394717,
          "standard_error": 0.01055009010830623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.53686882109747,
            "upper_bound": 33.55909098211603
          },
          "point_estimate": 33.548764399101856,
          "standard_error": 0.005581898247775472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012736246746404804,
            "upper_bound": 0.03851844353344466
          },
          "point_estimate": 0.028575642893454857,
          "standard_error": 0.006519673153460723
        }
      }
    },
    "memchr3/krate/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/krate/small/uncommon",
        "directory_name": "memchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.49867091953053,
            "upper_bound": 172.73008746584114
          },
          "point_estimate": 172.62139379850433,
          "standard_error": 0.059590516656997945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.40432515104305,
            "upper_bound": 172.76504042064064
          },
          "point_estimate": 172.7063431748806,
          "standard_error": 0.0917862227442794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0154116498242462,
            "upper_bound": 0.3158336898178253
          },
          "point_estimate": 0.09427688280207457,
          "standard_error": 0.0777710484783467
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.45699645543897,
            "upper_bound": 172.73070544227602
          },
          "point_estimate": 172.6270956899619,
          "standard_error": 0.06994796673637094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0663592660590867,
            "upper_bound": 0.23208435295748517
          },
          "point_estimate": 0.19782856791299833,
          "standard_error": 0.035882003463017155
        }
      }
    },
    "memchr3/krate/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/never",
        "directory_name": "memchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.379287347742016,
            "upper_bound": 5.387213237649684
          },
          "point_estimate": 5.383298994536859,
          "standard_error": 0.0020253642894130916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.378488176427241,
            "upper_bound": 5.388552105198712
          },
          "point_estimate": 5.383241171728744,
          "standard_error": 0.002553226851097532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014651036449406525,
            "upper_bound": 0.012117208751182136
          },
          "point_estimate": 0.007193988959315198,
          "standard_error": 0.002716212875447636
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.380356105646137,
            "upper_bound": 5.386676972673032
          },
          "point_estimate": 5.3832672828640655,
          "standard_error": 0.0015863661229051282
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003697117896041421,
            "upper_bound": 0.008660964380091098
          },
          "point_estimate": 0.0067246151553152535,
          "standard_error": 0.0012875312932537708
        }
      }
    },
    "memchr3/krate/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/rare",
        "directory_name": "memchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.11554945435879,
            "upper_bound": 20.146424351465257
          },
          "point_estimate": 20.13047248566087,
          "standard_error": 0.007886930104268402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.11534898497175,
            "upper_bound": 20.14668050084702
          },
          "point_estimate": 20.128186825320483,
          "standard_error": 0.007175016969542929
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026520350570296983,
            "upper_bound": 0.043942196507851514
          },
          "point_estimate": 0.023226052305992343,
          "standard_error": 0.0108078228730424
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.125646453347738,
            "upper_bound": 20.14500084672583
          },
          "point_estimate": 20.134525721555807,
          "standard_error": 0.004963216326341778
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011612485680411976,
            "upper_bound": 0.035746287686054895
          },
          "point_estimate": 0.026288331363801,
          "standard_error": 0.006227547473343558
        }
      }
    },
    "memchr3/krate/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/krate/tiny/uncommon",
        "directory_name": "memchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.54271087091627,
            "upper_bound": 82.77050876430084
          },
          "point_estimate": 82.62937778864116,
          "standard_error": 0.0635264591963005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.5279063069618,
            "upper_bound": 82.62833573970282
          },
          "point_estimate": 82.56229417961748,
          "standard_error": 0.027911172203766193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01040246568087692,
            "upper_bound": 0.11733543052905206
          },
          "point_estimate": 0.049518882944380414,
          "standard_error": 0.032036503684444925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.54940819514187,
            "upper_bound": 82.63888575314839
          },
          "point_estimate": 82.59121740641551,
          "standard_error": 0.025386680555165683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024940485223236983,
            "upper_bound": 0.3232669696064354
          },
          "point_estimate": 0.2117924505632859,
          "standard_error": 0.09903816436501152
        }
      }
    },
    "memchr3/naive/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memchr3/naive/empty/never",
        "directory_name": "memchr3/naive_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6519773713638908,
            "upper_bound": 0.6535675741831561
          },
          "point_estimate": 0.6527440723870696,
          "standard_error": 0.0004057624689287528
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6519650839662025,
            "upper_bound": 0.6535440605196584
          },
          "point_estimate": 0.6523487536536023,
          "standard_error": 0.0004631347403819366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00009513862083277052,
            "upper_bound": 0.002246176457949394
          },
          "point_estimate": 0.0012654716780345996,
          "standard_error": 0.0005717346823416369
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6521882292361513,
            "upper_bound": 0.6531493154936887
          },
          "point_estimate": 0.6526634864274542,
          "standard_error": 0.00025057665412058275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006206644859182098,
            "upper_bound": 0.0018372192744423848
          },
          "point_estimate": 0.001351578030287026,
          "standard_error": 0.0003210436176463527
        }
      }
    },
    "memchr3/naive/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/common",
        "directory_name": "memchr3/naive_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1224566.937110582,
            "upper_bound": 1227249.8833375664
          },
          "point_estimate": 1225743.334931217,
          "standard_error": 698.3950377963719
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1224374.677777778,
            "upper_bound": 1226791.911111111
          },
          "point_estimate": 1225164.0942857142,
          "standard_error": 517.2163118876813
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 246.55716006743225,
            "upper_bound": 3032.282654166447
          },
          "point_estimate": 940.22509275223,
          "standard_error": 737.3905991605759
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223922.4946509008,
            "upper_bound": 1225527.0400980392
          },
          "point_estimate": 1224682.999047619,
          "standard_error": 429.1738779077485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 671.930830994653,
            "upper_bound": 3287.668468694836
          },
          "point_estimate": 2330.8534747761305,
          "standard_error": 741.9540496757317
        }
      }
    },
    "memchr3/naive/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/never",
        "directory_name": "memchr3/naive_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290813.2209400476,
            "upper_bound": 291405.5373149603
          },
          "point_estimate": 291094.20066698414,
          "standard_error": 151.85788106434336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290649.3813333333,
            "upper_bound": 291614.288
          },
          "point_estimate": 290954.9504571429,
          "standard_error": 229.2424995930873
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.4625344204788,
            "upper_bound": 784.5580180713027
          },
          "point_estimate": 486.47395096344866,
          "standard_error": 195.72882351636403
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 290812.0454452482,
            "upper_bound": 291425.39792613644
          },
          "point_estimate": 291115.39808831166,
          "standard_error": 162.49383015865024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.09805535979208,
            "upper_bound": 603.9512870285861
          },
          "point_estimate": 506.4970119725627,
          "standard_error": 85.68987469836823
        }
      }
    },
    "memchr3/naive/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/rare",
        "directory_name": "memchr3/naive_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295115.6704717102,
            "upper_bound": 295751.88076110464
          },
          "point_estimate": 295431.8362282386,
          "standard_error": 162.85121184043723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295009.2489919355,
            "upper_bound": 295815.52419354836
          },
          "point_estimate": 295385.93093958014,
          "standard_error": 181.0595245365111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.17533003966855,
            "upper_bound": 923.592030377134
          },
          "point_estimate": 449.58784252893105,
          "standard_error": 238.47839761188465
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295309.01235393557,
            "upper_bound": 295688.54400983267
          },
          "point_estimate": 295479.5684122329,
          "standard_error": 95.16379305051476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 280.3108390623581,
            "upper_bound": 705.8342359695064
          },
          "point_estimate": 544.7375569094804,
          "standard_error": 108.44213305895076
        }
      }
    },
    "memchr3/naive/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memchr3/naive/huge/uncommon",
        "directory_name": "memchr3/naive_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463746.06794655416,
            "upper_bound": 464320.87420434
          },
          "point_estimate": 464020.6721267832,
          "standard_error": 146.02476399640747
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463798.40717299574,
            "upper_bound": 464200.38860759494
          },
          "point_estimate": 463956.289556962,
          "standard_error": 104.249237232051
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.277946348062144,
            "upper_bound": 743.6937289487206
          },
          "point_estimate": 289.10464898131784,
          "standard_error": 160.38022604311593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463855.2801915133,
            "upper_bound": 464464.6542977532
          },
          "point_estimate": 464114.3461121157,
          "standard_error": 156.33942755456926
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147.5619201322153,
            "upper_bound": 691.4424473143596
          },
          "point_estimate": 487.1268699922286,
          "standard_error": 143.16081822149332
        }
      }
    },
    "memchr3/naive/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/common",
        "directory_name": "memchr3/naive_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.4570686174464,
            "upper_bound": 424.9656115075678
          },
          "point_estimate": 420.0636677230856,
          "standard_error": 2.436468660020461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 412.7441547128173,
            "upper_bound": 426.9538150686258
          },
          "point_estimate": 418.71130727684533,
          "standard_error": 4.192931599939852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.700855611827125,
            "upper_bound": 13.518181501394624
          },
          "point_estimate": 9.784892908343956,
          "standard_error": 3.2024071574057316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.99067334755705,
            "upper_bound": 425.2538929864297
          },
          "point_estimate": 421.8874206761935,
          "standard_error": 2.09249473960845
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.857937145897269,
            "upper_bound": 9.92685193806811
          },
          "point_estimate": 8.129051907764387,
          "standard_error": 1.313810413641926
        }
      }
    },
    "memchr3/naive/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/never",
        "directory_name": "memchr3/naive_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.7203736923986,
            "upper_bound": 334.2692052328588
          },
          "point_estimate": 333.9657091048105,
          "standard_error": 0.14157882119599685
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.65652885871464,
            "upper_bound": 334.2104975069456
          },
          "point_estimate": 333.82212659891775,
          "standard_error": 0.13369582737489716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03710864453634091,
            "upper_bound": 0.6303764073657889
          },
          "point_estimate": 0.3592903086569058,
          "standard_error": 0.15708980501027422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 333.7809096018479,
            "upper_bound": 334.1579136772548
          },
          "point_estimate": 333.9804941760232,
          "standard_error": 0.09741311473995756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1815181561564004,
            "upper_bound": 0.6663245229834133
          },
          "point_estimate": 0.4726505815081865,
          "standard_error": 0.14257884697720916
        }
      }
    },
    "memchr3/naive/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/rare",
        "directory_name": "memchr3/naive_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359.65702466765083,
            "upper_bound": 360.4506692161187
          },
          "point_estimate": 360.0338686304657,
          "standard_error": 0.20275039141080844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359.54591821598103,
            "upper_bound": 360.4103571251978
          },
          "point_estimate": 359.9763404316104,
          "standard_error": 0.26432284632427117
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.16390687394430453,
            "upper_bound": 1.1502832242381762
          },
          "point_estimate": 0.6328424667612421,
          "standard_error": 0.24726230775397257
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359.74277690565333,
            "upper_bound": 360.24987545693807
          },
          "point_estimate": 360.0089826411721,
          "standard_error": 0.12862973193589358
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.34173744130970074,
            "upper_bound": 0.9046665815868794
          },
          "point_estimate": 0.6762312732637984,
          "standard_error": 0.15217581491396628
        }
      }
    },
    "memchr3/naive/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memchr3/naive/small/uncommon",
        "directory_name": "memchr3/naive_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.4039436298085,
            "upper_bound": 415.9987855884023
          },
          "point_estimate": 415.6876206725359,
          "standard_error": 0.15280441777736797
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.3011972670912,
            "upper_bound": 415.942699223796
          },
          "point_estimate": 415.7052582393086,
          "standard_error": 0.17441125238639468
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10487421307463816,
            "upper_bound": 0.8447993574082453
          },
          "point_estimate": 0.4088475732483918,
          "standard_error": 0.1833019812131234
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415.43224557523394,
            "upper_bound": 415.7995944959916
          },
          "point_estimate": 415.600014994665,
          "standard_error": 0.09211607978169524
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24222071677646603,
            "upper_bound": 0.6916168316312453
          },
          "point_estimate": 0.5103920308886792,
          "standard_error": 0.12115959871093988
        }
      }
    },
    "memchr3/naive/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/never",
        "directory_name": "memchr3/naive_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.01146557868327,
            "upper_bound": 43.12741801874576
          },
          "point_estimate": 43.06382107802544,
          "standard_error": 0.029802946745728955
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.01316018162463,
            "upper_bound": 43.09503516557743
          },
          "point_estimate": 43.04620288790269,
          "standard_error": 0.024302422691845193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01444985376830157,
            "upper_bound": 0.132997203896151
          },
          "point_estimate": 0.05398025045628165,
          "standard_error": 0.029256960078353024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.98973250867847,
            "upper_bound": 43.1167229100062
          },
          "point_estimate": 43.04909984293819,
          "standard_error": 0.03189660043766138
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0321132138339845,
            "upper_bound": 0.14198844001192504
          },
          "point_estimate": 0.09908233981070674,
          "standard_error": 0.03137337390249362
        }
      }
    },
    "memchr3/naive/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/rare",
        "directory_name": "memchr3/naive_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.599355515090586,
            "upper_bound": 43.84786672060148
          },
          "point_estimate": 43.730520226240536,
          "standard_error": 0.06361910676258108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.55967265364235,
            "upper_bound": 43.87145040805584
          },
          "point_estimate": 43.82076287264015,
          "standard_error": 0.08828433847607536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01773103068892369,
            "upper_bound": 0.36028584887069354
          },
          "point_estimate": 0.179839586491988,
          "standard_error": 0.08959498788121373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.49030276243548,
            "upper_bound": 43.84499215176301
          },
          "point_estimate": 43.636572575921235,
          "standard_error": 0.0882024493980316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10630111830411468,
            "upper_bound": 0.27141671603888945
          },
          "point_estimate": 0.21216322499371967,
          "standard_error": 0.043314715100515065
        }
      }
    },
    "memchr3/naive/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memchr3/naive/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memchr3",
        "function_id": "naive/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memchr3/naive/tiny/uncommon",
        "directory_name": "memchr3/naive_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.52796408726259,
            "upper_bound": 42.579873136151626
          },
          "point_estimate": 42.55317325332792,
          "standard_error": 0.013319178663135069
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.51454536732841,
            "upper_bound": 42.59491091736164
          },
          "point_estimate": 42.543475651113226,
          "standard_error": 0.019947432489977032
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008831597591932324,
            "upper_bound": 0.07503555189708172
          },
          "point_estimate": 0.04711668481902784,
          "standard_error": 0.01673833737771836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.520225768425554,
            "upper_bound": 42.57078301806539
          },
          "point_estimate": 42.545552103948616,
          "standard_error": 0.013116034604488232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0250268818665505,
            "upper_bound": 0.053850578935069095
          },
          "point_estimate": 0.044467965825209885,
          "standard_error": 0.007190751182822124
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46681.90942883537,
            "upper_bound": 46795.042147039625
          },
          "point_estimate": 46737.83635262374,
          "standard_error": 28.91288302408515
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46672.9043059126,
            "upper_bound": 46832.40706940874
          },
          "point_estimate": 46701.65359897172,
          "standard_error": 45.756548110123425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.992231135504941,
            "upper_bound": 168.25855864520648
          },
          "point_estimate": 113.33314348921512,
          "standard_error": 46.33114406534629
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46706.9949051031,
            "upper_bound": 46830.536675594914
          },
          "point_estimate": 46767.84591860582,
          "standard_error": 31.971451013062826
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.18921635437235,
            "upper_bound": 119.75424437415772
          },
          "point_estimate": 96.46068536157698,
          "standard_error": 15.864077663975864
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48979.09954517293,
            "upper_bound": 49032.67869118386
          },
          "point_estimate": 49007.24702760154,
          "standard_error": 13.705743624612994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48974.93935995214,
            "upper_bound": 49032.6369448183
          },
          "point_estimate": 49018.486361597126,
          "standard_error": 13.384987814575323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.7605988210095305,
            "upper_bound": 78.28688242574395
          },
          "point_estimate": 31.51783169706331,
          "standard_error": 18.187887162783376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48989.630526261986,
            "upper_bound": 49021.309678655285
          },
          "point_estimate": 49005.687853734424,
          "standard_error": 8.015455690346455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.731890270960655,
            "upper_bound": 60.80028524288841
          },
          "point_estimate": 45.633198284335734,
          "standard_error": 10.859150729292391
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48611.51713851223,
            "upper_bound": 48724.30625259952
          },
          "point_estimate": 48665.096507883456,
          "standard_error": 28.87945850751179
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48583.380124777184,
            "upper_bound": 48746.95060903149
          },
          "point_estimate": 48658.312277183606,
          "standard_error": 36.71836459338606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.88662294448419,
            "upper_bound": 177.17677682329204
          },
          "point_estimate": 86.91515231724763,
          "standard_error": 40.78563561168807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48623.5388826969,
            "upper_bound": 48717.85082519868
          },
          "point_estimate": 48670.56015348288,
          "standard_error": 23.69117574421551
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.68865828176092,
            "upper_bound": 119.79205032612502
          },
          "point_estimate": 96.05571226113872,
          "standard_error": 18.77329723013867
        }
      }
    },
    "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13941.256589833343,
            "upper_bound": 13955.926956728516
          },
          "point_estimate": 13948.379055083326,
          "standard_error": 3.7582723255315265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13937.393254729915,
            "upper_bound": 13955.875642994242
          },
          "point_estimate": 13949.033162721262,
          "standard_error": 4.206304917786477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8750628187585882,
            "upper_bound": 21.781015651697
          },
          "point_estimate": 10.766399306617556,
          "standard_error": 5.233795990434304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13944.738994421925,
            "upper_bound": 13958.984161722949
          },
          "point_estimate": 13951.945660941745,
          "standard_error": 3.694715070763407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.48057198768702,
            "upper_bound": 16.20572147316612
          },
          "point_estimate": 12.455339805823948,
          "standard_error": 2.5344608852130963
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26575.904880516948,
            "upper_bound": 26647.804779322112
          },
          "point_estimate": 26607.416982443312,
          "standard_error": 18.70410513465171
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26567.795354791517,
            "upper_bound": 26632.297488417466
          },
          "point_estimate": 26592.394135576687,
          "standard_error": 14.598791322969229
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.934192270059699,
            "upper_bound": 79.10051361031726
          },
          "point_estimate": 29.404176434810896,
          "standard_error": 19.196143839179005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26579.71958075151,
            "upper_bound": 26610.635492343783
          },
          "point_estimate": 26592.16125746967,
          "standard_error": 7.85545414134999
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.10304154372716,
            "upper_bound": 89.08118502012017
          },
          "point_estimate": 62.47948863899239,
          "standard_error": 20.096837634286626
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17720.56075566907,
            "upper_bound": 17736.48437613129
          },
          "point_estimate": 17728.95310008926,
          "standard_error": 4.064384723696282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17723.674083129583,
            "upper_bound": 17736.34866398882
          },
          "point_estimate": 17730.417273838633,
          "standard_error": 2.883231636194855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3830853340422984,
            "upper_bound": 20.649295635845483
          },
          "point_estimate": 6.931380428699126,
          "standard_error": 4.890569564233262
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17728.32139648604,
            "upper_bound": 17739.2166221933
          },
          "point_estimate": 17733.53407423872,
          "standard_error": 2.7521947268311173
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.4266610211763,
            "upper_bound": 19.144036107676307
          },
          "point_estimate": 13.531801507508916,
          "standard_error": 3.808745125714161
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17478.278243197827,
            "upper_bound": 17519.10324163191
          },
          "point_estimate": 17497.339419275253,
          "standard_error": 10.458793664941846
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17473.280558229068,
            "upper_bound": 17518.213426371512
          },
          "point_estimate": 17490.80571978551,
          "standard_error": 10.69826514148169
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.800864638751885,
            "upper_bound": 55.864231448058725
          },
          "point_estimate": 30.69804989292097,
          "standard_error": 12.934308344172342
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17467.633026744814,
            "upper_bound": 17511.39124966212
          },
          "point_estimate": 17484.162461407697,
          "standard_error": 11.053517278757012
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.643986302333587,
            "upper_bound": 46.950575305180955
          },
          "point_estimate": 34.8311214738831,
          "standard_error": 8.427756043171138
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17309.793465336857,
            "upper_bound": 17335.00882427006
          },
          "point_estimate": 17320.287975717838,
          "standard_error": 6.669239062970508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17308.260498196418,
            "upper_bound": 17322.74946403049
          },
          "point_estimate": 17315.026774654598,
          "standard_error": 3.402543728970959
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5896495750458673,
            "upper_bound": 22.229499381435172
          },
          "point_estimate": 7.852027143431124,
          "standard_error": 6.232715713095458
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17307.16876800678,
            "upper_bound": 17318.332240884258
          },
          "point_estimate": 17312.770554933395,
          "standard_error": 2.851884433645216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.600443946504119,
            "upper_bound": 33.13736147790858
          },
          "point_estimate": 22.305486069072824,
          "standard_error": 8.634785610406986
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39214.85598917984,
            "upper_bound": 39355.0920890576
          },
          "point_estimate": 39283.31331684916,
          "standard_error": 36.051032870766505
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39188.700001936784,
            "upper_bound": 39413.96800433839
          },
          "point_estimate": 39233.23499638467,
          "standard_error": 67.46535700513253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.918903749119412,
            "upper_bound": 181.9326246550772
          },
          "point_estimate": 125.3428702259138,
          "standard_error": 50.59389268944366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39190.3756654125,
            "upper_bound": 39402.6554792828
          },
          "point_estimate": 39315.469819984784,
          "standard_error": 54.665566500004225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.25862555177949,
            "upper_bound": 138.96081853839962
          },
          "point_estimate": 119.74229716769344,
          "standard_error": 16.196864367775635
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16529.34821957371,
            "upper_bound": 16573.952128238874
          },
          "point_estimate": 16549.06794777359,
          "standard_error": 11.530551846912068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16523.34076433121,
            "upper_bound": 16565.989877161053
          },
          "point_estimate": 16535.912647861693,
          "standard_error": 11.02089145538384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7903767798400354,
            "upper_bound": 53.16176533007369
          },
          "point_estimate": 20.40193817282702,
          "standard_error": 12.599843166375074
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16524.7089228352,
            "upper_bound": 16550.129674340824
          },
          "point_estimate": 16537.142627890764,
          "standard_error": 6.530550941135659
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.861747336765365,
            "upper_bound": 52.402898790461435
          },
          "point_estimate": 38.403210365140865,
          "standard_error": 11.32082228279999
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18628.96295357876,
            "upper_bound": 18644.71215732049
          },
          "point_estimate": 18636.73237107249,
          "standard_error": 4.0342661292302
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18628.66912217659,
            "upper_bound": 18646.00218172485
          },
          "point_estimate": 18637.00806229833,
          "standard_error": 4.183493184166881
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4163787143743858,
            "upper_bound": 22.975685494309175
          },
          "point_estimate": 11.555823107217371,
          "standard_error": 5.840652163222126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18633.572027897757,
            "upper_bound": 18650.55410438043
          },
          "point_estimate": 18642.036890317075,
          "standard_error": 4.44435896683633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.603503425016658,
            "upper_bound": 17.727169653649405
          },
          "point_estimate": 13.4582401330243,
          "standard_error": 2.9148586639818044
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18035.07061320755,
            "upper_bound": 18084.20441823899
          },
          "point_estimate": 18061.084877799847,
          "standard_error": 12.571899400478758
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18032.17626613704,
            "upper_bound": 18089.2545183714
          },
          "point_estimate": 18070.180217643167,
          "standard_error": 13.57280431606804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.6579093952051425,
            "upper_bound": 69.25985284934129
          },
          "point_estimate": 31.05485632800774,
          "standard_error": 15.93608098545514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18055.453236848265,
            "upper_bound": 18084.28930935956
          },
          "point_estimate": 18069.848692915824,
          "standard_error": 7.281488911933604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.066674520436976,
            "upper_bound": 56.16994284045704
          },
          "point_estimate": 41.90895810616944,
          "standard_error": 9.936948183602256
        }
      }
    },
    "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17345.305567814354,
            "upper_bound": 17376.58675974835
          },
          "point_estimate": 17360.955953554767,
          "standard_error": 8.0309854448558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17338.71301685751,
            "upper_bound": 17383.322936545803
          },
          "point_estimate": 17361.64229825518,
          "standard_error": 15.641933433012976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.627984382158252,
            "upper_bound": 40.0705888210471
          },
          "point_estimate": 32.53927332498367,
          "standard_error": 10.536829150636812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17348.598595562868,
            "upper_bound": 17384.85516238312
          },
          "point_estimate": 17367.284227223157,
          "standard_error": 9.417794794318212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.837512595077506,
            "upper_bound": 31.67360983313608
          },
          "point_estimate": 26.837036919814896,
          "standard_error": 3.566732058025269
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17700.385847688733,
            "upper_bound": 17726.394385255513
          },
          "point_estimate": 17713.263321370498,
          "standard_error": 6.674441654196329
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17694.489126016262,
            "upper_bound": 17732.321780487808
          },
          "point_estimate": 17716.0236097561,
          "standard_error": 8.161823503074972
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.74382245860543,
            "upper_bound": 42.83969007846955
          },
          "point_estimate": 22.95066567303357,
          "standard_error": 10.870174865375924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17700.939754437317,
            "upper_bound": 17726.864650306292
          },
          "point_estimate": 17713.492358568263,
          "standard_error": 6.6679309568472735
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.33987505943108,
            "upper_bound": 28.11620733563201
          },
          "point_estimate": 22.27784057892937,
          "standard_error": 4.092771219247166
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17497.490650864136,
            "upper_bound": 17587.535052416955
          },
          "point_estimate": 17539.082491751058,
          "standard_error": 23.16058034307159
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17475.890782525203,
            "upper_bound": 17589.03168506961
          },
          "point_estimate": 17518.567964207607,
          "standard_error": 30.13909768561269
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.528712024781829,
            "upper_bound": 122.37384097433204
          },
          "point_estimate": 68.03754445139928,
          "standard_error": 29.545522122067386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17478.91911708962,
            "upper_bound": 17517.458178493845
          },
          "point_estimate": 17492.302399760585,
          "standard_error": 9.932514141939846
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.209877238355546,
            "upper_bound": 102.32856748546608
          },
          "point_estimate": 77.1697896084156,
          "standard_error": 18.38603632670097
        }
      }
    },
    "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17479.90483317292,
            "upper_bound": 17502.97980816891
          },
          "point_estimate": 17491.291487751503,
          "standard_error": 5.896933664276363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17479.122232916267,
            "upper_bound": 17506.088787295477
          },
          "point_estimate": 17486.935480544482,
          "standard_error": 8.432904401771955
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7123030831723105,
            "upper_bound": 34.21688055710984
          },
          "point_estimate": 15.88962371212748,
          "standard_error": 8.815600050701017
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17478.0474190398,
            "upper_bound": 17500.689732254985
          },
          "point_estimate": 17487.811898303815,
          "standard_error": 5.818682333491875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.717041822689286,
            "upper_bound": 25.56819330736741
          },
          "point_estimate": 19.694440202239907,
          "standard_error": 3.8639738569634656
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18091.313413709817,
            "upper_bound": 18112.17863565985
          },
          "point_estimate": 18101.049411187265,
          "standard_error": 5.3653102588894335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18086.25507246377,
            "upper_bound": 18118.053173413293
          },
          "point_estimate": 18095.2567153923,
          "standard_error": 8.05526600748936
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9625741180849443,
            "upper_bound": 28.771259637878636
          },
          "point_estimate": 14.36626840436283,
          "standard_error": 7.371649871872868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18096.15856863235,
            "upper_bound": 18120.38292807864
          },
          "point_estimate": 18110.893827112417,
          "standard_error": 6.013750560854347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.752864226438498,
            "upper_bound": 22.43507468034419
          },
          "point_estimate": 17.886236098777825,
          "standard_error": 3.555546208360665
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18135.875461252657,
            "upper_bound": 18152.685945456786
          },
          "point_estimate": 18144.66828124753,
          "standard_error": 4.320815047715597
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18134.80091867255,
            "upper_bound": 18156.52594300432
          },
          "point_estimate": 18146.99421180902,
          "standard_error": 6.916488112079789
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2734082389680346,
            "upper_bound": 25.93955094983269
          },
          "point_estimate": 14.188561404797168,
          "standard_error": 5.827970191336173
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18129.729397415947,
            "upper_bound": 18152.96460394601
          },
          "point_estimate": 18140.151920860793,
          "standard_error": 5.767822695985282
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.212096147395904,
            "upper_bound": 18.31285885612565
          },
          "point_estimate": 14.425015093555514,
          "standard_error": 2.696215896821598
        }
      }
    },
    "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18273.787369774203,
            "upper_bound": 18293.106759527906
          },
          "point_estimate": 18283.815501241577,
          "standard_error": 4.953564396007015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18272.476609657948,
            "upper_bound": 18295.902550221967
          },
          "point_estimate": 18286.734595070426,
          "standard_error": 5.033807913297598
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6076933106556972,
            "upper_bound": 27.01087411804636
          },
          "point_estimate": 13.685113700703248,
          "standard_error": 7.882618108577509
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18275.506010817775,
            "upper_bound": 18293.262325786487
          },
          "point_estimate": 18285.900738195407,
          "standard_error": 4.431833005633114
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.210733872343138,
            "upper_bound": 21.63091925620093
          },
          "point_estimate": 16.52106378438383,
          "standard_error": 3.5039864750047456
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68570.44010873763,
            "upper_bound": 68758.60562809973
          },
          "point_estimate": 68643.00324880205,
          "standard_error": 51.92870756084905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68557.40678908356,
            "upper_bound": 68651.53815513627
          },
          "point_estimate": 68586.60062893081,
          "standard_error": 24.851827563648833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.184327195446718,
            "upper_bound": 110.58464549584812
          },
          "point_estimate": 48.42326534313986,
          "standard_error": 29.328503042904263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68566.70647274633,
            "upper_bound": 68631.86488469137
          },
          "point_estimate": 68599.92150943396,
          "standard_error": 16.610179412624287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.85369545478907,
            "upper_bound": 263.5093532230544
          },
          "point_estimate": 172.904423152984,
          "standard_error": 78.57191882233333
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227272.131015873,
            "upper_bound": 1230744.242786111
          },
          "point_estimate": 1229293.3701269843,
          "standard_error": 911.674492616438
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228678.0706349206,
            "upper_bound": 1231052.5533333332
          },
          "point_estimate": 1229997.0391666666,
          "standard_error": 665.7090840276791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 507.37536299259045,
            "upper_bound": 3124.475250696381
          },
          "point_estimate": 1612.3583588749746,
          "standard_error": 674.1711563041351
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228782.945542451,
            "upper_bound": 1230170.396949891
          },
          "point_estimate": 1229477.4425108226,
          "standard_error": 350.8105697958335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 873.7742478357407,
            "upper_bound": 4473.361539275971
          },
          "point_estimate": 3026.5403451736015,
          "standard_error": 1125.853680863367
        }
      }
    },
    "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207613.77904149663,
            "upper_bound": 207890.91395620184
          },
          "point_estimate": 207765.6133399093,
          "standard_error": 71.5466043473161
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207643.0165079365,
            "upper_bound": 207903.52775510203
          },
          "point_estimate": 207845.90342857144,
          "standard_error": 72.41941979962915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.12857874520086,
            "upper_bound": 343.90982029434775
          },
          "point_estimate": 139.65562252060474,
          "standard_error": 83.85450687473542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207696.1724107143,
            "upper_bound": 207871.01513345525
          },
          "point_estimate": 207788.81781076067,
          "standard_error": 44.96618478869759
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.42636374033414,
            "upper_bound": 332.73771147453965
          },
          "point_estimate": 238.540801231581,
          "standard_error": 68.84508717782339
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6204.214282604984,
            "upper_bound": 6218.19245679494
          },
          "point_estimate": 6210.642057483457,
          "standard_error": 3.572846820339671
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6204.386298651417,
            "upper_bound": 6215.157277796745
          },
          "point_estimate": 6208.775520655514,
          "standard_error": 2.4008968749508695
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8558863880138848,
            "upper_bound": 16.9413663602068
          },
          "point_estimate": 6.880971390299369,
          "standard_error": 4.056513143260077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6207.304329257336,
            "upper_bound": 6220.310326017979
          },
          "point_estimate": 6212.652723991611,
          "standard_error": 3.328042057399917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5051544329428865,
            "upper_bound": 17.085156042868014
          },
          "point_estimate": 11.939070419885551,
          "standard_error": 3.67561856357212
        }
      }
    },
    "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6489.285198831456,
            "upper_bound": 6501.523291226439
          },
          "point_estimate": 6495.467429137815,
          "standard_error": 3.1204889038617742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6489.358403386395,
            "upper_bound": 6500.87995215525
          },
          "point_estimate": 6496.779070955873,
          "standard_error": 2.4834409311614944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.135815835674749,
            "upper_bound": 16.918057419641244
          },
          "point_estimate": 6.011849387097704,
          "standard_error": 4.104611615511072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6495.558982401739,
            "upper_bound": 6507.415492656849
          },
          "point_estimate": 6501.2463690485865,
          "standard_error": 3.1849932120081483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.892339914450961,
            "upper_bound": 14.178003247703389
          },
          "point_estimate": 10.376313766257132,
          "standard_error": 2.6155747835243854
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14214.426228705206,
            "upper_bound": 14260.45985082224
          },
          "point_estimate": 14232.3555525413,
          "standard_error": 12.684847621531995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14211.401070216654,
            "upper_bound": 14234.141816758029
          },
          "point_estimate": 14216.540341947271,
          "standard_error": 7.1966705535417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.313727347467564,
            "upper_bound": 27.8952011570714
          },
          "point_estimate": 14.095527478805996,
          "standard_error": 7.103326532902283
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14212.374246643556,
            "upper_bound": 14230.124352215704
          },
          "point_estimate": 14221.833015692217,
          "standard_error": 4.5929387127418355
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.840901575989956,
            "upper_bound": 64.64633870586525
          },
          "point_estimate": 42.42882526206859,
          "standard_error": 19.27898106039454
        }
      }
    },
    "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.17318107160093,
            "upper_bound": 51.38096760715675
          },
          "point_estimate": 51.26814185908705,
          "standard_error": 0.05326966156665173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.14958132793029,
            "upper_bound": 51.40347003065368
          },
          "point_estimate": 51.178544001983255,
          "standard_error": 0.06281860705410884
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006295581067464138,
            "upper_bound": 0.27040088206359447
          },
          "point_estimate": 0.061937407469564144,
          "standard_error": 0.06048805443777575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.1633945698497,
            "upper_bound": 51.24147065272145
          },
          "point_estimate": 51.19119332458689,
          "standard_error": 0.02013612980223827
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03333201499788504,
            "upper_bound": 0.2197090098945217
          },
          "point_estimate": 0.1772673275349977,
          "standard_error": 0.045225046670504726
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.394997896341366,
            "upper_bound": 33.452236635803956
          },
          "point_estimate": 33.42008136059138,
          "standard_error": 0.014760578404059688
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.38747391508028,
            "upper_bound": 33.44239579575601
          },
          "point_estimate": 33.403574416862696,
          "standard_error": 0.013955562627421845
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006855377192118413,
            "upper_bound": 0.06490903816478001
          },
          "point_estimate": 0.025615553754268813,
          "standard_error": 0.01546854467922065
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.38817348374859,
            "upper_bound": 33.44591152459857
          },
          "point_estimate": 33.41570752765373,
          "standard_error": 0.015231416165143062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014637690820819591,
            "upper_bound": 0.06829277791138852
          },
          "point_estimate": 0.04895750231736855,
          "standard_error": 0.014927961923359987
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.92646135359413,
            "upper_bound": 36.98478811624325
          },
          "point_estimate": 36.95286914038497,
          "standard_error": 0.015012877260975482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.92410644534806,
            "upper_bound": 36.972684776318346
          },
          "point_estimate": 36.93985988841142,
          "standard_error": 0.012012567925818294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003901892737461289,
            "upper_bound": 0.07320704519532212
          },
          "point_estimate": 0.024349890208669844,
          "standard_error": 0.018523437862980347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.93300952613944,
            "upper_bound": 36.95527456602638
          },
          "point_estimate": 36.94295682898836,
          "standard_error": 0.0057108875384986445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016368812385194696,
            "upper_bound": 0.06907488798842308
          },
          "point_estimate": 0.05011221662298471,
          "standard_error": 0.01430892296752284
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.33788267720188,
            "upper_bound": 29.37938294728026
          },
          "point_estimate": 29.35766347784853,
          "standard_error": 0.010643220239843066
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.32869869668029,
            "upper_bound": 29.379768974551112
          },
          "point_estimate": 29.354152120067837,
          "standard_error": 0.012010682580126568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006324061392552101,
            "upper_bound": 0.059674081581365813
          },
          "point_estimate": 0.032914520073626705,
          "standard_error": 0.012962373420529066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.330656512303797,
            "upper_bound": 29.375802238711977
          },
          "point_estimate": 29.35152378379865,
          "standard_error": 0.011867495551114864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01759921875415853,
            "upper_bound": 0.04706172178463528
          },
          "point_estimate": 0.035502491697602126,
          "standard_error": 0.007915699085004215
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.97239597228964,
            "upper_bound": 23.00295398198708
          },
          "point_estimate": 22.98828742491083,
          "standard_error": 0.007811037829626336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.973487002511327,
            "upper_bound": 23.012678050540934
          },
          "point_estimate": 22.988841578301216,
          "standard_error": 0.00825672052647996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021533016023077083,
            "upper_bound": 0.043770667349216655
          },
          "point_estimate": 0.024772677790218408,
          "standard_error": 0.011182784236275936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.985160795114425,
            "upper_bound": 23.00797890713498
          },
          "point_estimate": 22.9967556978796,
          "standard_error": 0.005836136119788534
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012914607167094862,
            "upper_bound": 0.03471590811410566
          },
          "point_estimate": 0.0261063094105178,
          "standard_error": 0.0058654255084181605
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.57853884906283,
            "upper_bound": 32.62905277572591
          },
          "point_estimate": 32.602856402275954,
          "standard_error": 0.012958130686151854
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.573048189019104,
            "upper_bound": 32.635139067463804
          },
          "point_estimate": 32.59236095052216,
          "standard_error": 0.013951312291474924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005256894753540745,
            "upper_bound": 0.07617467044491842
          },
          "point_estimate": 0.03611846343669264,
          "standard_error": 0.019082608049625825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.58081229184878,
            "upper_bound": 32.634628706168876
          },
          "point_estimate": 32.60997147059251,
          "standard_error": 0.013741362514082726
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020908357929964573,
            "upper_bound": 0.05550087319845625
          },
          "point_estimate": 0.04320331911689253,
          "standard_error": 0.008729295652479813
        }
      }
    },
    "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.366549132464215,
            "upper_bound": 44.42199387167116
          },
          "point_estimate": 44.39456453523847,
          "standard_error": 0.01421930358745879
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.36021822354772,
            "upper_bound": 44.44319990348412
          },
          "point_estimate": 44.38490481624282,
          "standard_error": 0.02597364888224096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004113951762008996,
            "upper_bound": 0.08642829480953938
          },
          "point_estimate": 0.05526021163662659,
          "standard_error": 0.02196158102869866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.3670030172912,
            "upper_bound": 44.4129028826071
          },
          "point_estimate": 44.387267943913464,
          "standard_error": 0.011764555740292452
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030385186369479934,
            "upper_bound": 0.05806833921464497
          },
          "point_estimate": 0.04752548034592257,
          "standard_error": 0.007324933171319964
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.16541888358653,
            "upper_bound": 53.69913874111353
          },
          "point_estimate": 53.43749544455468,
          "standard_error": 0.13679052818276363
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.97817774471995,
            "upper_bound": 53.797364919102435
          },
          "point_estimate": 53.47370733796026,
          "standard_error": 0.21058468479972384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07472270457005482,
            "upper_bound": 0.7436892824271162
          },
          "point_estimate": 0.488857769197005,
          "standard_error": 0.1747190319065975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.178261203792054,
            "upper_bound": 53.64446182249736
          },
          "point_estimate": 53.39857868868937,
          "standard_error": 0.11924953176923486
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2644584270583796,
            "upper_bound": 0.5660543864951311
          },
          "point_estimate": 0.4556812430431015,
          "standard_error": 0.07644260699231928
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.9561924888467,
            "upper_bound": 39.98254997564885
          },
          "point_estimate": 39.9685963711609,
          "standard_error": 0.006745179960006289
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.95151236169465,
            "upper_bound": 39.97984051791822
          },
          "point_estimate": 39.968458847357766,
          "standard_error": 0.00607060941599018
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012702232746815525,
            "upper_bound": 0.03769313854844009
          },
          "point_estimate": 0.015201317418146471,
          "standard_error": 0.010949728442088025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.957043621085106,
            "upper_bound": 39.97912872065598
          },
          "point_estimate": 39.96699736836594,
          "standard_error": 0.0055167789982580075
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010440963816944294,
            "upper_bound": 0.030568094062962847
          },
          "point_estimate": 0.02256126296432565,
          "standard_error": 0.005575368428265243
        }
      }
    },
    "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.518337591701226,
            "upper_bound": 60.61409823845635
          },
          "point_estimate": 60.563418587011554,
          "standard_error": 0.024546138640544043
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.49650333780527,
            "upper_bound": 60.6204274685196
          },
          "point_estimate": 60.55182784309336,
          "standard_error": 0.0379293292256233
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009511261102192584,
            "upper_bound": 0.1513375853388971
          },
          "point_estimate": 0.0852553545236248,
          "standard_error": 0.03324779627824518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.51275318243174,
            "upper_bound": 60.576459958151034
          },
          "point_estimate": 60.539648806235725,
          "standard_error": 0.01628815040083491
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04383471604195225,
            "upper_bound": 0.10666786486768876
          },
          "point_estimate": 0.08189116686081524,
          "standard_error": 0.017334925547300144
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.53516325467013,
            "upper_bound": 42.580657570226066
          },
          "point_estimate": 42.55739600819118,
          "standard_error": 0.011671243098441322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.525780029447134,
            "upper_bound": 42.58844614299376
          },
          "point_estimate": 42.555560771232166,
          "standard_error": 0.01311916864539464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006055410684725497,
            "upper_bound": 0.07544157817736515
          },
          "point_estimate": 0.029769427714220947,
          "standard_error": 0.0180331015735591
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.526252256514894,
            "upper_bound": 42.59281709831112
          },
          "point_estimate": 42.55673543054334,
          "standard_error": 0.017521419718652023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020583874457180015,
            "upper_bound": 0.04906667077581236
          },
          "point_estimate": 0.03886052910358239,
          "standard_error": 0.007325422452958817
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.444597578293326,
            "upper_bound": 35.47289891116814
          },
          "point_estimate": 35.458693817917684,
          "standard_error": 0.007261738042186202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.438285943085546,
            "upper_bound": 35.47905054474659
          },
          "point_estimate": 35.45674684221614,
          "standard_error": 0.008923896716955725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004836358238413867,
            "upper_bound": 0.04672200536920637
          },
          "point_estimate": 0.020862953627493047,
          "standard_error": 0.010823879578454532
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.450007308654605,
            "upper_bound": 35.47964500650561
          },
          "point_estimate": 35.464718650898085,
          "standard_error": 0.007717889730556502
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013819574805618396,
            "upper_bound": 0.03041090976770937
          },
          "point_estimate": 0.024264259948214604,
          "standard_error": 0.004262840979136643
        }
      }
    },
    "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.00953341440938,
            "upper_bound": 67.5974016715063
          },
          "point_estimate": 67.30831510659283,
          "standard_error": 0.15044977587504552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.84018799450911,
            "upper_bound": 67.76911535833958
          },
          "point_estimate": 67.3308537691396,
          "standard_error": 0.2546479332901221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10251388509802672,
            "upper_bound": 0.8194056078741503
          },
          "point_estimate": 0.5319009673993478,
          "standard_error": 0.19165339962250352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.34503479198875,
            "upper_bound": 67.799228331301
          },
          "point_estimate": 67.63666393350667,
          "standard_error": 0.11611791200814096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3106573017874982,
            "upper_bound": 0.6017672928277334
          },
          "point_estimate": 0.5024785099045846,
          "standard_error": 0.07427627592873991
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98505.5437403536,
            "upper_bound": 98633.36979494127
          },
          "point_estimate": 98574.24010216372,
          "standard_error": 32.835694227044655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98490.89430894308,
            "upper_bound": 98651.35772357724
          },
          "point_estimate": 98621.27822944896,
          "standard_error": 43.51914460042712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.462561006036236,
            "upper_bound": 172.98603499122794
          },
          "point_estimate": 47.39498452440834,
          "standard_error": 42.3443429290208
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98513.45734538123,
            "upper_bound": 98630.86275253425
          },
          "point_estimate": 98587.39244008024,
          "standard_error": 30.511359009547284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.04618477849031,
            "upper_bound": 133.43338668429115
          },
          "point_estimate": 109.15693430973778,
          "standard_error": 23.754532047264348
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53394.7344132112,
            "upper_bound": 53472.36821492668
          },
          "point_estimate": 53433.47299955713,
          "standard_error": 19.798965162279295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53388.02437591777,
            "upper_bound": 53488.540871267745
          },
          "point_estimate": 53422.42204216488,
          "standard_error": 24.418436571138265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.597126689043425,
            "upper_bound": 116.648827194848
          },
          "point_estimate": 59.78105433514948,
          "standard_error": 26.209878952290214
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53363.299212598424,
            "upper_bound": 53458.37630406488
          },
          "point_estimate": 53407.58681846788,
          "standard_error": 24.15357561662696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.05327916054048,
            "upper_bound": 85.17348061310655
          },
          "point_estimate": 66.18270850206629,
          "standard_error": 12.696502056446567
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144463.8002941547,
            "upper_bound": 144690.48723903217
          },
          "point_estimate": 144576.0151111741,
          "standard_error": 58.138145050851904
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144395.56371252204,
            "upper_bound": 144739.40641534392
          },
          "point_estimate": 144575.36954365077,
          "standard_error": 113.3930325963773
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.02854726348196,
            "upper_bound": 301.19224381943354
          },
          "point_estimate": 256.56584686695135,
          "standard_error": 82.29402456209999
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144412.40828038694,
            "upper_bound": 144706.44246034496
          },
          "point_estimate": 144520.92329416616,
          "standard_error": 74.4395185926899
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.35092014446565,
            "upper_bound": 226.36697089217589
          },
          "point_estimate": 193.71510681401637,
          "standard_error": 25.622113181235875
        }
      }
    },
    "memmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389722.9937581371,
            "upper_bound": 390912.1775068073
          },
          "point_estimate": 390237.8296854948,
          "standard_error": 308.51378002390464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389573.1367781155,
            "upper_bound": 390662.1043882979
          },
          "point_estimate": 389914.1425531915,
          "standard_error": 273.8615795604691
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 107.44974319871372,
            "upper_bound": 1296.541563152008
          },
          "point_estimate": 542.4221562211816,
          "standard_error": 307.85106413229005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389646.1770132475,
            "upper_bound": 390300.8299308249
          },
          "point_estimate": 389949.6267753523,
          "standard_error": 168.84003546557358
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288.7638567315609,
            "upper_bound": 1450.114925304376
          },
          "point_estimate": 1025.8852206311574,
          "standard_error": 328.7666253332702
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570202.7973704891,
            "upper_bound": 570703.8567423115
          },
          "point_estimate": 570465.4999844991,
          "standard_error": 127.72695050827431
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570305.8271484375,
            "upper_bound": 570877.6
          },
          "point_estimate": 570408.5414806547,
          "standard_error": 136.5596981013056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.41946439256173,
            "upper_bound": 762.9720077826296
          },
          "point_estimate": 197.2969914972955,
          "standard_error": 191.7239212968433
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570402.1256768953,
            "upper_bound": 570742.0700363005
          },
          "point_estimate": 570561.3285308442,
          "standard_error": 89.44242631541725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197.51719175130228,
            "upper_bound": 584.1883454053959
          },
          "point_estimate": 426.7442584105621,
          "standard_error": 106.1940607245102
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39888.48131745596,
            "upper_bound": 39943.82207568867
          },
          "point_estimate": 39915.79065722301,
          "standard_error": 14.237059941017549
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39873.18659592634,
            "upper_bound": 39952.76308086352
          },
          "point_estimate": 39920.59228477341,
          "standard_error": 22.476003312720135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.295688489003185,
            "upper_bound": 84.35001110402635
          },
          "point_estimate": 52.47069404101636,
          "standard_error": 18.276856141326764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39884.70458128397,
            "upper_bound": 39942.1551325926
          },
          "point_estimate": 39907.22274366687,
          "standard_error": 14.550266809158105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.40893389308744,
            "upper_bound": 59.82138076388388
          },
          "point_estimate": 47.565969695369674,
          "standard_error": 8.219976520613018
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97914.2051212938,
            "upper_bound": 98091.6597458606
          },
          "point_estimate": 98006.2567439995,
          "standard_error": 45.48372664902032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97887.96978885894,
            "upper_bound": 98122.59299191374
          },
          "point_estimate": 98043.242266718,
          "standard_error": 60.49561470871213
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.33198014519952,
            "upper_bound": 251.3248394060292
          },
          "point_estimate": 124.62479239662638,
          "standard_error": 59.1457856648503
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97946.83062104975,
            "upper_bound": 98098.53384288492
          },
          "point_estimate": 98037.1049882732,
          "standard_error": 38.85145771955132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.07516132020066,
            "upper_bound": 188.21562078592888
          },
          "point_estimate": 151.79443531294774,
          "standard_error": 27.7055134722512
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68792.326471537,
            "upper_bound": 68876.95213909219
          },
          "point_estimate": 68834.06334718833,
          "standard_error": 21.628098384626863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68783.51138519924,
            "upper_bound": 68874.11764705883
          },
          "point_estimate": 68841.64974699556,
          "standard_error": 21.811968469755666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.201769226995086,
            "upper_bound": 128.9136775342181
          },
          "point_estimate": 60.01519011100269,
          "standard_error": 30.467676460358447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68751.52947151994,
            "upper_bound": 68857.6854467073
          },
          "point_estimate": 68796.02774834273,
          "standard_error": 28.061593922899103
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.49431639435615,
            "upper_bound": 96.14791642290103
          },
          "point_estimate": 72.39785682516504,
          "standard_error": 15.60792951007722
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306250.79009337066,
            "upper_bound": 306463.40786714683
          },
          "point_estimate": 306350.4536301187,
          "standard_error": 54.1908291140303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306224.02521008404,
            "upper_bound": 306446.1243830866
          },
          "point_estimate": 306333.46827731095,
          "standard_error": 61.07998487817977
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.078438887792935,
            "upper_bound": 291.2049420569628
          },
          "point_estimate": 149.7249473418241,
          "standard_error": 62.34720202835584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306255.29727166455,
            "upper_bound": 306424.6646345769
          },
          "point_estimate": 306337.38081414386,
          "standard_error": 43.021583381744215
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.53433960590625,
            "upper_bound": 244.5069029878563
          },
          "point_estimate": 180.43452660051724,
          "standard_error": 43.71598669802816
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36786.8466526233,
            "upper_bound": 36832.95694914897
          },
          "point_estimate": 36808.5357157703,
          "standard_error": 11.836393591657698
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36776.809571958205,
            "upper_bound": 36828.567003707445
          },
          "point_estimate": 36807.49117434638,
          "standard_error": 13.785416220851646
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.851076870163045,
            "upper_bound": 65.85520322668758
          },
          "point_estimate": 35.125769426950924,
          "standard_error": 13.876309634376245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36775.50115186061,
            "upper_bound": 36813.13887231803
          },
          "point_estimate": 36794.82459259648,
          "standard_error": 9.85141616277444
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.31631714827824,
            "upper_bound": 53.44457673709689
          },
          "point_estimate": 39.56105297318416,
          "standard_error": 9.62240281282446
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64904.410784525935,
            "upper_bound": 65050.055193956374
          },
          "point_estimate": 64973.46888231051,
          "standard_error": 37.40960575060397
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64891.43223834989,
            "upper_bound": 65132.22281639928
          },
          "point_estimate": 64927.29794266191,
          "standard_error": 55.37355354064952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.914918295816816,
            "upper_bound": 185.73929663117096
          },
          "point_estimate": 63.025545427417704,
          "standard_error": 50.281005533377126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64858.98557075088,
            "upper_bound": 64961.783331138104
          },
          "point_estimate": 64903.81781142209,
          "standard_error": 26.993489944712863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.951524692324426,
            "upper_bound": 148.92155760283032
          },
          "point_estimate": 124.87204769437444,
          "standard_error": 22.26772507354436
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167194.5400145624,
            "upper_bound": 167634.19319252405
          },
          "point_estimate": 167372.09888561236,
          "standard_error": 117.73614104407112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167160.4724770642,
            "upper_bound": 167401.33577981652
          },
          "point_estimate": 167290.35927624872,
          "standard_error": 64.6655529399123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.50277871392581,
            "upper_bound": 317.74682353320367
          },
          "point_estimate": 178.55196316036188,
          "standard_error": 73.09378898812487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 167205.64404150206,
            "upper_bound": 167382.69262489703
          },
          "point_estimate": 167304.48629810556,
          "standard_error": 45.395500119752654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.47416827431606,
            "upper_bound": 593.8271551581358
          },
          "point_estimate": 393.65558003442726,
          "standard_error": 164.81476088074157
        }
      }
    },
    "memmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34475.069145638314,
            "upper_bound": 34544.86264482431
          },
          "point_estimate": 34509.13350834351,
          "standard_error": 17.88069749460988
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34452.68240401574,
            "upper_bound": 34572.136942070276
          },
          "point_estimate": 34502.76495726495,
          "standard_error": 33.54789384784634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9507504521761905,
            "upper_bound": 101.8353691322277
          },
          "point_estimate": 74.98651813690284,
          "standard_error": 27.746943257922513
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34454.65252052822,
            "upper_bound": 34516.99606740205
          },
          "point_estimate": 34479.93872793873,
          "standard_error": 16.352347029805046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.415522900904755,
            "upper_bound": 70.34230760850737
          },
          "point_estimate": 59.53507111975932,
          "standard_error": 8.583244503429727
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11538.390492838957,
            "upper_bound": 11577.806025383185
          },
          "point_estimate": 11557.495653562084,
          "standard_error": 10.060070910506152
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11535.690192022066,
            "upper_bound": 11573.669632121891
          },
          "point_estimate": 11558.374363462764,
          "standard_error": 8.196763944096942
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.835521317717913,
            "upper_bound": 57.63958150692802
          },
          "point_estimate": 18.166969413308458,
          "standard_error": 13.72508866613499
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11542.24524814711,
            "upper_bound": 11593.085791425576
          },
          "point_estimate": 11570.397146329164,
          "standard_error": 12.777419405650674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.72350675786694,
            "upper_bound": 45.91659103547406
          },
          "point_estimate": 33.46310914550451,
          "standard_error": 8.184149420207715
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498003.530531583,
            "upper_bound": 498953.185371385
          },
          "point_estimate": 498474.29845509893,
          "standard_error": 242.96357074171576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497723.2020547945,
            "upper_bound": 499082.56506849313
          },
          "point_estimate": 498646.86270928464,
          "standard_error": 342.3082849411786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.96880321251696,
            "upper_bound": 1505.6361246394938
          },
          "point_estimate": 995.3103200009624,
          "standard_error": 380.7352515502397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498116.76438115607,
            "upper_bound": 498823.2456537221
          },
          "point_estimate": 498515.4148727984,
          "standard_error": 182.52556944621043
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479.3217671127825,
            "upper_bound": 1005.1726917069512
          },
          "point_estimate": 808.1113172380534,
          "standard_error": 137.1828518713694
        }
      }
    },
    "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 996.8135859689193,
            "upper_bound": 997.9215720207476
          },
          "point_estimate": 997.3744929039154,
          "standard_error": 0.28517976320072336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 996.3793046630416,
            "upper_bound": 998.0566677651452
          },
          "point_estimate": 997.703468751335,
          "standard_error": 0.4885723280557652
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12829039976556905,
            "upper_bound": 1.554968817709793
          },
          "point_estimate": 1.1124996342283742,
          "standard_error": 0.4117853592581109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 996.535128222412,
            "upper_bound": 997.9892037315016
          },
          "point_estimate": 997.2416995318628,
          "standard_error": 0.3822406369580811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.597509768477325,
            "upper_bound": 1.1426136860912963
          },
          "point_estimate": 0.9529132285325635,
          "standard_error": 0.13947152830582776
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46614.95351469629,
            "upper_bound": 46734.06302285184
          },
          "point_estimate": 46666.339542199,
          "standard_error": 30.942991468944125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46602.065949935815,
            "upper_bound": 46711.38479735925
          },
          "point_estimate": 46627.52638710597,
          "standard_error": 26.49505851784532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.441292215777873,
            "upper_bound": 125.33442863494452
          },
          "point_estimate": 60.581623289030446,
          "standard_error": 30.679676458023476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46609.5725939354,
            "upper_bound": 46685.28857168503
          },
          "point_estimate": 46646.7032125769,
          "standard_error": 18.998554324925067
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.870118945736294,
            "upper_bound": 146.50389150291028
          },
          "point_estimate": 102.78347450198105,
          "standard_error": 33.54763001482989
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48389.04174348205,
            "upper_bound": 48509.09566644415
          },
          "point_estimate": 48444.42342197004,
          "standard_error": 30.80407657779741
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48363.6064753004,
            "upper_bound": 48526.31775700935
          },
          "point_estimate": 48411.846545393855,
          "standard_error": 39.0807420170882
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.44002622682045,
            "upper_bound": 178.9442743932015
          },
          "point_estimate": 87.48944015235462,
          "standard_error": 38.24499467877933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48371.913462145625,
            "upper_bound": 48466.10920555945
          },
          "point_estimate": 48412.07421150278,
          "standard_error": 23.893780931860864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.90607954680818,
            "upper_bound": 135.5277414372063
          },
          "point_estimate": 102.76294168292635,
          "standard_error": 23.8469905573889
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48639.43752124713,
            "upper_bound": 48676.47379074357
          },
          "point_estimate": 48658.22489400305,
          "standard_error": 9.445575303620643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48638.95053475936,
            "upper_bound": 48680.435828877
          },
          "point_estimate": 48655.971122994655,
          "standard_error": 12.10166918012808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.26256026271502997,
            "upper_bound": 52.62775442663827
          },
          "point_estimate": 30.90728727214204,
          "standard_error": 12.948656087661783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48643.22623192047,
            "upper_bound": 48679.896314729966
          },
          "point_estimate": 48661.64051670255,
          "standard_error": 9.304482552391892
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.452135368252732,
            "upper_bound": 41.83951740822652
          },
          "point_estimate": 31.419694526061377,
          "standard_error": 6.872259658645852
        }
      }
    },
    "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13904.597045367536,
            "upper_bound": 13923.727733537517
          },
          "point_estimate": 13913.928726071976,
          "standard_error": 4.881813234698653
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13902.73802801174,
            "upper_bound": 13926.68319295559
          },
          "point_estimate": 13912.672543389484,
          "standard_error": 5.715980310254433
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.804942604730592,
            "upper_bound": 26.897667018641943
          },
          "point_estimate": 17.311568375660016,
          "standard_error": 6.2074627749636315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13896.722601657766,
            "upper_bound": 13914.36384299212
          },
          "point_estimate": 13903.828412322746,
          "standard_error": 4.41056789010948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.364789403557541,
            "upper_bound": 21.3092404305948
          },
          "point_estimate": 16.25968455927508,
          "standard_error": 3.352014712408755
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26484.877550152713,
            "upper_bound": 26538.885580487295
          },
          "point_estimate": 26512.32116722199,
          "standard_error": 13.664669349027564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26496.39882687769,
            "upper_bound": 26529.970918367348
          },
          "point_estimate": 26514.36118197279,
          "standard_error": 7.550147802559364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.185886532828186,
            "upper_bound": 74.50591666195054
          },
          "point_estimate": 16.149299458191717,
          "standard_error": 15.948954632642783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26501.77458534434,
            "upper_bound": 26522.30897272922
          },
          "point_estimate": 26513.542133959338,
          "standard_error": 5.152350610187896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.701939675098044,
            "upper_bound": 64.09408193960233
          },
          "point_estimate": 45.57450706295855,
          "standard_error": 13.559250209280483
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17432.44375765252,
            "upper_bound": 17455.883251681076
          },
          "point_estimate": 17443.19952945123,
          "standard_error": 5.986237100591267
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17429.67006884406,
            "upper_bound": 17451.803890489915
          },
          "point_estimate": 17443.118395773294,
          "standard_error": 5.3466676712201435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8538519380933638,
            "upper_bound": 30.330676718962245
          },
          "point_estimate": 14.288669996088744,
          "standard_error": 7.187779232133538
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17431.157769144214,
            "upper_bound": 17448.905308597503
          },
          "point_estimate": 17441.576790548548,
          "standard_error": 4.540749223830472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.291546207064444,
            "upper_bound": 27.877000843791475
          },
          "point_estimate": 19.98255829612839,
          "standard_error": 5.587631781917965
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17691.538772319014,
            "upper_bound": 17719.93901353078
          },
          "point_estimate": 17703.836294560588,
          "standard_error": 7.376864134718652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17690.02374796748,
            "upper_bound": 17712.51463414634
          },
          "point_estimate": 17696.89359272164,
          "standard_error": 5.8016721798199775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2748998498560815,
            "upper_bound": 29.67244850491568
          },
          "point_estimate": 11.197065093894707,
          "standard_error": 7.17640995943606
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17683.20290651582,
            "upper_bound": 17707.230107079118
          },
          "point_estimate": 17694.27636110231,
          "standard_error": 6.273625614664956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.855992346485938,
            "upper_bound": 35.37148483635951
          },
          "point_estimate": 24.529088749385465,
          "standard_error": 8.287574561334996
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17536.784202019157,
            "upper_bound": 17613.988041485773
          },
          "point_estimate": 17571.322335641926,
          "standard_error": 19.922255629607292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17529.626145682585,
            "upper_bound": 17609.60294489238
          },
          "point_estimate": 17542.89479015919,
          "standard_error": 20.2567526944732
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6342809888327443,
            "upper_bound": 96.44498779571929
          },
          "point_estimate": 20.6477410227219,
          "standard_error": 23.21526062449747
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17530.071694537288,
            "upper_bound": 17667.502491906598
          },
          "point_estimate": 17608.82822059754,
          "standard_error": 36.33943741626547
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.046459884291462,
            "upper_bound": 87.42094356921882
          },
          "point_estimate": 66.20887806625034,
          "standard_error": 18.315497683642445
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37098.47080957255,
            "upper_bound": 37123.755386338154
          },
          "point_estimate": 37110.85776112492,
          "standard_error": 6.468522768482958
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37096.96178778574,
            "upper_bound": 37125.75818833163
          },
          "point_estimate": 37107.321593069166,
          "standard_error": 7.3085314049864545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.549702334476832,
            "upper_bound": 36.450866067298655
          },
          "point_estimate": 19.692028775263218,
          "standard_error": 8.134447048598286
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37102.44001411365,
            "upper_bound": 37125.9454279709
          },
          "point_estimate": 37113.91458081325,
          "standard_error": 6.272712152160194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.876843152512498,
            "upper_bound": 28.193493782055256
          },
          "point_estimate": 21.47084051597586,
          "standard_error": 4.450261226749662
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15964.825062304208,
            "upper_bound": 15992.376993542648
          },
          "point_estimate": 15977.255073085642,
          "standard_error": 7.109645045018968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15958.235891330893,
            "upper_bound": 15988.052320251178
          },
          "point_estimate": 15972.908186813189,
          "standard_error": 8.595491945837331
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.03800184039985,
            "upper_bound": 35.80469161049318
          },
          "point_estimate": 20.32742317757687,
          "standard_error": 7.842369339321687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15960.819861605176,
            "upper_bound": 15984.193380841529
          },
          "point_estimate": 15972.71913115456,
          "standard_error": 5.978632503419283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.299089144660364,
            "upper_bound": 33.09295746719895
          },
          "point_estimate": 23.742202606205304,
          "standard_error": 6.748985969851086
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18523.363623015877,
            "upper_bound": 18550.431356816895
          },
          "point_estimate": 18536.012911848073,
          "standard_error": 6.9460040935390985
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18518.77057823129,
            "upper_bound": 18550.782933673472
          },
          "point_estimate": 18529.585367063493,
          "standard_error": 8.373443470605554
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.706782148581287,
            "upper_bound": 37.530108351567634
          },
          "point_estimate": 19.44282361910704,
          "standard_error": 8.09483970491969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18522.29311238583,
            "upper_bound": 18541.091079318325
          },
          "point_estimate": 18531.96068115558,
          "standard_error": 4.776908412283771
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.408914646316155,
            "upper_bound": 30.413505208299743
          },
          "point_estimate": 23.159506246124625,
          "standard_error": 5.343830335206508
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17828.675451875675,
            "upper_bound": 17847.2466092847
          },
          "point_estimate": 17838.192557810333,
          "standard_error": 4.750872208334045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17826.523879620545,
            "upper_bound": 17850.138432286556
          },
          "point_estimate": 17839.024954437125,
          "standard_error": 6.134822316598291
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7056722041758388,
            "upper_bound": 26.586776365982335
          },
          "point_estimate": 14.140579146647898,
          "standard_error": 5.943029356301376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17833.69395581623,
            "upper_bound": 17851.16395683108
          },
          "point_estimate": 17842.612806035966,
          "standard_error": 4.565126266149705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.609919899121257,
            "upper_bound": 20.238428583800538
          },
          "point_estimate": 15.834827838739368,
          "standard_error": 3.014435391395556
        }
      }
    },
    "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17294.56946061281,
            "upper_bound": 17316.633570723337
          },
          "point_estimate": 17305.61696617896,
          "standard_error": 5.647169458641652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17289.100226622137,
            "upper_bound": 17319.78681615776
          },
          "point_estimate": 17308.630751696353,
          "standard_error": 7.762332917056249
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6964865615404205,
            "upper_bound": 32.78800021941916
          },
          "point_estimate": 22.344463125095288,
          "standard_error": 7.568812097500908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17289.236447697564,
            "upper_bound": 17311.42285914145
          },
          "point_estimate": 17299.135269406168,
          "standard_error": 5.765951669575675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.029376787382676,
            "upper_bound": 23.24059184182167
          },
          "point_estimate": 18.80235627413311,
          "standard_error": 3.1222053741487263
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17385.033814712187,
            "upper_bound": 17409.547745241136
          },
          "point_estimate": 17396.857775972294,
          "standard_error": 6.271315914350878
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17381.377843630267,
            "upper_bound": 17416.474058109834
          },
          "point_estimate": 17391.8022296722,
          "standard_error": 9.043051086560537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5888631544695215,
            "upper_bound": 34.734204175809566
          },
          "point_estimate": 15.791710988400474,
          "standard_error": 8.513256619914927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17383.81514910918,
            "upper_bound": 17402.627704225808
          },
          "point_estimate": 17393.04308976464,
          "standard_error": 4.718265651479048
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.657017449694075,
            "upper_bound": 25.538476479610868
          },
          "point_estimate": 20.93537651244856,
          "standard_error": 3.613080402738545
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17423.08913201109,
            "upper_bound": 17452.396546810738
          },
          "point_estimate": 17437.57564497456,
          "standard_error": 7.489626121906121
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17417.794649712094,
            "upper_bound": 17453.700995681385
          },
          "point_estimate": 17438.989991774062,
          "standard_error": 10.288349966447791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.009574790621326,
            "upper_bound": 43.31393769071784
          },
          "point_estimate": 23.249838970628367,
          "standard_error": 10.209956685010903
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17414.309017046748,
            "upper_bound": 17444.76791166881
          },
          "point_estimate": 17429.168482189594,
          "standard_error": 7.791948122493413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.040366252805905,
            "upper_bound": 32.202677422828955
          },
          "point_estimate": 25.01501433550759,
          "standard_error": 4.683518661520272
        }
      }
    },
    "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17427.08877138607,
            "upper_bound": 17520.7833701569
          },
          "point_estimate": 17470.809432741717,
          "standard_error": 24.081424317158195
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17412.455865086988,
            "upper_bound": 17547.334614153056
          },
          "point_estimate": 17426.6333053154,
          "standard_error": 39.36539900290396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.901288219666706,
            "upper_bound": 129.85773355751195
          },
          "point_estimate": 36.0488213796964,
          "standard_error": 36.8048699687997
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17414.301924750587,
            "upper_bound": 17444.666299372777
          },
          "point_estimate": 17423.88133038911,
          "standard_error": 7.929505428773099
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.18901672947839,
            "upper_bound": 96.10741246236746
          },
          "point_estimate": 80.08489810107383,
          "standard_error": 14.603846304713551
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18047.77220604942,
            "upper_bound": 18061.79849773975
          },
          "point_estimate": 18054.70714688062,
          "standard_error": 3.5865930738213776
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18041.95129224652,
            "upper_bound": 18065.29796222664
          },
          "point_estimate": 18055.254804506298,
          "standard_error": 5.187811274397288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.032364128930253,
            "upper_bound": 22.049007709939612
          },
          "point_estimate": 14.239935532477723,
          "standard_error": 5.34949710592029
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18047.388628130964,
            "upper_bound": 18059.891174746394
          },
          "point_estimate": 18053.593573623195,
          "standard_error": 3.152835041490047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.161575712510214,
            "upper_bound": 14.596299031232022
          },
          "point_estimate": 11.957268490701288,
          "standard_error": 1.926465463194372
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17836.056842127844,
            "upper_bound": 17865.355326985104
          },
          "point_estimate": 17849.55133844234,
          "standard_error": 7.520939097808218
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17829.032309582308,
            "upper_bound": 17863.62520884521
          },
          "point_estimate": 17845.905596505596,
          "standard_error": 8.931591543719431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.599698290695193,
            "upper_bound": 38.88226759471339
          },
          "point_estimate": 22.17886384457804,
          "standard_error": 8.258863935178091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17839.384827251495,
            "upper_bound": 17861.783979926655
          },
          "point_estimate": 17851.111879766424,
          "standard_error": 5.935106481347101
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.698042327044968,
            "upper_bound": 33.97880069911421
          },
          "point_estimate": 25.119051773120248,
          "standard_error": 6.329170276073561
        }
      }
    },
    "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17969.033161146814,
            "upper_bound": 17988.285875411442
          },
          "point_estimate": 17977.960935570307,
          "standard_error": 4.963751850910726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17964.279238754327,
            "upper_bound": 17990.534808040862
          },
          "point_estimate": 17975.165293568407,
          "standard_error": 6.177022157583686
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3064541982598976,
            "upper_bound": 26.382992690283928
          },
          "point_estimate": 14.951862352773356,
          "standard_error": 6.46844549725842
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17965.927096110776,
            "upper_bound": 17979.403148464196
          },
          "point_estimate": 17971.355886525733,
          "standard_error": 3.429441024687359
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.543471243395104,
            "upper_bound": 20.656722862626115
          },
          "point_estimate": 16.480228159600546,
          "standard_error": 3.6242117251517665
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68242.57145072502,
            "upper_bound": 68399.07693139097
          },
          "point_estimate": 68315.7780789026,
          "standard_error": 40.19144790287733
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68211.72719298245,
            "upper_bound": 68446.69642857143
          },
          "point_estimate": 68270.61414138024,
          "standard_error": 57.72052533514064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.587946003543973,
            "upper_bound": 208.24575741193007
          },
          "point_estimate": 89.68723757596864,
          "standard_error": 50.6034429885145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68218.31108186286,
            "upper_bound": 68290.69614434894
          },
          "point_estimate": 68243.95323698857,
          "standard_error": 18.4823966941023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.39042355264525,
            "upper_bound": 166.202242514779
          },
          "point_estimate": 133.69556582928917,
          "standard_error": 26.45429236196993
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225993.8293005952,
            "upper_bound": 1229034.8003772488
          },
          "point_estimate": 1227599.4839867726,
          "standard_error": 783.4136258360995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1224624.2166666666,
            "upper_bound": 1229747.82
          },
          "point_estimate": 1228725.4933333334,
          "standard_error": 1340.3262710206825
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.6811728835379,
            "upper_bound": 4037.2910509904286
          },
          "point_estimate": 1723.8704685618172,
          "standard_error": 1109.2377912436498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228035.821803795,
            "upper_bound": 1229448.4275582572
          },
          "point_estimate": 1228816.5288311688,
          "standard_error": 362.46264737654815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1039.422118025783,
            "upper_bound": 3031.930922290701
          },
          "point_estimate": 2617.681563432248,
          "standard_error": 440.5435287218939
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207686.4575960827,
            "upper_bound": 208011.938631746
          },
          "point_estimate": 207864.1290106576,
          "standard_error": 84.13051646759625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207719.5330612245,
            "upper_bound": 208073.8878571429
          },
          "point_estimate": 207974.2234920635,
          "standard_error": 95.05501496699604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.64497641608491,
            "upper_bound": 425.3367524487657
          },
          "point_estimate": 180.73613799129268,
          "standard_error": 98.4426054385941
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207811.33720724343,
            "upper_bound": 208067.0821863354
          },
          "point_estimate": 207972.2235844156,
          "standard_error": 66.68525669026303
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.18669872940762,
            "upper_bound": 380.75140081263686
          },
          "point_estimate": 279.9205710413896,
          "standard_error": 75.38468726876158
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6131.001680743243,
            "upper_bound": 6146.849899126247
          },
          "point_estimate": 6138.751553450771,
          "standard_error": 4.063982634397871
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6127.7411599099105,
            "upper_bound": 6151.298262548263
          },
          "point_estimate": 6137.7044200450455,
          "standard_error": 6.1821577094472815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.105011286919216,
            "upper_bound": 21.924755075285848
          },
          "point_estimate": 17.052806580698572,
          "standard_error": 4.761489801022057
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6133.363940688704,
            "upper_bound": 6147.8300314877015
          },
          "point_estimate": 6139.894065022815,
          "standard_error": 3.663979356088682
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.029903392879817,
            "upper_bound": 16.625266435440775
          },
          "point_estimate": 13.545194330386773,
          "standard_error": 2.1903506954732364
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6426.605010329679,
            "upper_bound": 6444.534997226225
          },
          "point_estimate": 6434.495219711079,
          "standard_error": 4.591892492106143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6426.251902318174,
            "upper_bound": 6437.821553707308
          },
          "point_estimate": 6432.727629328143,
          "standard_error": 3.1765100199023273
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.278997185751809,
            "upper_bound": 18.497290873164733
          },
          "point_estimate": 7.916694399649263,
          "standard_error": 4.105853668790807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6429.007576127838,
            "upper_bound": 6437.019250324206
          },
          "point_estimate": 6434.081233754743,
          "standard_error": 2.08234356660614
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.168959903328367,
            "upper_bound": 22.153094030070235
          },
          "point_estimate": 15.245755424409042,
          "standard_error": 5.202436403877354
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14183.447369840653,
            "upper_bound": 14197.184258772988
          },
          "point_estimate": 14189.81733068576,
          "standard_error": 3.5092092920239466
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14184.2562084474,
            "upper_bound": 14193.343110415852
          },
          "point_estimate": 14188.487474160567,
          "standard_error": 2.6431861992568484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.39392045155328453,
            "upper_bound": 16.98930849775413
          },
          "point_estimate": 5.501031037456379,
          "standard_error": 3.634570574530669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14186.486794827922,
            "upper_bound": 14191.290261238397
          },
          "point_estimate": 14189.233852576832,
          "standard_error": 1.2143090466947106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.553578979715067,
            "upper_bound": 16.889470578016837
          },
          "point_estimate": 11.721019792898268,
          "standard_error": 3.556188365424291
        }
      }
    },
    "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.93187800721362,
            "upper_bound": 34.58087913238328
          },
          "point_estimate": 34.22667011768034,
          "standard_error": 0.16724819345918257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.8004598147727,
            "upper_bound": 34.52061993025548
          },
          "point_estimate": 34.04331605965106,
          "standard_error": 0.1980822975676996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08611172475475991,
            "upper_bound": 0.8578120021121561
          },
          "point_estimate": 0.42467691541931174,
          "standard_error": 0.19513842917107593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.82409704884957,
            "upper_bound": 34.21792551896745
          },
          "point_estimate": 33.95879690944337,
          "standard_error": 0.1012559596141536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22499877567440532,
            "upper_bound": 0.760628488042648
          },
          "point_estimate": 0.5589125833698473,
          "standard_error": 0.14859167209660773
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.042676871726652,
            "upper_bound": 9.054631143704553
          },
          "point_estimate": 9.04837327895815,
          "standard_error": 0.0030636951225047663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.041532397253132,
            "upper_bound": 9.057547902579898
          },
          "point_estimate": 9.04378437757541,
          "standard_error": 0.004522439912068783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010886889092542268,
            "upper_bound": 0.016460044633204243
          },
          "point_estimate": 0.007194512002747524,
          "standard_error": 0.0044596119549726735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.042573831008124,
            "upper_bound": 9.049673473040755
          },
          "point_estimate": 9.045399400546096,
          "standard_error": 0.0018104939024430945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005053703465152746,
            "upper_bound": 0.01248237291653718
          },
          "point_estimate": 0.010215755763192,
          "standard_error": 0.001843041823318871
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0441527706679,
            "upper_bound": 9.058128214013196
          },
          "point_estimate": 9.05040282731473,
          "standard_error": 0.0035964804406225415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.042492636574236,
            "upper_bound": 9.059769387798802
          },
          "point_estimate": 9.04531650267749,
          "standard_error": 0.0037159690667953674
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007726075154252082,
            "upper_bound": 0.015503899714591794
          },
          "point_estimate": 0.005642431806431801,
          "standard_error": 0.0037737278297519904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.041851143338333,
            "upper_bound": 9.049171751968636
          },
          "point_estimate": 9.044839885393108,
          "standard_error": 0.00188839202590701
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030685565737251738,
            "upper_bound": 0.015317040943463091
          },
          "point_estimate": 0.011995657263602869,
          "standard_error": 0.0032437005966542225
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049720612714882,
            "upper_bound": 9.057294704181103
          },
          "point_estimate": 9.053508387228566,
          "standard_error": 0.001922868272993988
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04996185056192,
            "upper_bound": 9.0569150033013
          },
          "point_estimate": 9.053482118358229,
          "standard_error": 0.0015844480175872578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011431306206694711,
            "upper_bound": 0.0105280078663615
          },
          "point_estimate": 0.0032529756582605684,
          "standard_error": 0.00243887200599543
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.052479602933335,
            "upper_bound": 9.057894763631465
          },
          "point_estimate": 9.054482618881565,
          "standard_error": 0.0014094742753617182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002451876495320487,
            "upper_bound": 0.008822391887056207
          },
          "point_estimate": 0.006411284140701756,
          "standard_error": 0.0015985909053920632
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.805349423207918,
            "upper_bound": 8.81576851396352
          },
          "point_estimate": 8.810584524599813,
          "standard_error": 0.00266095885115649
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.80325351104474,
            "upper_bound": 8.819116311471369
          },
          "point_estimate": 8.809064147690382,
          "standard_error": 0.003742124010141439
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009994288416243949,
            "upper_bound": 0.016148964068353026
          },
          "point_estimate": 0.011664920533740375,
          "standard_error": 0.004114531061633129
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.802818705202954,
            "upper_bound": 8.817078286400829
          },
          "point_estimate": 8.809293482780252,
          "standard_error": 0.003689715046807224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00529962196946346,
            "upper_bound": 0.010865313489385449
          },
          "point_estimate": 0.008871686908452818,
          "standard_error": 0.00143619274030062
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.538678840655372,
            "upper_bound": 9.550568866315002
          },
          "point_estimate": 9.544555306314871,
          "standard_error": 0.0030362968926408345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.53738657951731,
            "upper_bound": 9.551763570896156
          },
          "point_estimate": 9.544682090485908,
          "standard_error": 0.003562555133189807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002372580233550295,
            "upper_bound": 0.016922296691572605
          },
          "point_estimate": 0.007952574106801743,
          "standard_error": 0.0037345526314644306
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.539633803918786,
            "upper_bound": 9.550436161838466
          },
          "point_estimate": 9.545956578591854,
          "standard_error": 0.0027424019194281068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005147889716915847,
            "upper_bound": 0.013295307506015036
          },
          "point_estimate": 0.010119817982570764,
          "standard_error": 0.002080743032034014
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.33299406927747,
            "upper_bound": 10.359329008088263
          },
          "point_estimate": 10.34513748154516,
          "standard_error": 0.006771259795398897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.32892299145491,
            "upper_bound": 10.357360877638914
          },
          "point_estimate": 10.34098849247622,
          "standard_error": 0.006978941558182265
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005445962094596799,
            "upper_bound": 0.03504108670664701
          },
          "point_estimate": 0.017293434922782357,
          "standard_error": 0.00775408556856349
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.327088252272482,
            "upper_bound": 10.3427005104682
          },
          "point_estimate": 10.334977293834363,
          "standard_error": 0.0039802180926076175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009564798386173884,
            "upper_bound": 0.03024880378566247
          },
          "point_estimate": 0.0225185141721433,
          "standard_error": 0.005599923152703477
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.07133026069409,
            "upper_bound": 8.083255026387148
          },
          "point_estimate": 8.076911688983596,
          "standard_error": 0.0030568745079472795
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.068185118757226,
            "upper_bound": 8.081498050227975
          },
          "point_estimate": 8.077734676502216,
          "standard_error": 0.003635688334288937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009775692439079676,
            "upper_bound": 0.017235378152523635
          },
          "point_estimate": 0.009338014013447868,
          "standard_error": 0.0038958044868725662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.068345487905242,
            "upper_bound": 8.080530900301346
          },
          "point_estimate": 8.075109451853328,
          "standard_error": 0.0031588836566976983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005062047998446027,
            "upper_bound": 0.013723821207440475
          },
          "point_estimate": 0.01016624170144582,
          "standard_error": 0.002455311508013861
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.593383539367853,
            "upper_bound": 9.612200549736135
          },
          "point_estimate": 9.603465552573413,
          "standard_error": 0.004819716748261227
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.593651945672304,
            "upper_bound": 9.615168296601665
          },
          "point_estimate": 9.607982821701524,
          "standard_error": 0.006149563975154742
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003273854211102507,
            "upper_bound": 0.02679597135531479
          },
          "point_estimate": 0.01625365438036267,
          "standard_error": 0.005625396769443684
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.598596851863146,
            "upper_bound": 9.61612639173933
          },
          "point_estimate": 9.608900765775497,
          "standard_error": 0.004497886609345101
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007791801749616514,
            "upper_bound": 0.021419978478523352
          },
          "point_estimate": 0.016059186628873512,
          "standard_error": 0.0037650045566879385
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.471865041752263,
            "upper_bound": 12.478230651305754
          },
          "point_estimate": 12.474741393902814,
          "standard_error": 0.0016356232557112058
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.471096377069738,
            "upper_bound": 12.477505770866236
          },
          "point_estimate": 12.473334296574365,
          "standard_error": 0.00187015243279852
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000870090115083929,
            "upper_bound": 0.007893967479396469
          },
          "point_estimate": 0.004467429357562217,
          "standard_error": 0.0017816740552927212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.471302186848153,
            "upper_bound": 12.48278270277129
          },
          "point_estimate": 12.477432143410493,
          "standard_error": 0.00303557937264866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0023544116760034245,
            "upper_bound": 0.007530759191678144
          },
          "point_estimate": 0.005448859378642604,
          "standard_error": 0.0014984848182029195
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047841468458584,
            "upper_bound": 9.06077932375648
          },
          "point_estimate": 9.054361981056564,
          "standard_error": 0.0033068449896613865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.048463989070182,
            "upper_bound": 9.062660442797878
          },
          "point_estimate": 9.052467235041068,
          "standard_error": 0.00388976170931856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013752244038146857,
            "upper_bound": 0.01942650665174132
          },
          "point_estimate": 0.010119884780223591,
          "standard_error": 0.00441678897598096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049751214404738,
            "upper_bound": 9.0599669787442
          },
          "point_estimate": 9.053461732559429,
          "standard_error": 0.0026231326375096716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0054656358046133715,
            "upper_bound": 0.014722030051818798
          },
          "point_estimate": 0.011038619882702216,
          "standard_error": 0.002382213553326426
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.175464574540102,
            "upper_bound": 10.195231734342643
          },
          "point_estimate": 10.18629262868578,
          "standard_error": 0.005079306101365651
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.18111896389298,
            "upper_bound": 10.198664730892178
          },
          "point_estimate": 10.186793112838744,
          "standard_error": 0.0038926510823416383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010692623214858287,
            "upper_bound": 0.023402343097417094
          },
          "point_estimate": 0.009056023095877335,
          "standard_error": 0.006458296864816791
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.184422537572416,
            "upper_bound": 10.196893333327694
          },
          "point_estimate": 10.189342884057254,
          "standard_error": 0.003207026749410062
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006303512419625324,
            "upper_bound": 0.024052467193546416
          },
          "point_estimate": 0.016933562852367533,
          "standard_error": 0.005120181099846809
        }
      }
    },
    "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.5605869269213,
            "upper_bound": 19.590444549149947
          },
          "point_estimate": 19.57565970567974,
          "standard_error": 0.007625695962106945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.546864837308107,
            "upper_bound": 19.601461408889257
          },
          "point_estimate": 19.573297149372493,
          "standard_error": 0.01298380128551251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0035792160825266557,
            "upper_bound": 0.04320324118990932
          },
          "point_estimate": 0.03629889385925361,
          "standard_error": 0.010648068561461575
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.550357328141505,
            "upper_bound": 19.599389782971095
          },
          "point_estimate": 19.572417092331435,
          "standard_error": 0.012611478296504212
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016126490588141446,
            "upper_bound": 0.030061218424168288
          },
          "point_estimate": 0.025517349691065273,
          "standard_error": 0.003551201414182591
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98181.48859060487,
            "upper_bound": 98303.4216458896
          },
          "point_estimate": 98235.6659469112,
          "standard_error": 31.378163614551504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98164.1054054054,
            "upper_bound": 98253.13153153152
          },
          "point_estimate": 98235.69780405406,
          "standard_error": 24.174498000758028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8083545692598335,
            "upper_bound": 136.01608525413403
          },
          "point_estimate": 26.537203853196957,
          "standard_error": 35.43592938971977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98187.28554609403,
            "upper_bound": 98395.62533468047
          },
          "point_estimate": 98285.25477009478,
          "standard_error": 59.00428147081494
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.13555125587768,
            "upper_bound": 151.72745409092215
          },
          "point_estimate": 104.81764600488056,
          "standard_error": 34.849649071611644
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53315.004348996576,
            "upper_bound": 53418.34392216163
          },
          "point_estimate": 53365.43759777872,
          "standard_error": 26.403150971680763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53298.3362701909,
            "upper_bound": 53409.75854835326
          },
          "point_estimate": 53369.7012563224,
          "standard_error": 27.993521870524443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.564562452466667,
            "upper_bound": 155.10200030805836
          },
          "point_estimate": 73.74090906284131,
          "standard_error": 35.28819047670968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53313.27286001608,
            "upper_bound": 53407.36617967418
          },
          "point_estimate": 53374.196323206896,
          "standard_error": 23.993705107267875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.33828677680499,
            "upper_bound": 117.4518100623158
          },
          "point_estimate": 87.82769360679417,
          "standard_error": 19.39472007996003
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143914.52463895164,
            "upper_bound": 144123.23478293806
          },
          "point_estimate": 144014.0611666353,
          "standard_error": 53.533981178852045
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143866.0938735178,
            "upper_bound": 144141.79644268774
          },
          "point_estimate": 143985.71271723445,
          "standard_error": 63.269220337494815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.22070083134966,
            "upper_bound": 290.15497231906636
          },
          "point_estimate": 204.37831089724563,
          "standard_error": 76.20083859295279
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 143946.4713778968,
            "upper_bound": 144070.9512838794
          },
          "point_estimate": 144004.43353010627,
          "standard_error": 31.635830822525943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.76204458834783,
            "upper_bound": 230.89880191296913
          },
          "point_estimate": 178.28677963220255,
          "standard_error": 36.74521068877784
        }
      }
    },
    "memmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 364502.930015,
            "upper_bound": 365117.93351011904
          },
          "point_estimate": 364807.2626809523,
          "standard_error": 157.37788418691378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 364241.7833333333,
            "upper_bound": 365138.28
          },
          "point_estimate": 364860.196,
          "standard_error": 188.00573748011783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.46421552001325,
            "upper_bound": 983.569071638148
          },
          "point_estimate": 460.4483557254039,
          "standard_error": 262.4939400375964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 364601.686183714,
            "upper_bound": 364995.1291578194
          },
          "point_estimate": 364823.41197402595,
          "standard_error": 99.97851132350532
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.17877201844556,
            "upper_bound": 679.7181003816393
          },
          "point_estimate": 525.6153569701714,
          "standard_error": 101.67388130920972
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486793.4575370239,
            "upper_bound": 487248.43663257937
          },
          "point_estimate": 487020.2325253968,
          "standard_error": 116.60427417619135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486654.1911111111,
            "upper_bound": 487388.7411666667
          },
          "point_estimate": 486972.084,
          "standard_error": 252.86647569401336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.536753733969334,
            "upper_bound": 590.8583436101032
          },
          "point_estimate": 559.5270525664235,
          "standard_error": 161.36426662872822
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486863.7051672297,
            "upper_bound": 487325.6345961821
          },
          "point_estimate": 487104.1921038961,
          "standard_error": 117.23807787769572
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266.3928735282402,
            "upper_bound": 438.6727228363098
          },
          "point_estimate": 389.3603024685849,
          "standard_error": 43.74261030102582
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39618.198610297746,
            "upper_bound": 39704.68686456709
          },
          "point_estimate": 39659.29653016047,
          "standard_error": 22.191609200664654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39604.087677401745,
            "upper_bound": 39713.627577632215
          },
          "point_estimate": 39644.11939592431,
          "standard_error": 28.889446217784325
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.880707373657698,
            "upper_bound": 123.40154897730528
          },
          "point_estimate": 67.23053159390352,
          "standard_error": 26.922784676469476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39618.15658347552,
            "upper_bound": 39690.56480058046
          },
          "point_estimate": 39660.93638064992,
          "standard_error": 17.962512387062848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.10771676023915,
            "upper_bound": 96.57771772145338
          },
          "point_estimate": 74.05787967778089,
          "standard_error": 15.828599286094256
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96097.3263510852,
            "upper_bound": 96208.72224558968
          },
          "point_estimate": 96154.7395694918,
          "standard_error": 28.480648115936223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96038.61626878868,
            "upper_bound": 96217.3781314471
          },
          "point_estimate": 96178.2523051661,
          "standard_error": 40.954034948361965
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.255306484528754,
            "upper_bound": 175.22554516498087
          },
          "point_estimate": 61.32992552479941,
          "standard_error": 44.12129151661091
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96081.2039571532,
            "upper_bound": 96201.96293739328
          },
          "point_estimate": 96148.53708360605,
          "standard_error": 34.2593675311402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.77324721895448,
            "upper_bound": 116.1416906375203
          },
          "point_estimate": 94.63933120115136,
          "standard_error": 16.447857785227352
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66674.37756499475,
            "upper_bound": 66769.41564921803
          },
          "point_estimate": 66721.39737139648,
          "standard_error": 24.370049661480433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66646.74827665441,
            "upper_bound": 66793.30351307189
          },
          "point_estimate": 66733.79661239496,
          "standard_error": 36.748972838896094
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.819763540154476,
            "upper_bound": 136.19990507277905
          },
          "point_estimate": 91.05371386141702,
          "standard_error": 31.63424526532269
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66675.86909172668,
            "upper_bound": 66786.77757649207
          },
          "point_estimate": 66741.6375,
          "standard_error": 28.848511302995824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.07428126911768,
            "upper_bound": 100.54314769339372
          },
          "point_estimate": 81.11185077631876,
          "standard_error": 13.242323838760596
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265007.48821814614,
            "upper_bound": 265652.18439009663
          },
          "point_estimate": 265292.80890297913,
          "standard_error": 166.6251956653281
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264866.77554347826,
            "upper_bound": 265473.03743961354
          },
          "point_estimate": 265293.1000905797,
          "standard_error": 167.37477302022734
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.209070430101832,
            "upper_bound": 797.2014367526369
          },
          "point_estimate": 435.5079228841231,
          "standard_error": 181.10167068915115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264862.90524163004,
            "upper_bound": 265337.4317837498
          },
          "point_estimate": 265083.1499529456,
          "standard_error": 123.95108724334398
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.46277784479497,
            "upper_bound": 786.8404056538437
          },
          "point_estimate": 552.3734834778519,
          "standard_error": 169.44426177072927
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35081.02188798493,
            "upper_bound": 35147.716182340504
          },
          "point_estimate": 35115.741431099465,
          "standard_error": 17.09127003226516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35081.3958976834,
            "upper_bound": 35163.6546010296
          },
          "point_estimate": 35118.057794401546,
          "standard_error": 20.97878511898929
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.12899903797727,
            "upper_bound": 97.42058050016011
          },
          "point_estimate": 54.57675659863512,
          "standard_error": 20.48488705421981
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35071.78781774055,
            "upper_bound": 35146.14315301684
          },
          "point_estimate": 35109.76990924134,
          "standard_error": 18.92471224230483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.78645685037397,
            "upper_bound": 74.70052511302173
          },
          "point_estimate": 56.99700219782113,
          "standard_error": 11.948889269203022
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64351.60883333334,
            "upper_bound": 64459.37980059524
          },
          "point_estimate": 64403.13753750527,
          "standard_error": 27.675605553075727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64336.08734513275,
            "upper_bound": 64499.51268436578
          },
          "point_estimate": 64375.45088495575,
          "standard_error": 37.42868920899055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.609284855623171,
            "upper_bound": 147.67832835164631
          },
          "point_estimate": 69.43150160362737,
          "standard_error": 37.78373527001542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64329.06986348955,
            "upper_bound": 64464.150297666936
          },
          "point_estimate": 64383.65789679347,
          "standard_error": 34.114521827168964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.99988447440451,
            "upper_bound": 112.79123873993736
          },
          "point_estimate": 92.24362085252602,
          "standard_error": 16.626487101984203
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153043.24530245428,
            "upper_bound": 153248.98510037348
          },
          "point_estimate": 153146.18406929442,
          "standard_error": 52.40835755725592
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153022.6799519808,
            "upper_bound": 153279.83893557423
          },
          "point_estimate": 153148.34844771243,
          "standard_error": 52.74315916078032
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.086930088839797,
            "upper_bound": 321.61977748338586
          },
          "point_estimate": 114.36161561672438,
          "standard_error": 81.01214535540075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153049.33621751246,
            "upper_bound": 153324.96673768017
          },
          "point_estimate": 153199.13229291717,
          "standard_error": 71.84469689814823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 89.76910521940825,
            "upper_bound": 227.28310352997536
          },
          "point_estimate": 175.24869195527828,
          "standard_error": 35.38291449180114
        }
      }
    },
    "memmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34292.30581069838,
            "upper_bound": 34358.61039944313
          },
          "point_estimate": 34325.277346174,
          "standard_error": 16.966883776192336
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34284.43396226415,
            "upper_bound": 34377.05283018868
          },
          "point_estimate": 34310.92059748428,
          "standard_error": 25.585234004592586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.645808501905792,
            "upper_bound": 100.42268148757978
          },
          "point_estimate": 53.74005300818969,
          "standard_error": 23.161406736262673
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34304.82119401409,
            "upper_bound": 34362.40560651846
          },
          "point_estimate": 34340.35773584906,
          "standard_error": 14.449416618186762
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.341912392431894,
            "upper_bound": 71.74056715928457
          },
          "point_estimate": 56.50533699609347,
          "standard_error": 10.13195562211295
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11262.324934879698,
            "upper_bound": 11300.829256754512
          },
          "point_estimate": 11282.689154422613,
          "standard_error": 9.853539181675384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11259.729421136484,
            "upper_bound": 11314.865164188352
          },
          "point_estimate": 11284.455106361009,
          "standard_error": 13.529784872298029
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4922068663215668,
            "upper_bound": 55.264991718661854
          },
          "point_estimate": 40.87212559872402,
          "standard_error": 13.968058032349024
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11268.272588609212,
            "upper_bound": 11299.096122978344
          },
          "point_estimate": 11281.4934405124,
          "standard_error": 7.733546188509229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.45564340361048,
            "upper_bound": 43.14576990694465
          },
          "point_estimate": 32.89932502046559,
          "standard_error": 7.190603564752367
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495800.0554871835,
            "upper_bound": 496479.513900311
          },
          "point_estimate": 496119.9202885028,
          "standard_error": 174.20457962308402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495598.2451737452,
            "upper_bound": 496442.19313063065
          },
          "point_estimate": 496090.3804804805,
          "standard_error": 232.1734081499807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.08915305983182,
            "upper_bound": 1012.9062914768224
          },
          "point_estimate": 521.0635451637099,
          "standard_error": 214.74899721006577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495800.00864378933,
            "upper_bound": 496294.38139652775
          },
          "point_estimate": 496073.684977185,
          "standard_error": 124.49097741751652
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298.18854293205214,
            "upper_bound": 775.6853542593666
          },
          "point_estimate": 579.4495017240233,
          "standard_error": 132.88122661156405
        }
      }
    },
    "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964.4537955211808,
            "upper_bound": 965.0701903282496
          },
          "point_estimate": 964.7465084030516,
          "standard_error": 0.15809795370347432
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964.3233221278604,
            "upper_bound": 965.1316082802548
          },
          "point_estimate": 964.662384364574,
          "standard_error": 0.18555365224313425
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09312482716836656,
            "upper_bound": 0.8941369301627025
          },
          "point_estimate": 0.5167222901894128,
          "standard_error": 0.21602369563479343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 964.3709216813952,
            "upper_bound": 965.2587366182806
          },
          "point_estimate": 964.7197435685334,
          "standard_error": 0.23174692931666507
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.25865495811874734,
            "upper_bound": 0.6752219926811934
          },
          "point_estimate": 0.527604835464734,
          "standard_error": 0.10587477652817326
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46684.682123825594,
            "upper_bound": 46786.71747366569
          },
          "point_estimate": 46729.98611473253,
          "standard_error": 26.227641565899702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46671.17572530297,
            "upper_bound": 46760.52148136246
          },
          "point_estimate": 46721.61161096829,
          "standard_error": 19.81504202374618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.051387178635087,
            "upper_bound": 118.6008516861878
          },
          "point_estimate": 49.77996383345235,
          "standard_error": 31.241179999983196
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46692.90445361459,
            "upper_bound": 46752.46524140074
          },
          "point_estimate": 46726.07644309418,
          "standard_error": 14.820265172222811
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.79405782056446,
            "upper_bound": 123.7503453386273
          },
          "point_estimate": 87.22749931928735,
          "standard_error": 26.877245029484055
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48814.0247895092,
            "upper_bound": 48873.688699716775
          },
          "point_estimate": 48844.723679222145,
          "standard_error": 15.368833208262744
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48796.960920698926,
            "upper_bound": 48895.40188172043
          },
          "point_estimate": 48854.81175328554,
          "standard_error": 26.100179349836303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.979328825294887,
            "upper_bound": 86.11986725914366
          },
          "point_estimate": 62.56096952103915,
          "standard_error": 20.74333584310192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48801.67867530407,
            "upper_bound": 48883.04098612445
          },
          "point_estimate": 48839.68421309873,
          "standard_error": 21.28798676046781
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.406751888951533,
            "upper_bound": 61.04678474642351
          },
          "point_estimate": 51.06276339495249,
          "standard_error": 7.778230018441954
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48226.38531356574,
            "upper_bound": 48400.35201149425
          },
          "point_estimate": 48289.65866031956,
          "standard_error": 50.82265560173891
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48215.495855437664,
            "upper_bound": 48262.68777630416
          },
          "point_estimate": 48239.68027661993,
          "standard_error": 16.626960320970227
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.637538529586799,
            "upper_bound": 55.33171904418185
          },
          "point_estimate": 30.471091716990248,
          "standard_error": 19.586404627550095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48215.75875884095,
            "upper_bound": 48250.8220178655
          },
          "point_estimate": 48229.668435013264,
          "standard_error": 9.132225916034011
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.441801774814982,
            "upper_bound": 259.81865327072035
          },
          "point_estimate": 168.96418434864324,
          "standard_error": 86.93219986036448
        }
      }
    },
    "memmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13952.27354326554,
            "upper_bound": 13967.406107486991
          },
          "point_estimate": 13959.101656220688,
          "standard_error": 3.8909964170092217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13951.084804297774,
            "upper_bound": 13966.795898354225
          },
          "point_estimate": 13954.955487336914,
          "standard_error": 4.022551504606289
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.705046861518816,
            "upper_bound": 18.32450645774079
          },
          "point_estimate": 7.406514204655743,
          "standard_error": 4.416304268278693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13952.041945293156,
            "upper_bound": 13966.665079420452
          },
          "point_estimate": 13958.984476383172,
          "standard_error": 3.7962847132918394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.089312805029233,
            "upper_bound": 18.112653735475305
          },
          "point_estimate": 13.01618365403325,
          "standard_error": 3.732067140043854
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25850.234006665534,
            "upper_bound": 25921.17561328588
          },
          "point_estimate": 25885.195217646728,
          "standard_error": 18.132014534994298
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25828.320284697507,
            "upper_bound": 25929.639304072756
          },
          "point_estimate": 25889.57583629893,
          "standard_error": 23.88236998899963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.594228242640904,
            "upper_bound": 104.91512060001968
          },
          "point_estimate": 66.87218230270393,
          "standard_error": 22.68236854635867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25834.082831516906,
            "upper_bound": 25906.074497142818
          },
          "point_estimate": 25875.92892545177,
          "standard_error": 18.204653724566715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.82734657986932,
            "upper_bound": 77.895442444435
          },
          "point_estimate": 60.636942373646015,
          "standard_error": 11.466987931633485
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17462.67205263138,
            "upper_bound": 17486.09274138494
          },
          "point_estimate": 17474.73247404124,
          "standard_error": 6.013681661251867
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17463.75414862915,
            "upper_bound": 17493.00384800385
          },
          "point_estimate": 17473.054704261052,
          "standard_error": 7.000831347485695
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6790690618991924,
            "upper_bound": 34.56106163389426
          },
          "point_estimate": 18.616635015007414,
          "standard_error": 8.294910563274701
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17462.37642713273,
            "upper_bound": 17488.25366138619
          },
          "point_estimate": 17473.438201433008,
          "standard_error": 6.731923884067648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.659390967991742,
            "upper_bound": 26.271957459492643
          },
          "point_estimate": 20.015966489224592,
          "standard_error": 4.173848644339419
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17487.70680145356,
            "upper_bound": 17513.76001365819
          },
          "point_estimate": 17499.69175965411,
          "standard_error": 6.7082518760672105
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17482.098424926062,
            "upper_bound": 17517.406355320174
          },
          "point_estimate": 17491.84274862248,
          "standard_error": 8.003301087526951
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4091758616762675,
            "upper_bound": 36.68180399730076
          },
          "point_estimate": 15.491715037517467,
          "standard_error": 8.53137727847708
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17486.604294827885,
            "upper_bound": 17505.19779781656
          },
          "point_estimate": 17494.282994328732,
          "standard_error": 4.685672874692689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.1245335181426,
            "upper_bound": 29.258848790306097
          },
          "point_estimate": 22.323988641935173,
          "standard_error": 5.267719131556163
        }
      }
    },
    "memmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17534.07139451409,
            "upper_bound": 17565.70826100932
          },
          "point_estimate": 17549.24865932712,
          "standard_error": 8.108416618192004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17532.53363066538,
            "upper_bound": 17569.811395049823
          },
          "point_estimate": 17539.89076931319,
          "standard_error": 10.556224960289708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.005696229188749,
            "upper_bound": 44.97082907142353
          },
          "point_estimate": 16.97691345944917,
          "standard_error": 11.977759361631945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17534.64748421049,
            "upper_bound": 17564.514683456302
          },
          "point_estimate": 17545.57825395434,
          "standard_error": 7.724018927387552
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.102397416424845,
            "upper_bound": 34.07614982900168
          },
          "point_estimate": 26.995396692304737,
          "standard_error": 5.159812086323371
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39793.35959612769,
            "upper_bound": 39897.15527114949
          },
          "point_estimate": 39838.40359558696,
          "standard_error": 27.009678686816628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39772.56747252747,
            "upper_bound": 39869.43537414966
          },
          "point_estimate": 39816.60085470085,
          "standard_error": 28.22336197670344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.814779873076242,
            "upper_bound": 116.24206069269916
          },
          "point_estimate": 66.48593316579083,
          "standard_error": 24.759152457374693
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39790.35878102896,
            "upper_bound": 39950.91100632698
          },
          "point_estimate": 39853.42060796347,
          "standard_error": 42.911330449914935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.530299831355975,
            "upper_bound": 130.02906037430864
          },
          "point_estimate": 90.1029294779272,
          "standard_error": 29.887343975838323
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16546.912803593113,
            "upper_bound": 16571.730319352286
          },
          "point_estimate": 16558.75134833897,
          "standard_error": 6.33963770912505
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16546.006635441063,
            "upper_bound": 16570.070431086824
          },
          "point_estimate": 16555.65478901032,
          "standard_error": 4.681406386482711
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3947277302492624,
            "upper_bound": 36.13246691316398
          },
          "point_estimate": 8.540646774602953,
          "standard_error": 10.25614705285722
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16548.938247556915,
            "upper_bound": 16560.529318911693
          },
          "point_estimate": 16555.85875026613,
          "standard_error": 2.9156732681423008
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.76890824856241,
            "upper_bound": 28.58096002477583
          },
          "point_estimate": 21.15343245328052,
          "standard_error": 5.136715389573601
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18601.606490651346,
            "upper_bound": 18624.822357433415
          },
          "point_estimate": 18612.471304383165,
          "standard_error": 5.959484117315637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18600.213802127782,
            "upper_bound": 18624.807804842367
          },
          "point_estimate": 18606.913543266768,
          "standard_error": 6.264811215652705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0680485654364733,
            "upper_bound": 31.408187998463344
          },
          "point_estimate": 11.947054937238084,
          "standard_error": 8.267606987678711
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18603.39643062251,
            "upper_bound": 18629.259223402623
          },
          "point_estimate": 18613.295049241595,
          "standard_error": 6.627260938714869
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.795255123598129,
            "upper_bound": 25.413806385706092
          },
          "point_estimate": 19.75493888710052,
          "standard_error": 4.341648263939355
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18024.50499270455,
            "upper_bound": 18057.826671092116
          },
          "point_estimate": 18043.058292057987,
          "standard_error": 8.578688296428846
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18028.83763981311,
            "upper_bound": 18063.707916941967
          },
          "point_estimate": 18050.03970928312,
          "standard_error": 7.920057948729223
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.704940649873582,
            "upper_bound": 41.11557587436334
          },
          "point_estimate": 16.631494997100805,
          "standard_error": 9.329827865751785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18036.505174985712,
            "upper_bound": 18066.013320070422
          },
          "point_estimate": 18055.403556304944,
          "standard_error": 7.638860040500819
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.785282143373808,
            "upper_bound": 39.53813555583153
          },
          "point_estimate": 28.65102636856152,
          "standard_error": 8.255971163217126
        }
      }
    },
    "memmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17347.83444997274,
            "upper_bound": 17369.730710840948
          },
          "point_estimate": 17358.296280859988,
          "standard_error": 5.621662044964369
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17342.36564544711,
            "upper_bound": 17370.70676288168
          },
          "point_estimate": 17356.02302003817,
          "standard_error": 7.708866247178791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.424646563997813,
            "upper_bound": 31.2541285123998
          },
          "point_estimate": 19.405896769025176,
          "standard_error": 6.809042753336843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17342.71748121422,
            "upper_bound": 17370.64548237466
          },
          "point_estimate": 17354.45006939625,
          "standard_error": 7.120848936394473
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.791338645091022,
            "upper_bound": 23.74330134549628
          },
          "point_estimate": 18.76275074817388,
          "standard_error": 3.6153779651142655
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17445.53832722466,
            "upper_bound": 17468.26551755028
          },
          "point_estimate": 17457.64928449446,
          "standard_error": 5.823464117374544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17442.67495196926,
            "upper_bound": 17473.86565878353
          },
          "point_estimate": 17460.973943323726,
          "standard_error": 7.813725341982632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.9357946582376688,
            "upper_bound": 30.607464658337395
          },
          "point_estimate": 20.40944404208188,
          "standard_error": 7.871830805253398
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17450.462187009165,
            "upper_bound": 17468.706642689285
          },
          "point_estimate": 17460.55033247252,
          "standard_error": 4.653940410960016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.093188505017645,
            "upper_bound": 24.390307409717348
          },
          "point_estimate": 19.357267702049516,
          "standard_error": 3.918465746029608
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17480.805448431307,
            "upper_bound": 17503.248510335787
          },
          "point_estimate": 17491.767323976732,
          "standard_error": 5.748628506068873
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17478.795107537964,
            "upper_bound": 17505.65496232163
          },
          "point_estimate": 17489.871653038317,
          "standard_error": 7.732114284967064
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9153047789761586,
            "upper_bound": 32.75031726368506
          },
          "point_estimate": 19.83370281286515,
          "standard_error": 7.169957756681246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17476.130598369247,
            "upper_bound": 17500.93898590572
          },
          "point_estimate": 17487.896327530092,
          "standard_error": 6.38678428259489
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.62248220091266,
            "upper_bound": 24.198032059248728
          },
          "point_estimate": 19.14091953848375,
          "standard_error": 3.4625876586891184
        }
      }
    },
    "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17478.255218158487,
            "upper_bound": 17495.94266545213
          },
          "point_estimate": 17487.116600726433,
          "standard_error": 4.560621964858359
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17473.593198588387,
            "upper_bound": 17499.726479788256
          },
          "point_estimate": 17489.151179018285,
          "standard_error": 6.805690850268412
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.374668786627223,
            "upper_bound": 25.91202569492959
          },
          "point_estimate": 16.224525661188224,
          "standard_error": 5.634623396143726
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17478.03168174527,
            "upper_bound": 17499.246978989428
          },
          "point_estimate": 17488.998398810043,
          "standard_error": 5.387821853124077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.074827326808917,
            "upper_bound": 18.7403497994615
          },
          "point_estimate": 15.176455907366432,
          "standard_error": 2.469318862904135
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17816.328154570205,
            "upper_bound": 17835.85801065532
          },
          "point_estimate": 17824.915846976033,
          "standard_error": 5.060741779951021
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17812.8880170018,
            "upper_bound": 17832.718538499263
          },
          "point_estimate": 17821.318604669264,
          "standard_error": 4.255699633257073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5846825858969456,
            "upper_bound": 21.844324317920375
          },
          "point_estimate": 10.032309450790528,
          "standard_error": 5.2131079378689105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17820.26006145113,
            "upper_bound": 17839.182016765037
          },
          "point_estimate": 17829.607407501768,
          "standard_error": 4.983819389030206
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.868568363788492,
            "upper_bound": 23.52268837810868
          },
          "point_estimate": 16.91674722143999,
          "standard_error": 5.21445301010878
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17871.917055461767,
            "upper_bound": 17894.060426352844
          },
          "point_estimate": 17882.50711413566,
          "standard_error": 5.651646204866216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17865.847078614803,
            "upper_bound": 17896.122107336287
          },
          "point_estimate": 17880.713582331013,
          "standard_error": 8.180390343733242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.09672636200072,
            "upper_bound": 32.87756081463302
          },
          "point_estimate": 19.780044405111344,
          "standard_error": 6.899379141570678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17880.578319384953,
            "upper_bound": 17899.340787869394
          },
          "point_estimate": 17891.8691118827,
          "standard_error": 4.749261700530568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.625219416581604,
            "upper_bound": 24.17090261197765
          },
          "point_estimate": 18.887925426870012,
          "standard_error": 3.585015131413759
        }
      }
    },
    "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18025.8821758591,
            "upper_bound": 18092.085657641295
          },
          "point_estimate": 18055.791808258055,
          "standard_error": 17.084459894155373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18020.226058790693,
            "upper_bound": 18088.28553739475
          },
          "point_estimate": 18039.03045236916,
          "standard_error": 17.546414131206976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.466183891310775,
            "upper_bound": 86.61064128553494
          },
          "point_estimate": 28.8883034816806,
          "standard_error": 20.981268622469475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18019.65884409484,
            "upper_bound": 18040.296745768752
          },
          "point_estimate": 18029.043064909336,
          "standard_error": 5.229899647604491
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.11167195312592,
            "upper_bound": 77.90433558759094
          },
          "point_estimate": 56.931852429620875,
          "standard_error": 15.549390145375906
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68614.95586635222,
            "upper_bound": 68729.73019429468
          },
          "point_estimate": 68668.14609636119,
          "standard_error": 29.596404841863553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68593.8662735849,
            "upper_bound": 68752.57113207548
          },
          "point_estimate": 68637.16588050315,
          "standard_error": 35.40010133938091
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.700807403834293,
            "upper_bound": 156.34063345080227
          },
          "point_estimate": 71.1014502235477,
          "standard_error": 34.36158427286477
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68613.94487943918,
            "upper_bound": 68729.1620167322
          },
          "point_estimate": 68661.71240872335,
          "standard_error": 29.39598386721874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.643301644072466,
            "upper_bound": 122.40943037727504
          },
          "point_estimate": 98.39853068039184,
          "standard_error": 21.80644641815207
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226726.7984656086,
            "upper_bound": 1228954.972060516
          },
          "point_estimate": 1227962.0941084656,
          "standard_error": 571.7048024056959
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227351.379166667,
            "upper_bound": 1229096.8994444443
          },
          "point_estimate": 1228293.866931217,
          "standard_error": 516.0361245869278
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157.62332220153624,
            "upper_bound": 2531.708895053094
          },
          "point_estimate": 1098.3471255003717,
          "standard_error": 570.8529366397544
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227704.8673374658,
            "upper_bound": 1228793.331138546
          },
          "point_estimate": 1228115.377142857,
          "standard_error": 276.65072553084985
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 686.0833891417093,
            "upper_bound": 2730.739471219359
          },
          "point_estimate": 1913.128634893302,
          "standard_error": 598.6041962233712
        }
      }
    },
    "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208411.16048942745,
            "upper_bound": 209119.83506213152
          },
          "point_estimate": 208761.52088594105,
          "standard_error": 181.200329870652
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208310.3358730159,
            "upper_bound": 209202.60761904763
          },
          "point_estimate": 208718.71078571427,
          "standard_error": 211.2049833312905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.62101893683242,
            "upper_bound": 1023.7928208240536
          },
          "point_estimate": 661.441033590422,
          "standard_error": 249.3654107463631
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208457.16973544977,
            "upper_bound": 208934.71874065197
          },
          "point_estimate": 208687.3921038961,
          "standard_error": 120.59294639566382
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 327.0283912587343,
            "upper_bound": 774.926043419728
          },
          "point_estimate": 602.0582462164958,
          "standard_error": 115.47848610121548
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5578.2506048510895,
            "upper_bound": 5605.641467656963
          },
          "point_estimate": 5592.209147928028,
          "standard_error": 7.02547294549033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5572.757660423702,
            "upper_bound": 5607.183623733497
          },
          "point_estimate": 5597.754604612288,
          "standard_error": 8.966510204796739
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.898243309668931,
            "upper_bound": 40.289466512996626
          },
          "point_estimate": 20.686463201209826,
          "standard_error": 9.525809703531026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5591.13347756111,
            "upper_bound": 5610.234440275951
          },
          "point_estimate": 5601.111470997532,
          "standard_error": 4.839381115981528
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.519956842497558,
            "upper_bound": 29.542292133917183
          },
          "point_estimate": 23.44727959215861,
          "standard_error": 4.267325963696868
        }
      }
    },
    "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6002.33804709621,
            "upper_bound": 6016.632643810678
          },
          "point_estimate": 6009.251005546768,
          "standard_error": 3.661362981372391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6000.035758552305,
            "upper_bound": 6018.389935547843
          },
          "point_estimate": 6007.549510641033,
          "standard_error": 4.144586595212095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7673101322334812,
            "upper_bound": 21.028185124939903
          },
          "point_estimate": 11.485133406107366,
          "standard_error": 5.0516258196472785
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6003.665945284308,
            "upper_bound": 6014.7498431760905
          },
          "point_estimate": 6008.688821210189,
          "standard_error": 2.781396079373519
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.081152462053412,
            "upper_bound": 15.795480814329188
          },
          "point_estimate": 12.16809049115044,
          "standard_error": 2.4698004293325058
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14226.395704486176,
            "upper_bound": 14258.843999739174
          },
          "point_estimate": 14239.931562336986,
          "standard_error": 8.530065543720394
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14222.44619848722,
            "upper_bound": 14248.348982785605
          },
          "point_estimate": 14231.405581637977,
          "standard_error": 7.467334058217692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2518494766965342,
            "upper_bound": 31.32942754629464
          },
          "point_estimate": 16.834566938997703,
          "standard_error": 6.892621903890557
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14223.928537467593,
            "upper_bound": 14240.669467314094
          },
          "point_estimate": 14231.395790907058,
          "standard_error": 4.315762819929344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.758362265517206,
            "upper_bound": 41.77852364412493
          },
          "point_estimate": 28.286336248167004,
          "standard_error": 10.517701487200734
        }
      }
    },
    "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.30108637327058,
            "upper_bound": 65.46754330967539
          },
          "point_estimate": 65.38651636610055,
          "standard_error": 0.042431799948773774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.3130589951324,
            "upper_bound": 65.51370098692279
          },
          "point_estimate": 65.37425932265299,
          "standard_error": 0.04781427909095487
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019800252396342283,
            "upper_bound": 0.24698404937741003
          },
          "point_estimate": 0.09890435237697316,
          "standard_error": 0.05723552797885299
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65.27405302200837,
            "upper_bound": 65.45688527493397
          },
          "point_estimate": 65.36548893986071,
          "standard_error": 0.04623625684841382
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06858602328359797,
            "upper_bound": 0.19033601745234183
          },
          "point_estimate": 0.14160940665554866,
          "standard_error": 0.031995518272902645
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04198949423219,
            "upper_bound": 24.077119848488255
          },
          "point_estimate": 24.05718730628131,
          "standard_error": 0.009130578614440816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04161117452021,
            "upper_bound": 24.068316342927577
          },
          "point_estimate": 24.04931670738557,
          "standard_error": 0.005989594140470602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004241968281269741,
            "upper_bound": 0.03959308197784579
          },
          "point_estimate": 0.011275438755375464,
          "standard_error": 0.009166369578962393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.04041221315406,
            "upper_bound": 24.065346449538627
          },
          "point_estimate": 24.05257638515895,
          "standard_error": 0.0063180960350804095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007054131173236621,
            "upper_bound": 0.043393690394315475
          },
          "point_estimate": 0.03035316604498613,
          "standard_error": 0.010195071894403
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.461626593133467,
            "upper_bound": 24.51906845947795
          },
          "point_estimate": 24.48755638271955,
          "standard_error": 0.0147186441714257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.45466747433312,
            "upper_bound": 24.507176293866607
          },
          "point_estimate": 24.482467236839653,
          "standard_error": 0.014942214956151932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008811985726014633,
            "upper_bound": 0.06898925941675185
          },
          "point_estimate": 0.03505707076917149,
          "standard_error": 0.01509788448394868
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.45358340886466,
            "upper_bound": 24.488261737779016
          },
          "point_estimate": 24.472751484169365,
          "standard_error": 0.00877034012799148
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019585907869974425,
            "upper_bound": 0.06916886430666579
          },
          "point_estimate": 0.04916284276894178,
          "standard_error": 0.014350552781904004
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.242358708633457,
            "upper_bound": 24.262892876540505
          },
          "point_estimate": 24.252332732278067,
          "standard_error": 0.005271823975264015
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.23962337652792,
            "upper_bound": 24.266509201948853
          },
          "point_estimate": 24.249020100639623,
          "standard_error": 0.006522302089680518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034571331176272225,
            "upper_bound": 0.03122107222468025
          },
          "point_estimate": 0.017783510919372497,
          "standard_error": 0.007188236767295789
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.24742315047,
            "upper_bound": 24.27113031208256
          },
          "point_estimate": 24.26160021056761,
          "standard_error": 0.005958861337849322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009092320497692189,
            "upper_bound": 0.02186384944353211
          },
          "point_estimate": 0.01758013837162646,
          "standard_error": 0.0031854997804279493
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.77267279741981,
            "upper_bound": 23.795115476004533
          },
          "point_estimate": 23.783000255866597,
          "standard_error": 0.005786878796203675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.76841114374602,
            "upper_bound": 23.79588414930584
          },
          "point_estimate": 23.776583753049707,
          "standard_error": 0.00784850480595239
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032435577651069667,
            "upper_bound": 0.032698797841751184
          },
          "point_estimate": 0.018374975073252572,
          "standard_error": 0.007118715464134845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.772837270557137,
            "upper_bound": 23.79054813949239
          },
          "point_estimate": 23.781197502589556,
          "standard_error": 0.004467802429156869
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008918778990710379,
            "upper_bound": 0.025756660376249664
          },
          "point_estimate": 0.019297073104580342,
          "standard_error": 0.004644328301944449
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.911564121435855,
            "upper_bound": 19.938181089679567
          },
          "point_estimate": 19.924788258777603,
          "standard_error": 0.006813780730134886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.90747262161035,
            "upper_bound": 19.94274745070381
          },
          "point_estimate": 19.92562347088052,
          "standard_error": 0.00984525888121366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003477058512799462,
            "upper_bound": 0.03761505487809916
          },
          "point_estimate": 0.025171919393799775,
          "standard_error": 0.00859913806410954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.917052015657664,
            "upper_bound": 19.939926174950106
          },
          "point_estimate": 19.92956891433074,
          "standard_error": 0.005813704887471871
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012901466801838716,
            "upper_bound": 0.02874900690728581
          },
          "point_estimate": 0.022638418575827792,
          "standard_error": 0.004054122451760706
        }
      }
    },
    "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.963560599464333,
            "upper_bound": 26.034360990946205
          },
          "point_estimate": 25.998677212413103,
          "standard_error": 0.01810877850100829
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.971458106961148,
            "upper_bound": 26.033249090905628
          },
          "point_estimate": 25.989564963340975,
          "standard_error": 0.01729870517532574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004806266451236171,
            "upper_bound": 0.10527027678112512
          },
          "point_estimate": 0.02857485486383163,
          "standard_error": 0.025617118795675423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.975177640863865,
            "upper_bound": 26.05417225644511
          },
          "point_estimate": 26.003138619670757,
          "standard_error": 0.021627408592932676
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024563424642895213,
            "upper_bound": 0.08202280177338621
          },
          "point_estimate": 0.060403052355069826,
          "standard_error": 0.014383246376069869
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.02398473853312,
            "upper_bound": 33.084124029249395
          },
          "point_estimate": 33.050758507419175,
          "standard_error": 0.015502536899310458
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.00597215814002,
            "upper_bound": 33.070078782435075
          },
          "point_estimate": 33.040110985828505,
          "standard_error": 0.014686083109444527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022127250598901037,
            "upper_bound": 0.0675980084027264
          },
          "point_estimate": 0.044810152637735697,
          "standard_error": 0.01792764992726058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.01550636899566,
            "upper_bound": 33.07311536609087
          },
          "point_estimate": 33.04205745958052,
          "standard_error": 0.01456438557434996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018667267565949032,
            "upper_bound": 0.07258492928907155
          },
          "point_estimate": 0.051775854682606016,
          "standard_error": 0.015456855641577416
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.318569466159826,
            "upper_bound": 27.403381907973987
          },
          "point_estimate": 27.353981601703644,
          "standard_error": 0.02231017845466612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.307195547292707,
            "upper_bound": 27.36923292638569
          },
          "point_estimate": 27.33779194370063,
          "standard_error": 0.014341039007920355
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006598460355245181,
            "upper_bound": 0.07363100634277583
          },
          "point_estimate": 0.02826878983373281,
          "standard_error": 0.017724831046602888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.323633460640245,
            "upper_bound": 27.35770141969508
          },
          "point_estimate": 27.340192690165942,
          "standard_error": 0.008749908769186233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020446987031720657,
            "upper_bound": 0.11006005168749786
          },
          "point_estimate": 0.07420737328486965,
          "standard_error": 0.028251610699826908
        }
      }
    },
    "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.97402402767902,
            "upper_bound": 35.05922471665518
          },
          "point_estimate": 35.008826019272945,
          "standard_error": 0.02265951032898414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.96807334590751,
            "upper_bound": 35.01192974105483
          },
          "point_estimate": 34.99559565513984,
          "standard_error": 0.012775204924518318
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002885230161514869,
            "upper_bound": 0.06423507520751887
          },
          "point_estimate": 0.021371137493258016,
          "standard_error": 0.016461039554811645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.96940167160186,
            "upper_bound": 35.00763445891161
          },
          "point_estimate": 34.9903666815691,
          "standard_error": 0.010054862009738314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015112059772794008,
            "upper_bound": 0.1136472445242458
          },
          "point_estimate": 0.07543254669927092,
          "standard_error": 0.03105096196187808
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.728216373477288,
            "upper_bound": 26.813191938940225
          },
          "point_estimate": 26.77620609051413,
          "standard_error": 0.02216548764480332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.748685685858767,
            "upper_bound": 26.826398356994915
          },
          "point_estimate": 26.79301572001957,
          "standard_error": 0.01515634494581192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00904268224208924,
            "upper_bound": 0.09511601772133343
          },
          "point_estimate": 0.025048079270117648,
          "standard_error": 0.024576055884585925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.768537832434923,
            "upper_bound": 26.818551327772347
          },
          "point_estimate": 26.79687225977294,
          "standard_error": 0.012559122933238328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02171545675297265,
            "upper_bound": 0.10548186198864452
          },
          "point_estimate": 0.07404612946136713,
          "standard_error": 0.024390709804209498
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.71688789360374,
            "upper_bound": 18.765201732264412
          },
          "point_estimate": 18.7361249408656,
          "standard_error": 0.0130559119062968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.71231268043069,
            "upper_bound": 18.7407234138624
          },
          "point_estimate": 18.72365927245996,
          "standard_error": 0.006851665244723067
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004275522847962823,
            "upper_bound": 0.033758481735522365
          },
          "point_estimate": 0.01345317541856351,
          "standard_error": 0.008492188238843816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.71982194672564,
            "upper_bound": 18.732661688774726
          },
          "point_estimate": 18.726157604779107,
          "standard_error": 0.0032479351529792247
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00865035205897503,
            "upper_bound": 0.0656246603288556
          },
          "point_estimate": 0.04344860974873311,
          "standard_error": 0.018468178434353492
        }
      }
    },
    "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.85212263417012,
            "upper_bound": 28.897368442132553
          },
          "point_estimate": 28.874787999044297,
          "standard_error": 0.011572112371720004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.83966325377449,
            "upper_bound": 28.90940264414867
          },
          "point_estimate": 28.876591834368213,
          "standard_error": 0.020840894588188098
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005415753180179804,
            "upper_bound": 0.06322653062046993
          },
          "point_estimate": 0.04986831215940569,
          "standard_error": 0.014749258475145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.846330691117416,
            "upper_bound": 28.89191782728741
          },
          "point_estimate": 28.86790342425734,
          "standard_error": 0.011568909692475218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02496041267731782,
            "upper_bound": 0.04590146027455925
          },
          "point_estimate": 0.03869868719103352,
          "standard_error": 0.005363516866556877
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99131.18275184896,
            "upper_bound": 99284.56473953876
          },
          "point_estimate": 99196.81453029714,
          "standard_error": 39.98529363879103
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99108.0408719346,
            "upper_bound": 99241.94570844687
          },
          "point_estimate": 99155.61943687555,
          "standard_error": 34.1830986803623
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.4034197418182,
            "upper_bound": 151.3150680356894
          },
          "point_estimate": 74.7416890000508,
          "standard_error": 39.941609634969865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99120.37006792106,
            "upper_bound": 99194.71856211458
          },
          "point_estimate": 99155.7106125482,
          "standard_error": 18.913820015477715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.43342615077589,
            "upper_bound": 191.13059681176813
          },
          "point_estimate": 133.3226225870323,
          "standard_error": 45.26254757000375
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52440.94154624277,
            "upper_bound": 52573.17909200386
          },
          "point_estimate": 52504.5431093449,
          "standard_error": 33.68991948330772
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52429.64354527938,
            "upper_bound": 52571.51276493256
          },
          "point_estimate": 52486.38969171484,
          "standard_error": 33.647906496976844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.85037050947464,
            "upper_bound": 186.8493751509733
          },
          "point_estimate": 90.75432934254178,
          "standard_error": 43.2253866163965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52457.49804170493,
            "upper_bound": 52541.71145374486
          },
          "point_estimate": 52507.808922002856,
          "standard_error": 21.507811618302668
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.540811287680064,
            "upper_bound": 152.1327038095543
          },
          "point_estimate": 111.94023196569276,
          "standard_error": 26.491335066524
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146144.0785025658,
            "upper_bound": 146375.3592426372
          },
          "point_estimate": 146268.3971508255,
          "standard_error": 58.67654665074537
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146210.859437751,
            "upper_bound": 146349.08500669344
          },
          "point_estimate": 146301.14307228915,
          "standard_error": 39.81887259120929
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.999916391927453,
            "upper_bound": 286.1715660037677
          },
          "point_estimate": 94.78302649666212,
          "standard_error": 59.739001697059344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146211.19851026873,
            "upper_bound": 146324.76820183962
          },
          "point_estimate": 146261.91409795024,
          "standard_error": 29.38441163023276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.72205878886369,
            "upper_bound": 285.97531791020475
          },
          "point_estimate": 195.6203659188482,
          "standard_error": 61.64930049661553
        }
      }
    },
    "memmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401657.1119989535,
            "upper_bound": 402121.71302649134
          },
          "point_estimate": 401914.5079618001,
          "standard_error": 119.71161459283304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401739.1978021978,
            "upper_bound": 402247.6769230769
          },
          "point_estimate": 401970.23129251704,
          "standard_error": 115.05956255069556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.382917620292034,
            "upper_bound": 559.4526219358318
          },
          "point_estimate": 358.2767617712147,
          "standard_error": 129.84540535265168
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401862.6853948287,
            "upper_bound": 402192.1909751779
          },
          "point_estimate": 402052.2987012987,
          "standard_error": 85.95204866825964
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.80456910015857,
            "upper_bound": 569.567277921822
          },
          "point_estimate": 401.5403824541588,
          "standard_error": 122.78380904917849
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570672.711783854,
            "upper_bound": 571506.457517361
          },
          "point_estimate": 571117.8394525049,
          "standard_error": 213.97716465006255
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570794.0017578125,
            "upper_bound": 571795.40234375
          },
          "point_estimate": 571118.1609375,
          "standard_error": 210.5281331624025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.11333677609196,
            "upper_bound": 1147.7560611988213
          },
          "point_estimate": 541.3886305055736,
          "standard_error": 333.28134775103564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 570857.9737413195,
            "upper_bound": 571418.0970839969
          },
          "point_estimate": 571122.6150162338,
          "standard_error": 139.9047920127779
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332.2680746373988,
            "upper_bound": 967.8718798962144
          },
          "point_estimate": 711.1947914755635,
          "standard_error": 179.19990697591757
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40857.20895155963,
            "upper_bound": 40915.00166050672
          },
          "point_estimate": 40884.7233062385,
          "standard_error": 14.812653206613971
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40844.95658243612,
            "upper_bound": 40932.25646794151
          },
          "point_estimate": 40870.82370953631,
          "standard_error": 21.386221478700044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.252862391689593,
            "upper_bound": 78.05467167387015
          },
          "point_estimate": 43.53079579635422,
          "standard_error": 19.05479883548052
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40851.79995904217,
            "upper_bound": 40889.97791912041
          },
          "point_estimate": 40870.864911691235,
          "standard_error": 9.777165008620935
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.200645160164925,
            "upper_bound": 62.419421773035175
          },
          "point_estimate": 49.26938681660102,
          "standard_error": 9.489237296352233
        }
      }
    },
    "memmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100103.62359524082,
            "upper_bound": 100287.1222125284
          },
          "point_estimate": 100195.00752186363,
          "standard_error": 46.7330428832121
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100086.87052341596,
            "upper_bound": 100309.6396694215
          },
          "point_estimate": 100194.77670645854,
          "standard_error": 66.17124763782132
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.3903679844746,
            "upper_bound": 269.4894505255362
          },
          "point_estimate": 165.13876500209372,
          "standard_error": 60.83456710878859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100113.1028639742,
            "upper_bound": 100281.18450367
          },
          "point_estimate": 100204.55220922329,
          "standard_error": 43.73860284783967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.24631634401739,
            "upper_bound": 201.09142236217252
          },
          "point_estimate": 155.2175703440486,
          "standard_error": 29.959508261517573
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69554.3922449616,
            "upper_bound": 69762.92978048928
          },
          "point_estimate": 69635.5784358529,
          "standard_error": 57.405343277061206
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69551.09652911068,
            "upper_bound": 69616.75047984645
          },
          "point_estimate": 69600.32397632758,
          "standard_error": 20.310497510891786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.000047945315492,
            "upper_bound": 116.41132952317832
          },
          "point_estimate": 32.45779383834644,
          "standard_error": 31.47736651594429
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69576.14281763128,
            "upper_bound": 69608.72328951629
          },
          "point_estimate": 69593.93219333449,
          "standard_error": 8.370875987397925
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.05454265324865,
            "upper_bound": 292.1195392213507
          },
          "point_estimate": 190.84127843925165,
          "standard_error": 88.49166918091422
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307816.8088761098,
            "upper_bound": 308268.6242449556
          },
          "point_estimate": 308048.3504956955,
          "standard_error": 115.22496665274294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307868.5374764595,
            "upper_bound": 308263.91490112996
          },
          "point_estimate": 308091.8529661017,
          "standard_error": 111.32836443859964
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.244712212044904,
            "upper_bound": 634.5904819540898
          },
          "point_estimate": 324.965182959576,
          "standard_error": 146.1907876565699
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307948.93015536724,
            "upper_bound": 308237.54319665214
          },
          "point_estimate": 308088.16680607526,
          "standard_error": 73.20218188716505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144.0743421556697,
            "upper_bound": 534.9251186883307
          },
          "point_estimate": 384.2425426180023,
          "standard_error": 100.5754643235649
        }
      }
    },
    "memmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35387.883429540314,
            "upper_bound": 35435.58709251692
          },
          "point_estimate": 35411.12074052179,
          "standard_error": 12.181923465386092
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35379.35140106027,
            "upper_bound": 35429.95846432049
          },
          "point_estimate": 35412.712207887045,
          "standard_error": 10.221717036128965
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.654291003349238,
            "upper_bound": 69.25753323976028
          },
          "point_estimate": 17.83770637350842,
          "standard_error": 17.147348861834235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35400.40777685867,
            "upper_bound": 35442.473321095255
          },
          "point_estimate": 35421.71988770723,
          "standard_error": 10.629588951277109
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.972042226363904,
            "upper_bound": 55.46791710990587
          },
          "point_estimate": 40.62314624276435,
          "standard_error": 9.97318597700412
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66634.1730733945,
            "upper_bound": 66727.70889738605
          },
          "point_estimate": 66682.09828105432,
          "standard_error": 23.929466093090817
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66629.41712538226,
            "upper_bound": 66734.15072083879
          },
          "point_estimate": 66687.62792048929,
          "standard_error": 25.659693303842968
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.111974258217973,
            "upper_bound": 135.23932380087595
          },
          "point_estimate": 67.4843366000783,
          "standard_error": 30.874144061741028
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66665.65363627726,
            "upper_bound": 66761.31558341539
          },
          "point_estimate": 66720.52965566544,
          "standard_error": 24.442415104643057
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.14760914692385,
            "upper_bound": 104.44754382657948
          },
          "point_estimate": 79.93444808640204,
          "standard_error": 16.553323729479924
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171387.33693003145,
            "upper_bound": 171609.71809542528
          },
          "point_estimate": 171491.24794736452,
          "standard_error": 56.97608245649697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171342.0495283019,
            "upper_bound": 171598.878032345
          },
          "point_estimate": 171464.245086478,
          "standard_error": 60.32099121616361
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.785859459269023,
            "upper_bound": 302.742868445976
          },
          "point_estimate": 190.3869666671112,
          "standard_error": 68.55789156048326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171332.1085162018,
            "upper_bound": 171538.9718153862
          },
          "point_estimate": 171431.02520215634,
          "standard_error": 53.27206075151642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.25052733716902,
            "upper_bound": 256.0543713635734
          },
          "point_estimate": 190.75185228666712,
          "standard_error": 46.21890390818109
        }
      }
    },
    "memmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34627.66450346749,
            "upper_bound": 34714.79270958288
          },
          "point_estimate": 34672.19756084908,
          "standard_error": 22.24506767920638
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34608.383778625954,
            "upper_bound": 34744.64515744275
          },
          "point_estimate": 34673.92193066158,
          "standard_error": 32.427869785259404
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.19941173374466,
            "upper_bound": 126.57668981388233
          },
          "point_estimate": 101.01055832359395,
          "standard_error": 27.14216227104756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34654.01284134365,
            "upper_bound": 34712.139236127645
          },
          "point_estimate": 34684.33431644691,
          "standard_error": 14.715493764848064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.99498864437746,
            "upper_bound": 91.82413921357823
          },
          "point_estimate": 74.03305411369631,
          "standard_error": 12.495678510143398
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11412.696500695352,
            "upper_bound": 11464.506019312252
          },
          "point_estimate": 11434.554839631515,
          "standard_error": 13.597002178556425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11404.557670185945,
            "upper_bound": 11448.488480932872
          },
          "point_estimate": 11418.044446195328,
          "standard_error": 12.743470009841449
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0985778743947714,
            "upper_bound": 52.83229002721581
          },
          "point_estimate": 24.727539051490705,
          "standard_error": 12.52589581340526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11408.076116052243,
            "upper_bound": 11432.385524198471
          },
          "point_estimate": 11419.880070890344,
          "standard_error": 6.424520352357953
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.73516650161179,
            "upper_bound": 66.79173964489213
          },
          "point_estimate": 45.60022707249183,
          "standard_error": 16.450680876254978
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496655.60301858094,
            "upper_bound": 497275.12713588576
          },
          "point_estimate": 496975.7367605105,
          "standard_error": 159.1031628776724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496439.5016891892,
            "upper_bound": 497442.75
          },
          "point_estimate": 497067.1621621622,
          "standard_error": 210.5409725950878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.74652865124227,
            "upper_bound": 911.0133560897988
          },
          "point_estimate": 592.3215072770187,
          "standard_error": 233.06060756123864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496717.51681117894,
            "upper_bound": 497496.1419352972
          },
          "point_estimate": 497195.4272376272,
          "standard_error": 201.94310631933143
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267.5902505207095,
            "upper_bound": 661.7599306327747
          },
          "point_estimate": 532.3810237082872,
          "standard_error": 96.63880740356448
        }
      }
    },
    "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 991.9248218497828,
            "upper_bound": 993.329936079696
          },
          "point_estimate": 992.6238154456404,
          "standard_error": 0.3604595945443055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 991.4370749875096,
            "upper_bound": 993.9396602625244
          },
          "point_estimate": 992.63091020575,
          "standard_error": 0.6299135743340805
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2152520754808484,
            "upper_bound": 2.0001257016936593
          },
          "point_estimate": 1.6356403576627825,
          "standard_error": 0.4604652402632339
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 991.6088760808362,
            "upper_bound": 993.4662563088768
          },
          "point_estimate": 992.4038628710164,
          "standard_error": 0.47476916127953234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7572042378617853,
            "upper_bound": 1.4035766057949546
          },
          "point_estimate": 1.1957964950572726,
          "standard_error": 0.1642691736132919
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47210.284093700335,
            "upper_bound": 47305.04735822136
          },
          "point_estimate": 47256.29609919087,
          "standard_error": 24.217042192028824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47191.43579323797,
            "upper_bound": 47309.83875162549
          },
          "point_estimate": 47256.828413524054,
          "standard_error": 33.434198501342756
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.243500331290514,
            "upper_bound": 143.07764531422285
          },
          "point_estimate": 79.34820808521486,
          "standard_error": 31.011636828456727
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47220.741285771466,
            "upper_bound": 47320.975742532646
          },
          "point_estimate": 47266.60567780724,
          "standard_error": 25.363887301331513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.60894058259269,
            "upper_bound": 104.19153692956236
          },
          "point_estimate": 80.41697335630795,
          "standard_error": 16.0308350659822
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48761.62996185682,
            "upper_bound": 48830.74822881111
          },
          "point_estimate": 48793.738878768505,
          "standard_error": 17.72767790433383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48744.37472035794,
            "upper_bound": 48817.546275167784
          },
          "point_estimate": 48800.07871524448,
          "standard_error": 22.92910896074538
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.2266268870764385,
            "upper_bound": 103.26121655600244
          },
          "point_estimate": 48.742464201765166,
          "standard_error": 24.092392353680403
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48751.83789756992,
            "upper_bound": 48877.79847528005
          },
          "point_estimate": 48815.39037392138,
          "standard_error": 33.842065795922295
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.215485588035847,
            "upper_bound": 80.63205452332718
          },
          "point_estimate": 58.94386882664757,
          "standard_error": 14.98404918938962
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48180.579495351936,
            "upper_bound": 48231.05272209576
          },
          "point_estimate": 48205.48484854234,
          "standard_error": 12.937180816292733
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48162.16135458167,
            "upper_bound": 48240.34803642573
          },
          "point_estimate": 48209.37450199203,
          "standard_error": 18.979901527112062
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.927808389479951,
            "upper_bound": 78.73728147066625
          },
          "point_estimate": 53.06693909771183,
          "standard_error": 18.31282106415789
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48174.26951168362,
            "upper_bound": 48249.4715939076
          },
          "point_estimate": 48214.86683568755,
          "standard_error": 19.116868647154597
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.9465904429166,
            "upper_bound": 52.90891157778783
          },
          "point_estimate": 43.176858070934095,
          "standard_error": 6.974046408202595
        }
      }
    },
    "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13758.71444393394,
            "upper_bound": 13786.224045303354
          },
          "point_estimate": 13772.62708171686,
          "standard_error": 7.037180470568552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13756.483800682076,
            "upper_bound": 13786.456328154603
          },
          "point_estimate": 13773.466190897225,
          "standard_error": 6.347282907801904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.766376616220329,
            "upper_bound": 41.277791423292925
          },
          "point_estimate": 15.403210906908688,
          "standard_error": 10.511949844250326
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13767.09125135154,
            "upper_bound": 13778.20798663006
          },
          "point_estimate": 13772.029353897336,
          "standard_error": 2.8224288356192955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.349739265752076,
            "upper_bound": 31.040696344583623
          },
          "point_estimate": 23.407967552000063,
          "standard_error": 5.109256668936329
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25589.580706607034,
            "upper_bound": 25653.491628553493
          },
          "point_estimate": 25625.054699410364,
          "standard_error": 16.54862422550438
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25606.31920903955,
            "upper_bound": 25657.111279257464
          },
          "point_estimate": 25638.856167608283,
          "standard_error": 13.721323436421455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.859210806803803,
            "upper_bound": 73.30955415400616
          },
          "point_estimate": 32.990766151589234,
          "standard_error": 16.492006649517887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25622.147969379377,
            "upper_bound": 25672.592572154623
          },
          "point_estimate": 25649.747494313597,
          "standard_error": 13.42299341959028
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.50189664884745,
            "upper_bound": 78.39783353795933
          },
          "point_estimate": 55.087843711938405,
          "standard_error": 17.242518740986693
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17433.036833843416,
            "upper_bound": 17448.004565946263
          },
          "point_estimate": 17441.02604742473,
          "standard_error": 3.841025084875635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17430.352216354615,
            "upper_bound": 17450.351256200993
          },
          "point_estimate": 17445.03948631781,
          "standard_error": 4.862039556754347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7691010727705183,
            "upper_bound": 20.74050361641715
          },
          "point_estimate": 9.858717449793591,
          "standard_error": 4.985935442405445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17442.69452825639,
            "upper_bound": 17450.09768452632
          },
          "point_estimate": 17446.832959455332,
          "standard_error": 1.9068192952560723
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.436267555532731,
            "upper_bound": 16.301813107337303
          },
          "point_estimate": 12.823107642131378,
          "standard_error": 2.780762795155464
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17434.807825335836,
            "upper_bound": 17456.619382675388
          },
          "point_estimate": 17445.518269311407,
          "standard_error": 5.585962707636414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17431.55402857829,
            "upper_bound": 17457.86887608069
          },
          "point_estimate": 17445.514115700717,
          "standard_error": 7.381679120144754
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.892884185211837,
            "upper_bound": 31.27083379281512
          },
          "point_estimate": 18.411477641431045,
          "standard_error": 7.060241122630278
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17430.206815585843,
            "upper_bound": 17448.889260534685
          },
          "point_estimate": 17438.1845166361,
          "standard_error": 4.695840921830178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.93715527558356,
            "upper_bound": 24.30631397172941
          },
          "point_estimate": 18.596936019069577,
          "standard_error": 3.7442752905175407
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17310.863529519094,
            "upper_bound": 17328.19937301209
          },
          "point_estimate": 17318.896776834932,
          "standard_error": 4.466078467818933
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17307.425572519085,
            "upper_bound": 17329.826932251908
          },
          "point_estimate": 17314.51488284563,
          "standard_error": 5.093311663706891
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.528118251871858,
            "upper_bound": 24.193694966464687
          },
          "point_estimate": 12.540796342241594,
          "standard_error": 5.3860919847519515
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17307.392635632496,
            "upper_bound": 17323.05071533139
          },
          "point_estimate": 17312.956069693664,
          "standard_error": 4.019296949630883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.9848634751639445,
            "upper_bound": 19.299874572936822
          },
          "point_estimate": 14.94191204613365,
          "standard_error": 3.443176160033209
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37717.305763145094,
            "upper_bound": 37832.154468674285
          },
          "point_estimate": 37774.3403997099,
          "standard_error": 29.44360798002303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37688.004450378285,
            "upper_bound": 37893.03888311988
          },
          "point_estimate": 37770.32294911734,
          "standard_error": 47.70134583647712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.470040708507838,
            "upper_bound": 176.18768534556193
          },
          "point_estimate": 141.16581551041912,
          "standard_error": 42.86713790625066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37725.189176484666,
            "upper_bound": 37866.606225488715
          },
          "point_estimate": 37814.2349772761,
          "standard_error": 35.847997554740324
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.68903738392782,
            "upper_bound": 116.54631407630012
          },
          "point_estimate": 97.94906136890818,
          "standard_error": 14.312086037461244
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15783.04379110532,
            "upper_bound": 15796.952492636052
          },
          "point_estimate": 15789.59175117099,
          "standard_error": 3.5542874003368676
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15780.623424598,
            "upper_bound": 15795.818883094307
          },
          "point_estimate": 15789.086397218602,
          "standard_error": 3.752728784813467
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.713398457192916,
            "upper_bound": 19.59553974554786
          },
          "point_estimate": 11.264393183328645,
          "standard_error": 4.3832475284765104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15787.93855214605,
            "upper_bound": 15804.933009036928
          },
          "point_estimate": 15797.684381155568,
          "standard_error": 4.257053512360633
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.666239060697691,
            "upper_bound": 15.833334773052808
          },
          "point_estimate": 11.83238383184344,
          "standard_error": 2.7306912402791443
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18322.24228061425,
            "upper_bound": 18361.37051503169
          },
          "point_estimate": 18337.82076454492,
          "standard_error": 10.59695471349636
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18323.035966683492,
            "upper_bound": 18337.849129227663
          },
          "point_estimate": 18328.016910651186,
          "standard_error": 4.830779956676985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.38272883293122,
            "upper_bound": 25.20558411081386
          },
          "point_estimate": 9.102401529465917,
          "standard_error": 6.251139201399225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18322.315857604855,
            "upper_bound": 18333.585063586434
          },
          "point_estimate": 18328.51891278837,
          "standard_error": 2.826604240054235
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.882303047641348,
            "upper_bound": 53.5735644168152
          },
          "point_estimate": 35.357465848130296,
          "standard_error": 15.490492761299516
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17833.100120488496,
            "upper_bound": 17895.07799258532
          },
          "point_estimate": 17861.1181168902,
          "standard_error": 15.965242533634155
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17827.07545523934,
            "upper_bound": 17890.19179342493
          },
          "point_estimate": 17839.64611138371,
          "standard_error": 18.334883662009176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.718089400073755,
            "upper_bound": 83.19305181547313
          },
          "point_estimate": 29.460309045178487,
          "standard_error": 20.758271315790964
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17826.461020050236,
            "upper_bound": 17877.389562091426
          },
          "point_estimate": 17843.71752928132,
          "standard_error": 13.273474783676978
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.078262108360416,
            "upper_bound": 71.60642735834506
          },
          "point_estimate": 53.19020970886176,
          "standard_error": 13.839947552990465
        }
      }
    },
    "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17285.86393240262,
            "upper_bound": 17304.603380546858
          },
          "point_estimate": 17295.82424238299,
          "standard_error": 4.7927745786195075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17284.14278908324,
            "upper_bound": 17306.820509766556
          },
          "point_estimate": 17298.281955693186,
          "standard_error": 5.342130126468776
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.006470420436276,
            "upper_bound": 26.743777324153847
          },
          "point_estimate": 15.463179861254714,
          "standard_error": 6.525359869158547
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17285.909174613127,
            "upper_bound": 17309.031448941398
          },
          "point_estimate": 17299.939091589687,
          "standard_error": 5.895213735536189
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.348408989040681,
            "upper_bound": 20.822269405606583
          },
          "point_estimate": 15.918003276521366,
          "standard_error": 3.514127860435421
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17629.53469888635,
            "upper_bound": 17652.454102377487
          },
          "point_estimate": 17641.191561385676,
          "standard_error": 5.85879134667283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17626.081136449404,
            "upper_bound": 17658.194177583697
          },
          "point_estimate": 17639.50055451584,
          "standard_error": 8.672960084089611
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.629111693617342,
            "upper_bound": 34.2977311669284
          },
          "point_estimate": 25.525362749163545,
          "standard_error": 8.007625624962317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17633.38160718157,
            "upper_bound": 17653.979473173928
          },
          "point_estimate": 17645.35025236772,
          "standard_error": 5.315131598281596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.357987344148617,
            "upper_bound": 24.457243970107765
          },
          "point_estimate": 19.516336102963756,
          "standard_error": 3.3954976814226616
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17661.01251841156,
            "upper_bound": 17686.294263379834
          },
          "point_estimate": 17672.151371537428,
          "standard_error": 6.527092793608185
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17655.590953307394,
            "upper_bound": 17678.609030479896
          },
          "point_estimate": 17671.6332641398,
          "standard_error": 6.2562801481465184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7132238817903036,
            "upper_bound": 29.81606377513332
          },
          "point_estimate": 15.96031611256233,
          "standard_error": 6.6634698484587735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17664.493525131922,
            "upper_bound": 17682.697409842203
          },
          "point_estimate": 17672.472195411592,
          "standard_error": 4.699381513026548
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.633346253012828,
            "upper_bound": 31.099055882157483
          },
          "point_estimate": 21.747416460556295,
          "standard_error": 6.844531776925414
        }
      }
    },
    "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17383.846889227963,
            "upper_bound": 17404.870884869346
          },
          "point_estimate": 17392.95806188121,
          "standard_error": 5.472317517374715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17381.082431785544,
            "upper_bound": 17397.474036335454
          },
          "point_estimate": 17391.325109036752,
          "standard_error": 4.620752433910565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1223807785023134,
            "upper_bound": 21.29963772621183
          },
          "point_estimate": 13.60378527707032,
          "standard_error": 4.8900328307900365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17386.46518239619,
            "upper_bound": 17395.74893111982
          },
          "point_estimate": 17392.06180425606,
          "standard_error": 2.341426529279967
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.249066433342198,
            "upper_bound": 26.592999979862626
          },
          "point_estimate": 18.32636877414543,
          "standard_error": 6.217952703827152
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18002.057541259615,
            "upper_bound": 18023.924306243804
          },
          "point_estimate": 18012.908654490537,
          "standard_error": 5.631557879019349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17996.123224314502,
            "upper_bound": 18031.073422530557
          },
          "point_estimate": 18012.310790386524,
          "standard_error": 9.398173937411558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.7783195208057725,
            "upper_bound": 30.7569086957779
          },
          "point_estimate": 23.442663112057243,
          "standard_error": 6.829229725068793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17999.083164811494,
            "upper_bound": 18026.621365346695
          },
          "point_estimate": 18013.298490211473,
          "standard_error": 6.962472396501848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.774674690501564,
            "upper_bound": 22.25476824140821
          },
          "point_estimate": 18.75242540037905,
          "standard_error": 2.6739301868497156
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17832.231923262232,
            "upper_bound": 17859.478740484767
          },
          "point_estimate": 17844.939225352384,
          "standard_error": 6.967692571642459
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17833.49518664047,
            "upper_bound": 17853.995930395733
          },
          "point_estimate": 17841.552326866404,
          "standard_error": 5.489893974265939
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.931742017627692,
            "upper_bound": 34.63164000402593
          },
          "point_estimate": 13.48984231330623,
          "standard_error": 7.153933005344536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17838.576412690385,
            "upper_bound": 17850.549531394096
          },
          "point_estimate": 17844.69020998648,
          "standard_error": 3.0953608695965418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.279301884856659,
            "upper_bound": 33.52999119967464
          },
          "point_estimate": 23.22182103154275,
          "standard_error": 7.07483928289108
        }
      }
    },
    "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18174.249860069915,
            "upper_bound": 18198.69252126808
          },
          "point_estimate": 18186.014465744774,
          "standard_error": 6.253788004800823
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18169.953426713357,
            "upper_bound": 18195.526138069035
          },
          "point_estimate": 18187.68257342957,
          "standard_error": 6.3212450746200775
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.107526578145444,
            "upper_bound": 36.04910500407241
          },
          "point_estimate": 12.395404064581516,
          "standard_error": 8.628927057555384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18167.69643601244,
            "upper_bound": 18187.49326040577
          },
          "point_estimate": 18175.71094378358,
          "standard_error": 4.935521881987425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.456535055442425,
            "upper_bound": 28.38418129407036
          },
          "point_estimate": 20.851461745504373,
          "standard_error": 4.9845611506148595
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68254.17202690878,
            "upper_bound": 68385.16135428975
          },
          "point_estimate": 68315.66049588258,
          "standard_error": 33.49150284733128
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68227.79517991407,
            "upper_bound": 68391.09649122808
          },
          "point_estimate": 68295.70136278195,
          "standard_error": 42.428134680398344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.477622956208176,
            "upper_bound": 191.99789095226677
          },
          "point_estimate": 112.72822563026024,
          "standard_error": 39.474804043647985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68252.92374633164,
            "upper_bound": 68340.53084196163
          },
          "point_estimate": 68291.35570745045,
          "standard_error": 22.063584032558374
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.510469862442406,
            "upper_bound": 148.82078885430286
          },
          "point_estimate": 111.92879244284708,
          "standard_error": 25.783200586545373
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227772.0053448414,
            "upper_bound": 1229954.4170613426
          },
          "point_estimate": 1228840.2203108466,
          "standard_error": 558.0068246537596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227574.8542857142,
            "upper_bound": 1230374.72
          },
          "point_estimate": 1228690.5104166665,
          "standard_error": 644.2515339820925
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 416.6960480189174,
            "upper_bound": 3187.1140700007027
          },
          "point_estimate": 1847.964527608612,
          "standard_error": 739.0772491120985
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228022.549416888,
            "upper_bound": 1229498.963898501
          },
          "point_estimate": 1228650.4605194803,
          "standard_error": 370.0174606582547
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 937.3358149318706,
            "upper_bound": 2444.133543211148
          },
          "point_estimate": 1855.8442006098935,
          "standard_error": 387.1099086969024
        }
      }
    },
    "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207826.5758172959,
            "upper_bound": 208038.77823041385
          },
          "point_estimate": 207936.07941337867,
          "standard_error": 54.17859914633441
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207836.5994285714,
            "upper_bound": 208038.23469387752
          },
          "point_estimate": 207949.66992857144,
          "standard_error": 47.381692288924526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.721822168469373,
            "upper_bound": 299.9251335324358
          },
          "point_estimate": 117.5506452463828,
          "standard_error": 66.6441172546338
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207832.6015766359,
            "upper_bound": 207994.47968253968
          },
          "point_estimate": 207915.97148794064,
          "standard_error": 43.62269396083081
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.13168776228238,
            "upper_bound": 247.24896595376907
          },
          "point_estimate": 180.6091168840512,
          "standard_error": 44.13162347944856
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5518.993274891201,
            "upper_bound": 5532.828803306657
          },
          "point_estimate": 5525.988156072205,
          "standard_error": 3.535285045666033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5519.130003542331,
            "upper_bound": 5533.970206467284
          },
          "point_estimate": 5524.977556803806,
          "standard_error": 3.743262985386066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1391199407698047,
            "upper_bound": 20.67084121112744
          },
          "point_estimate": 10.012839195244457,
          "standard_error": 4.568884571870657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5519.801745078389,
            "upper_bound": 5538.169597021974
          },
          "point_estimate": 5530.509276635009,
          "standard_error": 4.7124171399358215
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.748490192054392,
            "upper_bound": 15.754191354206132
          },
          "point_estimate": 11.81367705430804,
          "standard_error": 2.5651780130254775
        }
      }
    },
    "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5918.754011592933,
            "upper_bound": 5932.468180657397
          },
          "point_estimate": 5925.823519473348,
          "standard_error": 3.5184474068947496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5917.57691054261,
            "upper_bound": 5937.465265330509
          },
          "point_estimate": 5927.098685665402,
          "standard_error": 4.532674784007495
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7460057275161478,
            "upper_bound": 20.11718089264488
          },
          "point_estimate": 14.248755303494896,
          "standard_error": 5.0351666301181774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5924.780228393148,
            "upper_bound": 5935.653990108776
          },
          "point_estimate": 5930.133947167384,
          "standard_error": 2.8055566079776955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.534567085029792,
            "upper_bound": 14.718119001375674
          },
          "point_estimate": 11.721989400323569,
          "standard_error": 2.115087751183573
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14172.885937418683,
            "upper_bound": 14188.694472363171
          },
          "point_estimate": 14181.458799982653,
          "standard_error": 4.054174888452906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14174.136774655217,
            "upper_bound": 14191.156030444963
          },
          "point_estimate": 14183.725930262815,
          "standard_error": 4.705975244517977
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9250914589877052,
            "upper_bound": 21.093614288537477
          },
          "point_estimate": 11.141461031707758,
          "standard_error": 4.72129470438595
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14182.8911889105,
            "upper_bound": 14193.198453695832
          },
          "point_estimate": 14189.26871559354,
          "standard_error": 2.599900658716777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.849134207038212,
            "upper_bound": 18.34247985152578
          },
          "point_estimate": 13.504627542522265,
          "standard_error": 3.462378990612364
        }
      }
    },
    "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.85504319903727,
            "upper_bound": 35.988318732941494
          },
          "point_estimate": 35.92805340879753,
          "standard_error": 0.034299187473774764
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.86380697475531,
            "upper_bound": 36.016891914822374
          },
          "point_estimate": 35.9545975407283,
          "standard_error": 0.03006251054539893
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034911209925429887,
            "upper_bound": 0.17284261163486772
          },
          "point_estimate": 0.07596881285841538,
          "standard_error": 0.04412796966566142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.80622886437081,
            "upper_bound": 35.9777306051684
          },
          "point_estimate": 35.907176925375786,
          "standard_error": 0.04577788336820461
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04126972625932858,
            "upper_bound": 0.15507470815008642
          },
          "point_estimate": 0.11463380057653694,
          "standard_error": 0.031109131472986665
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-haystack/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192444.0353968254,
            "upper_bound": 192946.73199395312
          },
          "point_estimate": 192720.29619614512,
          "standard_error": 129.76127314211715
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192470.51534391535,
            "upper_bound": 193000.8975812547
          },
          "point_estimate": 192863.3587962963,
          "standard_error": 125.58440300188984
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.217376761953496,
            "upper_bound": 633.0084137618686
          },
          "point_estimate": 280.3035112140866,
          "standard_error": 146.883684989314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192807.3329784968,
            "upper_bound": 192980.47888096853
          },
          "point_estimate": 192912.82729334157,
          "standard_error": 44.04242881396066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.3555595598349,
            "upper_bound": 585.3429146051225
          },
          "point_estimate": 432.7994209860902,
          "standard_error": 117.65597162903408
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23528773.291944444,
            "upper_bound": 23550785.23282837
          },
          "point_estimate": 23539778.825158738,
          "standard_error": 5625.805608310521
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23522304.875,
            "upper_bound": 23557909.5
          },
          "point_estimate": 23541159.11388889,
          "standard_error": 10150.435055592116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1216.1158659088385,
            "upper_bound": 30573.675103038855
          },
          "point_estimate": 27166.41738186654,
          "standard_error": 7749.540572779632
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23524286.428977273,
            "upper_bound": 23541304.63162112
          },
          "point_estimate": 23533629.812987015,
          "standard_error": 4322.353741623779
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11956.327925912548,
            "upper_bound": 22145.151555450073
          },
          "point_estimate": 18699.46268714244,
          "standard_error": 2590.497089347595
        }
      }
    },
    "memmem/krate/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81096661.97000001,
            "upper_bound": 81173127.5075
          },
          "point_estimate": 81133443.23333332,
          "standard_error": 19607.276787188777
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81072915.33333333,
            "upper_bound": 81177921.66666667
          },
          "point_estimate": 81136542.83333334,
          "standard_error": 26526.03546928548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5699.60849882645,
            "upper_bound": 111786.8025153949
          },
          "point_estimate": 78046.03941442055,
          "standard_error": 26123.480409305183
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35937.244265763155,
            "upper_bound": 83088.16052889789
          },
          "point_estimate": 65323.54669671964,
          "standard_error": 12309.947604615472
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.037975885641629,
            "upper_bound": 9.048629805239356
          },
          "point_estimate": 9.04359160158836,
          "standard_error": 0.0027346844946689887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.03638519907564,
            "upper_bound": 9.050886237761455
          },
          "point_estimate": 9.045614291042156,
          "standard_error": 0.0036709975744283383
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019856641834158674,
            "upper_bound": 0.015378700568029344
          },
          "point_estimate": 0.008191293854946033,
          "standard_error": 0.003416994641885302
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0397356265799,
            "upper_bound": 9.05012799999049
          },
          "point_estimate": 9.044695296398098,
          "standard_error": 0.002724173758170794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004425078201762205,
            "upper_bound": 0.011454107773396269
          },
          "point_estimate": 0.009107464566572163,
          "standard_error": 0.001799612017502921
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04388924766562,
            "upper_bound": 9.05368597302682
          },
          "point_estimate": 9.048012441620685,
          "standard_error": 0.002559881955524801
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.04194759652956,
            "upper_bound": 9.049856773117654
          },
          "point_estimate": 9.046768690848564,
          "standard_error": 0.002167223719453152
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012743661283338102,
            "upper_bound": 0.009304602023337638
          },
          "point_estimate": 0.005916112334653475,
          "standard_error": 0.002117458078384064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0445602951554,
            "upper_bound": 9.050731811677167
          },
          "point_estimate": 9.047664834567357,
          "standard_error": 0.001608927584112885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002746022178083561,
            "upper_bound": 0.012506887067745652
          },
          "point_estimate": 0.008516565694389548,
          "standard_error": 0.003062113011216483
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.046124499608354,
            "upper_bound": 9.06030434392814
          },
          "point_estimate": 9.053057967263982,
          "standard_error": 0.0036386509453378737
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.0419592644945,
            "upper_bound": 9.061689407959818
          },
          "point_estimate": 9.052210680675808,
          "standard_error": 0.005422063286775604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002456333662055136,
            "upper_bound": 0.020787440847834578
          },
          "point_estimate": 0.01394740529371993,
          "standard_error": 0.004504731166742468
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.047474903645648,
            "upper_bound": 9.062220069852746
          },
          "point_estimate": 9.055026169008231,
          "standard_error": 0.003715712079262134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006976736243528544,
            "upper_bound": 0.015569360381629014
          },
          "point_estimate": 0.012166510750052135,
          "standard_error": 0.002263825949277053
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.797399781216937,
            "upper_bound": 8.811996282780605
          },
          "point_estimate": 8.804677571847533,
          "standard_error": 0.0037252120297331993
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.793953533543595,
            "upper_bound": 8.812735527083582
          },
          "point_estimate": 8.80680058732552,
          "standard_error": 0.005153865700440846
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003071349867898649,
            "upper_bound": 0.02203978417994995
          },
          "point_estimate": 0.012235359563161876,
          "standard_error": 0.004781646949270736
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.798368961211692,
            "upper_bound": 8.81100425280614
          },
          "point_estimate": 8.805137843167692,
          "standard_error": 0.0031978480098173705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007056643767259181,
            "upper_bound": 0.01583304033457686
          },
          "point_estimate": 0.012391888744486351,
          "standard_error": 0.002256870766731666
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.537890564059566,
            "upper_bound": 9.55308134305754
          },
          "point_estimate": 9.545840935512787,
          "standard_error": 0.003877924863218366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.539582816833349,
            "upper_bound": 9.554828093047036
          },
          "point_estimate": 9.544797847158025,
          "standard_error": 0.0035128721084502364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009222186023127184,
            "upper_bound": 0.02195066068344423
          },
          "point_estimate": 0.009946192099296147,
          "standard_error": 0.005440411205365859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.542552710677407,
            "upper_bound": 9.550640488136962
          },
          "point_estimate": 9.546034987524871,
          "standard_error": 0.002052445493574329
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005665595662610821,
            "upper_bound": 0.01754143129442939
          },
          "point_estimate": 0.012891082694518536,
          "standard_error": 0.003136322747841554
        }
      }
    },
    "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.323898432574026,
            "upper_bound": 10.34234905418202
          },
          "point_estimate": 10.333054139624403,
          "standard_error": 0.004724165707464928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.320677002415728,
            "upper_bound": 10.347788819219977
          },
          "point_estimate": 10.332001375054308,
          "standard_error": 0.007613259697189686
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037166076955103145,
            "upper_bound": 0.025612541627179724
          },
          "point_estimate": 0.01632577361703614,
          "standard_error": 0.005926269774163287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.325317801234112,
            "upper_bound": 10.341214702438204
          },
          "point_estimate": 10.333890493518547,
          "standard_error": 0.003996592402740266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009526865498927268,
            "upper_bound": 0.019355456985707113
          },
          "point_estimate": 0.015727515578249963,
          "standard_error": 0.00250742493519413
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.073114946742352,
            "upper_bound": 8.084554888920481
          },
          "point_estimate": 8.078701733753629,
          "standard_error": 0.002935749911513415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.069030854887291,
            "upper_bound": 8.08720271679091
          },
          "point_estimate": 8.078580936063254,
          "standard_error": 0.0046695272275209905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0027953726280106543,
            "upper_bound": 0.016859258623628106
          },
          "point_estimate": 0.012947618302095534,
          "standard_error": 0.003638109270825443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.073181579818597,
            "upper_bound": 8.084002429900018
          },
          "point_estimate": 8.07896702145447,
          "standard_error": 0.002749419401586165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005882414021773914,
            "upper_bound": 0.012111420824143938
          },
          "point_estimate": 0.00976376480935308,
          "standard_error": 0.0016165862430219365
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.534987732331322,
            "upper_bound": 9.54679168245069
          },
          "point_estimate": 9.540593099191412,
          "standard_error": 0.0030067899736270004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.535232981739536,
            "upper_bound": 9.544497291552888
          },
          "point_estimate": 9.5403580271311,
          "standard_error": 0.002782784587640806
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013205944710947284,
            "upper_bound": 0.01551281845734856
          },
          "point_estimate": 0.00648589475562883,
          "standard_error": 0.0033647791079491666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.536044550082618,
            "upper_bound": 9.549529799961128
          },
          "point_estimate": 9.54160608231378,
          "standard_error": 0.0034775410638007835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003568964093608427,
            "upper_bound": 0.014238300779812764
          },
          "point_estimate": 0.010021220011667747,
          "standard_error": 0.0028295421782188177
        }
      }
    },
    "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.473804578612825,
            "upper_bound": 12.489127638050109
          },
          "point_estimate": 12.481303306139615,
          "standard_error": 0.003930958364585175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.470340769281467,
            "upper_bound": 12.492444498149023
          },
          "point_estimate": 12.480934537843224,
          "standard_error": 0.005055626673326131
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003123623626169717,
            "upper_bound": 0.024569054329665753
          },
          "point_estimate": 0.013370499317046862,
          "standard_error": 0.005676163744140871
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.477277957025972,
            "upper_bound": 12.486378405162336
          },
          "point_estimate": 12.481658946177788,
          "standard_error": 0.0022946195584790463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007249063788201018,
            "upper_bound": 0.016235360065890762
          },
          "point_estimate": 0.013110228504243943,
          "standard_error": 0.0022996402782782476
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044376586266155,
            "upper_bound": 9.052266227744031
          },
          "point_estimate": 9.048471462781215,
          "standard_error": 0.0020151552498557687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.043920882424878,
            "upper_bound": 9.05262147510783
          },
          "point_estimate": 9.049994921814616,
          "standard_error": 0.002278098334497999
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009768678213884264,
            "upper_bound": 0.011284330094466551
          },
          "point_estimate": 0.004068444955045252,
          "standard_error": 0.0028239235402221356
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045266715410548,
            "upper_bound": 9.055668874904317
          },
          "point_estimate": 9.05075106791817,
          "standard_error": 0.0027491083967426146
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003190551639979424,
            "upper_bound": 0.008768639131752859
          },
          "point_estimate": 0.006705018339502191,
          "standard_error": 0.0014242744744461923
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.173455053194957,
            "upper_bound": 10.18818087298378
          },
          "point_estimate": 10.180997791284115,
          "standard_error": 0.0037819947212655186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.167240902572088,
            "upper_bound": 10.193187487415512
          },
          "point_estimate": 10.18452793813944,
          "standard_error": 0.00635285817050365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002563562547977404,
            "upper_bound": 0.02036471262239222
          },
          "point_estimate": 0.013049900045892284,
          "standard_error": 0.00475219341123612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.174751488549724,
            "upper_bound": 10.1880629675569
          },
          "point_estimate": 10.182318569061486,
          "standard_error": 0.0033216245489829993
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00751557832920648,
            "upper_bound": 0.01499878080037606
          },
          "point_estimate": 0.012629664466195026,
          "standard_error": 0.001893052082473693
        }
      }
    },
    "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.52336877584447,
            "upper_bound": 19.557373769877984
          },
          "point_estimate": 19.540835972467647,
          "standard_error": 0.00871716343456947
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.519328334786728,
            "upper_bound": 19.562020845459863
          },
          "point_estimate": 19.54055091853764,
          "standard_error": 0.011548416822568693
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010114629919212427,
            "upper_bound": 0.04851482094558733
          },
          "point_estimate": 0.026380222765412395,
          "standard_error": 0.011714104915392078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.511678730608804,
            "upper_bound": 19.55160029033015
          },
          "point_estimate": 19.53270639401258,
          "standard_error": 0.010224772996221744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01533307756770484,
            "upper_bound": 0.03754712470238138
          },
          "point_estimate": 0.02910272576216969,
          "standard_error": 0.005716780763412014
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98407.98668989546,
            "upper_bound": 98533.23287477416
          },
          "point_estimate": 98467.50246676987,
          "standard_error": 32.150457679299535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98373.22384823849,
            "upper_bound": 98557.41237579045
          },
          "point_estimate": 98436.42994579946,
          "standard_error": 50.508076814217915
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.18745087793661,
            "upper_bound": 173.94985890364782
          },
          "point_estimate": 99.74043522112692,
          "standard_error": 41.2631995259158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98413.03953371193,
            "upper_bound": 98583.52544003256
          },
          "point_estimate": 98503.92941963184,
          "standard_error": 43.18247442823825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.33371142935384,
            "upper_bound": 132.24719704266383
          },
          "point_estimate": 107.27768581010302,
          "standard_error": 18.985953457300855
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52944.3152186589,
            "upper_bound": 52994.20705534731
          },
          "point_estimate": 52970.14881866352,
          "standard_error": 12.75476146225324
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52943.87730806608,
            "upper_bound": 53002.09843815077
          },
          "point_estimate": 52974.68019112407,
          "standard_error": 14.665460260810429
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.41158161936501,
            "upper_bound": 72.18834434522311
          },
          "point_estimate": 33.526858792535556,
          "standard_error": 16.03647638335925
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52960.36269555843,
            "upper_bound": 53000.14372225611
          },
          "point_estimate": 52979.06680171141,
          "standard_error": 10.405117339078178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.613906552365787,
            "upper_bound": 55.87880060526468
          },
          "point_estimate": 42.55845630637632,
          "standard_error": 8.9206394402663
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144884.5116854803,
            "upper_bound": 145162.2839053508
          },
          "point_estimate": 145018.50139331564,
          "standard_error": 70.86502161187339
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144910.44057104914,
            "upper_bound": 145113.78461155377
          },
          "point_estimate": 144986.8185258964,
          "standard_error": 47.575332596902975
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.859519988642369,
            "upper_bound": 357.3569653023973
          },
          "point_estimate": 121.33258709226602,
          "standard_error": 82.88916947301797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144953.8923179873,
            "upper_bound": 145082.12232551276
          },
          "point_estimate": 145022.4037874476,
          "standard_error": 32.62561380030877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.89311548997372,
            "upper_bound": 332.57471219870035
          },
          "point_estimate": 236.25793165222333,
          "standard_error": 68.02138709656379
        }
      }
    },
    "memmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397157.8556219807,
            "upper_bound": 397859.78845939017
          },
          "point_estimate": 397478.8781064528,
          "standard_error": 180.71451639364005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397038.67119565216,
            "upper_bound": 397835.89782608696
          },
          "point_estimate": 397209.2910628019,
          "standard_error": 204.434692998022
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.03280362659377,
            "upper_bound": 910.3117027517102
          },
          "point_estimate": 285.9468007929613,
          "standard_error": 220.5095988464387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397129.1230062129,
            "upper_bound": 397493.5632411067
          },
          "point_estimate": 397307.1934217956,
          "standard_error": 91.63121817368418
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.82281701050198,
            "upper_bound": 789.1567527746171
          },
          "point_estimate": 605.0700242290012,
          "standard_error": 152.63058625683803
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572735.4991364088,
            "upper_bound": 574253.7871176525
          },
          "point_estimate": 573340.1294295635,
          "standard_error": 410.81815042503905
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572760.8078125,
            "upper_bound": 573349.98046875
          },
          "point_estimate": 572993.4815228174,
          "standard_error": 186.70295429503432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 101.19845187527784,
            "upper_bound": 968.9109355327672
          },
          "point_estimate": 436.75368232416264,
          "standard_error": 232.17152641335997
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572883.7000807481,
            "upper_bound": 573343.1103515625
          },
          "point_estimate": 573061.8093344156,
          "standard_error": 118.79392199657254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.05012137854695,
            "upper_bound": 2072.9968834203946
          },
          "point_estimate": 1365.274889080423,
          "standard_error": 595.7393453148285
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40683.6892281969,
            "upper_bound": 40722.01628027465
          },
          "point_estimate": 40702.61573738184,
          "standard_error": 9.841152571918302
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40676.39719101124,
            "upper_bound": 40735.96269662921
          },
          "point_estimate": 40696.198651685394,
          "standard_error": 15.428408129888137
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.622956925564415,
            "upper_bound": 56.43010304311226
          },
          "point_estimate": 38.52851153508194,
          "standard_error": 12.5778801732685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40676.2340848552,
            "upper_bound": 40726.25221325692
          },
          "point_estimate": 40699.270987888514,
          "standard_error": 13.074459724395592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.06414291566977,
            "upper_bound": 39.2195722921025
          },
          "point_estimate": 32.80576649697265,
          "standard_error": 4.848203002977271
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97601.6863011192,
            "upper_bound": 97738.0260493851
          },
          "point_estimate": 97669.32073513768,
          "standard_error": 34.80843581151315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97590.81888591003,
            "upper_bound": 97782.700178731
          },
          "point_estimate": 97651.92823630793,
          "standard_error": 43.91117421301914
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.98546797827734,
            "upper_bound": 221.71219820859275
          },
          "point_estimate": 94.22304381871234,
          "standard_error": 52.90746195917038
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97597.71291500443,
            "upper_bound": 97687.29163881186
          },
          "point_estimate": 97630.8376310017,
          "standard_error": 23.0476554682937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.93534311110012,
            "upper_bound": 147.33206653875416
          },
          "point_estimate": 115.68979397500496,
          "standard_error": 21.565324976846963
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68853.7023801994,
            "upper_bound": 68979.41789096527
          },
          "point_estimate": 68915.69052686666,
          "standard_error": 32.12607741194034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68856.66386554622,
            "upper_bound": 68985.02922201139
          },
          "point_estimate": 68901.47224857684,
          "standard_error": 31.955475043903498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.28230421727855,
            "upper_bound": 189.4062258973371
          },
          "point_estimate": 81.45125986008114,
          "standard_error": 49.2314414819764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68863.62970441632,
            "upper_bound": 68984.77886851931
          },
          "point_estimate": 68914.47994282757,
          "standard_error": 31.24320951636341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.31287089852477,
            "upper_bound": 139.95876694967907
          },
          "point_estimate": 106.51389751012692,
          "standard_error": 22.521409301514083
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307586.4372891445,
            "upper_bound": 308315.96900360676
          },
          "point_estimate": 307918.8191394269,
          "standard_error": 187.0151550353593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307383.3177966102,
            "upper_bound": 308277.45338983054
          },
          "point_estimate": 307862.240960452,
          "standard_error": 167.67010749092282
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.809869825799725,
            "upper_bound": 918.5593674635594
          },
          "point_estimate": 416.19737320427896,
          "standard_error": 289.12893162062653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307517.44099670014,
            "upper_bound": 308287.8460369528
          },
          "point_estimate": 307906.9313229144,
          "standard_error": 197.66838531444023
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247.833753911862,
            "upper_bound": 838.9594917416678
          },
          "point_estimate": 624.4422758959917,
          "standard_error": 162.56855264407105
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35141.629159420285,
            "upper_bound": 35188.18911042098
          },
          "point_estimate": 35164.02904976612,
          "standard_error": 11.91562535315843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35128.13661835749,
            "upper_bound": 35187.142167011734
          },
          "point_estimate": 35163.887740203965,
          "standard_error": 16.29625687920428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.920557584014189,
            "upper_bound": 71.3775792883498
          },
          "point_estimate": 43.3929227046762,
          "standard_error": 15.09612013057316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35152.391700141125,
            "upper_bound": 35204.90407053573
          },
          "point_estimate": 35175.96605809649,
          "standard_error": 13.632784719895556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.586055549347847,
            "upper_bound": 51.33701724121933
          },
          "point_estimate": 39.561468558700135,
          "standard_error": 8.051414363744916
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65062.95389688249,
            "upper_bound": 65164.48030610869
          },
          "point_estimate": 65114.20050038541,
          "standard_error": 26.18578912365587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65042.704766187046,
            "upper_bound": 65196.0457348407
          },
          "point_estimate": 65110.727930155874,
          "standard_error": 46.35676669383597
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.037680182204417,
            "upper_bound": 151.00920703128236
          },
          "point_estimate": 110.2795153982654,
          "standard_error": 34.36153006163883
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65110.73948840927,
            "upper_bound": 65190.67865159927
          },
          "point_estimate": 65161.09444548257,
          "standard_error": 20.293092859976326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.886139586938775,
            "upper_bound": 105.13101227150308
          },
          "point_estimate": 87.19024583530225,
          "standard_error": 12.743801256143968
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169478.85910401904,
            "upper_bound": 169803.51776664823
          },
          "point_estimate": 169635.5692290513,
          "standard_error": 83.30276205887958
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169434.53023255814,
            "upper_bound": 169877.77873754152
          },
          "point_estimate": 169530.41262919898,
          "standard_error": 138.8543115988737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.50579645468523,
            "upper_bound": 436.38113029533406
          },
          "point_estimate": 297.63170527801174,
          "standard_error": 118.22187857250611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169498.5455311563,
            "upper_bound": 169825.57301587303
          },
          "point_estimate": 169669.27499848988,
          "standard_error": 85.2008138388164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162.8512765730892,
            "upper_bound": 345.33604826061253
          },
          "point_estimate": 278.41291918857087,
          "standard_error": 47.41563472299409
        }
      }
    },
    "memmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34449.834140826,
            "upper_bound": 34485.263660587334
          },
          "point_estimate": 34466.464557774765,
          "standard_error": 9.083094723461125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34448.430568720374,
            "upper_bound": 34482.98887440759
          },
          "point_estimate": 34462.57708192282,
          "standard_error": 6.776054678999135
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.4119122380021993,
            "upper_bound": 51.20541955538207
          },
          "point_estimate": 10.86180144223736,
          "standard_error": 12.725461176667908
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34462.302583014585,
            "upper_bound": 34488.4067965044
          },
          "point_estimate": 34475.05500092325,
          "standard_error": 7.185406479565972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.409103541619483,
            "upper_bound": 41.52534555634774
          },
          "point_estimate": 30.380363637628204,
          "standard_error": 7.8531955556614115
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11254.71815433713,
            "upper_bound": 11294.488952174104
          },
          "point_estimate": 11274.29511188186,
          "standard_error": 10.180469823981182
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11243.271796462925,
            "upper_bound": 11290.684539507703
          },
          "point_estimate": 11283.045252869995,
          "standard_error": 13.319897808316895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7423844455721764,
            "upper_bound": 68.65893551314178
          },
          "point_estimate": 21.642820621038386,
          "standard_error": 16.72548937166887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11270.82559611094,
            "upper_bound": 11314.163472172944
          },
          "point_estimate": 11291.514250254864,
          "standard_error": 11.54542068698
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.869663978343343,
            "upper_bound": 44.79140218940867
          },
          "point_estimate": 33.900614065353395,
          "standard_error": 7.072056927016543
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498686.4312071917,
            "upper_bound": 499247.5481406964
          },
          "point_estimate": 498967.29505490325,
          "standard_error": 142.7451896608781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498608.0447488584,
            "upper_bound": 499336.9608610567
          },
          "point_estimate": 498953.0657343988,
          "standard_error": 162.16867794975624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.27318791146728,
            "upper_bound": 929.263537611852
          },
          "point_estimate": 443.87766609219045,
          "standard_error": 236.8377649847614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498737.4990546016,
            "upper_bound": 499179.3870437553
          },
          "point_estimate": 498950.72520903754,
          "standard_error": 109.52346837458433
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265.6926777725473,
            "upper_bound": 599.2398510381439
          },
          "point_estimate": 476.76819720052623,
          "standard_error": 86.5241767787611
        }
      }
    },
    "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971.4551412947544,
            "upper_bound": 973.5086258997932
          },
          "point_estimate": 972.3765220824808,
          "standard_error": 0.5301021619662803
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971.1858173419572,
            "upper_bound": 973.1863262597108
          },
          "point_estimate": 972.0413521844488,
          "standard_error": 0.4820021687371345
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3722013027250696,
            "upper_bound": 2.445380851390107
          },
          "point_estimate": 0.967389614533874,
          "standard_error": 0.5398721246708945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971.1401484671684,
            "upper_bound": 972.4360148608736
          },
          "point_estimate": 971.7320684393648,
          "standard_error": 0.32227005724230556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6707325010789416,
            "upper_bound": 2.4882731880490847
          },
          "point_estimate": 1.7720582314767994,
          "standard_error": 0.5276033654718754
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47301.3639349263,
            "upper_bound": 47371.44489637645
          },
          "point_estimate": 47334.36760320557,
          "standard_error": 17.99181651840125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47289.14986996099,
            "upper_bound": 47405.37525543377
          },
          "point_estimate": 47305.84228254587,
          "standard_error": 27.472069983439344
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9150304562309888,
            "upper_bound": 94.21076941
          },
          "point_estimate": 38.202976759996574,
          "standard_error": 23.518752937408415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47296.50335077899,
            "upper_bound": 47382.42670332907
          },
          "point_estimate": 47341.63726884299,
          "standard_error": 22.88990587216773
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.568497454132572,
            "upper_bound": 71.8309525572421
          },
          "point_estimate": 60.10479761879243,
          "standard_error": 11.0168947932579
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48895.60754416283,
            "upper_bound": 48972.00326612904
          },
          "point_estimate": 48929.1582718894,
          "standard_error": 19.697670832900144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48884.23286290323,
            "upper_bound": 48949.34038978495
          },
          "point_estimate": 48924.59375,
          "standard_error": 18.304560740139927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2683802194225693,
            "upper_bound": 83.03934844510538
          },
          "point_estimate": 45.271907583360374,
          "standard_error": 19.26493034764551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48896.78170859136,
            "upper_bound": 49030.6255710737
          },
          "point_estimate": 48964.440423823486,
          "standard_error": 36.47686453718723
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.151927059047832,
            "upper_bound": 94.21741001423636
          },
          "point_estimate": 65.6011462942235,
          "standard_error": 21.170739262147503
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48852.55261776581,
            "upper_bound": 48931.43955413221
          },
          "point_estimate": 48892.31424229315,
          "standard_error": 20.25507287328614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48842.28936742934,
            "upper_bound": 48945.34259437288
          },
          "point_estimate": 48889.28573351279,
          "standard_error": 31.437770304898983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.756262474488564,
            "upper_bound": 113.58212366318976
          },
          "point_estimate": 75.78203865458661,
          "standard_error": 25.56842305372411
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48882.2570983005,
            "upper_bound": 48968.343937648824
          },
          "point_estimate": 48932.67402772194,
          "standard_error": 22.27210761471941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.871477724698195,
            "upper_bound": 85.26140783353657
          },
          "point_estimate": 67.68525881615888,
          "standard_error": 11.675180224519693
        }
      }
    },
    "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13960.834758080517,
            "upper_bound": 13994.371306963643
          },
          "point_estimate": 13976.950060346717,
          "standard_error": 8.54826120289977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13951.884595128373,
            "upper_bound": 13998.701740911418
          },
          "point_estimate": 13976.855113927291,
          "standard_error": 10.807881106119618
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.810920783931393,
            "upper_bound": 49.122179101025765
          },
          "point_estimate": 29.90481674058792,
          "standard_error": 10.720486546132689
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13958.448333726146,
            "upper_bound": 13982.411197033094
          },
          "point_estimate": 13971.12911804018,
          "standard_error": 6.053379577160361
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.19081168632912,
            "upper_bound": 37.124600397654405
          },
          "point_estimate": 28.527290420728953,
          "standard_error": 5.815364740755264
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25733.92645184136,
            "upper_bound": 25768.487529846218
          },
          "point_estimate": 25753.06972177256,
          "standard_error": 8.931300239192055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25738.996901558072,
            "upper_bound": 25772.215509915015
          },
          "point_estimate": 25762.44487302711,
          "standard_error": 7.027991768952679
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4605962063161235,
            "upper_bound": 40.19714928635798
          },
          "point_estimate": 14.252332246972664,
          "standard_error": 10.519205534235226
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25739.145654677628,
            "upper_bound": 25769.60657387277
          },
          "point_estimate": 25755.70517273095,
          "standard_error": 7.943178173797943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.304204433415935,
            "upper_bound": 40.63390000695475
          },
          "point_estimate": 29.843086386911104,
          "standard_error": 8.736461357155168
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17481.66564829846,
            "upper_bound": 17495.36092113501
          },
          "point_estimate": 17488.836286695376,
          "standard_error": 3.5153802123011793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17479.70132618704,
            "upper_bound": 17497.87518037518
          },
          "point_estimate": 17490.109658088826,
          "standard_error": 4.146109958769491
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.039025623169434,
            "upper_bound": 21.473294680500963
          },
          "point_estimate": 9.497621324086907,
          "standard_error": 4.870634452393749
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17482.135579783728,
            "upper_bound": 17496.346878666263
          },
          "point_estimate": 17489.71394464122,
          "standard_error": 3.765968347322852
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.732875966812993,
            "upper_bound": 14.858287718620009
          },
          "point_estimate": 11.722117383368394,
          "standard_error": 2.355426613949018
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17481.77101385407,
            "upper_bound": 17535.444377246928
          },
          "point_estimate": 17506.510777720545,
          "standard_error": 13.828217609677573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17472.308125,
            "upper_bound": 17552.856009615385
          },
          "point_estimate": 17486.09016826923,
          "standard_error": 18.70702477870416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.1945905442464206,
            "upper_bound": 68.72670584716484
          },
          "point_estimate": 21.59870966141833,
          "standard_error": 16.76481005129097
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17474.986745912443,
            "upper_bound": 17493.46055450779
          },
          "point_estimate": 17481.360216033965,
          "standard_error": 4.825757063902427
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.341575076221952,
            "upper_bound": 58.21153326945986
          },
          "point_estimate": 45.83918261542577,
          "standard_error": 10.553105827947864
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17314.540284272698,
            "upper_bound": 17328.869350646517
          },
          "point_estimate": 17322.543036214804,
          "standard_error": 3.714756965343398
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17318.61957269025,
            "upper_bound": 17329.57187053784
          },
          "point_estimate": 17325.795614333314,
          "standard_error": 2.69736894087456
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1449145679674877,
            "upper_bound": 16.19809273480117
          },
          "point_estimate": 6.6202060144701855,
          "standard_error": 3.939048348667939
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17304.422257170852,
            "upper_bound": 17327.54489801095
          },
          "point_estimate": 17317.207750174624,
          "standard_error": 6.894671254805222
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.061803118303083,
            "upper_bound": 17.7360404262279
          },
          "point_estimate": 12.396404855362931,
          "standard_error": 3.99719652180759
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95802.56463218984,
            "upper_bound": 95971.22088596493
          },
          "point_estimate": 95889.38836340854,
          "standard_error": 43.19078712974836
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95789.69863721804,
            "upper_bound": 96024.35877192982
          },
          "point_estimate": 95894.9259868421,
          "standard_error": 48.732891550772905
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.65551509904673,
            "upper_bound": 263.667725187388
          },
          "point_estimate": 146.34731334917808,
          "standard_error": 74.02289699094267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95803.92051870568,
            "upper_bound": 95949.978824508
          },
          "point_estimate": 95873.59345181136,
          "standard_error": 36.69017090794341
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.64303481970103,
            "upper_bound": 181.88919049231865
          },
          "point_estimate": 144.10241528914992,
          "standard_error": 27.452714275030925
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104272.93229703917,
            "upper_bound": 104494.82867990177
          },
          "point_estimate": 104378.7744529722,
          "standard_error": 56.8434999981861
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104232.45272206304,
            "upper_bound": 104536.15200573066
          },
          "point_estimate": 104337.86057670442,
          "standard_error": 61.158520175999726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.00125262319535,
            "upper_bound": 353.5839035984635
          },
          "point_estimate": 130.73434336189771,
          "standard_error": 86.46956078461841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104284.39992386314,
            "upper_bound": 104429.52552127912
          },
          "point_estimate": 104355.05235738472,
          "standard_error": 36.63580649494874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.66189464692104,
            "upper_bound": 240.4956369315836
          },
          "point_estimate": 189.31673687731077,
          "standard_error": 38.27014094406388
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18402.39280814215,
            "upper_bound": 18433.90445943987
          },
          "point_estimate": 18418.842155950773,
          "standard_error": 8.064569833496826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18404.3106865722,
            "upper_bound": 18430.83071862348
          },
          "point_estimate": 18421.432433648224,
          "standard_error": 5.431238347026276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.728241045566372,
            "upper_bound": 45.85859957383645
          },
          "point_estimate": 9.994186676480869,
          "standard_error": 11.237254707109065
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18411.33818767937,
            "upper_bound": 18430.030080000302
          },
          "point_estimate": 18420.780841526896,
          "standard_error": 4.649854059604553
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.8883421616615035,
            "upper_bound": 37.31529571609941
          },
          "point_estimate": 26.91785952973187,
          "standard_error": 7.176434998436923
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18269.65197870557,
            "upper_bound": 18330.868025465297
          },
          "point_estimate": 18299.368706405097,
          "standard_error": 15.516918320361086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18273.743293091888,
            "upper_bound": 18319.575737759893
          },
          "point_estimate": 18295.440077967807,
          "standard_error": 10.615877455972347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.0317789778654087,
            "upper_bound": 82.33414114391121
          },
          "point_estimate": 26.851491424699145,
          "standard_error": 21.66209558451569
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18288.215929545506,
            "upper_bound": 18311.572128539727
          },
          "point_estimate": 18297.65740285871,
          "standard_error": 5.898326135110401
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.407096333571427,
            "upper_bound": 72.12905676378804
          },
          "point_estimate": 51.784713661980234,
          "standard_error": 13.611940285083277
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17351.637756806776,
            "upper_bound": 17371.661393771145
          },
          "point_estimate": 17362.28185058398,
          "standard_error": 5.11234924341611
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17354.13289818789,
            "upper_bound": 17373.98712446352
          },
          "point_estimate": 17361.765498330948,
          "standard_error": 4.795994116281142
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6141265627703161,
            "upper_bound": 27.69274850692458
          },
          "point_estimate": 11.93319368577525,
          "standard_error": 6.52683963654496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17361.02202237759,
            "upper_bound": 17375.39192903025
          },
          "point_estimate": 17368.278064520124,
          "standard_error": 3.605094658566601
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.319977443025796,
            "upper_bound": 23.35399767928142
          },
          "point_estimate": 17.03902369393589,
          "standard_error": 4.310673186317894
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17706.794798692175,
            "upper_bound": 17725.679426998893
          },
          "point_estimate": 17715.983811743405,
          "standard_error": 4.8433102572885565
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17703.80131322957,
            "upper_bound": 17728.452780479896
          },
          "point_estimate": 17716.094652507567,
          "standard_error": 6.028260835312747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.025072019900386,
            "upper_bound": 27.11996966443492
          },
          "point_estimate": 14.921577718634255,
          "standard_error": 6.444871503073239
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17706.965754139746,
            "upper_bound": 17718.46204823914
          },
          "point_estimate": 17712.805348931226,
          "standard_error": 2.876904231402292
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.55125940867024,
            "upper_bound": 20.618118318777253
          },
          "point_estimate": 16.126715994606663,
          "standard_error": 3.1039142963418307
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17707.785384552233,
            "upper_bound": 17720.708654152742
          },
          "point_estimate": 17714.58016720041,
          "standard_error": 3.3179911319227164
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17709.363072031934,
            "upper_bound": 17722.873477095516
          },
          "point_estimate": 17715.652412280702,
          "standard_error": 3.069558457503699
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3990592222382985,
            "upper_bound": 18.437549209706937
          },
          "point_estimate": 8.033567664392267,
          "standard_error": 4.686517991296352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17704.78908960254,
            "upper_bound": 17718.10875273482
          },
          "point_estimate": 17712.23933318144,
          "standard_error": 3.441866604810184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.062912884911364,
            "upper_bound": 14.928646407354938
          },
          "point_estimate": 11.061707523296286,
          "standard_error": 2.636606736741195
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17479.4549633328,
            "upper_bound": 17502.03296960218
          },
          "point_estimate": 17490.789995321356,
          "standard_error": 5.794109881732966
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17474.759955486043,
            "upper_bound": 17511.039781841515
          },
          "point_estimate": 17486.855931183833,
          "standard_error": 11.497968182349087
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6763875704927222,
            "upper_bound": 31.645937690094463
          },
          "point_estimate": 27.02248927001465,
          "standard_error": 7.996737171175494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17475.99401270992,
            "upper_bound": 17496.782978769254
          },
          "point_estimate": 17482.851539317275,
          "standard_error": 5.327305558579424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.855167766943792,
            "upper_bound": 22.4474941599604
          },
          "point_estimate": 19.32070768221072,
          "standard_error": 2.4662541689334976
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18041.395497392627,
            "upper_bound": 18073.24433468633
          },
          "point_estimate": 18054.551996848273,
          "standard_error": 8.428225390682558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18037.214649602385,
            "upper_bound": 18060.757731389444
          },
          "point_estimate": 18047.483562908263,
          "standard_error": 6.472884545637139
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.550307284855559,
            "upper_bound": 26.264040949812543
          },
          "point_estimate": 12.35227596112388,
          "standard_error": 6.874055622719283
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18039.29406365587,
            "upper_bound": 18057.1344032616
          },
          "point_estimate": 18047.343528439753,
          "standard_error": 4.6295301693061734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.332377269802776,
            "upper_bound": 41.77341030833635
          },
          "point_estimate": 28.08016631148064,
          "standard_error": 10.874246256240763
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18131.04677157726,
            "upper_bound": 18177.26676848293
          },
          "point_estimate": 18151.02204969116,
          "standard_error": 11.99757359870642
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18123.762082710513,
            "upper_bound": 18162.068323368218
          },
          "point_estimate": 18149.424101359527,
          "standard_error": 10.319101984980236
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.72344331985848,
            "upper_bound": 47.576118517589826
          },
          "point_estimate": 32.6488035027447,
          "standard_error": 12.30720664777185
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18127.91744260328,
            "upper_bound": 18149.44378609477
          },
          "point_estimate": 18135.042799552215,
          "standard_error": 5.215035581778762
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.764627376561156,
            "upper_bound": 57.914771310691414
          },
          "point_estimate": 39.91980223815284,
          "standard_error": 13.472390622236048
        }
      }
    },
    "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18242.30227004068,
            "upper_bound": 18269.768534745155
          },
          "point_estimate": 18256.22377339076,
          "standard_error": 7.003714227960189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18245.226264656616,
            "upper_bound": 18271.324087102177
          },
          "point_estimate": 18254.56195561139,
          "standard_error": 5.703518715572208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.88818297636233,
            "upper_bound": 40.19866809286349
          },
          "point_estimate": 11.53922832528874,
          "standard_error": 9.520231002680312
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18251.732555022743,
            "upper_bound": 18274.20870762481
          },
          "point_estimate": 18261.46671408993,
          "standard_error": 5.7286990196246546
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.872064072065038,
            "upper_bound": 32.04970611157271
          },
          "point_estimate": 23.293632436967524,
          "standard_error": 5.720843186329155
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58192.91734686667,
            "upper_bound": 58314.52167300001
          },
          "point_estimate": 58243.655781333335,
          "standard_error": 32.179990933635395
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58179.3614,
            "upper_bound": 58268.323866666666
          },
          "point_estimate": 58209.9082,
          "standard_error": 25.424287666642748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.802034101485637,
            "upper_bound": 104.46952918529064
          },
          "point_estimate": 47.92949194907801,
          "standard_error": 27.55240207253056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58193.018347003155,
            "upper_bound": 58280.28173040826
          },
          "point_estimate": 58234.756417662335,
          "standard_error": 22.151731963036298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.868113028797435,
            "upper_bound": 158.6907104571919
          },
          "point_estimate": 107.4896034691627,
          "standard_error": 40.59234288605252
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881588.4186904762,
            "upper_bound": 883241.5501785714
          },
          "point_estimate": 882290.8644075964,
          "standard_error": 429.28330843785454
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881166.5654761905,
            "upper_bound": 882651.537202381
          },
          "point_estimate": 882104.1809523809,
          "standard_error": 366.51314344148415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.00660802302077,
            "upper_bound": 1618.791783760635
          },
          "point_estimate": 1060.1375236786027,
          "standard_error": 433.8227611202428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 881692.8023851615,
            "upper_bound": 882716.8027897764
          },
          "point_estimate": 882212.9115646258,
          "standard_error": 259.8944847707015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473.495326360644,
            "upper_bound": 2078.0476578867133
          },
          "point_estimate": 1429.5168605497342,
          "standard_error": 493.2576128473487
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208286.26062237527,
            "upper_bound": 208663.30502653055
          },
          "point_estimate": 208479.2970011338,
          "standard_error": 96.63978186765203
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208221.33333333337,
            "upper_bound": 208676.66857142857
          },
          "point_estimate": 208525.0248469388,
          "standard_error": 113.2574761689644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.839906297406685,
            "upper_bound": 543.6542548815191
          },
          "point_estimate": 237.2198233170552,
          "standard_error": 130.15572829612722
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208207.703588624,
            "upper_bound": 208551.912755529
          },
          "point_estimate": 208375.36424489797,
          "standard_error": 87.85295161525575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.44630173163796,
            "upper_bound": 425.0006119368465
          },
          "point_estimate": 322.4186815082101,
          "standard_error": 70.60852308810661
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5617.503687868236,
            "upper_bound": 5624.324098786841
          },
          "point_estimate": 5620.953903379567,
          "standard_error": 1.7499040397580587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5615.264733107066,
            "upper_bound": 5626.065669032192
          },
          "point_estimate": 5622.5905353286025,
          "standard_error": 3.040440673153521
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9499893135879744,
            "upper_bound": 9.78070294267995
          },
          "point_estimate": 7.636936621976572,
          "standard_error": 2.322834672690203
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5618.150196212204,
            "upper_bound": 5625.871272403471
          },
          "point_estimate": 5622.081711192232,
          "standard_error": 1.9653605564507368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.701247995729739,
            "upper_bound": 6.981398470372719
          },
          "point_estimate": 5.841244953545288,
          "standard_error": 0.8364037572345964
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6036.6403360297945,
            "upper_bound": 6052.934169773332
          },
          "point_estimate": 6044.676336646581,
          "standard_error": 4.14519035793786
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6036.795859625817,
            "upper_bound": 6052.289577106167
          },
          "point_estimate": 6043.8976787493475,
          "standard_error": 3.1371615757798716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.30681016144191114,
            "upper_bound": 24.20275404557271
          },
          "point_estimate": 8.615000604276707,
          "standard_error": 6.257845936273323
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6031.2939164787385,
            "upper_bound": 6053.921179800345
          },
          "point_estimate": 6042.953261418091,
          "standard_error": 5.909035289592812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.984951399437717,
            "upper_bound": 18.97221390052974
          },
          "point_estimate": 13.839583950796426,
          "standard_error": 3.3750598630871087
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14052.404577239668,
            "upper_bound": 14070.86147679247
          },
          "point_estimate": 14060.50086684235,
          "standard_error": 4.762306760501774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14048.948658410733,
            "upper_bound": 14068.322271671826
          },
          "point_estimate": 14057.337862118531,
          "standard_error": 4.270565410888953
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0868479102713229,
            "upper_bound": 20.686875435753755
          },
          "point_estimate": 13.861373976049029,
          "standard_error": 5.168287510418485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14051.096193556008,
            "upper_bound": 14061.827250234765
          },
          "point_estimate": 14056.011835068955,
          "standard_error": 2.8144350029450824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.509347321318089,
            "upper_bound": 22.78020688351351
          },
          "point_estimate": 15.9135141121041,
          "standard_error": 5.106991534080945
        }
      }
    },
    "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.0252649101246,
            "upper_bound": 61.131192129827774
          },
          "point_estimate": 61.07807424539212,
          "standard_error": 0.027083432222280487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.01640357476537,
            "upper_bound": 61.15162606214815
          },
          "point_estimate": 61.083194130975606,
          "standard_error": 0.026104159736405463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00919474842366023,
            "upper_bound": 0.17629370946195436
          },
          "point_estimate": 0.051507174071088846,
          "standard_error": 0.04798726893300395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.04774782079051,
            "upper_bound": 61.12816252458099
          },
          "point_estimate": 61.0868446627839,
          "standard_error": 0.02019247903385356
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0465438012425712,
            "upper_bound": 0.11602100769446431
          },
          "point_estimate": 0.09006263338286476,
          "standard_error": 0.018055430414858385
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.777254269790674,
            "upper_bound": 34.816039637250164
          },
          "point_estimate": 34.796246564032465,
          "standard_error": 0.009901602214612976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.76577177956906,
            "upper_bound": 34.825343551387384
          },
          "point_estimate": 34.792710400196356,
          "standard_error": 0.014824330750717507
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008745912057946666,
            "upper_bound": 0.056254705684352584
          },
          "point_estimate": 0.04416055366491898,
          "standard_error": 0.012366319288189188
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.79386656471697,
            "upper_bound": 34.82952951215218
          },
          "point_estimate": 34.81128807675041,
          "standard_error": 0.009124848679204055
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019648033162322612,
            "upper_bound": 0.04036199051861793
          },
          "point_estimate": 0.03295496450704824,
          "standard_error": 0.005290271933347985
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.48079277252638,
            "upper_bound": 38.55641914324204
          },
          "point_estimate": 38.515647616866794,
          "standard_error": 0.019484104876071048
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.46863885331243,
            "upper_bound": 38.54816614658405
          },
          "point_estimate": 38.501574302921526,
          "standard_error": 0.019817510874148205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009791252773722331,
            "upper_bound": 0.10209020679170688
          },
          "point_estimate": 0.0491522267104557,
          "standard_error": 0.023429025259499428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.46037271793528,
            "upper_bound": 38.55264364642304
          },
          "point_estimate": 38.50552080380057,
          "standard_error": 0.024748599262784183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026076547654526307,
            "upper_bound": 0.08736723075603128
          },
          "point_estimate": 0.06504157875232217,
          "standard_error": 0.016415015259961786
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.836983855498794,
            "upper_bound": 30.903987681884225
          },
          "point_estimate": 30.866436280222796,
          "standard_error": 0.017373968320649844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.83768743933925,
            "upper_bound": 30.89145931760454
          },
          "point_estimate": 30.85311382522534,
          "standard_error": 0.012327364274233482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014496336054904916,
            "upper_bound": 0.08503636629996879
          },
          "point_estimate": 0.018271340480430963,
          "standard_error": 0.01969784246649675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.84635107828806,
            "upper_bound": 30.877319119927325
          },
          "point_estimate": 30.860803401706686,
          "standard_error": 0.007690024959010439
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013231028603704084,
            "upper_bound": 0.0810754923375357
          },
          "point_estimate": 0.05774196068695497,
          "standard_error": 0.01821599488114462
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.4701006311341,
            "upper_bound": 24.497647173487525
          },
          "point_estimate": 24.485008034169503,
          "standard_error": 0.007079219947949641
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.470505532011025,
            "upper_bound": 24.503858131220507
          },
          "point_estimate": 24.48983343293014,
          "standard_error": 0.007734561698937539
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005371340546319284,
            "upper_bound": 0.037010770250240355
          },
          "point_estimate": 0.01764399863583998,
          "standard_error": 0.008212703803995428
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.47150532447109,
            "upper_bound": 24.49847187669999
          },
          "point_estimate": 24.487416699739228,
          "standard_error": 0.006828104750545607
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010112217438257508,
            "upper_bound": 0.031602609704658544
          },
          "point_estimate": 0.023662205640885947,
          "standard_error": 0.0058266531890490535
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.32736108657003,
            "upper_bound": 34.37162745680732
          },
          "point_estimate": 34.34647381723159,
          "standard_error": 0.011479264216354606
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.32399405891949,
            "upper_bound": 34.35364855324792
          },
          "point_estimate": 34.344335240893656,
          "standard_error": 0.00868150478706454
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032592984701015023,
            "upper_bound": 0.043623883428115295
          },
          "point_estimate": 0.02501258926601632,
          "standard_error": 0.0103315518144932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.33278914184858,
            "upper_bound": 34.40153725872046
          },
          "point_estimate": 34.362250577577086,
          "standard_error": 0.019073808593805485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01145241158651893,
            "upper_bound": 0.056056615299059885
          },
          "point_estimate": 0.03829860174926507,
          "standard_error": 0.01352794052334369
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.302832664371394,
            "upper_bound": 46.381634589121504
          },
          "point_estimate": 46.33642174667152,
          "standard_error": 0.020485341678128446
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.2930693668392,
            "upper_bound": 46.348187946802746
          },
          "point_estimate": 46.33145125964447,
          "standard_error": 0.01604573419823263
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007657367227792188,
            "upper_bound": 0.07924446464134031
          },
          "point_estimate": 0.03113982844503552,
          "standard_error": 0.018268434095448063
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.30874606230972,
            "upper_bound": 46.34714888106391
          },
          "point_estimate": 46.332892890757115,
          "standard_error": 0.009811993501557151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02152742077936501,
            "upper_bound": 0.10021078560596508
          },
          "point_estimate": 0.06819031889542829,
          "standard_error": 0.02446349439221956
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.15166476755121,
            "upper_bound": 53.27474682762269
          },
          "point_estimate": 53.20883778034562,
          "standard_error": 0.03164060675007023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.13808641159011,
            "upper_bound": 53.281758835950846
          },
          "point_estimate": 53.17350935966382,
          "standard_error": 0.03693163461900948
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017870364048428854,
            "upper_bound": 0.17127588171194916
          },
          "point_estimate": 0.08326015073024631,
          "standard_error": 0.04039911490831229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.121483787892146,
            "upper_bound": 53.24130014667938
          },
          "point_estimate": 53.16853322586198,
          "standard_error": 0.03077929885482146
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045217171042219985,
            "upper_bound": 0.1393897692566545
          },
          "point_estimate": 0.10545319503730163,
          "standard_error": 0.024800702809601907
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.837510199492925,
            "upper_bound": 43.93333055718983
          },
          "point_estimate": 43.887257131583695,
          "standard_error": 0.02451800760961805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.827845640440145,
            "upper_bound": 43.94575195176917
          },
          "point_estimate": 43.904243720459945,
          "standard_error": 0.02802763057825709
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01133556641113022,
            "upper_bound": 0.14334626544134674
          },
          "point_estimate": 0.07346871179635207,
          "standard_error": 0.03263180334095837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.86681027577444,
            "upper_bound": 43.94512007397995
          },
          "point_estimate": 43.91073605213237,
          "standard_error": 0.019953847493842467
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04181737058145569,
            "upper_bound": 0.10479397150275764
          },
          "point_estimate": 0.08205637765651654,
          "standard_error": 0.016268709456557784
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.93191518577177,
            "upper_bound": 61.07042859417231
          },
          "point_estimate": 61.00075664211708,
          "standard_error": 0.03538390661382536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.92565613486734,
            "upper_bound": 61.0459697345082
          },
          "point_estimate": 61.027957423874966,
          "standard_error": 0.0335455257864091
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005960732051403408,
            "upper_bound": 0.1969261518176668
          },
          "point_estimate": 0.05883397669981498,
          "standard_error": 0.05170042565513078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.963154973932305,
            "upper_bound": 61.03252326795643
          },
          "point_estimate": 60.994563108745346,
          "standard_error": 0.017377510932921503
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04482378685604953,
            "upper_bound": 0.1612887414491968
          },
          "point_estimate": 0.1178064135218072,
          "standard_error": 0.02913491042667565
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.3953682991402,
            "upper_bound": 44.4640039998808
          },
          "point_estimate": 44.42704280340321,
          "standard_error": 0.01770032001530389
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.381238682888835,
            "upper_bound": 44.46612403309911
          },
          "point_estimate": 44.41279534548638,
          "standard_error": 0.0236882065887258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006563956296154533,
            "upper_bound": 0.09133525777561578
          },
          "point_estimate": 0.04584018198181741,
          "standard_error": 0.025778912180395724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.40774654915426,
            "upper_bound": 44.470466830996095
          },
          "point_estimate": 44.43412958037079,
          "standard_error": 0.015990481366555016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025815654122189415,
            "upper_bound": 0.0776603983830098
          },
          "point_estimate": 0.059165422027522445,
          "standard_error": 0.013965063554782026
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.31746032274273,
            "upper_bound": 37.40604100210616
          },
          "point_estimate": 37.35351796163049,
          "standard_error": 0.02361569378901033
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.31073686157593,
            "upper_bound": 37.359872556215656
          },
          "point_estimate": 37.34404204714336,
          "standard_error": 0.01625539824918617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00017085987490599055,
            "upper_bound": 0.06563806946189558
          },
          "point_estimate": 0.04246891578479156,
          "standard_error": 0.017686301181879794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.318974791560784,
            "upper_bound": 37.3565436662287
          },
          "point_estimate": 37.337531516224054,
          "standard_error": 0.009899740251366772
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019082285928672477,
            "upper_bound": 0.11812968165177634
          },
          "point_estimate": 0.07852571315114762,
          "standard_error": 0.03222308830643373
        }
      }
    },
    "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.43040826580327,
            "upper_bound": 66.54566596341893
          },
          "point_estimate": 66.48960159592522,
          "standard_error": 0.02957412965156387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.38980920848043,
            "upper_bound": 66.56069813598697
          },
          "point_estimate": 66.52192928701655,
          "standard_error": 0.04516092853493851
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011291376388708942,
            "upper_bound": 0.16276814407281273
          },
          "point_estimate": 0.08454849966054488,
          "standard_error": 0.04105920593369501
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.45147175792083,
            "upper_bound": 66.58091320407135
          },
          "point_estimate": 66.52819658290015,
          "standard_error": 0.03273799874590581
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05467684505157112,
            "upper_bound": 0.12119756981212948
          },
          "point_estimate": 0.09866241782444644,
          "standard_error": 0.016524680713088928
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157222.2780154915,
            "upper_bound": 157410.06761124678
          },
          "point_estimate": 157314.4785097002,
          "standard_error": 48.048730842701154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157178.68304843304,
            "upper_bound": 157451.94444444444
          },
          "point_estimate": 157304.51001899334,
          "standard_error": 64.08382913396858
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.95870609215666,
            "upper_bound": 267.57557419543
          },
          "point_estimate": 162.80167371224593,
          "standard_error": 60.60503083419422
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 157261.93875306912,
            "upper_bound": 157433.00438708477
          },
          "point_estimate": 157337.15896325896,
          "standard_error": 42.87763423094848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.75502884061305,
            "upper_bound": 204.1708411490042
          },
          "point_estimate": 159.9016433602897,
          "standard_error": 29.721565375556676
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53012.16351882317,
            "upper_bound": 53104.166405664306
          },
          "point_estimate": 53060.32652500116,
          "standard_error": 23.59701766660169
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53014.403162455455,
            "upper_bound": 53122.00100652505
          },
          "point_estimate": 53062.98727526725,
          "standard_error": 25.168719791319944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.508013025490758,
            "upper_bound": 131.61465187255533
          },
          "point_estimate": 54.20715373514757,
          "standard_error": 29.338760727801503
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53041.11912498842,
            "upper_bound": 53091.06406886553
          },
          "point_estimate": 53069.32317216312,
          "standard_error": 12.987647449283234
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.189879550336045,
            "upper_bound": 105.7048168253011
          },
          "point_estimate": 78.67427826112909,
          "standard_error": 18.42933142921663
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251147.96068986465,
            "upper_bound": 251641.3548970768
          },
          "point_estimate": 251398.57415606655,
          "standard_error": 125.77137403656894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251094.8687214612,
            "upper_bound": 251624.99563356163
          },
          "point_estimate": 251438.30699608612,
          "standard_error": 123.28145080130376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.18421526838288,
            "upper_bound": 731.5818486556842
          },
          "point_estimate": 357.64871955797975,
          "standard_error": 167.4133425195299
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251198.74471339275,
            "upper_bound": 251631.5693093903
          },
          "point_estimate": 251435.874025974,
          "standard_error": 112.59967948175682
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191.4701076805995,
            "upper_bound": 565.8978749188867
          },
          "point_estimate": 419.957742044784,
          "standard_error": 97.23364495746016
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 816576.0961307321,
            "upper_bound": 817324.1436234568
          },
          "point_estimate": 816945.0052336862,
          "standard_error": 191.5480649755714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 816522.2977777778,
            "upper_bound": 817359.7281746031
          },
          "point_estimate": 816957.3341666667,
          "standard_error": 227.0719333534503
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.16644725376306,
            "upper_bound": 1111.9845742583022
          },
          "point_estimate": 489.8162542670032,
          "standard_error": 245.86564119872463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 816503.131405378,
            "upper_bound": 817142.1125181465
          },
          "point_estimate": 816761.2163924964,
          "standard_error": 164.7852992115779
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323.6212756416383,
            "upper_bound": 846.3623160334054
          },
          "point_estimate": 640.6699237236451,
          "standard_error": 134.73856393832463
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1869084.3441550592,
            "upper_bound": 1871296.8114583336
          },
          "point_estimate": 1870126.528873016,
          "standard_error": 570.5508570910978
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1868638.5333333337,
            "upper_bound": 1872658.85
          },
          "point_estimate": 1869434.8368055555,
          "standard_error": 921.9159730612288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.2561009653188,
            "upper_bound": 2801.9106222560767
          },
          "point_estimate": 1217.4297613027416,
          "standard_error": 678.3580788891646
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1869263.4004045164,
            "upper_bound": 1870710.9171262255
          },
          "point_estimate": 1869777.4472727273,
          "standard_error": 370.56472821138806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 612.1750491711985,
            "upper_bound": 2200.091890234683
          },
          "point_estimate": 1904.073882060138,
          "standard_error": 332.2822763977423
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63353.30574327123,
            "upper_bound": 63453.94277826087
          },
          "point_estimate": 63405.834100759144,
          "standard_error": 25.74872577369974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63337.01971014493,
            "upper_bound": 63467.75596273292
          },
          "point_estimate": 63439.45695652174,
          "standard_error": 34.5424327072759
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.5692757851839385,
            "upper_bound": 145.3609772454033
          },
          "point_estimate": 95.98932377410624,
          "standard_error": 37.98070360650128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63370.479368671826,
            "upper_bound": 63446.668122620715
          },
          "point_estimate": 63415.62438396386,
          "standard_error": 19.40888030603032
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.33709958957483,
            "upper_bound": 110.46206525274756
          },
          "point_estimate": 85.94182027565746,
          "standard_error": 17.475082902484797
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220860.26271810543,
            "upper_bound": 221389.20023286756
          },
          "point_estimate": 221102.39985861612,
          "standard_error": 136.55289730348312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220851.08083832337,
            "upper_bound": 221355.14471057884
          },
          "point_estimate": 220943.34659846977,
          "standard_error": 135.18530023552793
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.11500388778805,
            "upper_bound": 654.9984638205432
          },
          "point_estimate": 140.24003655715902,
          "standard_error": 188.8215082478162
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220866.15225972637,
            "upper_bound": 221111.3733734454
          },
          "point_estimate": 220961.2317443036,
          "standard_error": 63.01958236621462
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.27736936572933,
            "upper_bound": 591.9528377300478
          },
          "point_estimate": 453.6258256646505,
          "standard_error": 113.35144713608688
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149303.8156938153,
            "upper_bound": 149678.7401277584
          },
          "point_estimate": 149487.53909794812,
          "standard_error": 95.8343869301614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149238.30081300813,
            "upper_bound": 149809.6056910569
          },
          "point_estimate": 149416.95809136663,
          "standard_error": 141.9720090548006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.55689679382621,
            "upper_bound": 550.0751261691638
          },
          "point_estimate": 309.27783064744204,
          "standard_error": 121.54174766393136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 149223.49642380755,
            "upper_bound": 149507.65500658532
          },
          "point_estimate": 149337.01918488016,
          "standard_error": 73.28552350429098
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.1651192053933,
            "upper_bound": 392.00612344132816
          },
          "point_estimate": 320.5875252612174,
          "standard_error": 52.1502130329144
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 931574.258343006,
            "upper_bound": 932740.412875
          },
          "point_estimate": 932120.2838035714,
          "standard_error": 298.50834071461117
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 931356.90625,
            "upper_bound": 932693.5996428572
          },
          "point_estimate": 932147.29125,
          "standard_error": 338.2040098194134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.87140634982265,
            "upper_bound": 1536.7283749675378
          },
          "point_estimate": 1044.9868080728124,
          "standard_error": 408.60582347485393
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 931702.389910314,
            "upper_bound": 932765.3660722396
          },
          "point_estimate": 932245.7558441558,
          "standard_error": 286.4157134488617
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 468.5129848101767,
            "upper_bound": 1316.3011673097756
          },
          "point_estimate": 993.8415027570207,
          "standard_error": 228.86367897395647
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64909.382090791405,
            "upper_bound": 64994.74872073061
          },
          "point_estimate": 64949.96731558877,
          "standard_error": 21.854063552654303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64893.0938449848,
            "upper_bound": 65006.66134751773
          },
          "point_estimate": 64944.7722320725,
          "standard_error": 28.844515008324176
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.499580889128866,
            "upper_bound": 121.27204472198004
          },
          "point_estimate": 80.44955186370032,
          "standard_error": 26.748370000260948
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64932.311006944605,
            "upper_bound": 65012.30523661824
          },
          "point_estimate": 64966.20251450677,
          "standard_error": 20.297806700101987
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.38189481874886,
            "upper_bound": 94.56898868965936
          },
          "point_estimate": 72.7232632657531,
          "standard_error": 15.04320623919551
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128711.74991508492,
            "upper_bound": 128830.74540615636
          },
          "point_estimate": 128769.95576007328,
          "standard_error": 30.45852132240663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128684.05122377622,
            "upper_bound": 128842.99537962038
          },
          "point_estimate": 128754.31628787878,
          "standard_error": 37.30801219279365
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.66022141777895,
            "upper_bound": 172.26541634727147
          },
          "point_estimate": 100.16865442795245,
          "standard_error": 43.46415188716347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128738.44064814676,
            "upper_bound": 128796.6305858862
          },
          "point_estimate": 128763.31994369268,
          "standard_error": 14.844920658497584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.57374301948092,
            "upper_bound": 130.84445410779523
          },
          "point_estimate": 101.28338519672016,
          "standard_error": 20.15263434308314
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408200.4704981684,
            "upper_bound": 408883.2197854526
          },
          "point_estimate": 408535.60299668583,
          "standard_error": 175.40316806466842
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408012.4018838304,
            "upper_bound": 409053.1391941392
          },
          "point_estimate": 408506.70915750915,
          "standard_error": 303.3348586690477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.05933308144012,
            "upper_bound": 950.1587690287216
          },
          "point_estimate": 748.4558009979729,
          "standard_error": 210.85868975612613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408216.6407052293,
            "upper_bound": 409017.7483806981
          },
          "point_estimate": 408610.556300842,
          "standard_error": 208.10166807294544
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369.7942546585625,
            "upper_bound": 692.4073315247789
          },
          "point_estimate": 585.2011394309088,
          "standard_error": 82.23249017142003
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59749.77549202927,
            "upper_bound": 59805.32036026565
          },
          "point_estimate": 59780.46024741314,
          "standard_error": 14.280387297122186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59766.2170771757,
            "upper_bound": 59807.97257799672
          },
          "point_estimate": 59785.705371803895,
          "standard_error": 12.048071863095188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.51418884985766,
            "upper_bound": 62.06605483680195
          },
          "point_estimate": 30.953352209089054,
          "standard_error": 13.73083781515238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59757.24481516745,
            "upper_bound": 59797.51979679803
          },
          "point_estimate": 59781.28480156953,
          "standard_error": 10.478012573496654
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.057998742637174,
            "upper_bound": 68.16822273992331
          },
          "point_estimate": 47.59821761544098,
          "standard_error": 15.043553513777455
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23638.258839931885,
            "upper_bound": 23661.68642081737
          },
          "point_estimate": 23650.62306637576,
          "standard_error": 5.983039093255562
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23643.00638569604,
            "upper_bound": 23664.43446679438
          },
          "point_estimate": 23647.737402440755,
          "standard_error": 6.03111381045902
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.92835826717082,
            "upper_bound": 33.572640903010644
          },
          "point_estimate": 9.738201359679897,
          "standard_error": 8.344676472077007
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23645.680098375306,
            "upper_bound": 23660.28090202785
          },
          "point_estimate": 23651.818699308355,
          "standard_error": 3.721512919494853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.645572340940426,
            "upper_bound": 27.61056859153383
          },
          "point_estimate": 19.924491515336335,
          "standard_error": 5.163589705961451
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2008244.41627005,
            "upper_bound": 2010784.1409022552
          },
          "point_estimate": 2009483.8171010856,
          "standard_error": 651.90720999775
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2007564.7178362573,
            "upper_bound": 2011245.3157894737
          },
          "point_estimate": 2009074.71679198,
          "standard_error": 1246.6057361539697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.32937798688891,
            "upper_bound": 3790.2095476224545
          },
          "point_estimate": 2420.6138517625136,
          "standard_error": 1059.91417672457
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2007706.1432748537,
            "upper_bound": 2010381.33851389
          },
          "point_estimate": 2009033.5809979497,
          "standard_error": 712.6981801743229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1360.8577833397362,
            "upper_bound": 2565.0693426476796
          },
          "point_estimate": 2168.8070507546063,
          "standard_error": 312.59407262949014
        }
      }
    },
    "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3986.02672818221,
            "upper_bound": 3991.9741908086894
          },
          "point_estimate": 3988.560809441228,
          "standard_error": 1.5605569936344283
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3984.843702081051,
            "upper_bound": 3990.002468966776
          },
          "point_estimate": 3987.512156157096,
          "standard_error": 1.600715287733952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5059798677966197,
            "upper_bound": 6.487831863642496
          },
          "point_estimate": 3.5672320929302095,
          "standard_error": 1.4777922211986538
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3985.724018891469,
            "upper_bound": 3989.2851318383023
          },
          "point_estimate": 3987.3840386338743,
          "standard_error": 0.9205899867480316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.8141740811571785,
            "upper_bound": 7.648293580323157
          },
          "point_estimate": 5.211005146887539,
          "standard_error": 1.856765484576424
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47221.16956509175,
            "upper_bound": 47325.96224462429
          },
          "point_estimate": 47268.17619826821,
          "standard_error": 26.98932040202633
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47209.605981794535,
            "upper_bound": 47333.60208062419
          },
          "point_estimate": 47227.439965322934,
          "standard_error": 29.96423886994577
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.659018469518742,
            "upper_bound": 142.33239301275307
          },
          "point_estimate": 32.13102187863281,
          "standard_error": 33.40077960596071
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47219.631262582814,
            "upper_bound": 47372.06082832928
          },
          "point_estimate": 47284.00637022275,
          "standard_error": 39.95282870213339
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.133289331357076,
            "upper_bound": 123.36499694194016
          },
          "point_estimate": 90.09187976980536,
          "standard_error": 25.566810360228075
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48739.915655633966,
            "upper_bound": 48786.08434111783
          },
          "point_estimate": 48764.3721751792,
          "standard_error": 11.77380753439998
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48745.006888440854,
            "upper_bound": 48788.34968637992
          },
          "point_estimate": 48766.85887096774,
          "standard_error": 9.6859488725067
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.639957762780165,
            "upper_bound": 62.98758234949937
          },
          "point_estimate": 19.97923029046009,
          "standard_error": 14.61602106755947
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48749.63894265483,
            "upper_bound": 48787.13598116359
          },
          "point_estimate": 48766.2126344086,
          "standard_error": 9.443243346411144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.297365336477696,
            "upper_bound": 54.13599113547486
          },
          "point_estimate": 39.22075405344279,
          "standard_error": 10.204097600809437
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48158.479899370766,
            "upper_bound": 48272.34547239613
          },
          "point_estimate": 48212.9641144944,
          "standard_error": 29.068913347600635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48152.10296591412,
            "upper_bound": 48251.613035951435
          },
          "point_estimate": 48218.92989154493,
          "standard_error": 25.739617277280637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.94622489557956,
            "upper_bound": 150.74500012586626
          },
          "point_estimate": 63.2564667455017,
          "standard_error": 34.838140365920516
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48175.38478409594,
            "upper_bound": 48235.41230880673
          },
          "point_estimate": 48201.689446542834,
          "standard_error": 15.314416964448991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.44995229052631,
            "upper_bound": 136.33548745002685
          },
          "point_estimate": 96.63725977384378,
          "standard_error": 25.96424884437627
        }
      }
    },
    "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13928.999344804615,
            "upper_bound": 13950.943470039603
          },
          "point_estimate": 13938.436798429731,
          "standard_error": 5.718823093201912
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13925.941740130318,
            "upper_bound": 13945.84210904561
          },
          "point_estimate": 13931.876625782548,
          "standard_error": 4.695076235088543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1090728103922043,
            "upper_bound": 23.748554558255016
          },
          "point_estimate": 9.292172130930382,
          "standard_error": 5.571112310131035
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13926.918315071143,
            "upper_bound": 13965.904580226836
          },
          "point_estimate": 13944.049801635696,
          "standard_error": 11.416165045326986
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.241461891154698,
            "upper_bound": 26.838507960028107
          },
          "point_estimate": 19.01350069860673,
          "standard_error": 6.263670189079402
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25786.129742243156,
            "upper_bound": 25835.965421199297
          },
          "point_estimate": 25810.133159355835,
          "standard_error": 12.775273407189886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25772.570154671717,
            "upper_bound": 25849.13068181818
          },
          "point_estimate": 25807.69872159091,
          "standard_error": 18.637937817788607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.047193252017657,
            "upper_bound": 70.76613939706132
          },
          "point_estimate": 56.75431776608235,
          "standard_error": 17.535093405647324
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25775.49396993773,
            "upper_bound": 25814.858602590302
          },
          "point_estimate": 25793.254340687723,
          "standard_error": 10.143747111285974
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.597359204843738,
            "upper_bound": 52.587211894932175
          },
          "point_estimate": 42.60952084901095,
          "standard_error": 7.396400626606504
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17422.952140609166,
            "upper_bound": 17440.070839372394
          },
          "point_estimate": 17431.209648667595,
          "standard_error": 4.378870273849722
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17420.06949111858,
            "upper_bound": 17442.10178961967
          },
          "point_estimate": 17428.217978876623,
          "standard_error": 6.545811094991449
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4601337249260187,
            "upper_bound": 24.76994855208636
          },
          "point_estimate": 15.16161522930712,
          "standard_error": 5.460631935628651
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17424.628537159006,
            "upper_bound": 17442.198981467704
          },
          "point_estimate": 17433.18886845272,
          "standard_error": 4.395867350870047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.18025829688572,
            "upper_bound": 18.39944275264004
          },
          "point_estimate": 14.533507556479876,
          "standard_error": 2.656219727750736
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17690.38656242031,
            "upper_bound": 17723.698223001284
          },
          "point_estimate": 17705.246693617562,
          "standard_error": 8.573849624831169
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17689.612672137988,
            "upper_bound": 17714.782375851995
          },
          "point_estimate": 17697.22536514119,
          "standard_error": 6.439663801891056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6172086307188065,
            "upper_bound": 37.64785277245933
          },
          "point_estimate": 11.70881344232136,
          "standard_error": 9.132934134767853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17693.46152037597,
            "upper_bound": 17708.911228767716
          },
          "point_estimate": 17699.644326559515,
          "standard_error": 3.9307090589323934
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.376894407660547,
            "upper_bound": 40.69552681221143
          },
          "point_estimate": 28.573884174753392,
          "standard_error": 9.08163991643255
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17513.34735085102,
            "upper_bound": 17535.89138741633
          },
          "point_estimate": 17523.144286727864,
          "standard_error": 5.900533416829126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17511.971777108432,
            "upper_bound": 17528.259586919106
          },
          "point_estimate": 17519.27716198126,
          "standard_error": 3.009598176244856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6588766148081358,
            "upper_bound": 24.921153099728645
          },
          "point_estimate": 2.688019248665385,
          "standard_error": 7.203532627125111
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17518.7983268252,
            "upper_bound": 17535.783163149837
          },
          "point_estimate": 17524.398670630573,
          "standard_error": 4.445553742448903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.570111452004312,
            "upper_bound": 28.640861458561012
          },
          "point_estimate": 19.68695464404202,
          "standard_error": 6.894814372112486
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93910.57382141832,
            "upper_bound": 94062.26657725275
          },
          "point_estimate": 93989.66263976049,
          "standard_error": 38.88864663379008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93877.61141257537,
            "upper_bound": 94075.205749354
          },
          "point_estimate": 94039.02680878554,
          "standard_error": 48.4425154533974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.438389050739262,
            "upper_bound": 215.54191019146268
          },
          "point_estimate": 119.52811016477264,
          "standard_error": 58.76992766813152
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93925.42688815226,
            "upper_bound": 94051.52417051936
          },
          "point_estimate": 93978.87532467532,
          "standard_error": 31.637402243281688
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.422534409472945,
            "upper_bound": 164.51492758970906
          },
          "point_estimate": 129.43957863228698,
          "standard_error": 25.72919824549331
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103377.39890827922,
            "upper_bound": 103629.2758633207
          },
          "point_estimate": 103488.2021248196,
          "standard_error": 65.26030254629167
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103340.2974431818,
            "upper_bound": 103581.5615530303
          },
          "point_estimate": 103458.35744160353,
          "standard_error": 61.02153815915712
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.39361122175103,
            "upper_bound": 284.82987178037655
          },
          "point_estimate": 162.59212470809936,
          "standard_error": 71.75625635638768
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103338.40367812694,
            "upper_bound": 103478.05066350472
          },
          "point_estimate": 103412.91906729636,
          "standard_error": 35.89505475186435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.66019126534117,
            "upper_bound": 306.73954638169664
          },
          "point_estimate": 217.0887791890013,
          "standard_error": 66.92027693520498
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18519.088363429302,
            "upper_bound": 18542.889023809526
          },
          "point_estimate": 18532.429243379494,
          "standard_error": 6.121737704119172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18519.211734693876,
            "upper_bound": 18546.618664965987
          },
          "point_estimate": 18541.643731778426,
          "standard_error": 7.342713731258973
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6206496318368903,
            "upper_bound": 30.62083001886724
          },
          "point_estimate": 8.133966335184676,
          "standard_error": 8.292222074667631
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18524.429352593612,
            "upper_bound": 18543.59112457483
          },
          "point_estimate": 18533.969369202227,
          "standard_error": 5.056906211950669
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.529831900345606,
            "upper_bound": 28.322841521338347
          },
          "point_estimate": 20.427132993885788,
          "standard_error": 5.944967634757474
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17817.996363051385,
            "upper_bound": 17850.32013740177
          },
          "point_estimate": 17833.49711535613,
          "standard_error": 8.247853069583849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17819.604076620824,
            "upper_bound": 17844.244386752736
          },
          "point_estimate": 17832.174148657497,
          "standard_error": 8.304988746332626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.755981133540728,
            "upper_bound": 43.11557700959493
          },
          "point_estimate": 17.744843872916764,
          "standard_error": 10.032001717328232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17819.613689573216,
            "upper_bound": 17841.008516015518
          },
          "point_estimate": 17828.812944913632,
          "standard_error": 5.502109891341739
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.349599555495288,
            "upper_bound": 38.744435913571216
          },
          "point_estimate": 27.49211608076481,
          "standard_error": 7.487327038355272
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17289.320935113094,
            "upper_bound": 17335.495202964572
          },
          "point_estimate": 17309.188357018204,
          "standard_error": 12.006684430482355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17283.18413000612,
            "upper_bound": 17324.24458591147
          },
          "point_estimate": 17294.275170553705,
          "standard_error": 10.275868339325829
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8164812684157012,
            "upper_bound": 48.70796570831935
          },
          "point_estimate": 25.12931096509945,
          "standard_error": 12.196137396403763
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17287.60468582354,
            "upper_bound": 17311.479559372026
          },
          "point_estimate": 17301.162131823436,
          "standard_error": 6.188308016620382
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.445192730796553,
            "upper_bound": 57.21921648138285
          },
          "point_estimate": 39.98493678676818,
          "standard_error": 13.154982807257738
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17387.96326711345,
            "upper_bound": 17406.162475144745
          },
          "point_estimate": 17396.94396350498,
          "standard_error": 4.656594060350111
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17382.28386787937,
            "upper_bound": 17412.054913492444
          },
          "point_estimate": 17395.797327269825,
          "standard_error": 7.97340291314593
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.20113758457971,
            "upper_bound": 25.783766737076263
          },
          "point_estimate": 19.679390567327943,
          "standard_error": 5.507721195603936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17391.198470790805,
            "upper_bound": 17410.63736894101
          },
          "point_estimate": 17400.445441490057,
          "standard_error": 4.972997963268763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.698431064604442,
            "upper_bound": 18.55428445115688
          },
          "point_estimate": 15.504006367485346,
          "standard_error": 2.2572832290637734
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17628.695693183305,
            "upper_bound": 17658.12989446389
          },
          "point_estimate": 17643.229836802908,
          "standard_error": 7.532791231480613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17622.842212518193,
            "upper_bound": 17663.18775270904
          },
          "point_estimate": 17644.24065120954,
          "standard_error": 7.804441334050705
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.891465596410369,
            "upper_bound": 45.35423905409434
          },
          "point_estimate": 19.67595360184072,
          "standard_error": 13.286784412027464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17637.270850728317,
            "upper_bound": 17655.006562235183
          },
          "point_estimate": 17645.63716012275,
          "standard_error": 4.448156475398972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.819540482109108,
            "upper_bound": 32.12645193418143
          },
          "point_estimate": 25.092591725706416,
          "standard_error": 4.921937887184712
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17414.605083084843,
            "upper_bound": 17491.753174008223
          },
          "point_estimate": 17452.271220046947,
          "standard_error": 19.819537593263448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17387.834104046244,
            "upper_bound": 17518.931599229287
          },
          "point_estimate": 17442.182731213874,
          "standard_error": 35.28960879544733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.087822757551152,
            "upper_bound": 107.06639273792024
          },
          "point_estimate": 81.45657604301931,
          "standard_error": 26.21858023021051
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17392.8395499177,
            "upper_bound": 17438.752737349758
          },
          "point_estimate": 17407.097826739733,
          "standard_error": 11.881019147790578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.339950507708416,
            "upper_bound": 77.16295757700061
          },
          "point_estimate": 65.75280805155698,
          "standard_error": 9.298179566578703
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17994.996790972516,
            "upper_bound": 18021.337087443168
          },
          "point_estimate": 18006.611643797096,
          "standard_error": 6.831010411849755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17993.825528576148,
            "upper_bound": 18012.90113974232
          },
          "point_estimate": 18000.7448154113,
          "standard_error": 4.736005681095599
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7028349990894716,
            "upper_bound": 27.84899981942299
          },
          "point_estimate": 13.78410617376264,
          "standard_error": 7.154475038213256
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17995.333978068353,
            "upper_bound": 18006.472164191342
          },
          "point_estimate": 18000.86967165639,
          "standard_error": 2.759528319978969
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.095825214611314,
            "upper_bound": 32.812480825872434
          },
          "point_estimate": 22.70965113433493,
          "standard_error": 7.58597807296528
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18076.509780857617,
            "upper_bound": 18118.15077272368
          },
          "point_estimate": 18096.756650003947,
          "standard_error": 10.677158489038309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18073.893993919293,
            "upper_bound": 18125.68252487562
          },
          "point_estimate": 18088.779021558872,
          "standard_error": 13.916423290441042
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.491983854405165,
            "upper_bound": 62.648976835518965
          },
          "point_estimate": 23.99386092228192,
          "standard_error": 15.374711594909993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18074.225498864373,
            "upper_bound": 18105.59958955224
          },
          "point_estimate": 18086.065826710605,
          "standard_error": 8.306386657547353
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.01800707945479,
            "upper_bound": 45.01760237899533
          },
          "point_estimate": 35.54228264064448,
          "standard_error": 6.580083924030919
        }
      }
    },
    "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18185.17893220386,
            "upper_bound": 18211.70328198548
          },
          "point_estimate": 18198.71671952373,
          "standard_error": 6.784911800581143
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18181.580871306964,
            "upper_bound": 18217.111834418298
          },
          "point_estimate": 18200.72467033884,
          "standard_error": 10.702321462957226
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.3768373586730855,
            "upper_bound": 38.289482726996795
          },
          "point_estimate": 25.1081497920984,
          "standard_error": 8.641044043541227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18175.048803522302,
            "upper_bound": 18212.55278672726
          },
          "point_estimate": 18190.604681047545,
          "standard_error": 9.418940592396734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.227115545960704,
            "upper_bound": 28.71427299680998
          },
          "point_estimate": 22.68103998045805,
          "standard_error": 4.045456847317933
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57712.396186696904,
            "upper_bound": 57754.8327744709
          },
          "point_estimate": 57734.67473696145,
          "standard_error": 10.875506117139349
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57706.91349206349,
            "upper_bound": 57759.413095238095
          },
          "point_estimate": 57741.61428571429,
          "standard_error": 9.643077154572309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.1450632952519944,
            "upper_bound": 65.32424574502919
          },
          "point_estimate": 15.510063296071122,
          "standard_error": 16.96655243523319
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57732.82763078562,
            "upper_bound": 57770.25456950457
          },
          "point_estimate": 57754.93102040816,
          "standard_error": 9.768160018169706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.360290600345774,
            "upper_bound": 46.95391771114711
          },
          "point_estimate": 36.15219386606015,
          "standard_error": 8.12750641155652
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 880888.0981041667,
            "upper_bound": 881782.7304992676
          },
          "point_estimate": 881329.1483947467,
          "standard_error": 229.76600084179344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 880630.7642857142,
            "upper_bound": 882006.9973544973
          },
          "point_estimate": 881187.5550595238,
          "standard_error": 400.3184143172413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.8132249812074,
            "upper_bound": 1254.1216732704736
          },
          "point_estimate": 856.6131710420466,
          "standard_error": 292.2115065821389
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 880832.9703787026,
            "upper_bound": 881911.1322229743
          },
          "point_estimate": 881364.5643166357,
          "standard_error": 279.04622155691106
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471.9786156366609,
            "upper_bound": 905.5707892199732
          },
          "point_estimate": 766.6549781614026,
          "standard_error": 109.74924128987789
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207999.68096968252,
            "upper_bound": 208209.16210782315
          },
          "point_estimate": 208102.25122879815,
          "standard_error": 53.760904162273945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207983.59892857145,
            "upper_bound": 208298.53015873017
          },
          "point_estimate": 208055.9262857143,
          "standard_error": 76.82627268795588
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.68942677761272,
            "upper_bound": 321.09877243271524
          },
          "point_estimate": 125.20076697724602,
          "standard_error": 76.68577506790095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207966.58175622544,
            "upper_bound": 208201.818081356
          },
          "point_estimate": 208078.1277625232,
          "standard_error": 60.74449438682865
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.61556610481747,
            "upper_bound": 219.54606828131972
          },
          "point_estimate": 178.07709261450643,
          "standard_error": 30.466624245413858
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5547.6456714619935,
            "upper_bound": 5561.755765240601
          },
          "point_estimate": 5555.252701240245,
          "standard_error": 3.62617968682264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5546.0845952853215,
            "upper_bound": 5563.921856881294
          },
          "point_estimate": 5560.687105838673,
          "standard_error": 4.428770867979121
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1444303962221754,
            "upper_bound": 18.561576793215735
          },
          "point_estimate": 8.017733184633071,
          "standard_error": 4.55403481696398
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5544.500245677371,
            "upper_bound": 5561.583324234611
          },
          "point_estimate": 5552.861339362499,
          "standard_error": 4.615600552076668
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.319776486441146,
            "upper_bound": 15.209079510209936
          },
          "point_estimate": 12.09489909547406,
          "standard_error": 2.7852560206851775
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5923.15496545883,
            "upper_bound": 5938.795026463933
          },
          "point_estimate": 5930.357059909881,
          "standard_error": 4.014440087334165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5921.094427440417,
            "upper_bound": 5937.661255305256
          },
          "point_estimate": 5926.377942830195,
          "standard_error": 4.238625095106376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.3462600383320056,
            "upper_bound": 20.99146328387229
          },
          "point_estimate": 8.320130332209724,
          "standard_error": 4.976206655474247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5921.585363652347,
            "upper_bound": 5931.0318046684915
          },
          "point_estimate": 5926.813969412892,
          "standard_error": 2.37410977983952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.289809946798809,
            "upper_bound": 17.858946369441583
          },
          "point_estimate": 13.409669682599388,
          "standard_error": 3.3129011475558623
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14026.159226410042,
            "upper_bound": 14055.778130309014
          },
          "point_estimate": 14040.488215910416,
          "standard_error": 7.592126286347955
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14015.296794129008,
            "upper_bound": 14061.970087120724
          },
          "point_estimate": 14036.281717292944,
          "standard_error": 10.933500570993615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.118400069797132,
            "upper_bound": 43.940363903821726
          },
          "point_estimate": 30.33921233506746,
          "standard_error": 9.97770047256725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14025.970986780094,
            "upper_bound": 14044.997710312442
          },
          "point_estimate": 14035.562320105542,
          "standard_error": 4.839757824548286
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.048567417833487,
            "upper_bound": 31.08345843298155
          },
          "point_estimate": 25.250986782613243,
          "standard_error": 4.328709094955742
        }
      }
    },
    "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.549828762879756,
            "upper_bound": 35.89882665532209
          },
          "point_estimate": 35.74636356215624,
          "standard_error": 0.090571968723158
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.63995942941188,
            "upper_bound": 35.98473347463503
          },
          "point_estimate": 35.81930047408699,
          "standard_error": 0.0971773581694202
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027867336727675104,
            "upper_bound": 0.41174032843931246
          },
          "point_estimate": 0.2392021740311485,
          "standard_error": 0.08923857216727812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.60243978198848,
            "upper_bound": 35.902170240857515
          },
          "point_estimate": 35.7848978658368,
          "standard_error": 0.07653980842969287
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11676781129524948,
            "upper_bound": 0.4364565351132537
          },
          "point_estimate": 0.3025618562509021,
          "standard_error": 0.09860374525319478
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-haystack/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192428.66023639453,
            "upper_bound": 192908.8032771374
          },
          "point_estimate": 192675.97947929788,
          "standard_error": 122.4425776395564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192461.4914021164,
            "upper_bound": 192944.40343915345
          },
          "point_estimate": 192708.06837154616,
          "standard_error": 128.99288439985813
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.56701249402687,
            "upper_bound": 676.9973959174081
          },
          "point_estimate": 348.035441043377,
          "standard_error": 144.24069014875212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192559.79546207737,
            "upper_bound": 193005.40050545693
          },
          "point_estimate": 192759.241723356,
          "standard_error": 111.79111244882267
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.28755183281322,
            "upper_bound": 556.2809423618332
          },
          "point_estimate": 407.7534608616334,
          "standard_error": 100.3982477280102
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23493414.162752483,
            "upper_bound": 23540827.133781247
          },
          "point_estimate": 23517142.394742064,
          "standard_error": 12180.435888038666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23469739.0,
            "upper_bound": 23557740.5
          },
          "point_estimate": 23513397.732638888,
          "standard_error": 20952.506438387965
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3500.284692026646,
            "upper_bound": 67092.67111636588
          },
          "point_estimate": 65235.51079183817,
          "standard_error": 18417.804747043163
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23503460.03789876,
            "upper_bound": 23540300.260484897
          },
          "point_estimate": 23523141.3012987,
          "standard_error": 9448.836194606927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25333.7517624122,
            "upper_bound": 47269.09642278539
          },
          "point_estimate": 40472.942053867286,
          "standard_error": 5518.590698488061
        }
      }
    },
    "memmem/krate_nopre/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/krate_nopre_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81026268.76666667,
            "upper_bound": 81103024.19083333
          },
          "point_estimate": 81065149.1,
          "standard_error": 19711.365043034915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81005575.66666667,
            "upper_bound": 81131815.33333333
          },
          "point_estimate": 81053940.5,
          "standard_error": 35317.5803404634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5573.58750106377,
            "upper_bound": 115239.77785408129
          },
          "point_estimate": 106251.02131365992,
          "standard_error": 32873.55644836444
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41875.66723476255,
            "upper_bound": 77251.31880932851
          },
          "point_estimate": 65892.3820248935,
          "standard_error": 8976.414185623891
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049819445100695,
            "upper_bound": 9.063287996546304
          },
          "point_estimate": 9.056697417597336,
          "standard_error": 0.0034528208295227345
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.050464318284064,
            "upper_bound": 9.06564954743682
          },
          "point_estimate": 9.054899569773172,
          "standard_error": 0.003954047763841006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011864586499584465,
            "upper_bound": 0.01999590331918902
          },
          "point_estimate": 0.01095536034414434,
          "standard_error": 0.0047107242391497455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.053466748071964,
            "upper_bound": 9.062032051061932
          },
          "point_estimate": 9.057470958431184,
          "standard_error": 0.0021840230428907936
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005723460110344729,
            "upper_bound": 0.015378234909409112
          },
          "point_estimate": 0.011513467225285533,
          "standard_error": 0.002529381307129315
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045660684971564,
            "upper_bound": 9.062292085305335
          },
          "point_estimate": 9.053295722358412,
          "standard_error": 0.00425500701670953
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044929256729011,
            "upper_bound": 9.058040647135355
          },
          "point_estimate": 9.05290511566556,
          "standard_error": 0.003873157391720572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017666855856216246,
            "upper_bound": 0.0197474287381493
          },
          "point_estimate": 0.008060323488594675,
          "standard_error": 0.0046326272003572215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.044258342631087,
            "upper_bound": 9.05540676984651
          },
          "point_estimate": 9.0508893294334,
          "standard_error": 0.0028386414824345422
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005205707116004994,
            "upper_bound": 0.02006572889621791
          },
          "point_estimate": 0.01413739882541254,
          "standard_error": 0.004169487426349696
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.041909050058337,
            "upper_bound": 9.049843943674617
          },
          "point_estimate": 9.04556176404898,
          "standard_error": 0.00204704633133088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.040400994526127,
            "upper_bound": 9.050688680735473
          },
          "point_estimate": 9.04388617053959,
          "standard_error": 0.002294324251463501
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007976802063854019,
            "upper_bound": 0.010781552561632816
          },
          "point_estimate": 0.005224763092860554,
          "standard_error": 0.002401886022577823
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.041529290842806,
            "upper_bound": 9.047044545840492
          },
          "point_estimate": 9.043628380833084,
          "standard_error": 0.001407301826181089
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002431099092293872,
            "upper_bound": 0.00855242808158895
          },
          "point_estimate": 0.006841120251812748,
          "standard_error": 0.0015497603404249671
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.797817343465494,
            "upper_bound": 8.80677633920845
          },
          "point_estimate": 8.80244054613037,
          "standard_error": 0.00229221301959553
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.796497381998186,
            "upper_bound": 8.807714078656758
          },
          "point_estimate": 8.804923409093938,
          "standard_error": 0.003074892920317667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005241324059601587,
            "upper_bound": 0.01269122374494044
          },
          "point_estimate": 0.007229926268101126,
          "standard_error": 0.0031872098479042554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.800307457298928,
            "upper_bound": 8.806776564333141
          },
          "point_estimate": 8.803383090283859,
          "standard_error": 0.0016630150167469366
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004031170704479684,
            "upper_bound": 0.009908951846099075
          },
          "point_estimate": 0.007635940894648891,
          "standard_error": 0.0015304235768649962
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.530180385954242,
            "upper_bound": 9.544411583341873
          },
          "point_estimate": 9.535897351547812,
          "standard_error": 0.0038079627101922976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.529156828109487,
            "upper_bound": 9.535529386428683
          },
          "point_estimate": 9.534123359124244,
          "standard_error": 0.001698141410852926
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00023625823088433592,
            "upper_bound": 0.009804877984309584
          },
          "point_estimate": 0.003005270581825816,
          "standard_error": 0.0025540907857327898
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.532065563534903,
            "upper_bound": 9.540158405441868
          },
          "point_estimate": 9.535163852593303,
          "standard_error": 0.002076988629917578
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002312614609416337,
            "upper_bound": 0.019177904055672473
          },
          "point_estimate": 0.012673285025295857,
          "standard_error": 0.0054365490995312805
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.33012218191173,
            "upper_bound": 10.341038396079528
          },
          "point_estimate": 10.335633715149015,
          "standard_error": 0.0027924282509483997
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.326428995417023,
            "upper_bound": 10.34233291335192
          },
          "point_estimate": 10.337579305605464,
          "standard_error": 0.0034131199139469056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006347784281613165,
            "upper_bound": 0.01726837487138632
          },
          "point_estimate": 0.008287885540213681,
          "standard_error": 0.00465772980574445
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.33224770573642,
            "upper_bound": 10.341430659341889
          },
          "point_estimate": 10.33713995907618,
          "standard_error": 0.002281066033264205
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005083455925524209,
            "upper_bound": 0.011751271119382744
          },
          "point_estimate": 0.009306722192111651,
          "standard_error": 0.0017048348196800572
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.06844052327396,
            "upper_bound": 8.090302956802068
          },
          "point_estimate": 8.078328515119198,
          "standard_error": 0.005633848705797183
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.06659640277797,
            "upper_bound": 8.091621913724609
          },
          "point_estimate": 8.072906919953986,
          "standard_error": 0.004897573888688613
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001655833061922562,
            "upper_bound": 0.025288120175232937
          },
          "point_estimate": 0.00819122060610659,
          "standard_error": 0.006023801722310053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.069255307394942,
            "upper_bound": 8.07468646365226
          },
          "point_estimate": 8.07202361479817,
          "standard_error": 0.0013630247345481552
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005316170433209752,
            "upper_bound": 0.024439505714568995
          },
          "point_estimate": 0.018800358124153132,
          "standard_error": 0.005012966964011675
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.54005396530314,
            "upper_bound": 9.55681981462558
          },
          "point_estimate": 9.548375132753474,
          "standard_error": 0.004291944573282552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.53718121821034,
            "upper_bound": 9.55898710565609
          },
          "point_estimate": 9.549237726430274,
          "standard_error": 0.005603624177368945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002993934867168302,
            "upper_bound": 0.024787865879496593
          },
          "point_estimate": 0.015303920808243018,
          "standard_error": 0.005136901787707488
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.538701836198513,
            "upper_bound": 9.549592472032346
          },
          "point_estimate": 9.544534553308834,
          "standard_error": 0.002765956354012839
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008040688618372784,
            "upper_bound": 0.01826060627421133
          },
          "point_estimate": 0.014300744672196712,
          "standard_error": 0.0026277102796947163
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.474587211476704,
            "upper_bound": 12.485222582278984
          },
          "point_estimate": 12.480148207898475,
          "standard_error": 0.002701744330004763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.476424490124828,
            "upper_bound": 12.484557554940348
          },
          "point_estimate": 12.480958131962144,
          "standard_error": 0.0022390987755589336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015841929887797416,
            "upper_bound": 0.01382262263938592
          },
          "point_estimate": 0.005360104673944837,
          "standard_error": 0.002908527708166727
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.479049465657292,
            "upper_bound": 12.486205115907929
          },
          "point_estimate": 12.4824870795383,
          "standard_error": 0.0018559629423357368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030895220614417795,
            "upper_bound": 0.012634714015266377
          },
          "point_estimate": 0.009010744951159552,
          "standard_error": 0.002526012533494103
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049652396333686,
            "upper_bound": 9.062491559732257
          },
          "point_estimate": 9.056157621207968,
          "standard_error": 0.0032862181954021728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.045810318006277,
            "upper_bound": 9.067668050284452
          },
          "point_estimate": 9.058432594584712,
          "standard_error": 0.0057631089475907865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0019392510959348748,
            "upper_bound": 0.017954527700376283
          },
          "point_estimate": 0.014708144047241106,
          "standard_error": 0.0042211373778962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.049373083337796,
            "upper_bound": 9.059318220045297
          },
          "point_estimate": 9.054865705152997,
          "standard_error": 0.0025438521164710404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006966595314930769,
            "upper_bound": 0.012870655783499866
          },
          "point_estimate": 0.010971442102243377,
          "standard_error": 0.001505804147998709
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.18043234400312,
            "upper_bound": 10.192653239490165
          },
          "point_estimate": 10.186140033005357,
          "standard_error": 0.003147917591358294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.17831541118257,
            "upper_bound": 10.192928775803273
          },
          "point_estimate": 10.18374240936515,
          "standard_error": 0.003714086816361523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002448152070814464,
            "upper_bound": 0.017012472223808823
          },
          "point_estimate": 0.008878724012181285,
          "standard_error": 0.0038371522835884274
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.180070500111457,
            "upper_bound": 10.19357217055142
          },
          "point_estimate": 10.188042673100147,
          "standard_error": 0.0034817408433051485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004580717023962864,
            "upper_bound": 0.013691289481228736
          },
          "point_estimate": 0.010504605117787075,
          "standard_error": 0.002357064553585456
        }
      }
    },
    "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.522272741512474,
            "upper_bound": 19.54800680851298
          },
          "point_estimate": 19.532633402527647,
          "standard_error": 0.006987892477366692
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.521465890630655,
            "upper_bound": 19.536941816322923
          },
          "point_estimate": 19.52435594328695,
          "standard_error": 0.0035987391435649578
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0009582384240258272,
            "upper_bound": 0.017569771726555182
          },
          "point_estimate": 0.004381785805907475,
          "standard_error": 0.004499481026608611
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.519323069030563,
            "upper_bound": 19.52790600145605
          },
          "point_estimate": 19.522370792669097,
          "standard_error": 0.0022090477078450564
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002719004046176203,
            "upper_bound": 0.034817896210311575
          },
          "point_estimate": 0.02333016015243365,
          "standard_error": 0.009778642686459471
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98882.8010607531,
            "upper_bound": 98991.87944644476
          },
          "point_estimate": 98938.9458110119,
          "standard_error": 28.014589221818383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98872.21396221532,
            "upper_bound": 99020.48278985509
          },
          "point_estimate": 98937.08192934784,
          "standard_error": 30.02452651817518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.1391246924603,
            "upper_bound": 173.67159400910322
          },
          "point_estimate": 81.94319311043506,
          "standard_error": 49.87304328245952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98902.74498227518,
            "upper_bound": 99028.16676720323
          },
          "point_estimate": 98980.59405702991,
          "standard_error": 32.35078828462034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.92383565153769,
            "upper_bound": 118.71609285038907
          },
          "point_estimate": 93.3161484447872,
          "standard_error": 18.14089519333229
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52421.83899038956,
            "upper_bound": 52495.00744330229
          },
          "point_estimate": 52456.1275626115,
          "standard_error": 18.74870363816312
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52403.44650576369,
            "upper_bound": 52492.24543707973
          },
          "point_estimate": 52444.3394092219,
          "standard_error": 25.305986033763112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.913922679331157,
            "upper_bound": 106.89503083999
          },
          "point_estimate": 65.81420387479005,
          "standard_error": 23.912259123948267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52416.82038836536,
            "upper_bound": 52475.98391747987
          },
          "point_estimate": 52449.104992701825,
          "standard_error": 14.959164647937476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.42389435091692,
            "upper_bound": 84.3190460292453
          },
          "point_estimate": 62.62672501325573,
          "standard_error": 14.922479169532274
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144763.64072709164,
            "upper_bound": 144904.11110225762
          },
          "point_estimate": 144834.01906706506,
          "standard_error": 35.867861750201605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144745.49037184595,
            "upper_bound": 144916.6544820717
          },
          "point_estimate": 144834.46010956174,
          "standard_error": 39.49520949042698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.955689406259967,
            "upper_bound": 198.35327014386627
          },
          "point_estimate": 86.39049009973087,
          "standard_error": 47.91155566654016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 144788.5865684687,
            "upper_bound": 144935.1461964641
          },
          "point_estimate": 144863.45526982978,
          "standard_error": 36.60930683726434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.95962181147024,
            "upper_bound": 155.4810502851814
          },
          "point_estimate": 119.34863517056236,
          "standard_error": 24.049705694286043
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397384.2445436077,
            "upper_bound": 398154.6766279115
          },
          "point_estimate": 397778.693631815,
          "standard_error": 197.49737139960465
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397200.0608695652,
            "upper_bound": 398341.2422360248
          },
          "point_estimate": 397789.5665760869,
          "standard_error": 277.52910111941287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.58157725780737,
            "upper_bound": 1191.4749507492336
          },
          "point_estimate": 692.3827569106056,
          "standard_error": 265.17834209076796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397523.2505903598,
            "upper_bound": 398108.4590261283
          },
          "point_estimate": 397828.36708074535,
          "standard_error": 150.80642621803818
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369.8262064340666,
            "upper_bound": 811.6503202898269
          },
          "point_estimate": 657.3468028775901,
          "standard_error": 112.21030218194646
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572663.8811625744,
            "upper_bound": 573503.7963362631
          },
          "point_estimate": 573108.0052101933,
          "standard_error": 215.33573918822376
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572584.869140625,
            "upper_bound": 573643.7447916666
          },
          "point_estimate": 573222.5723958333,
          "standard_error": 239.2440594488851
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.83969817784288,
            "upper_bound": 1166.0284761738687
          },
          "point_estimate": 784.9445061816081,
          "standard_error": 297.4976320713962
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 572743.9819647493,
            "upper_bound": 573279.4832444561
          },
          "point_estimate": 573024.0071834415,
          "standard_error": 136.6172543649839
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 348.3862671391339,
            "upper_bound": 946.8406452098684
          },
          "point_estimate": 720.14840035437,
          "standard_error": 158.81353795439497
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40164.255990307625,
            "upper_bound": 40259.79463311209
          },
          "point_estimate": 40208.89676016646,
          "standard_error": 24.526724877374956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40143.157079646015,
            "upper_bound": 40252.11128318584
          },
          "point_estimate": 40196.71805862832,
          "standard_error": 27.570965263792836
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.80062993840305,
            "upper_bound": 131.0118475634626
          },
          "point_estimate": 77.6437934224326,
          "standard_error": 27.311974789704696
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40147.56978845855,
            "upper_bound": 40236.25336272958
          },
          "point_estimate": 40195.88615389036,
          "standard_error": 22.356181961594753
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.515680021619595,
            "upper_bound": 110.40383111468854
          },
          "point_estimate": 81.47446228104977,
          "standard_error": 19.908893906325037
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98079.9491935406,
            "upper_bound": 98170.99016366372
          },
          "point_estimate": 98124.61676603346,
          "standard_error": 23.362450300075228
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98063.70655884995,
            "upper_bound": 98181.35714285714
          },
          "point_estimate": 98110.68628331835,
          "standard_error": 34.72694585004579
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.607116423572621,
            "upper_bound": 132.15775834555564
          },
          "point_estimate": 95.97107246509042,
          "standard_error": 31.438065723587343
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 98058.31473708485,
            "upper_bound": 98153.58786166237
          },
          "point_estimate": 98098.4676934925,
          "standard_error": 23.953448634507204
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.14560266312825,
            "upper_bound": 99.90326254727954
          },
          "point_estimate": 78.15103568298775,
          "standard_error": 14.516360426275314
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68867.26296681473,
            "upper_bound": 68977.76861742426
          },
          "point_estimate": 68920.33961843133,
          "standard_error": 28.2989593214502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68872.30976430976,
            "upper_bound": 68984.98846726191
          },
          "point_estimate": 68892.34647253787,
          "standard_error": 28.86403649518309
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6679077651423606,
            "upper_bound": 157.69402248445715
          },
          "point_estimate": 58.60094062628875,
          "standard_error": 40.625636228768094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68860.73282643902,
            "upper_bound": 68955.0491026638
          },
          "point_estimate": 68900.41533844943,
          "standard_error": 23.646815370332902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.609733687230445,
            "upper_bound": 124.64450590215031
          },
          "point_estimate": 94.3967441005938,
          "standard_error": 20.995968233339365
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308219.46499303874,
            "upper_bound": 308643.69087964087
          },
          "point_estimate": 308413.09001109767,
          "standard_error": 109.5375571688888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308199.47669491527,
            "upper_bound": 308695.46101694915
          },
          "point_estimate": 308271.2015738499,
          "standard_error": 123.40502308469932
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.518075307436844,
            "upper_bound": 628.0530118159328
          },
          "point_estimate": 109.96492018757714,
          "standard_error": 154.3341650913954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308183.7960494235,
            "upper_bound": 308543.2015413491
          },
          "point_estimate": 308350.5638784944,
          "standard_error": 100.62557712700752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 129.5325760230784,
            "upper_bound": 484.6690762954386
          },
          "point_estimate": 364.3952342799968,
          "standard_error": 92.45442610815428
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34961.74417712494,
            "upper_bound": 35050.88881480461
          },
          "point_estimate": 35004.60428295909,
          "standard_error": 22.758588991194713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34954.4305689489,
            "upper_bound": 35069.8725168756
          },
          "point_estimate": 34989.614134178264,
          "standard_error": 28.226709728418655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.006142067242113,
            "upper_bound": 131.01577747149932
          },
          "point_estimate": 59.7235447159689,
          "standard_error": 29.976408969222295
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34948.34171409156,
            "upper_bound": 35015.728489270485
          },
          "point_estimate": 34984.64953349447,
          "standard_error": 17.193119264908084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.30839669847167,
            "upper_bound": 99.4960387156696
          },
          "point_estimate": 75.96212171956327,
          "standard_error": 15.833945915714022
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65213.37469233564,
            "upper_bound": 65345.930708418826
          },
          "point_estimate": 65278.37129755493,
          "standard_error": 33.98840897494758
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65188.08438061041,
            "upper_bound": 65368.479802513466
          },
          "point_estimate": 65251.30771992818,
          "standard_error": 48.022104619731024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.104906122807511,
            "upper_bound": 187.18106687435997
          },
          "point_estimate": 129.36616386128193,
          "standard_error": 43.53102944951066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65255.44030274431,
            "upper_bound": 65393.19707291958
          },
          "point_estimate": 65320.81233416494,
          "standard_error": 35.470682261460105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.95155551072378,
            "upper_bound": 142.92337927405484
          },
          "point_estimate": 113.27155930871452,
          "standard_error": 20.04521712892595
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169726.44925122385,
            "upper_bound": 169982.3801869159
          },
          "point_estimate": 169840.01554795282,
          "standard_error": 65.88912182195041
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169692.6964953271,
            "upper_bound": 169897.85046728974
          },
          "point_estimate": 169829.06347352025,
          "standard_error": 56.435510621255034
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.270122484061005,
            "upper_bound": 288.4740031962989
          },
          "point_estimate": 114.37288871713533,
          "standard_error": 65.58832119771255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169684.40075368033,
            "upper_bound": 169842.47204906898
          },
          "point_estimate": 169773.2103531982,
          "standard_error": 40.84989283528824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.17822935145726,
            "upper_bound": 315.8668740435635
          },
          "point_estimate": 219.79065341356232,
          "standard_error": 71.36514205590642
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34439.88166613957,
            "upper_bound": 34494.96841736507
          },
          "point_estimate": 34465.36618068733,
          "standard_error": 14.112082779993845
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34429.88111954459,
            "upper_bound": 34505.03795066414
          },
          "point_estimate": 34447.27910868649,
          "standard_error": 18.818183475118637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.949652210581904,
            "upper_bound": 74.41327505276455
          },
          "point_estimate": 28.66078230110744,
          "standard_error": 17.733942149813124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34435.503511302886,
            "upper_bound": 34471.538321656204
          },
          "point_estimate": 34448.956778136475,
          "standard_error": 9.237341964576734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.01913373527941,
            "upper_bound": 59.140625541180285
          },
          "point_estimate": 47.089718202546685,
          "standard_error": 10.14451072210781
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11389.651388416623,
            "upper_bound": 11409.488854039591
          },
          "point_estimate": 11399.335431474608,
          "standard_error": 5.091551636396123
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11385.195584090196,
            "upper_bound": 11418.905174513691
          },
          "point_estimate": 11396.339686516636,
          "standard_error": 8.696575457079163
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.219256678419585,
            "upper_bound": 27.790159434594116
          },
          "point_estimate": 17.30552895991159,
          "standard_error": 6.779166929791372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11393.92457586336,
            "upper_bound": 11417.496870509767
          },
          "point_estimate": 11409.820387129314,
          "standard_error": 5.948347798649242
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.116728874619431,
            "upper_bound": 20.10220241459949
          },
          "point_estimate": 16.9627059393905,
          "standard_error": 2.493507444027181
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498287.2801645738,
            "upper_bound": 498843.861575886
          },
          "point_estimate": 498590.2211573169,
          "standard_error": 142.52352249448526
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498398.0068493151,
            "upper_bound": 498816.5029354207
          },
          "point_estimate": 498717.93179223745,
          "standard_error": 122.29029951883766
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.36943416157779,
            "upper_bound": 678.7537877213888
          },
          "point_estimate": 249.28192242368408,
          "standard_error": 163.7320707130103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498449.12794085674,
            "upper_bound": 499027.00529759214
          },
          "point_estimate": 498717.1073830279,
          "standard_error": 153.74801673605387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.1108907597636,
            "upper_bound": 680.3284788803892
          },
          "point_estimate": 474.80673165973553,
          "standard_error": 145.7343128091583
        }
      }
    },
    "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 972.9402109180932,
            "upper_bound": 973.9513498456366
          },
          "point_estimate": 973.460545228198,
          "standard_error": 0.25967048173644075
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 972.7955898094626,
            "upper_bound": 974.2442419539
          },
          "point_estimate": 973.4485881307494,
          "standard_error": 0.35260547046240603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2109172128579931,
            "upper_bound": 1.4859620605647297
          },
          "point_estimate": 0.9001682789056914,
          "standard_error": 0.3389099297108005
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 973.2640864596772,
            "upper_bound": 974.1063139737838
          },
          "point_estimate": 973.7317921855496,
          "standard_error": 0.2111417035944285
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49102015512979874,
            "upper_bound": 1.0916359868901857
          },
          "point_estimate": 0.8657862760312292,
          "standard_error": 0.15682716666780888
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247007.9698077542,
            "upper_bound": 247354.7599341417
          },
          "point_estimate": 247187.82693935008,
          "standard_error": 88.60172870227794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247013.647240991,
            "upper_bound": 247379.8072916667
          },
          "point_estimate": 247239.55067567568,
          "standard_error": 89.20954610899896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.69537737849724,
            "upper_bound": 495.18588681680535
          },
          "point_estimate": 205.9072814795689,
          "standard_error": 122.24302050999246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247121.4197398333,
            "upper_bound": 247271.7096524161
          },
          "point_estimate": 247195.31379431376,
          "standard_error": 37.248876898120315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.69080156142343,
            "upper_bound": 389.3517513913241
          },
          "point_estimate": 294.2888440485638,
          "standard_error": 64.96717998341967
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195077.2128504796,
            "upper_bound": 195459.15556446815
          },
          "point_estimate": 195253.6686157797,
          "standard_error": 98.21024169516076
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195035.20320855617,
            "upper_bound": 195460.38522727272
          },
          "point_estimate": 195181.522875817,
          "standard_error": 115.58257191562724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.87493592310703,
            "upper_bound": 512.8192621968906
          },
          "point_estimate": 225.3580275498973,
          "standard_error": 126.11737259489618
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195143.6892678169,
            "upper_bound": 195531.5515884389
          },
          "point_estimate": 195343.65650392388,
          "standard_error": 100.16766312858344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.58739588759445,
            "upper_bound": 440.84159152477554
          },
          "point_estimate": 327.3381560057497,
          "standard_error": 81.42263585561012
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/libc_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174429.0324963925,
            "upper_bound": 174640.1701398382
          },
          "point_estimate": 174544.02582175136,
          "standard_error": 54.28155325641769
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174422.13157894736,
            "upper_bound": 174658.44552289817
          },
          "point_estimate": 174611.57509303562,
          "standard_error": 56.522704621365214
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.372522548268677,
            "upper_bound": 285.43631852099435
          },
          "point_estimate": 92.11070362786326,
          "standard_error": 65.01387841342658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174483.22183702065,
            "upper_bound": 174641.6195465649
          },
          "point_estimate": 174587.5885291742,
          "standard_error": 40.781615828791296
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.40777792845559,
            "upper_bound": 233.76432732021024
          },
          "point_estimate": 181.4133801433316,
          "standard_error": 44.56505934913982
        }
      }
    },
    "memmem/libc/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/libc_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37413.42510465758,
            "upper_bound": 37468.803354412135
          },
          "point_estimate": 37438.74608206513,
          "standard_error": 14.26151363182556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37406.15567010309,
            "upper_bound": 37472.17843642612
          },
          "point_estimate": 37425.63543814433,
          "standard_error": 14.385989294787125
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.868545460535536,
            "upper_bound": 73.8696520906156
          },
          "point_estimate": 29.207874533002865,
          "standard_error": 16.44060102028991
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37414.23010532584,
            "upper_bound": 37450.68527433164
          },
          "point_estimate": 37429.72334716829,
          "standard_error": 9.152501423324123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.96020543294936,
            "upper_bound": 62.763044835575755
          },
          "point_estimate": 47.5347709899464,
          "standard_error": 12.093847226145575
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71681.52039916886,
            "upper_bound": 71758.98393056923
          },
          "point_estimate": 71720.2384552712,
          "standard_error": 19.757818612843383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71664.94537401575,
            "upper_bound": 71757.6198600175
          },
          "point_estimate": 71723.0754675197,
          "standard_error": 22.45543648622744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.374972124072107,
            "upper_bound": 109.48271179644348
          },
          "point_estimate": 42.53340007165875,
          "standard_error": 28.15912311704426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71712.70376359206,
            "upper_bound": 71754.71429150143
          },
          "point_estimate": 71733.79365988342,
          "standard_error": 10.588071083035183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.5914995560569,
            "upper_bound": 86.1234671832711
          },
          "point_estimate": 65.74865052260918,
          "standard_error": 13.644996965928009
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68739.09336365783,
            "upper_bound": 68848.11619880277
          },
          "point_estimate": 68787.86640779246,
          "standard_error": 28.100617299818246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68726.24373818527,
            "upper_bound": 68841.36055450534
          },
          "point_estimate": 68755.98651543794,
          "standard_error": 26.704861639615117
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.06220090232796,
            "upper_bound": 136.10439031208708
          },
          "point_estimate": 47.025406315417094,
          "standard_error": 31.06283832724724
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68739.18145068067,
            "upper_bound": 68795.88087300223
          },
          "point_estimate": 68765.31791422189,
          "standard_error": 14.517384038652931
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.974343644383698,
            "upper_bound": 125.25782941562449
          },
          "point_estimate": 93.39107637522184,
          "standard_error": 25.814731541299057
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82381.05521098182,
            "upper_bound": 82438.77356065503
          },
          "point_estimate": 82409.04586655174,
          "standard_error": 14.79531872395808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82367.81023755655,
            "upper_bound": 82457.18438914027
          },
          "point_estimate": 82399.07117269986,
          "standard_error": 22.28384570901675
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.655301353535111,
            "upper_bound": 80.71532537697396
          },
          "point_estimate": 47.80234204351271,
          "standard_error": 18.528738286882493
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82373.40365278353,
            "upper_bound": 82444.37136066548
          },
          "point_estimate": 82408.74632426396,
          "standard_error": 18.82994856975643
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.057831802449993,
            "upper_bound": 60.27937735395937
          },
          "point_estimate": 49.20163272102452,
          "standard_error": 8.311930533491795
        }
      }
    },
    "memmem/libc/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321376.9462406015,
            "upper_bound": 321922.22188387637
          },
          "point_estimate": 321628.7916144528,
          "standard_error": 139.8689508303858
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321311.5811403509,
            "upper_bound": 321864.8333333333
          },
          "point_estimate": 321576.31140350876,
          "standard_error": 123.75992630812068
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.34670687663079,
            "upper_bound": 686.0859263283036
          },
          "point_estimate": 410.1258433766745,
          "standard_error": 163.2669464745593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321386.7525704632,
            "upper_bound": 321943.78052240267
          },
          "point_estimate": 321694.6120984279,
          "standard_error": 141.00186404548563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.2373881524069,
            "upper_bound": 649.4673479058964
          },
          "point_estimate": 465.2423822195954,
          "standard_error": 128.35876541036285
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47635.70575147511,
            "upper_bound": 47733.336405996786
          },
          "point_estimate": 47681.47157810812,
          "standard_error": 25.0573611701448
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47619.54793848168,
            "upper_bound": 47742.383944153575
          },
          "point_estimate": 47662.82395287958,
          "standard_error": 29.731019732622208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.9346562128679903,
            "upper_bound": 129.33891470311278
          },
          "point_estimate": 77.04733917860278,
          "standard_error": 33.66988842135952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47619.49015482903,
            "upper_bound": 47684.563691477575
          },
          "point_estimate": 47643.526874957504,
          "standard_error": 16.646499492445077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.28589744247656,
            "upper_bound": 105.7432183507751
          },
          "point_estimate": 83.55779781150703,
          "standard_error": 17.45917358289477
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33952.870289070604,
            "upper_bound": 34007.32039692553
          },
          "point_estimate": 33980.348753968254,
          "standard_error": 13.922593818146773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33951.96394080997,
            "upper_bound": 34016.63217623498
          },
          "point_estimate": 33978.03355399793,
          "standard_error": 15.899135736609368
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.11025797471278,
            "upper_bound": 81.87648787351145
          },
          "point_estimate": 40.96500320575472,
          "standard_error": 17.30738540611463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33924.11681907394,
            "upper_bound": 33996.24989880405
          },
          "point_estimate": 33960.7944119432,
          "standard_error": 19.697284854736232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.36952898781627,
            "upper_bound": 62.22641556448199
          },
          "point_estimate": 46.525645765108614,
          "standard_error": 10.261744064635284
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36933.14676536943,
            "upper_bound": 37002.30876447708
          },
          "point_estimate": 36966.79401998227,
          "standard_error": 17.808271238043723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36902.87436548223,
            "upper_bound": 37016.280315848846
          },
          "point_estimate": 36958.33627024414,
          "standard_error": 27.38743309097429
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1593630758697724,
            "upper_bound": 99.00565558747552
          },
          "point_estimate": 82.37645483193926,
          "standard_error": 24.30819930625282
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36927.87749158165,
            "upper_bound": 36995.84287868756
          },
          "point_estimate": 36959.59579669062,
          "standard_error": 17.502067436426483
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.555746868184094,
            "upper_bound": 72.92206999002215
          },
          "point_estimate": 59.47359507624631,
          "standard_error": 10.005371680675893
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84773.06717906865,
            "upper_bound": 86127.59209501292
          },
          "point_estimate": 85464.86419968854,
          "standard_error": 346.13901744919224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84155.52587339112,
            "upper_bound": 86445.21985815602
          },
          "point_estimate": 86114.39314420804,
          "standard_error": 737.7302829351205
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128.34303672855418,
            "upper_bound": 1792.9520916616425
          },
          "point_estimate": 685.5314455598884,
          "standard_error": 509.16668944904666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84195.94121073249,
            "upper_bound": 85615.08928116722
          },
          "point_estimate": 84641.8327837647,
          "standard_error": 367.4753741989127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 732.260064153869,
            "upper_bound": 1264.7374510593502
          },
          "point_estimate": 1156.0948132117696,
          "standard_error": 139.61508153692742
        }
      }
    },
    "memmem/libc/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67147.00332674399,
            "upper_bound": 67283.44103936039
          },
          "point_estimate": 67208.03955675628,
          "standard_error": 35.149724704544425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67131.13653136531,
            "upper_bound": 67257.9971402214
          },
          "point_estimate": 67182.13906826568,
          "standard_error": 34.510402564096445
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.055991142579852,
            "upper_bound": 160.9465305743604
          },
          "point_estimate": 86.03367624934671,
          "standard_error": 34.19364583055489
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67119.48257268855,
            "upper_bound": 67219.32477780302
          },
          "point_estimate": 67168.9311113241,
          "standard_error": 26.351195935846956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.6616131971385,
            "upper_bound": 164.91822099223918
          },
          "point_estimate": 117.12305232490246,
          "standard_error": 34.68984682729893
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148795.782495987,
            "upper_bound": 149088.013613205
          },
          "point_estimate": 148931.56876967865,
          "standard_error": 74.59978546886286
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148783.0364298725,
            "upper_bound": 149030.3512880562
          },
          "point_estimate": 148930.10356898909,
          "standard_error": 54.827953877379336
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.12543022670319,
            "upper_bound": 376.11382569972176
          },
          "point_estimate": 128.95671955827063,
          "standard_error": 97.67293287180284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 148797.06676849554,
            "upper_bound": 148949.06663469365
          },
          "point_estimate": 148868.02300404513,
          "standard_error": 37.97514008773389
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.47564142768454,
            "upper_bound": 345.34202242059865
          },
          "point_estimate": 247.8201715399022,
          "standard_error": 67.81017718853963
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198598.99950949775,
            "upper_bound": 198924.3685398452
          },
          "point_estimate": 198760.4540823142,
          "standard_error": 83.2519847502863
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198490.84192037472,
            "upper_bound": 198957.36778992103
          },
          "point_estimate": 198821.13606557375,
          "standard_error": 146.21611391829023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.83591901539923,
            "upper_bound": 467.01737138089953
          },
          "point_estimate": 350.1162602687099,
          "standard_error": 117.03971059453522
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198723.564378819,
            "upper_bound": 198961.44619470648
          },
          "point_estimate": 198868.71400184516,
          "standard_error": 61.17728339347648
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.1168762328734,
            "upper_bound": 340.8329433395952
          },
          "point_estimate": 278.1789279873664,
          "standard_error": 42.85267237166141
        }
      }
    },
    "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119668.53400723814,
            "upper_bound": 119843.74797436951
          },
          "point_estimate": 119753.4329070071,
          "standard_error": 44.81501001350181
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119592.36220760236,
            "upper_bound": 119863.7379111842
          },
          "point_estimate": 119742.71820175438,
          "standard_error": 63.557865422841594
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.099991908204586,
            "upper_bound": 264.52975736505107
          },
          "point_estimate": 177.22813676584877,
          "standard_error": 63.333499071424555
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119609.9376650851,
            "upper_bound": 119890.02521090391
          },
          "point_estimate": 119714.41954887216,
          "standard_error": 73.45834867524253
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.80347524562191,
            "upper_bound": 188.45671031946253
          },
          "point_estimate": 149.04298331180266,
          "standard_error": 27.538938322324015
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52411.44383494813,
            "upper_bound": 52536.795830527495
          },
          "point_estimate": 52474.42956807303,
          "standard_error": 31.85266371768248
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52418.31387571864,
            "upper_bound": 52539.65930735931
          },
          "point_estimate": 52464.02576358826,
          "standard_error": 38.58516142315132
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.87877019498542,
            "upper_bound": 184.74290328581355
          },
          "point_estimate": 84.25238811461087,
          "standard_error": 43.32805339647389
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52435.69717520182,
            "upper_bound": 52524.97876886249
          },
          "point_estimate": 52480.50161353798,
          "standard_error": 22.803094750125965
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.83594494169481,
            "upper_bound": 142.6118443422183
          },
          "point_estimate": 106.22602433497666,
          "standard_error": 24.70482110879419
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77386.49250771278,
            "upper_bound": 77487.10480496453
          },
          "point_estimate": 77435.45517907801,
          "standard_error": 25.76659303359514
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77368.16263297873,
            "upper_bound": 77520.16808510639
          },
          "point_estimate": 77417.3214893617,
          "standard_error": 35.52701138155542
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.157207287688896,
            "upper_bound": 141.57178910360727
          },
          "point_estimate": 91.54146613544226,
          "standard_error": 34.1000603469548
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77373.93824338034,
            "upper_bound": 77464.76895987714
          },
          "point_estimate": 77422.69220226583,
          "standard_error": 22.976386218985112
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.51825060311,
            "upper_bound": 107.27314726890975
          },
          "point_estimate": 85.90161017067112,
          "standard_error": 15.224747686138464
        }
      }
    },
    "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40718.99226766341,
            "upper_bound": 40794.09747937933
          },
          "point_estimate": 40757.288147791856,
          "standard_error": 19.291524821967332
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40676.25084175084,
            "upper_bound": 40811.10628329147
          },
          "point_estimate": 40772.53995510662,
          "standard_error": 29.260940159312263
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.709028343292408,
            "upper_bound": 115.45402945047756
          },
          "point_estimate": 58.93302540341622,
          "standard_error": 30.23705579579179
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40745.78744771824,
            "upper_bound": 40807.92196841392
          },
          "point_estimate": 40780.56720159751,
          "standard_error": 15.671586975135128
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.19826161474721,
            "upper_bound": 76.77046011833238
          },
          "point_estimate": 63.96223419365437,
          "standard_error": 10.172220594885266
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93580.914889748,
            "upper_bound": 93706.17432028308
          },
          "point_estimate": 93635.49006770576,
          "standard_error": 32.464531636287376
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93556.85077319588,
            "upper_bound": 93671.9038107511
          },
          "point_estimate": 93615.72820733103,
          "standard_error": 26.770983947036253
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.335957796463916,
            "upper_bound": 132.97418784541844
          },
          "point_estimate": 71.74494239895245,
          "standard_error": 31.088580330725385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93578.93089997214,
            "upper_bound": 93652.66698044613
          },
          "point_estimate": 93610.39801178205,
          "standard_error": 18.923535142169857
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.319048149843375,
            "upper_bound": 155.48984550182675
          },
          "point_estimate": 107.94561347317672,
          "standard_error": 35.47651718199699
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1320384.3352849702,
            "upper_bound": 1321770.7529464285
          },
          "point_estimate": 1321065.2631249998,
          "standard_error": 355.2070196203208
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1319860.4142857145,
            "upper_bound": 1322128.057142857
          },
          "point_estimate": 1321107.1904761903,
          "standard_error": 633.1828030386903
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.903609114089726,
            "upper_bound": 2160.428796644909
          },
          "point_estimate": 1767.6051086188284,
          "standard_error": 581.0149511886184
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1320817.340715845,
            "upper_bound": 1322109.5943865343
          },
          "point_estimate": 1321529.9396103895,
          "standard_error": 323.7680401602464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 731.5993795094267,
            "upper_bound": 1420.92353077781
          },
          "point_estimate": 1183.228330127998,
          "standard_error": 177.52963444615904
        }
      }
    },
    "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/libc_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156919.4611885006,
            "upper_bound": 157067.85997690886
          },
          "point_estimate": 156990.0352704228,
          "standard_error": 37.99252918658802
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156898.40229885056,
            "upper_bound": 157084.95021551725
          },
          "point_estimate": 156969.0459770115,
          "standard_error": 41.308044494656954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.755287435838206,
            "upper_bound": 217.1593577825855
          },
          "point_estimate": 99.84209219298154,
          "standard_error": 50.17892113470113
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156908.64934839658,
            "upper_bound": 157052.02531097466
          },
          "point_estimate": 156979.62424988803,
          "standard_error": 36.322295148942246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.79899238282629,
            "upper_bound": 163.4924645638731
          },
          "point_estimate": 126.4187396115665,
          "standard_error": 26.786859185449345
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6833.476750123098,
            "upper_bound": 6849.757781328988
          },
          "point_estimate": 6841.490704154321,
          "standard_error": 4.173183836726589
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6828.72936642226,
            "upper_bound": 6853.211505922166
          },
          "point_estimate": 6841.343297612333,
          "standard_error": 7.422354486739417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.532487922703508,
            "upper_bound": 23.44013136050354
          },
          "point_estimate": 18.148609689078256,
          "standard_error": 5.274844777214066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6835.870692713682,
            "upper_bound": 6854.229167199498
          },
          "point_estimate": 6846.968709575816,
          "standard_error": 4.632427116747136
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.87672655800733,
            "upper_bound": 16.540490292267503
          },
          "point_estimate": 13.941792416781428,
          "standard_error": 1.9620409878411005
        }
      }
    },
    "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/libc_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7479.523262311636,
            "upper_bound": 7490.123845423557
          },
          "point_estimate": 7484.607378337925,
          "standard_error": 2.7139486762484193
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7477.715338686432,
            "upper_bound": 7492.911902637659
          },
          "point_estimate": 7481.152863564616,
          "standard_error": 4.03230789364927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.35972458183829165,
            "upper_bound": 14.40860831430105
          },
          "point_estimate": 9.3725422980881,
          "standard_error": 3.614266284548249
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7476.067695164747,
            "upper_bound": 7488.536121495834
          },
          "point_estimate": 7481.448787531184,
          "standard_error": 3.2983265513018316
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.035710450625385,
            "upper_bound": 11.209212051790397
          },
          "point_estimate": 9.061429325825795,
          "standard_error": 1.5790509169739908
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47657.54726063056,
            "upper_bound": 47730.59036901902
          },
          "point_estimate": 47692.52042511806,
          "standard_error": 18.74788926119379
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47647.07601572739,
            "upper_bound": 47757.63931847968
          },
          "point_estimate": 47674.39678899082,
          "standard_error": 27.741589606285967
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.692858290963638,
            "upper_bound": 102.12842528165092
          },
          "point_estimate": 41.348362798029605,
          "standard_error": 26.154054877519425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47650.89331266563,
            "upper_bound": 47717.06235286336
          },
          "point_estimate": 47679.07242770336,
          "standard_error": 16.869921440536938
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.94293793316153,
            "upper_bound": 77.7644384823043
          },
          "point_estimate": 62.32884119845339,
          "standard_error": 11.50860696402168
        }
      }
    },
    "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/libc_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.97221115757414,
            "upper_bound": 116.104223052324
          },
          "point_estimate": 116.02886938829464,
          "standard_error": 0.03467157530503034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.95966840078977,
            "upper_bound": 116.06328184834202
          },
          "point_estimate": 115.99001178055424,
          "standard_error": 0.027762076979274634
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011175463820411416,
            "upper_bound": 0.13229159236415053
          },
          "point_estimate": 0.05349460207857101,
          "standard_error": 0.03147473514857344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.950913513566,
            "upper_bound": 116.0535974980634
          },
          "point_estimate": 115.99864394658452,
          "standard_error": 0.026284803816278884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03348665864407564,
            "upper_bound": 0.16770159489872297
          },
          "point_estimate": 0.1157276349233016,
          "standard_error": 0.03997680150472829
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.08224543114949,
            "upper_bound": 19.11490225562287
          },
          "point_estimate": 19.096425867747477,
          "standard_error": 0.008461002562883374
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.074250163678453,
            "upper_bound": 19.10566866351003
          },
          "point_estimate": 19.092915220570532,
          "standard_error": 0.007764627453168937
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003110803305393005,
            "upper_bound": 0.03425159078749185
          },
          "point_estimate": 0.022528228854268192,
          "standard_error": 0.008678021783910764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.07682969124971,
            "upper_bound": 19.09457953889277
          },
          "point_estimate": 19.085605770645692,
          "standard_error": 0.004732027470392034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010030539861689246,
            "upper_bound": 0.040857736633960674
          },
          "point_estimate": 0.02826575983328616,
          "standard_error": 0.009353356379284436
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.334976541557342,
            "upper_bound": 19.406266475020917
          },
          "point_estimate": 19.37306522331275,
          "standard_error": 0.01826133818924805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.339324499161272,
            "upper_bound": 19.424408144321458
          },
          "point_estimate": 19.378782081067776,
          "standard_error": 0.02378611634568091
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010759617948095214,
            "upper_bound": 0.10347928644683356
          },
          "point_estimate": 0.06250195258236917,
          "standard_error": 0.022010554453732022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.36635543809674,
            "upper_bound": 19.41321454826196
          },
          "point_estimate": 19.387481116897018,
          "standard_error": 0.01189355005048656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030390192432033468,
            "upper_bound": 0.08130081334316658
          },
          "point_estimate": 0.06086605083765679,
          "standard_error": 0.014359048929282404
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.663480900525272,
            "upper_bound": 18.716450591208513
          },
          "point_estimate": 18.690864285554326,
          "standard_error": 0.013567916695731628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.66690653304254,
            "upper_bound": 18.72698144980176
          },
          "point_estimate": 18.691349959143103,
          "standard_error": 0.01545076213729907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00661684624502314,
            "upper_bound": 0.07875107171181797
          },
          "point_estimate": 0.03657299182506242,
          "standard_error": 0.018083824173119142
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.672925432143455,
            "upper_bound": 18.716819499692733
          },
          "point_estimate": 18.692277416429196,
          "standard_error": 0.011252705886909022
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02135572723473683,
            "upper_bound": 0.060768168162050974
          },
          "point_estimate": 0.045274561002584666,
          "standard_error": 0.010272051688257552
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/libc_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.92640141658318,
            "upper_bound": 19.548124845040913
          },
          "point_estimate": 19.20350095705038,
          "standard_error": 0.16008153773350475
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.882502091193963,
            "upper_bound": 19.4518423610667
          },
          "point_estimate": 19.075716748330933,
          "standard_error": 0.12683989501055332
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05726089629619941,
            "upper_bound": 0.7609028844384493
          },
          "point_estimate": 0.29198632531438423,
          "standard_error": 0.18917483227782453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.954621557189473,
            "upper_bound": 19.119771456462612
          },
          "point_estimate": 19.043710357967157,
          "standard_error": 0.04118794932555328
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1807651499937853,
            "upper_bound": 0.7332739963615864
          },
          "point_estimate": 0.5326698358544463,
          "standard_error": 0.15321626891214096
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.944921740005476,
            "upper_bound": 22.974915550348065
          },
          "point_estimate": 22.96026487607944,
          "standard_error": 0.007701928578376809
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.938774484564576,
            "upper_bound": 22.981384806153883
          },
          "point_estimate": 22.962650674243548,
          "standard_error": 0.012184164606651974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0049543598981889704,
            "upper_bound": 0.044945696855387886
          },
          "point_estimate": 0.02495822196882819,
          "standard_error": 0.009808820136200078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.948683241126897,
            "upper_bound": 22.9749005739444
          },
          "point_estimate": 22.96230199427488,
          "standard_error": 0.006813941822989094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014932797757823286,
            "upper_bound": 0.03207282034092346
          },
          "point_estimate": 0.025684150865809455,
          "standard_error": 0.004411351879589519
        }
      }
    },
    "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.76039963041708,
            "upper_bound": 27.79750388726363
          },
          "point_estimate": 27.778999254604372,
          "standard_error": 0.009533282926740742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.75176792622908,
            "upper_bound": 27.80394892586928
          },
          "point_estimate": 27.78119567525234,
          "standard_error": 0.015523931303125786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008533597561526378,
            "upper_bound": 0.05216317748883104
          },
          "point_estimate": 0.03626141376840656,
          "standard_error": 0.011395484398954532
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.758840021269425,
            "upper_bound": 27.803484792171425
          },
          "point_estimate": 27.78062495637866,
          "standard_error": 0.011394001734068454
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01954174772871146,
            "upper_bound": 0.039153562306120875
          },
          "point_estimate": 0.031749731541394155,
          "standard_error": 0.0050350718036867425
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.177189762046535,
            "upper_bound": 24.223907877126624
          },
          "point_estimate": 24.200693063628343,
          "standard_error": 0.011943000554245548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.174378864425123,
            "upper_bound": 24.22945603372664
          },
          "point_estimate": 24.19973210295851,
          "standard_error": 0.012857863092747413
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006125488659200961,
            "upper_bound": 0.06887604920305714
          },
          "point_estimate": 0.03674920013396455,
          "standard_error": 0.015955275565408805
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.176810209319612,
            "upper_bound": 24.22494491319983
          },
          "point_estimate": 24.205104273298527,
          "standard_error": 0.012319534514324248
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020400311674526325,
            "upper_bound": 0.05198032819688028
          },
          "point_estimate": 0.039835037748413425,
          "standard_error": 0.008088964852003066
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.972870082114575,
            "upper_bound": 29.0344369473379
          },
          "point_estimate": 29.00242452654319,
          "standard_error": 0.015749692219679576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.96457183862223,
            "upper_bound": 29.04532331366608
          },
          "point_estimate": 28.988646514378352,
          "standard_error": 0.02150436454819101
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011501202506709609,
            "upper_bound": 0.085772285791859
          },
          "point_estimate": 0.03891954962796795,
          "standard_error": 0.020297568728888573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.97339027459909,
            "upper_bound": 29.028692655534545
          },
          "point_estimate": 28.994004560261935,
          "standard_error": 0.014151329169326732
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02644605205876548,
            "upper_bound": 0.06656852292512173
          },
          "point_estimate": 0.052336400015893465,
          "standard_error": 0.010146365765735564
        }
      }
    },
    "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.566899279024334,
            "upper_bound": 37.64518049737934
          },
          "point_estimate": 37.602998660941545,
          "standard_error": 0.02012420210266864
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.557182909465745,
            "upper_bound": 37.6647488338901
          },
          "point_estimate": 37.57809908285557,
          "standard_error": 0.02524870444202405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004536551367924119,
            "upper_bound": 0.10271916840255992
          },
          "point_estimate": 0.034432639895033035,
          "standard_error": 0.024941150649356043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.56050389217864,
            "upper_bound": 37.594662989355335
          },
          "point_estimate": 37.57581614529809,
          "standard_error": 0.008635607854291513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018417816818233693,
            "upper_bound": 0.08590766126310237
          },
          "point_estimate": 0.06694623155058604,
          "standard_error": 0.01545129363723842
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/libc_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.82722587553393,
            "upper_bound": 20.91492045079683
          },
          "point_estimate": 20.87573697774896,
          "standard_error": 0.022699388997865184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.843795590645513,
            "upper_bound": 20.940907184262713
          },
          "point_estimate": 20.882786327026835,
          "standard_error": 0.02122669764436805
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003442097403205352,
            "upper_bound": 0.1001972428373598
          },
          "point_estimate": 0.06404925400610424,
          "standard_error": 0.02903488499710487
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.87805003218313,
            "upper_bound": 20.91576700280993
          },
          "point_estimate": 20.892179984262494,
          "standard_error": 0.009595629976884908
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028297418225517016,
            "upper_bound": 0.10584536444525156
          },
          "point_estimate": 0.07602498634867932,
          "standard_error": 0.02247866422995936
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.241609596207333,
            "upper_bound": 23.27888991174185
          },
          "point_estimate": 23.26079428619419,
          "standard_error": 0.009529850483348925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.241318292687343,
            "upper_bound": 23.29140647840776
          },
          "point_estimate": 23.258881494186276,
          "standard_error": 0.012656255952684794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007251885098190886,
            "upper_bound": 0.0573393897325931
          },
          "point_estimate": 0.03098701618050025,
          "standard_error": 0.012555599549046554
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.246130296599254,
            "upper_bound": 23.279003545771147
          },
          "point_estimate": 23.261303788324877,
          "standard_error": 0.008267410834935613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01782666571133243,
            "upper_bound": 0.04107880677630332
          },
          "point_estimate": 0.0317929679472536,
          "standard_error": 0.006184319490778142
        }
      }
    },
    "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/libc_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.80621464685847,
            "upper_bound": 31.869929782937035
          },
          "point_estimate": 31.836159818484255,
          "standard_error": 0.01635079586650361
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.793353163280894,
            "upper_bound": 31.885561924565096
          },
          "point_estimate": 31.82407782633608,
          "standard_error": 0.019963761674528895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01090821443130823,
            "upper_bound": 0.0939098037786318
          },
          "point_estimate": 0.041549405544923826,
          "standard_error": 0.021100482038038455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.808017850443548,
            "upper_bound": 31.84737496161984
          },
          "point_estimate": 31.83008111612793,
          "standard_error": 0.009967976512224056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023917539052217225,
            "upper_bound": 0.06783379134229921
          },
          "point_estimate": 0.054342832654427166,
          "standard_error": 0.01120254335166324
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 891376.0509485096,
            "upper_bound": 892422.0364972899
          },
          "point_estimate": 891941.9068399149,
          "standard_error": 269.25716440296355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 891336.737804878,
            "upper_bound": 892653.875
          },
          "point_estimate": 892117.7303523035,
          "standard_error": 301.37865754880994
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.0808612712072,
            "upper_bound": 1407.385791477397
          },
          "point_estimate": 703.6928957386987,
          "standard_error": 321.47835799591246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 891909.1173408163,
            "upper_bound": 892586.2130033617
          },
          "point_estimate": 892296.8039277795,
          "standard_error": 173.51165122909254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 339.97249414158154,
            "upper_bound": 1159.863108100894
          },
          "point_estimate": 897.4264158857234,
          "standard_error": 214.4733403437338
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166826.230334493,
            "upper_bound": 167026.33975755595
          },
          "point_estimate": 166930.15482097556,
          "standard_error": 51.308510322882995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166813.87671232875,
            "upper_bound": 167066.04109589042
          },
          "point_estimate": 166948.97539320143,
          "standard_error": 66.63208328149396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.3308412804628,
            "upper_bound": 290.3189212156893
          },
          "point_estimate": 181.44163410755027,
          "standard_error": 59.781693697914704
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166872.21609909582,
            "upper_bound": 167046.41528946706
          },
          "point_estimate": 166968.23239044062,
          "standard_error": 44.88694018159472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.42041362416568,
            "upper_bound": 222.00460939220537
          },
          "point_estimate": 171.5879680521128,
          "standard_error": 34.0627120990995
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697713.9462835776,
            "upper_bound": 699682.3447916667
          },
          "point_estimate": 698583.2678930098,
          "standard_error": 508.29007658407016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697554.8024725275,
            "upper_bound": 699336.8365384615
          },
          "point_estimate": 698105.0053418804,
          "standard_error": 368.9410032772706
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.85061674523476,
            "upper_bound": 2346.5642338209623
          },
          "point_estimate": 622.8387045674601,
          "standard_error": 605.5356697584901
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697743.510952283,
            "upper_bound": 698595.9595341708
          },
          "point_estimate": 698189.2847152847,
          "standard_error": 214.1721174365113
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 498.8399495173955,
            "upper_bound": 2342.2080500167476
          },
          "point_estimate": 1694.1677770032743,
          "standard_error": 508.08938610929744
        }
      }
    },
    "memmem/libc/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/libc_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378329.8273339061,
            "upper_bound": 379799.5177462772
          },
          "point_estimate": 378936.4008832433,
          "standard_error": 387.8462300430763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378136.94845360826,
            "upper_bound": 379142.9373343151
          },
          "point_estimate": 378728.47164948453,
          "standard_error": 270.58913481287897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.09387136622152,
            "upper_bound": 1210.4845425302342
          },
          "point_estimate": 750.2328427373824,
          "standard_error": 287.2414089419855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 378286.8312264196,
            "upper_bound": 379137.9117670013
          },
          "point_estimate": 378793.9363502477,
          "standard_error": 218.73686478023248
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.68719078957486,
            "upper_bound": 1920.5580717223336
          },
          "point_estimate": 1288.411784864147,
          "standard_error": 501.2827986413837
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579410.2985679327,
            "upper_bound": 579940.7802380953
          },
          "point_estimate": 579666.7645540439,
          "standard_error": 135.96095792405805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579362.6507936509,
            "upper_bound": 580008.2403628118
          },
          "point_estimate": 579556.8833333333,
          "standard_error": 177.7975743274659
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.53439821516176,
            "upper_bound": 745.2544272453199
          },
          "point_estimate": 448.1015539493454,
          "standard_error": 174.41064651988086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579411.7157526711,
            "upper_bound": 579735.1447985643
          },
          "point_estimate": 579565.193609565,
          "standard_error": 80.9537963368318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.07242392902631,
            "upper_bound": 583.5086704730942
          },
          "point_estimate": 451.241998086737,
          "standard_error": 90.18886675750016
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228148.1985677952,
            "upper_bound": 228545.89489839412
          },
          "point_estimate": 228321.2452065972,
          "standard_error": 103.25502006389492
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228141.9140625,
            "upper_bound": 228426.15381944444
          },
          "point_estimate": 228279.786328125,
          "standard_error": 64.13895457513954
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.44601467249476,
            "upper_bound": 445.5858469642365
          },
          "point_estimate": 162.00358329572353,
          "standard_error": 110.40185764257131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228094.33040027195,
            "upper_bound": 228412.95313914923
          },
          "point_estimate": 228246.0816233766,
          "standard_error": 83.62669101920159
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.7530509201626,
            "upper_bound": 493.2075318103015
          },
          "point_estimate": 342.3732522899601,
          "standard_error": 114.89885691916116
        }
      }
    },
    "memmem/libc/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/libc_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389605.62009793986,
            "upper_bound": 390346.4340341101
          },
          "point_estimate": 389961.3062947485,
          "standard_error": 189.29338141410057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389582.7055851064,
            "upper_bound": 390410.892287234
          },
          "point_estimate": 389789.8950945627,
          "standard_error": 228.1094370481526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.89104307634344,
            "upper_bound": 1079.3713354473025
          },
          "point_estimate": 434.5462218597186,
          "standard_error": 261.9008961622678
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389631.2084090666,
            "upper_bound": 390203.3141902933
          },
          "point_estimate": 389878.2253937552,
          "standard_error": 147.40537453392102
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298.89225186594905,
            "upper_bound": 841.1875107852436
          },
          "point_estimate": 631.3805320925832,
          "standard_error": 143.64286341649935
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383262.8714160401,
            "upper_bound": 384049.717556391
          },
          "point_estimate": 383606.59553884715,
          "standard_error": 202.86583306908412
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383149.875877193,
            "upper_bound": 383827.42236842104
          },
          "point_estimate": 383484.5080701754,
          "standard_error": 190.13364010709287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.49496348904821,
            "upper_bound": 864.9260654339178
          },
          "point_estimate": 371.00617041334937,
          "standard_error": 195.73151276246296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383257.4147658567,
            "upper_bound": 383647.9702929379
          },
          "point_estimate": 383464.2570061517,
          "standard_error": 100.37245347106123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247.05355804121015,
            "upper_bound": 978.0508701229886
          },
          "point_estimate": 677.8458667020899,
          "standard_error": 221.52269555003693
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330966.8189155303,
            "upper_bound": 331411.4309090909
          },
          "point_estimate": 331187.5573560606,
          "standard_error": 113.47080318004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330941.0575757576,
            "upper_bound": 331476.8090909091
          },
          "point_estimate": 331138.2971590909,
          "standard_error": 140.61481346952382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.37001548372902,
            "upper_bound": 666.4869813038381
          },
          "point_estimate": 365.6953577121093,
          "standard_error": 143.04574098483593
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330943.1531868132,
            "upper_bound": 331389.07151129126
          },
          "point_estimate": 331173.8648406139,
          "standard_error": 113.29559303433795
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 206.57981834942768,
            "upper_bound": 486.2226178594645
          },
          "point_estimate": 379.4914989475136,
          "standard_error": 71.70946230542371
        }
      }
    },
    "memmem/libc/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288636.40448097757,
            "upper_bound": 288939.94973544974
          },
          "point_estimate": 288784.71586482745,
          "standard_error": 77.62637156745309
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288627.3645754598,
            "upper_bound": 288978.3088624339
          },
          "point_estimate": 288744.7263888889,
          "standard_error": 83.6210172222586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.709489490302207,
            "upper_bound": 419.05903978237177
          },
          "point_estimate": 178.3826634997345,
          "standard_error": 107.14273583441802
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288659.59784765355,
            "upper_bound": 289057.73322225804
          },
          "point_estimate": 288860.80107194395,
          "standard_error": 108.76749817918626
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.4115198208093,
            "upper_bound": 335.7788734583106
          },
          "point_estimate": 257.78628133749214,
          "standard_error": 53.79603480158431
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318404.0015947723,
            "upper_bound": 318921.4254353001
          },
          "point_estimate": 318642.61588129745,
          "standard_error": 132.1741315791662
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318340.5379710145,
            "upper_bound": 318787.30217391305
          },
          "point_estimate": 318652.2045031056,
          "standard_error": 112.32992568910284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.00545173141221,
            "upper_bound": 642.7364182848421
          },
          "point_estimate": 252.98555939268456,
          "standard_error": 150.03299718921272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318539.9675073495,
            "upper_bound": 318743.8592461625
          },
          "point_estimate": 318667.385770751,
          "standard_error": 51.76019916046299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 156.0748268419058,
            "upper_bound": 619.8029441809437
          },
          "point_estimate": 437.9800764805065,
          "standard_error": 125.85560407857244
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166094.77888019133,
            "upper_bound": 166278.26749934768
          },
          "point_estimate": 166189.14054685802,
          "standard_error": 46.93235167775214
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166074.93283866058,
            "upper_bound": 166327.5691780822
          },
          "point_estimate": 166198.0020874103,
          "standard_error": 78.71696785245385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.76823730005294,
            "upper_bound": 287.1143985556328
          },
          "point_estimate": 168.45562668969905,
          "standard_error": 63.54416094449731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166104.02729016845,
            "upper_bound": 166310.0139199902
          },
          "point_estimate": 166222.1039672656,
          "standard_error": 52.495609960634134
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.71686378649017,
            "upper_bound": 196.7442572500696
          },
          "point_estimate": 157.03961684536245,
          "standard_error": 27.33716932953188
        }
      }
    },
    "memmem/libc/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/libc_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221746.2329878049,
            "upper_bound": 222092.0183016841
          },
          "point_estimate": 221904.64670150983,
          "standard_error": 88.96870116446811
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221686.73780487804,
            "upper_bound": 222041.28038617887
          },
          "point_estimate": 221861.27240853655,
          "standard_error": 87.83861878964025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.02497304284645,
            "upper_bound": 442.18845556423696
          },
          "point_estimate": 232.1285986228044,
          "standard_error": 92.43792776964288
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221814.6252535423,
            "upper_bound": 222193.49051347608
          },
          "point_estimate": 221973.59472600571,
          "standard_error": 97.2449756849956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.24840236617396,
            "upper_bound": 410.2218646602491
          },
          "point_estimate": 295.32582317354553,
          "standard_error": 81.37681001789981
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/libc_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84043.8162210097,
            "upper_bound": 84165.10794532628
          },
          "point_estimate": 84109.83165270429,
          "standard_error": 31.13835481611007
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84076.00491898149,
            "upper_bound": 84179.20400683422
          },
          "point_estimate": 84112.32199074075,
          "standard_error": 28.594630082172543
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.741488106348616,
            "upper_bound": 149.9799734947193
          },
          "point_estimate": 64.92329308349576,
          "standard_error": 33.36871820343477
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84095.51175075339,
            "upper_bound": 84201.93699363426
          },
          "point_estimate": 84159.07012987013,
          "standard_error": 27.049156247954876
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.28781464686203,
            "upper_bound": 146.47718721092642
          },
          "point_estimate": 103.94172907842992,
          "standard_error": 30.209917539381216
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1034184.1697530864,
            "upper_bound": 1035638.468497685
          },
          "point_estimate": 1034922.3602469136,
          "standard_error": 373.17388073221326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1033556.4629629628,
            "upper_bound": 1035910.824074074
          },
          "point_estimate": 1035011.3840277776,
          "standard_error": 578.2161460839675
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205.60572884990097,
            "upper_bound": 2137.194173631316
          },
          "point_estimate": 1393.045100963062,
          "standard_error": 518.5031737406133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1033693.6332589666,
            "upper_bound": 1035635.958921296
          },
          "point_estimate": 1034695.4401875902,
          "standard_error": 506.0973714392229
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 726.9146476802978,
            "upper_bound": 1511.1243801885123
          },
          "point_estimate": 1243.8935844774294,
          "standard_error": 196.70814046307257
        }
      }
    },
    "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/libc/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/libc_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2073.1325553105003,
            "upper_bound": 2078.058137512134
          },
          "point_estimate": 2075.744142700343,
          "standard_error": 1.2620570680226213
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2072.779064123794,
            "upper_bound": 2078.90556538952
          },
          "point_estimate": 2076.65502134922,
          "standard_error": 1.5222626803004615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.004339035063478,
            "upper_bound": 6.817775725474435
          },
          "point_estimate": 4.170469811948878,
          "standard_error": 1.5369159271637818
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2073.4451394038365,
            "upper_bound": 2077.6014187059263
          },
          "point_estimate": 2075.7253482199867,
          "standard_error": 1.0562352556419383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.051400181403357,
            "upper_bound": 5.514809181722495
          },
          "point_estimate": 4.212351097426711,
          "standard_error": 0.9231742305323652
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47217.873338981655,
            "upper_bound": 47316.46416955267
          },
          "point_estimate": 47267.65898026181,
          "standard_error": 25.42770673198375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47208.69466089466,
            "upper_bound": 47303.87989718615
          },
          "point_estimate": 47277.29207792207,
          "standard_error": 19.11107337857196
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.467418163111136,
            "upper_bound": 171.75746487277377
          },
          "point_estimate": 32.88683181874135,
          "standard_error": 41.7857228765475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47263.044276119894,
            "upper_bound": 47320.401818198625
          },
          "point_estimate": 47287.44553887671,
          "standard_error": 14.5179184063675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.06010909952552,
            "upper_bound": 114.79069648085353
          },
          "point_estimate": 84.90862881240292,
          "standard_error": 20.259822827723045
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48963.30184872372,
            "upper_bound": 49128.5016874075
          },
          "point_estimate": 49032.4433722115,
          "standard_error": 42.844963629526234
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48982.150844594595,
            "upper_bound": 49040.93378378378
          },
          "point_estimate": 49005.45265765766,
          "standard_error": 15.778325613522515
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.58055083839317,
            "upper_bound": 144.18985973741275
          },
          "point_estimate": 34.715632132552116,
          "standard_error": 29.255639900021155
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48986.97651619426,
            "upper_bound": 49143.167067711904
          },
          "point_estimate": 49034.18835731836,
          "standard_error": 43.11322215312981
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.458420378910596,
            "upper_bound": 212.68929632118585
          },
          "point_estimate": 143.1273584756356,
          "standard_error": 58.25707811540228
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49261.495635243264,
            "upper_bound": 49370.79728723465
          },
          "point_estimate": 49311.90838963092,
          "standard_error": 28.121628523924176
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49242.24880226481,
            "upper_bound": 49380.123532068654
          },
          "point_estimate": 49281.824593495934,
          "standard_error": 32.363212442705716
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.367662636244443,
            "upper_bound": 156.73974599780212
          },
          "point_estimate": 81.42677282606346,
          "standard_error": 34.5846787057915
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49248.119894194715,
            "upper_bound": 49329.76465145709
          },
          "point_estimate": 49286.41129412593,
          "standard_error": 20.673961350656192
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.90917201256697,
            "upper_bound": 120.855112112691
          },
          "point_estimate": 93.47514409465283,
          "standard_error": 21.270708122182437
        }
      }
    },
    "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/regex_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13898.283866934687,
            "upper_bound": 13924.811494334102
          },
          "point_estimate": 13909.357327135504,
          "standard_error": 6.982369649538102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13896.391037916506,
            "upper_bound": 13917.79439664606
          },
          "point_estimate": 13899.600834078046,
          "standard_error": 5.210173534793274
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1669252339757696,
            "upper_bound": 25.6278130647408
          },
          "point_estimate": 9.695942153598828,
          "standard_error": 6.395564004493841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13899.334896206115,
            "upper_bound": 13914.508921534603
          },
          "point_estimate": 13905.111281441654,
          "standard_error": 3.947751367545149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.176210113089875,
            "upper_bound": 33.95659883618344
          },
          "point_estimate": 23.292604403040965,
          "standard_error": 8.524385996073443
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26722.534643873987,
            "upper_bound": 26757.17853888947
          },
          "point_estimate": 26738.209143924287,
          "standard_error": 8.91464745247383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26719.07665931913,
            "upper_bound": 26751.92111498898
          },
          "point_estimate": 26733.109058465416,
          "standard_error": 7.716084483518626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.317840937303981,
            "upper_bound": 44.32179606471315
          },
          "point_estimate": 19.896941909801473,
          "standard_error": 10.741839512267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26728.21533699602,
            "upper_bound": 26777.97320910709
          },
          "point_estimate": 26752.904646125367,
          "standard_error": 13.17202744607258
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.47932342913601,
            "upper_bound": 40.76193570265575
          },
          "point_estimate": 29.729099419056816,
          "standard_error": 8.135303476252
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17694.556336978476,
            "upper_bound": 17712.829973241856
          },
          "point_estimate": 17703.173735402008,
          "standard_error": 4.662933068453753
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17691.440191776368,
            "upper_bound": 17708.65407118479
          },
          "point_estimate": 17705.277906247822,
          "standard_error": 4.814264220101112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5673302988455071,
            "upper_bound": 25.22750770668128
          },
          "point_estimate": 9.19414386407496,
          "standard_error": 6.275662561327297
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17698.081653492143,
            "upper_bound": 17720.336221968246
          },
          "point_estimate": 17709.015048725043,
          "standard_error": 5.656104765082568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.701399270405932,
            "upper_bound": 21.582569346918852
          },
          "point_estimate": 15.573173929269789,
          "standard_error": 4.092362424741197
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17451.412607747923,
            "upper_bound": 17467.398996541375
          },
          "point_estimate": 17459.40997804958,
          "standard_error": 4.080735876781329
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17447.024317246538,
            "upper_bound": 17469.06168831169
          },
          "point_estimate": 17459.966482283147,
          "standard_error": 6.3610080863417435
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.103517705843439,
            "upper_bound": 22.612323840972756
          },
          "point_estimate": 15.389593346310736,
          "standard_error": 5.183324116592243
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17449.76221070364,
            "upper_bound": 17469.294981850537
          },
          "point_estimate": 17457.77766158805,
          "standard_error": 5.120117723033408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.030698987897495,
            "upper_bound": 17.078398801037196
          },
          "point_estimate": 13.595409669397617,
          "standard_error": 2.3309127282556785
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17295.84285874226,
            "upper_bound": 17330.938921048953
          },
          "point_estimate": 17313.902219915755,
          "standard_error": 8.986949893306297
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17287.6017746546,
            "upper_bound": 17336.066460219154
          },
          "point_estimate": 17324.942806098145,
          "standard_error": 15.507310840293428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4250645496104104,
            "upper_bound": 47.88505177810407
          },
          "point_estimate": 30.410907416266816,
          "standard_error": 12.988678849177616
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17302.471992561,
            "upper_bound": 17333.362904894588
          },
          "point_estimate": 17318.231233178445,
          "standard_error": 7.938332269907522
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.685245095644895,
            "upper_bound": 36.449294175753714
          },
          "point_estimate": 30.03624431936351,
          "standard_error": 4.799337612705927
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23022.382250451337,
            "upper_bound": 23075.976362339858
          },
          "point_estimate": 23045.851547241356,
          "standard_error": 13.96323612676638
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23020.540873519458,
            "upper_bound": 23072.657571912016
          },
          "point_estimate": 23028.242787648054,
          "standard_error": 10.48621312069841
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.4605397139190368,
            "upper_bound": 60.56897532062587
          },
          "point_estimate": 12.71969177925539,
          "standard_error": 13.051058737233332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23014.86627718773,
            "upper_bound": 23033.333612136594
          },
          "point_estimate": 23022.958560221505,
          "standard_error": 4.75776809685298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.422023948190024,
            "upper_bound": 62.463082086302194
          },
          "point_estimate": 46.41898697093655,
          "standard_error": 14.540057089287515
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15987.4811971831,
            "upper_bound": 16008.258414209213
          },
          "point_estimate": 15997.95380355047,
          "standard_error": 5.3057979668094735
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15980.7724471831,
            "upper_bound": 16007.575044014084
          },
          "point_estimate": 16001.266175176055,
          "standard_error": 6.546856599393751
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.750917919911444,
            "upper_bound": 35.09307177134003
          },
          "point_estimate": 10.85304074041647,
          "standard_error": 8.57797941800267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15984.494867983114,
            "upper_bound": 16005.910168699707
          },
          "point_estimate": 15996.174158587892,
          "standard_error": 5.669487439902196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.1967807696377,
            "upper_bound": 22.813758759845637
          },
          "point_estimate": 17.69995519681207,
          "standard_error": 3.4314060584337014
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18533.4615856414,
            "upper_bound": 18572.427898323614
          },
          "point_estimate": 18551.93242662779,
          "standard_error": 9.985762910895671
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18523.57582908163,
            "upper_bound": 18584.60301020408
          },
          "point_estimate": 18541.34400510204,
          "standard_error": 16.14586338675708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.852427610178029,
            "upper_bound": 57.53102946330719
          },
          "point_estimate": 31.823138542167182,
          "standard_error": 16.121973509302634
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18528.269781785657,
            "upper_bound": 18565.5117631559
          },
          "point_estimate": 18546.70184866154,
          "standard_error": 9.53106744761529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.692935714972272,
            "upper_bound": 40.34521597081164
          },
          "point_estimate": 33.27957669775352,
          "standard_error": 6.082117547342345
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17836.646241618895,
            "upper_bound": 17935.239299130302
          },
          "point_estimate": 17875.040298038297,
          "standard_error": 27.150731710721587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17832.48779006279,
            "upper_bound": 17875.1800982801
          },
          "point_estimate": 17852.36445945946,
          "standard_error": 13.049154547499976
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.997130727803313,
            "upper_bound": 57.032643471496385
          },
          "point_estimate": 31.59977884931104,
          "standard_error": 13.690839505150707
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17839.725716143137,
            "upper_bound": 17920.118176682892
          },
          "point_estimate": 17863.84856823766,
          "standard_error": 21.747294654657516
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.975322425355982,
            "upper_bound": 137.63265930989434
          },
          "point_estimate": 90.14210795829813,
          "standard_error": 40.90651791445661
        }
      }
    },
    "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17306.692192012688,
            "upper_bound": 17342.595881099704
          },
          "point_estimate": 17325.531923592695,
          "standard_error": 9.203223392151372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17306.362732032365,
            "upper_bound": 17347.14403855307
          },
          "point_estimate": 17331.17388828449,
          "standard_error": 11.173278733566791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.758768385290815,
            "upper_bound": 51.53577372332131
          },
          "point_estimate": 31.483091872445637,
          "standard_error": 11.356671716202316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17316.67644011436,
            "upper_bound": 17341.333208982425
          },
          "point_estimate": 17330.257390111077,
          "standard_error": 6.345478901622704
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.305610242655522,
            "upper_bound": 40.99450812310383
          },
          "point_estimate": 30.66286566759907,
          "standard_error": 6.892820919194644
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17385.644984417344,
            "upper_bound": 17412.411037780963
          },
          "point_estimate": 17398.804095594875,
          "standard_error": 6.87492717823536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17378.121506190553,
            "upper_bound": 17419.76661884266
          },
          "point_estimate": 17396.80368244859,
          "standard_error": 13.223485353780584
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.727742196473061,
            "upper_bound": 35.60886029958616
          },
          "point_estimate": 27.063759964287502,
          "standard_error": 9.183491134131698
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17389.734214077987,
            "upper_bound": 17418.808008467895
          },
          "point_estimate": 17405.763351903955,
          "standard_error": 7.510128430815798
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.047078976315436,
            "upper_bound": 26.82955969716721
          },
          "point_estimate": 22.910800906684443,
          "standard_error": 3.0520614907043644
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17425.62150835683,
            "upper_bound": 17463.74351506962
          },
          "point_estimate": 17444.420500626664,
          "standard_error": 9.767806638318548
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17422.475493500242,
            "upper_bound": 17467.75792408923
          },
          "point_estimate": 17445.631831625284,
          "standard_error": 10.564910971873044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.519890814376619,
            "upper_bound": 54.67769991396451
          },
          "point_estimate": 29.64130240105833,
          "standard_error": 13.757073308432464
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17430.144926322766,
            "upper_bound": 17476.332103645334
          },
          "point_estimate": 17451.158465318986,
          "standard_error": 11.793219364562743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.385691157373714,
            "upper_bound": 43.07324008975488
          },
          "point_estimate": 32.596436228375545,
          "standard_error": 7.026683323035539
        }
      }
    },
    "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17658.78638071051,
            "upper_bound": 17772.2975068635
          },
          "point_estimate": 17712.286792151375,
          "standard_error": 29.150489835001917
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17630.264108790674,
            "upper_bound": 17797.057498246184
          },
          "point_estimate": 17685.84033511413,
          "standard_error": 43.23351900937864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.117629572387552,
            "upper_bound": 164.96620425314842
          },
          "point_estimate": 88.41036526745584,
          "standard_error": 42.47447367794558
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17639.161562589572,
            "upper_bound": 17737.68563595879
          },
          "point_estimate": 17683.54613574866,
          "standard_error": 24.887029089094117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.77825400584383,
            "upper_bound": 120.1477292186837
          },
          "point_estimate": 97.27629385379257,
          "standard_error": 19.008013540285695
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17804.373376604814,
            "upper_bound": 17833.48915605742
          },
          "point_estimate": 17817.622777894492,
          "standard_error": 7.47165404988766
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17799.976421568626,
            "upper_bound": 17828.44458333333
          },
          "point_estimate": 17815.617320261437,
          "standard_error": 6.794198033658138
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.875347605815837,
            "upper_bound": 37.15563901192501
          },
          "point_estimate": 18.53762336206883,
          "standard_error": 8.547641597014536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17803.775915412676,
            "upper_bound": 17818.973092558062
          },
          "point_estimate": 17811.02446906035,
          "standard_error": 3.9505069299259135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.057498563045913,
            "upper_bound": 34.37486993060717
          },
          "point_estimate": 24.867349998383215,
          "standard_error": 6.748368689779404
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18102.614023785813,
            "upper_bound": 18135.73120711203
          },
          "point_estimate": 18118.78466765873,
          "standard_error": 8.395358337771793
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18105.547872794537,
            "upper_bound": 18131.527639442233
          },
          "point_estimate": 18115.14092795485,
          "standard_error": 7.21549836724996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.182225878188674,
            "upper_bound": 46.911294266758546
          },
          "point_estimate": 17.67812600421316,
          "standard_error": 9.604273298790762
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18110.917260845035,
            "upper_bound": 18127.504283902457
          },
          "point_estimate": 18119.2536011797,
          "standard_error": 4.2223054128838085
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.109860022117871,
            "upper_bound": 38.6647072441487
          },
          "point_estimate": 27.955375910687415,
          "standard_error": 7.51668469967264
        }
      }
    },
    "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18177.698141736415,
            "upper_bound": 18203.463060708626
          },
          "point_estimate": 18190.34661557813,
          "standard_error": 6.582692085676288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18173.400600901354,
            "upper_bound": 18206.06585711901
          },
          "point_estimate": 18190.07566975463,
          "standard_error": 7.907147088115311
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.549795567242867,
            "upper_bound": 37.45137480455798
          },
          "point_estimate": 19.278713728840295,
          "standard_error": 8.279363170946933
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18183.584091126195,
            "upper_bound": 18202.545355871644
          },
          "point_estimate": 18191.827697390243,
          "standard_error": 4.7837518478781815
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.780537231373286,
            "upper_bound": 28.2273031138133
          },
          "point_estimate": 21.879854263962965,
          "standard_error": 4.2755182856168155
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68323.56604099536,
            "upper_bound": 68397.52447074717
          },
          "point_estimate": 68358.35089785476,
          "standard_error": 18.986187765763237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68306.0373254565,
            "upper_bound": 68419.87969924812
          },
          "point_estimate": 68337.908625731,
          "standard_error": 27.38053817385316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.067047453893675,
            "upper_bound": 97.61084607407405
          },
          "point_estimate": 53.12058254690072,
          "standard_error": 23.88292269361492
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68330.81954887218,
            "upper_bound": 68408.27586865547
          },
          "point_estimate": 68374.63568987403,
          "standard_error": 19.933343154734334
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.11946126959234,
            "upper_bound": 78.65188724135656
          },
          "point_estimate": 63.29891463767417,
          "standard_error": 12.27636930245914
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227615.8558811836,
            "upper_bound": 1229939.744684954
          },
          "point_estimate": 1228806.0884695768,
          "standard_error": 592.4335909810186
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227392.6541666666,
            "upper_bound": 1230121.088888889
          },
          "point_estimate": 1228936.8447089947,
          "standard_error": 671.2634439656633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474.92195556838016,
            "upper_bound": 3399.1489289030387
          },
          "point_estimate": 2022.58862367533,
          "standard_error": 729.7403659309325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1228105.4085693536,
            "upper_bound": 1229417.812037037
          },
          "point_estimate": 1228681.2528138529,
          "standard_error": 330.42014443806573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 996.5096174534392,
            "upper_bound": 2609.158473639846
          },
          "point_estimate": 1972.314226428188,
          "standard_error": 417.5703061491917
        }
      }
    },
    "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/regex_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207654.36650068028,
            "upper_bound": 207852.22074273808
          },
          "point_estimate": 207754.5730959184,
          "standard_error": 50.62597582460304
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207642.7775510204,
            "upper_bound": 207877.4023809524
          },
          "point_estimate": 207749.77876190472,
          "standard_error": 66.1887564573059
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.822243435034395,
            "upper_bound": 283.0872153741621
          },
          "point_estimate": 161.63276673047542,
          "standard_error": 64.27964332721663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207706.342279198,
            "upper_bound": 207832.7777287881
          },
          "point_estimate": 207757.0679183673,
          "standard_error": 32.139602531456966
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.4990180072983,
            "upper_bound": 218.83377813738937
          },
          "point_estimate": 168.4258071971582,
          "standard_error": 33.26717004782516
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6109.668362661949,
            "upper_bound": 6128.224941406093
          },
          "point_estimate": 6118.384333402659,
          "standard_error": 4.761092427930749
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6107.202318158912,
            "upper_bound": 6127.228660219337
          },
          "point_estimate": 6116.968041323702,
          "standard_error": 6.011862646654296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.590823085198336,
            "upper_bound": 25.97759943241642
          },
          "point_estimate": 13.171993944155348,
          "standard_error": 5.770290030049899
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6108.551170772349,
            "upper_bound": 6121.906553890533
          },
          "point_estimate": 6112.983786413486,
          "standard_error": 3.446669382058995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.594290860566516,
            "upper_bound": 21.54612924008963
          },
          "point_estimate": 15.86437370137221,
          "standard_error": 3.832120801442565
        }
      }
    },
    "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/regex_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6419.088430567826,
            "upper_bound": 6434.0710694476
          },
          "point_estimate": 6426.77857222096,
          "standard_error": 3.8330500618846575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6416.004947870649,
            "upper_bound": 6437.948000235612
          },
          "point_estimate": 6427.743293622744,
          "standard_error": 4.934647573123626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.612648408814213,
            "upper_bound": 23.427825018335955
          },
          "point_estimate": 13.84453000690494,
          "standard_error": 5.724590356662934
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6426.3508004798,
            "upper_bound": 6434.592596858277
          },
          "point_estimate": 6429.599949052538,
          "standard_error": 2.091662371976233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.958610932540677,
            "upper_bound": 15.710283190265434
          },
          "point_estimate": 12.746588863600632,
          "standard_error": 2.189474141570118
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14036.17514007768,
            "upper_bound": 14064.570103785103
          },
          "point_estimate": 14052.30978080267,
          "standard_error": 7.355255854521698
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14045.146260146888,
            "upper_bound": 14070.612378129968
          },
          "point_estimate": 14053.645088998102,
          "standard_error": 6.190679518292468
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6350674526104807,
            "upper_bound": 28.93725294586618
          },
          "point_estimate": 18.77752696852617,
          "standard_error": 7.6405115127249745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14047.203312296837,
            "upper_bound": 14065.361449854065
          },
          "point_estimate": 14057.796307210378,
          "standard_error": 4.674294892069049
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.551985224537539,
            "upper_bound": 35.51290030143082
          },
          "point_estimate": 24.54949582117757,
          "standard_error": 8.252323075495529
        }
      }
    },
    "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/regex_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.9901080369567,
            "upper_bound": 37.08520948359894
          },
          "point_estimate": 37.030979082823464,
          "standard_error": 0.024715867138288855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.97235890292407,
            "upper_bound": 37.06685251638996
          },
          "point_estimate": 37.00342738502046,
          "standard_error": 0.02457267716180518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006092783109988399,
            "upper_bound": 0.1007080576826305
          },
          "point_estimate": 0.06130146434567059,
          "standard_error": 0.024391240978723923
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.9868493942059,
            "upper_bound": 37.03796485110198
          },
          "point_estimate": 37.00612649525544,
          "standard_error": 0.013082936286599052
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027544200426612205,
            "upper_bound": 0.11751687719503398
          },
          "point_estimate": 0.08201611981899269,
          "standard_error": 0.0266454615246764
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.240162407681256,
            "upper_bound": 15.273647189031418
          },
          "point_estimate": 15.254671765429094,
          "standard_error": 0.008748909192353035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.23840470624221,
            "upper_bound": 15.269594038349762
          },
          "point_estimate": 15.24402644055736,
          "standard_error": 0.007647887324323541
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0025773580723080056,
            "upper_bound": 0.03790651981404115
          },
          "point_estimate": 0.008951913897230556,
          "standard_error": 0.009724334469628197
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.232529572636947,
            "upper_bound": 15.259945411984411
          },
          "point_estimate": 15.243280848304368,
          "standard_error": 0.007080606486780175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008839593104547861,
            "upper_bound": 0.04168830688630308
          },
          "point_estimate": 0.02910954666610361,
          "standard_error": 0.009609070033620302
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.225673027782308,
            "upper_bound": 15.247000760444076
          },
          "point_estimate": 15.236153872291604,
          "standard_error": 0.005461647298827927
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.221566762699862,
            "upper_bound": 15.254613372543936
          },
          "point_estimate": 15.232074227456913,
          "standard_error": 0.009552356971462218
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007251903490778043,
            "upper_bound": 0.028687454655254623
          },
          "point_estimate": 0.017954883262717,
          "standard_error": 0.007337025479226612
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.22449178634101,
            "upper_bound": 15.250301229207786
          },
          "point_estimate": 15.237457212171472,
          "standard_error": 0.006615114481988884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01126327693243748,
            "upper_bound": 0.021972842582871233
          },
          "point_estimate": 0.018259284924003017,
          "standard_error": 0.0027267184153423555
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.237256732124148,
            "upper_bound": 15.272019101879874
          },
          "point_estimate": 15.253035810423064,
          "standard_error": 0.008921671918235894
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.237216946838036,
            "upper_bound": 15.265777903367743
          },
          "point_estimate": 15.245421711169817,
          "standard_error": 0.008192171862416957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003551650139494397,
            "upper_bound": 0.0412599776675062
          },
          "point_estimate": 0.020337998588276743,
          "standard_error": 0.010004533554148924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.242583039416632,
            "upper_bound": 15.262477023785488
          },
          "point_estimate": 15.249560591233342,
          "standard_error": 0.0051082976106001
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010834158160360569,
            "upper_bound": 0.042003844458398554
          },
          "point_estimate": 0.029651301835932724,
          "standard_error": 0.008823131249432885
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/regex_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.973714921921989,
            "upper_bound": 14.990314541886152
          },
          "point_estimate": 14.981948061652748,
          "standard_error": 0.004272855971039378
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.966776900786789,
            "upper_bound": 14.993258253567433
          },
          "point_estimate": 14.98396426287232,
          "standard_error": 0.005801143390418098
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003410876530382413,
            "upper_bound": 0.02721745306685171
          },
          "point_estimate": 0.01774600845244375,
          "standard_error": 0.00756708871027955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.973766901726632,
            "upper_bound": 14.99228834100326
          },
          "point_estimate": 14.984398957249873,
          "standard_error": 0.004610598328664156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008410973886363225,
            "upper_bound": 0.01747019384841904
          },
          "point_estimate": 0.014219794227693124,
          "standard_error": 0.002373109963868124
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.656808713596355,
            "upper_bound": 16.693531465125666
          },
          "point_estimate": 16.674051286900827,
          "standard_error": 0.009421866003226295
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.64649368153003,
            "upper_bound": 16.698349656104767
          },
          "point_estimate": 16.66894469830665,
          "standard_error": 0.01163418819487448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005645868179609568,
            "upper_bound": 0.051746041948845335
          },
          "point_estimate": 0.029595018225625103,
          "standard_error": 0.01195390974240835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.65978239720583,
            "upper_bound": 16.687352588610924
          },
          "point_estimate": 16.673376639164466,
          "standard_error": 0.00697611552250027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01367534943405205,
            "upper_bound": 0.03936559115123989
          },
          "point_estimate": 0.03134216281394282,
          "standard_error": 0.006579972988300984
        }
      }
    },
    "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.371733042117413,
            "upper_bound": 17.393419538111267
          },
          "point_estimate": 17.38210837851277,
          "standard_error": 0.005563412814324402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.370570306224625,
            "upper_bound": 17.39684240691243
          },
          "point_estimate": 17.37514354254305,
          "standard_error": 0.007335788190599109
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0017333538701463196,
            "upper_bound": 0.030283259344015073
          },
          "point_estimate": 0.012607387506808655,
          "standard_error": 0.007937612229305897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.372091658415258,
            "upper_bound": 17.396691922924955
          },
          "point_estimate": 17.383981140855756,
          "standard_error": 0.006966777405426573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008818434126059474,
            "upper_bound": 0.023013670847797214
          },
          "point_estimate": 0.018512142714705908,
          "standard_error": 0.00345709545539324
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.228086214539172,
            "upper_bound": 14.238951871734589
          },
          "point_estimate": 14.234066984733415,
          "standard_error": 0.002791438224992177
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.230811901340996,
            "upper_bound": 14.241135817845947
          },
          "point_estimate": 14.234520008816611,
          "standard_error": 0.002147455475220696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006445223827593519,
            "upper_bound": 0.01244645195379494
          },
          "point_estimate": 0.005055651178859553,
          "standard_error": 0.00337266034048849
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.23110311098766,
            "upper_bound": 14.236816164728982
          },
          "point_estimate": 14.233609323983227,
          "standard_error": 0.0014513488150875318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0034202619726813794,
            "upper_bound": 0.013165727632679291
          },
          "point_estimate": 0.009320866491867436,
          "standard_error": 0.00282748130806579
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.82220356293802,
            "upper_bound": 16.895767950973145
          },
          "point_estimate": 16.863492853771668,
          "standard_error": 0.019050682211426785
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.83878671526483,
            "upper_bound": 16.90783925584832
          },
          "point_estimate": 16.87320012891498,
          "standard_error": 0.017755478593148293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012929327427824246,
            "upper_bound": 0.08268768153408253
          },
          "point_estimate": 0.04472111619870701,
          "standard_error": 0.017672492956254542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.8604420335115,
            "upper_bound": 16.910033867004874
          },
          "point_estimate": 16.884998050556884,
          "standard_error": 0.012728079232269353
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02365407237190385,
            "upper_bound": 0.09038043363692386
          },
          "point_estimate": 0.06334383910432118,
          "standard_error": 0.019974669527615733
        }
      }
    },
    "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.07048672006501,
            "upper_bound": 20.124829521831984
          },
          "point_estimate": 20.094086451000685,
          "standard_error": 0.014100791760790625
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.067981169485066,
            "upper_bound": 20.113756695551515
          },
          "point_estimate": 20.081790865721153,
          "standard_error": 0.008424833815232883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018430171546357475,
            "upper_bound": 0.05383176574510082
          },
          "point_estimate": 0.006623703915334486,
          "standard_error": 0.01618508534763093
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.05730501792934,
            "upper_bound": 20.09075964217189
          },
          "point_estimate": 20.069870182213553,
          "standard_error": 0.008623973007792385
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011194453357885593,
            "upper_bound": 0.06622013452592242
          },
          "point_estimate": 0.04709835424104381,
          "standard_error": 0.015429623509867204
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.34690737143652,
            "upper_bound": 15.363290390429569
          },
          "point_estimate": 15.355254644014485,
          "standard_error": 0.004189902670223742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.345081451304218,
            "upper_bound": 15.3660285603722
          },
          "point_estimate": 15.35618658905358,
          "standard_error": 0.004840541583726901
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016814661692536224,
            "upper_bound": 0.026050734061422536
          },
          "point_estimate": 0.0132066204422194,
          "standard_error": 0.006138428883446897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.34782413471108,
            "upper_bound": 15.368149535291094
          },
          "point_estimate": 15.35836049148021,
          "standard_error": 0.005272049636167497
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007679044884161365,
            "upper_bound": 0.017603537666571342
          },
          "point_estimate": 0.014000984123943893,
          "standard_error": 0.0025601252981494096
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.923105960915468,
            "upper_bound": 16.949167835520417
          },
          "point_estimate": 16.9356343099206,
          "standard_error": 0.006710128778476982
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.917170425410102,
            "upper_bound": 16.956826693301473
          },
          "point_estimate": 16.928196666604435,
          "standard_error": 0.01042425566292967
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004564653898311925,
            "upper_bound": 0.03748061739195651
          },
          "point_estimate": 0.02415898680754422,
          "standard_error": 0.008771257159813466
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.921292417053603,
            "upper_bound": 16.945847565132922
          },
          "point_estimate": 16.933678933175702,
          "standard_error": 0.006358047118827108
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01213649387876994,
            "upper_bound": 0.027093409336316585
          },
          "point_estimate": 0.02234285350274531,
          "standard_error": 0.0037447426459733153
        }
      }
    },
    "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/regex_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.062125236411593,
            "upper_bound": 26.11501263395896
          },
          "point_estimate": 26.088640271849577,
          "standard_error": 0.013538790007722366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.05665698675868,
            "upper_bound": 26.12267267508798
          },
          "point_estimate": 26.089358445510538,
          "standard_error": 0.020048905318260684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0022014507278591616,
            "upper_bound": 0.0741397585484038
          },
          "point_estimate": 0.04857352076001505,
          "standard_error": 0.01818031362289345
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.051418217032257,
            "upper_bound": 26.09434170222051
          },
          "point_estimate": 26.069929033454446,
          "standard_error": 0.010841783588434867
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025593311120636647,
            "upper_bound": 0.0577955458064646
          },
          "point_estimate": 0.04515193235028977,
          "standard_error": 0.008332809666417645
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131936.55355825397,
            "upper_bound": 132329.6101883117
          },
          "point_estimate": 132118.59304559886,
          "standard_error": 100.94015406085816
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131855.0550909091,
            "upper_bound": 132354.8712987013
          },
          "point_estimate": 132006.5727777778,
          "standard_error": 147.68258147505222
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.478375243398522,
            "upper_bound": 538.543465348033
          },
          "point_estimate": 253.28445281238777,
          "standard_error": 160.44097083392109
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131916.00161518093,
            "upper_bound": 132369.59746556473
          },
          "point_estimate": 132161.21667060212,
          "standard_error": 119.7603751324474
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.15178860923817,
            "upper_bound": 431.1220017101101
          },
          "point_estimate": 335.4466659053617,
          "standard_error": 73.63951495349386
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53753.7749811714,
            "upper_bound": 53858.54510126114
          },
          "point_estimate": 53805.22971227216,
          "standard_error": 26.751168249521655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53738.70162144977,
            "upper_bound": 53882.41635756676
          },
          "point_estimate": 53791.09025717112,
          "standard_error": 39.752727154327694
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.53881561117497,
            "upper_bound": 145.32559326566005
          },
          "point_estimate": 88.73254523476673,
          "standard_error": 33.48480491267018
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53779.68347614443,
            "upper_bound": 53908.86579715444
          },
          "point_estimate": 53855.65029480905,
          "standard_error": 33.20151114122292
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.62350705365026,
            "upper_bound": 111.83580531940792
          },
          "point_estimate": 89.24231586177304,
          "standard_error": 15.397559019795851
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198165.96774320657,
            "upper_bound": 198599.63494708636
          },
          "point_estimate": 198358.97218706863,
          "standard_error": 111.81137943477718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198081.30672554343,
            "upper_bound": 198489.93097826088
          },
          "point_estimate": 198335.28416149068,
          "standard_error": 132.70710352740855
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.037718581554614,
            "upper_bound": 554.5710525592895
          },
          "point_estimate": 289.2813413723757,
          "standard_error": 125.9555498997367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198239.28695652177,
            "upper_bound": 198879.97301214255
          },
          "point_estimate": 198552.04003387917,
          "standard_error": 168.80357843352215
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.7109064811568,
            "upper_bound": 525.5117614373253
          },
          "point_estimate": 370.8995335332014,
          "standard_error": 111.1206793326894
        }
      }
    },
    "memmem/regex/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memmem/regex_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660975.3501669192,
            "upper_bound": 662423.009
          },
          "point_estimate": 661732.8341443001,
          "standard_error": 371.0927325188857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 661129.675,
            "upper_bound": 662866.1
          },
          "point_estimate": 661572.0383838385,
          "standard_error": 503.4261392902735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 210.49550080827115,
            "upper_bound": 2286.176272896996
          },
          "point_estimate": 1042.1626516797132,
          "standard_error": 522.0068860895432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 660540.1577771232,
            "upper_bound": 662467.9845100819
          },
          "point_estimate": 661639.2424557261,
          "standard_error": 495.8455924164313
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 658.1512208342452,
            "upper_bound": 1650.2034650691617
          },
          "point_estimate": 1235.4511473192938,
          "standard_error": 277.8987312315214
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332209.77996875,
            "upper_bound": 1334022.1616117137
          },
          "point_estimate": 1333111.3805229592,
          "standard_error": 463.6686518817256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1331710.7142857143,
            "upper_bound": 1334361.5494047618
          },
          "point_estimate": 1333398.6614583335,
          "standard_error": 647.4438484053559
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 257.7848641731901,
            "upper_bound": 2802.4732538176427
          },
          "point_estimate": 2001.4448800925052,
          "standard_error": 698.7109320478478
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1332869.6851983764,
            "upper_bound": 1334661.3694879315
          },
          "point_estimate": 1333935.2697588126,
          "standard_error": 459.4005557948533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 917.0999021161502,
            "upper_bound": 1906.52619280156
          },
          "point_estimate": 1546.269560784884,
          "standard_error": 254.86033176782485
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51998.081673819746,
            "upper_bound": 52073.1022595204
          },
          "point_estimate": 52035.71985608692,
          "standard_error": 19.014856959643645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52010.243383404864,
            "upper_bound": 52070.02902105048
          },
          "point_estimate": 52033.17000476872,
          "standard_error": 13.1804107293012
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.057383944549797,
            "upper_bound": 102.55979880509172
          },
          "point_estimate": 36.57243431136753,
          "standard_error": 24.073592418344564
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52028.08362420359,
            "upper_bound": 52082.24604194495
          },
          "point_estimate": 52047.933121527974,
          "standard_error": 13.8814334777702
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.030710886918552,
            "upper_bound": 88.46806158119
          },
          "point_estimate": 63.368926726992434,
          "standard_error": 17.718947336460857
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-en/common-you",
        "directory_name": "memmem/regex_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162928.32781019557,
            "upper_bound": 163469.0558522493
          },
          "point_estimate": 163175.0897257812,
          "standard_error": 139.29039806532265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162948.87623318387,
            "upper_bound": 163459.15657698055
          },
          "point_estimate": 163046.75924887892,
          "standard_error": 107.7170272553193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.17878467437798,
            "upper_bound": 730.7765465216293
          },
          "point_estimate": 152.96303237777883,
          "standard_error": 173.77088401965162
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162997.85224778272,
            "upper_bound": 163557.10266195974
          },
          "point_estimate": 163196.76108555123,
          "standard_error": 149.81078592520151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.17067417472649,
            "upper_bound": 620.541234026696
          },
          "point_estimate": 465.34084603603475,
          "standard_error": 126.02198276928117
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108345.67902724928,
            "upper_bound": 108648.59709567211
          },
          "point_estimate": 108494.19162993669,
          "standard_error": 77.46975441967433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108312.95271164022,
            "upper_bound": 108689.21031746033
          },
          "point_estimate": 108474.57842261906,
          "standard_error": 92.12331511674556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.024210550020342,
            "upper_bound": 456.5760752274832
          },
          "point_estimate": 240.12568403553945,
          "standard_error": 110.8996756335239
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108375.9701988315,
            "upper_bound": 108586.89827172457
          },
          "point_estimate": 108461.71739332096,
          "standard_error": 53.48648415715227
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.75180508170104,
            "upper_bound": 331.07834221009324
          },
          "point_estimate": 257.6973700827157,
          "standard_error": 50.38643870909842
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667204.6302452381,
            "upper_bound": 667995.3636190476
          },
          "point_estimate": 667592.2131948051,
          "standard_error": 202.70271503574617
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667098.9221212121,
            "upper_bound": 668000.8287878789
          },
          "point_estimate": 667558.6045454545,
          "standard_error": 219.6765123379401
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.70515230026578,
            "upper_bound": 1165.9597494818754
          },
          "point_estimate": 514.1862090532726,
          "standard_error": 257.9303071899873
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667319.7119822074,
            "upper_bound": 668321.887336047
          },
          "point_estimate": 667778.0167650532,
          "standard_error": 269.28857617017724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318.6971105200097,
            "upper_bound": 902.1165663922548
          },
          "point_estimate": 673.9650223436639,
          "standard_error": 147.3847233879974
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-ru/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47470.571785947715,
            "upper_bound": 47638.110476851856
          },
          "point_estimate": 47546.7470660857,
          "standard_error": 43.019143933764305
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47457.58052287582,
            "upper_bound": 47654.791013071896
          },
          "point_estimate": 47487.364705882355,
          "standard_error": 45.74240681482001
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.513862558444035,
            "upper_bound": 202.94581044927153
          },
          "point_estimate": 55.247198430931526,
          "standard_error": 46.64588834269224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47445.24716417943,
            "upper_bound": 47664.7567125617
          },
          "point_estimate": 47526.71687632629,
          "standard_error": 58.541805489101066
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.71643386529173,
            "upper_bound": 178.8052023228463
          },
          "point_estimate": 143.35637497736485,
          "standard_error": 36.2667331076403
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99407.66471111545,
            "upper_bound": 99745.49174818842
          },
          "point_estimate": 99568.81594817548,
          "standard_error": 86.08999936907368
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99343.88960597826,
            "upper_bound": 99712.71195652174
          },
          "point_estimate": 99555.85416666666,
          "standard_error": 89.85291229263908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.72130290386782,
            "upper_bound": 478.9037844665129
          },
          "point_estimate": 268.19917934179034,
          "standard_error": 106.44058458944367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99291.4607145594,
            "upper_bound": 99742.28948427472
          },
          "point_estimate": 99502.81645962731,
          "standard_error": 116.91459019879716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.20599521265282,
            "upper_bound": 389.8205965605039
          },
          "point_estimate": 287.33929918540343,
          "standard_error": 69.03456285474726
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312028.04520282184,
            "upper_bound": 314522.8945226564
          },
          "point_estimate": 312921.66140889976,
          "standard_error": 737.4361419861311
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 311888.20256410254,
            "upper_bound": 312456.7606837607
          },
          "point_estimate": 312201.71001221,
          "standard_error": 229.65560036973648
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.99976531613967,
            "upper_bound": 682.3186962625285
          },
          "point_estimate": 395.86912758731864,
          "standard_error": 256.35762888212435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 311893.3367748279,
            "upper_bound": 313492.2590611095
          },
          "point_estimate": 312320.3330669331,
          "standard_error": 438.8112392135208
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 196.38187455673443,
            "upper_bound": 3776.4383328713097
          },
          "point_estimate": 2457.358323820514,
          "standard_error": 1295.707303663061
        }
      }
    },
    "memmem/regex/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/huge-zh/common-that",
        "directory_name": "memmem/regex_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48032.37299594799,
            "upper_bound": 48199.609223131825
          },
          "point_estimate": 48116.32305801609,
          "standard_error": 42.80496627151036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47988.4741644679,
            "upper_bound": 48231.82058047493
          },
          "point_estimate": 48137.01510554089,
          "standard_error": 67.79754615807771
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.03609833836871,
            "upper_bound": 234.5854007165362
          },
          "point_estimate": 177.9484424608315,
          "standard_error": 50.23881269500269
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48051.84492599828,
            "upper_bound": 48210.472372320815
          },
          "point_estimate": 48127.67172326354,
          "standard_error": 39.769622222269675
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.00132928029099,
            "upper_bound": 175.04667080923485
          },
          "point_estimate": 142.8205201417117,
          "standard_error": 22.465909540067155
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/regex_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18664.19392982028,
            "upper_bound": 18697.718898407813
          },
          "point_estimate": 18683.27513196326,
          "standard_error": 8.738315198703367
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18671.159341363404,
            "upper_bound": 18700.53846379146
          },
          "point_estimate": 18692.492636254912,
          "standard_error": 6.835390097507944
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.296040110258227,
            "upper_bound": 36.56335215456138
          },
          "point_estimate": 12.41013182990676,
          "standard_error": 8.533068822339628
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18685.573329273928,
            "upper_bound": 18697.49085131705
          },
          "point_estimate": 18691.728596057965,
          "standard_error": 3.02487627265982
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.5564517691484605,
            "upper_bound": 41.69563763399718
          },
          "point_estimate": 29.16616729837668,
          "standard_error": 9.851676888984183
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1072285.8785616248,
            "upper_bound": 1074616.1198319327
          },
          "point_estimate": 1073418.7705532212,
          "standard_error": 598.2543343177415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1071558.9803921569,
            "upper_bound": 1075329.8109243698
          },
          "point_estimate": 1072842.0257352942,
          "standard_error": 1046.6554354143095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461.89384709384774,
            "upper_bound": 3138.280412519596
          },
          "point_estimate": 2462.4728259881504,
          "standard_error": 730.6940392827009
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1071927.824761318,
            "upper_bound": 1074651.020543896
          },
          "point_estimate": 1073229.9070282658,
          "standard_error": 706.6469088902123
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1242.9062571748502,
            "upper_bound": 2364.9989696656894
          },
          "point_estimate": 1993.773884206421,
          "standard_error": 287.0578516492216
        }
      }
    },
    "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/regex/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/regex_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2110.730482847646,
            "upper_bound": 2115.001225503322
          },
          "point_estimate": 2112.627849144111,
          "standard_error": 1.107519756812496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2110.3226701519993,
            "upper_bound": 2114.4571304095266
          },
          "point_estimate": 2110.567328234421,
          "standard_error": 1.4199678949732284
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08899044744725815,
            "upper_bound": 5.583984176205494
          },
          "point_estimate": 1.08237884030581,
          "standard_error": 1.526116441031107
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2110.4358641004433,
            "upper_bound": 2114.2582464877555
          },
          "point_estimate": 2111.713553643568,
          "standard_error": 0.9933770557560706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3992288576767435,
            "upper_bound": 5.243426235339931
          },
          "point_estimate": 3.697768043334553,
          "standard_error": 1.129592672057743
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52070.34950654648,
            "upper_bound": 52126.85858287284
          },
          "point_estimate": 52098.39347091464,
          "standard_error": 14.43207968597762
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52055.93151862464,
            "upper_bound": 52135.379656160454
          },
          "point_estimate": 52106.97582378224,
          "standard_error": 20.309214166273147
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.445959061469544,
            "upper_bound": 87.93669323824078
          },
          "point_estimate": 59.52254201871681,
          "standard_error": 20.94088723353437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52052.486949824415,
            "upper_bound": 52109.82374868309
          },
          "point_estimate": 52081.60324489264,
          "standard_error": 14.834994209849036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.38309292486879,
            "upper_bound": 59.62062189694555
          },
          "point_estimate": 48.027622408349494,
          "standard_error": 8.0966141465211
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48487.168626666666,
            "upper_bound": 48529.98757538889
          },
          "point_estimate": 48507.698229999995,
          "standard_error": 10.97680441009579
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48484.181777777776,
            "upper_bound": 48533.70125
          },
          "point_estimate": 48502.23644444444,
          "standard_error": 12.235651392352382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.110697733759242,
            "upper_bound": 60.10097879965437
          },
          "point_estimate": 28.331167630356955,
          "standard_error": 14.392463983026982
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48497.18344067797,
            "upper_bound": 48550.03034512857
          },
          "point_estimate": 48523.38780606061,
          "standard_error": 14.785732720842477
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.424619649994355,
            "upper_bound": 47.83056205238324
          },
          "point_estimate": 36.59469727632982,
          "standard_error": 7.880301670709339
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50182.720593059385,
            "upper_bound": 50248.26127532228
          },
          "point_estimate": 50214.65820146013,
          "standard_error": 16.718575210274032
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50172.991252302025,
            "upper_bound": 50249.37513812155
          },
          "point_estimate": 50217.884553407,
          "standard_error": 17.502679805283396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.6679673520077256,
            "upper_bound": 92.98338705087784
          },
          "point_estimate": 49.872816690549854,
          "standard_error": 24.13235057872518
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50177.826967377936,
            "upper_bound": 50255.45878013367
          },
          "point_estimate": 50222.30948195451,
          "standard_error": 20.035062494968344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.90775892772586,
            "upper_bound": 73.1092133846977
          },
          "point_estimate": 55.799479093186335,
          "standard_error": 11.67819143272776
        }
      }
    },
    "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13981.404164399968,
            "upper_bound": 14000.400830055478
          },
          "point_estimate": 13990.963334417806,
          "standard_error": 4.879161186424704
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13975.894991339494,
            "upper_bound": 14003.90623663502
          },
          "point_estimate": 13992.225002749368,
          "standard_error": 7.904744664126633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8057136081303597,
            "upper_bound": 26.71466723164729
          },
          "point_estimate": 18.17652766549715,
          "standard_error": 6.08866688435243
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13978.939788345531,
            "upper_bound": 14007.649402266892
          },
          "point_estimate": 13995.689743359027,
          "standard_error": 7.43155939619022
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.85453284985387,
            "upper_bound": 19.731584541744496
          },
          "point_estimate": 16.207036615908116,
          "standard_error": 2.514422637535651
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22996.71674155835,
            "upper_bound": 23016.454196766594
          },
          "point_estimate": 23006.404474149804,
          "standard_error": 5.040100500612151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22994.53912964806,
            "upper_bound": 23018.631808106395
          },
          "point_estimate": 23005.76293012455,
          "standard_error": 5.677507883514215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.358537199376921,
            "upper_bound": 28.33263312059891
          },
          "point_estimate": 14.593569696579683,
          "standard_error": 6.586998030445666
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22996.80585810945,
            "upper_bound": 23015.45659955353
          },
          "point_estimate": 23006.157040046717,
          "standard_error": 4.830184746477657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.700519852593336,
            "upper_bound": 21.76486413718849
          },
          "point_estimate": 16.827689534776766,
          "standard_error": 3.3467453677744263
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17824.958644580096,
            "upper_bound": 17842.16980887575
          },
          "point_estimate": 17834.034543144044,
          "standard_error": 4.424488167963251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17818.709774066796,
            "upper_bound": 17844.803024721674
          },
          "point_estimate": 17840.41721239904,
          "standard_error": 6.358479035397833
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6025619735903543,
            "upper_bound": 23.25785623983994
          },
          "point_estimate": 9.220373706639773,
          "standard_error": 5.786593446896114
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17824.288747096543,
            "upper_bound": 17842.719650540534
          },
          "point_estimate": 17833.92596892302,
          "standard_error": 4.9376147785541065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.87200715911488,
            "upper_bound": 18.20569459986392
          },
          "point_estimate": 14.790395201483689,
          "standard_error": 2.8578333856657068
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17493.32225794988,
            "upper_bound": 17513.13536011475
          },
          "point_estimate": 17502.898107274686,
          "standard_error": 5.075827420651319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17491.893871655546,
            "upper_bound": 17516.958497833413
          },
          "point_estimate": 17498.43442063874,
          "standard_error": 6.491517888471781
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.8127700632072217,
            "upper_bound": 29.362474895177503
          },
          "point_estimate": 10.854677575705784,
          "standard_error": 7.432519095240978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17496.751732623496,
            "upper_bound": 17523.64489392014
          },
          "point_estimate": 17512.96040117802,
          "standard_error": 6.9203611748436575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.079819648608153,
            "upper_bound": 21.291661471929466
          },
          "point_estimate": 16.956049185789297,
          "standard_error": 3.1733415826187894
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17277.8650469715,
            "upper_bound": 17298.694295348378
          },
          "point_estimate": 17287.293816366928,
          "standard_error": 5.369419158809264
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17276.35270783848,
            "upper_bound": 17297.652414885193
          },
          "point_estimate": 17280.621649134715,
          "standard_error": 5.223205306447621
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.0432209803527384,
            "upper_bound": 26.76932270764353
          },
          "point_estimate": 7.163385271876777,
          "standard_error": 6.486909060989345
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17274.336332128474,
            "upper_bound": 17292.029325287283
          },
          "point_estimate": 17280.68127710769,
          "standard_error": 4.526200836419184
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.279547007926819,
            "upper_bound": 23.53828130025436
          },
          "point_estimate": 17.9232468601463,
          "standard_error": 4.709220735562817
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30669.86022231567,
            "upper_bound": 30739.4222128773
          },
          "point_estimate": 30702.596656520756,
          "standard_error": 17.82706191902723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30653.74196804037,
            "upper_bound": 30736.416549855152
          },
          "point_estimate": 30706.845314189595,
          "standard_error": 20.42535815325324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.913537638214592,
            "upper_bound": 98.69701691893091
          },
          "point_estimate": 63.96811453296937,
          "standard_error": 25.02950123278563
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30682.3709254258,
            "upper_bound": 30713.48420173857
          },
          "point_estimate": 30699.895324019966,
          "standard_error": 7.869686825594099
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.041416987812507,
            "upper_bound": 78.42832382272817
          },
          "point_estimate": 59.34423883119752,
          "standard_error": 13.17732539716707
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32097.00362482724,
            "upper_bound": 32144.556202176784
          },
          "point_estimate": 32120.778684298704,
          "standard_error": 12.123937087642576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32100.996968840307,
            "upper_bound": 32148.85681618294
          },
          "point_estimate": 32116.134344766928,
          "standard_error": 10.989547006922605
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.93838663599833,
            "upper_bound": 67.01843038450133
          },
          "point_estimate": 24.61914164735158,
          "standard_error": 16.573491867366837
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32109.59178393433,
            "upper_bound": 32140.783476319673
          },
          "point_estimate": 32123.219534203705,
          "standard_error": 7.887616568946404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.084160382572566,
            "upper_bound": 55.18956941085457
          },
          "point_estimate": 40.46745166354473,
          "standard_error": 9.8637671808888
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51061.99487667289,
            "upper_bound": 51151.635637210435
          },
          "point_estimate": 51106.85673141478,
          "standard_error": 22.8872316092846
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51049.88691726691,
            "upper_bound": 51161.911414565824
          },
          "point_estimate": 51107.665227201986,
          "standard_error": 30.42064546509027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.68419986136318,
            "upper_bound": 130.09243739627507
          },
          "point_estimate": 77.75333597254362,
          "standard_error": 28.112898770921657
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51047.30669657235,
            "upper_bound": 51104.370339119254
          },
          "point_estimate": 51073.091363818254,
          "standard_error": 14.562719882023975
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.555918601313216,
            "upper_bound": 98.04698589799033
          },
          "point_estimate": 76.57612159873388,
          "standard_error": 14.223094626435266
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18010.754216077832,
            "upper_bound": 18061.696636811223
          },
          "point_estimate": 18038.449194572455,
          "standard_error": 13.093026292741929
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18012.125779865295,
            "upper_bound": 18072.380074441688
          },
          "point_estimate": 18047.318155500412,
          "standard_error": 15.203048723513108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.235765619646897,
            "upper_bound": 67.35348262227038
          },
          "point_estimate": 33.67674131113519,
          "standard_error": 15.034763916125971
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18033.36919332834,
            "upper_bound": 18063.225713615968
          },
          "point_estimate": 18052.05977635268,
          "standard_error": 7.614510525986915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.64711180089453,
            "upper_bound": 58.80257720452297
          },
          "point_estimate": 43.61632044075415,
          "standard_error": 11.044647800586926
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18418.261218622665,
            "upper_bound": 18458.696252350812
          },
          "point_estimate": 18436.49186642556,
          "standard_error": 10.416454668821896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18411.623821579265,
            "upper_bound": 18458.27032911393
          },
          "point_estimate": 18425.149620253163,
          "standard_error": 11.260365046721164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.449805872833708,
            "upper_bound": 50.33495723295849
          },
          "point_estimate": 26.24319560497607,
          "standard_error": 11.88186013406968
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18414.208440147515,
            "upper_bound": 18438.6287563725
          },
          "point_estimate": 18426.26279007069,
          "standard_error": 6.4940503330524875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.878017084550711,
            "upper_bound": 46.75617669342507
          },
          "point_estimate": 34.58598291653808,
          "standard_error": 9.201338559267995
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81468.24076432137,
            "upper_bound": 81592.38751339397
          },
          "point_estimate": 81529.39492500445,
          "standard_error": 31.960487885149508
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81434.36404494382,
            "upper_bound": 81634.59190074906
          },
          "point_estimate": 81498.58667290886,
          "standard_error": 54.257898344853615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.256418913489547,
            "upper_bound": 169.1963496534472
          },
          "point_estimate": 146.1462743904033,
          "standard_error": 41.53286443070852
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81506.83539817417,
            "upper_bound": 81618.23814829056
          },
          "point_estimate": 81554.80093973443,
          "standard_error": 28.35537014344169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.13495927553974,
            "upper_bound": 125.3973748216547
          },
          "point_estimate": 106.36486387639049,
          "standard_error": 14.893113154365576
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32561.17396154641,
            "upper_bound": 32600.814167165365
          },
          "point_estimate": 32579.283208372515,
          "standard_error": 10.190680789036628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32553.190253911256,
            "upper_bound": 32593.42948334331
          },
          "point_estimate": 32578.48713345302,
          "standard_error": 10.87864179067627
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.771183138314842,
            "upper_bound": 50.2586751400454
          },
          "point_estimate": 29.247111015770127,
          "standard_error": 10.931760559013677
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32551.74986973371,
            "upper_bound": 32605.06879360965
          },
          "point_estimate": 32575.097561146216,
          "standard_error": 13.714349827226432
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.907555436893,
            "upper_bound": 47.25483514919141
          },
          "point_estimate": 33.94633735011417,
          "standard_error": 9.324164582877255
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71871.9187862319,
            "upper_bound": 71980.5788780644
          },
          "point_estimate": 71926.3805474779,
          "standard_error": 27.814219739546573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71858.09387351779,
            "upper_bound": 71991.10331380577
          },
          "point_estimate": 71943.40704874836,
          "standard_error": 30.907419119710696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.603918301738347,
            "upper_bound": 150.15023210702785
          },
          "point_estimate": 91.81415425930612,
          "standard_error": 40.71604994289978
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71908.74578363704,
            "upper_bound": 71989.889364392
          },
          "point_estimate": 71943.45579282378,
          "standard_error": 20.854380392592496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.37214566134843,
            "upper_bound": 121.39714767889669
          },
          "point_estimate": 92.48691797006929,
          "standard_error": 19.08846854060373
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18625.1327727406,
            "upper_bound": 18645.718964056658
          },
          "point_estimate": 18635.550830645756,
          "standard_error": 5.264171216827133
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18622.197845048744,
            "upper_bound": 18646.51442131496
          },
          "point_estimate": 18637.71947152386,
          "standard_error": 6.0379351730050566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.7206356110146745,
            "upper_bound": 29.14941188372976
          },
          "point_estimate": 13.435868011852095,
          "standard_error": 6.9497138006662835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18630.537906122245,
            "upper_bound": 18651.80989029783
          },
          "point_estimate": 18642.263493100025,
          "standard_error": 5.34021597055745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.461282245631763,
            "upper_bound": 23.04986107321719
          },
          "point_estimate": 17.559598138324088,
          "standard_error": 3.668781154103052
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22141.002821740138,
            "upper_bound": 22167.511529432224
          },
          "point_estimate": 22154.330686268822,
          "standard_error": 6.793648097102499
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22139.425441265976,
            "upper_bound": 22178.395443874444
          },
          "point_estimate": 22148.348554473523,
          "standard_error": 10.040334887258542
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.995198859382824,
            "upper_bound": 44.54751743492882
          },
          "point_estimate": 18.636272645404617,
          "standard_error": 10.561492019875246
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22139.75332975846,
            "upper_bound": 22160.42824900428
          },
          "point_estimate": 22147.729743658656,
          "standard_error": 5.327899757604169
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.1846915107656,
            "upper_bound": 28.70516017583668
          },
          "point_estimate": 22.64942365904568,
          "standard_error": 4.034268245817751
        }
      }
    },
    "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35147.537724204434,
            "upper_bound": 35206.85666023786
          },
          "point_estimate": 35177.76894326584,
          "standard_error": 15.240141912239451
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35134.37078109932,
            "upper_bound": 35223.42068466731
          },
          "point_estimate": 35181.13577627772,
          "standard_error": 24.47239932236149
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.6294566051468,
            "upper_bound": 87.47111092538563
          },
          "point_estimate": 48.55234334969085,
          "standard_error": 20.308419418750507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35165.065151705465,
            "upper_bound": 35224.36248634235
          },
          "point_estimate": 35194.48059211762,
          "standard_error": 15.353715949299472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.58009365705481,
            "upper_bound": 62.29082771350769
          },
          "point_estimate": 50.89664579175263,
          "standard_error": 8.16407273592473
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-freq_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409011.2286516854,
            "upper_bound": 409531.2076048021
          },
          "point_estimate": 409259.946623863,
          "standard_error": 133.15482274641818
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 408861.4915730337,
            "upper_bound": 409590.9213483146
          },
          "point_estimate": 409218.2528089888,
          "standard_error": 150.21598066005916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.576617736467792,
            "upper_bound": 710.5207671610319
          },
          "point_estimate": 540.7262828159475,
          "standard_error": 196.80005720294812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 409061.2905423339,
            "upper_bound": 409664.1870572269
          },
          "point_estimate": 409362.85755143734,
          "standard_error": 151.6865069589039
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.4371856548316,
            "upper_bound": 585.9425970627935
          },
          "point_estimate": 443.4692282025927,
          "standard_error": 96.11916336944452
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector-repeated_ra"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2739698.1603741497,
            "upper_bound": 2744257.1125
          },
          "point_estimate": 2742098.334155329,
          "standard_error": 1164.099652885912
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2739602.0102040814,
            "upper_bound": 2745183.1428571427
          },
          "point_estimate": 2742604.7857142854,
          "standard_error": 1253.5994538933055
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.31258506265812,
            "upper_bound": 6447.306610537394
          },
          "point_estimate": 4137.293562262698,
          "standard_error": 1518.8143030992812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2740915.5151283033,
            "upper_bound": 2744149.827420901
          },
          "point_estimate": 2742388.7834879407,
          "standard_error": 811.228288171218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1858.0213130199936,
            "upper_bound": 5233.87598738595
          },
          "point_estimate": 3879.0097942065536,
          "standard_error": 914.7262752789322
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_oneshot_pathological-defeat-simple-vector_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164623.39073678898,
            "upper_bound": 164825.49576186883
          },
          "point_estimate": 164726.72404672124,
          "standard_error": 51.6709410191187
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164602.60583207643,
            "upper_bound": 164862.9556561086
          },
          "point_estimate": 164741.3747171946,
          "standard_error": 54.26925420707698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.354636928417364,
            "upper_bound": 291.54509075162537
          },
          "point_estimate": 163.1492815379139,
          "standard_error": 78.97944219032307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164729.28566543115,
            "upper_bound": 164830.35811247575
          },
          "point_estimate": 164768.96059234883,
          "standard_error": 25.700377323588196
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.83254338979638,
            "upper_bound": 220.5990750419606
          },
          "point_estimate": 172.01754550641152,
          "standard_error": 33.96434378201361
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11025.611199098628,
            "upper_bound": 11042.616892376507
          },
          "point_estimate": 11034.062917620691,
          "standard_error": 4.34981307207039
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11022.460902312843,
            "upper_bound": 11047.55124771759
          },
          "point_estimate": 11032.01916379252,
          "standard_error": 6.223954061244907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.5470492497681287,
            "upper_bound": 25.16096833297691
          },
          "point_estimate": 17.139371302026262,
          "standard_error": 5.641033362265777
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11019.838239948444,
            "upper_bound": 11034.853189333098
          },
          "point_estimate": 11026.13543328248,
          "standard_error": 3.853102351140959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.634727503096777,
            "upper_bound": 17.917192147944483
          },
          "point_estimate": 14.494414114207204,
          "standard_error": 2.394670299531427
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10951.344443282109,
            "upper_bound": 10976.79525064166
          },
          "point_estimate": 10963.4557848683,
          "standard_error": 6.514623291097853
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10950.01317374285,
            "upper_bound": 10980.044745558567
          },
          "point_estimate": 10956.774199359055,
          "standard_error": 7.985165318784876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.5493747770842714,
            "upper_bound": 36.37517294048136
          },
          "point_estimate": 12.243466238522098,
          "standard_error": 9.399331036853017
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10952.361728130152,
            "upper_bound": 10965.822155526805
          },
          "point_estimate": 10956.860353437589,
          "standard_error": 3.4542446860244707
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.197870972709762,
            "upper_bound": 28.305993151601783
          },
          "point_estimate": 21.852410736402316,
          "standard_error": 4.608963851391837
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14035.64484589513,
            "upper_bound": 14059.369352244954
          },
          "point_estimate": 14046.592247982946,
          "standard_error": 6.0615359956196375
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14034.15997684292,
            "upper_bound": 14055.500852309277
          },
          "point_estimate": 14043.917985333848,
          "standard_error": 5.591477845864049
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.329188653362942,
            "upper_bound": 30.46662836300881
          },
          "point_estimate": 13.262794237714091,
          "standard_error": 6.4402541715457025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14025.838996567045,
            "upper_bound": 14047.775257669991
          },
          "point_estimate": 14035.482830176385,
          "standard_error": 5.825972183350299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.896100134799038,
            "upper_bound": 28.425263744227312
          },
          "point_estimate": 20.267780545850783,
          "standard_error": 5.631236598220757
        }
      }
    },
    "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.91586235335937,
            "upper_bound": 32.49283680812618
          },
          "point_estimate": 32.21828514224151,
          "standard_error": 0.14745646214012192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.91433396449067,
            "upper_bound": 32.46431635340623
          },
          "point_estimate": 32.30429663098583,
          "standard_error": 0.11497331499635866
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04326449794790643,
            "upper_bound": 0.8709625416667885
          },
          "point_estimate": 0.24422526887150744,
          "standard_error": 0.20550542420661572
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.208270042453165,
            "upper_bound": 32.44754271471574
          },
          "point_estimate": 32.34454893276741,
          "standard_error": 0.05998995144305095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.160375608779539,
            "upper_bound": 0.6623151001643556
          },
          "point_estimate": 0.49165486569142985,
          "standard_error": 0.12219299269579696
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.22762293894295,
            "upper_bound": 12.244047565702232
          },
          "point_estimate": 12.235680940607905,
          "standard_error": 0.004193062410898848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.2286554158758,
            "upper_bound": 12.247494311796585
          },
          "point_estimate": 12.233277568223508,
          "standard_error": 0.004101892069220899
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0021580136254403604,
            "upper_bound": 0.026132772577206965
          },
          "point_estimate": 0.007637130156029538,
          "standard_error": 0.006090222116854685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.230056875999091,
            "upper_bound": 12.238581466015734
          },
          "point_estimate": 12.2345576125382,
          "standard_error": 0.002227479925935639
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0050152674302989045,
            "upper_bound": 0.018669872658228386
          },
          "point_estimate": 0.014016398301832728,
          "standard_error": 0.003135603737756958
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.866045067674037,
            "upper_bound": 16.897371157254103
          },
          "point_estimate": 16.88098865362427,
          "standard_error": 0.00799050606432519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.865300627595257,
            "upper_bound": 16.89416534011692
          },
          "point_estimate": 16.88099197082731,
          "standard_error": 0.008154616327009333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032163826293645768,
            "upper_bound": 0.04206828828456172
          },
          "point_estimate": 0.020721617216667425,
          "standard_error": 0.009387029402859542
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.86777031129905,
            "upper_bound": 16.88765658456987
          },
          "point_estimate": 16.876279595872596,
          "standard_error": 0.005127768710464971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010861147002072647,
            "upper_bound": 0.03688124283590782
          },
          "point_estimate": 0.02660106496754933,
          "standard_error": 0.006790479352611383
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.981845415563578,
            "upper_bound": 11.99564440478428
          },
          "point_estimate": 11.989070494079566,
          "standard_error": 0.0035458845713253135
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.978640841499573,
            "upper_bound": 11.999586466351346
          },
          "point_estimate": 11.993608011293976,
          "standard_error": 0.005838583070514115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014933557082087731,
            "upper_bound": 0.01915002255241267
          },
          "point_estimate": 0.010357013018901929,
          "standard_error": 0.004888173576808245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.980830790034435,
            "upper_bound": 11.998737865969725
          },
          "point_estimate": 11.990555114371944,
          "standard_error": 0.004594510506244586
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006299171511128234,
            "upper_bound": 0.014076291832205618
          },
          "point_estimate": 0.011817411559693833,
          "standard_error": 0.001959075124757929
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.753959149261709,
            "upper_bound": 10.769180228472928
          },
          "point_estimate": 10.761440265729084,
          "standard_error": 0.003903274446178834
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.752675169146835,
            "upper_bound": 10.771414606251962
          },
          "point_estimate": 10.758447114622086,
          "standard_error": 0.005206776532935662
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002437834694790655,
            "upper_bound": 0.0223360217189389
          },
          "point_estimate": 0.01386500954130354,
          "standard_error": 0.005288289330294265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.75565045381891,
            "upper_bound": 10.76797390059208
          },
          "point_estimate": 10.763063116562623,
          "standard_error": 0.003113667873868417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007176853246997127,
            "upper_bound": 0.016692403993611032
          },
          "point_estimate": 0.013006123093680917,
          "standard_error": 0.002446889692859828
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.256239403196002,
            "upper_bound": 11.267671705994932
          },
          "point_estimate": 11.26172457813828,
          "standard_error": 0.0029392699602909557
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.255390214796645,
            "upper_bound": 11.27019726794686
          },
          "point_estimate": 11.258619149336482,
          "standard_error": 0.0035817742313495075
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001373592876412244,
            "upper_bound": 0.01567213902378882
          },
          "point_estimate": 0.006548302578918352,
          "standard_error": 0.004298114280527332
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.25717612873461,
            "upper_bound": 11.27067110072168
          },
          "point_estimate": 11.26277992938918,
          "standard_error": 0.0034703581886144637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004439324457802205,
            "upper_bound": 0.01254374524810853
          },
          "point_estimate": 0.009823033013347528,
          "standard_error": 0.001972335780036186
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.46122014999289,
            "upper_bound": 29.57829442629749
          },
          "point_estimate": 29.515932024183012,
          "standard_error": 0.03008938544762925
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.42491508452071,
            "upper_bound": 29.568782338433863
          },
          "point_estimate": 29.50286270659388,
          "standard_error": 0.036055143637475986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0185120245674374,
            "upper_bound": 0.163474123075144
          },
          "point_estimate": 0.10664879343242636,
          "standard_error": 0.03599620025470687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.482641147379393,
            "upper_bound": 29.54520712103969
          },
          "point_estimate": 29.514372573302182,
          "standard_error": 0.015666383910059388
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04875014590067077,
            "upper_bound": 0.13413444102757907
          },
          "point_estimate": 0.10068085602894268,
          "standard_error": 0.02357174828137668
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.219764209570805,
            "upper_bound": 14.232465404917823
          },
          "point_estimate": 14.226130503179164,
          "standard_error": 0.0032394864119003855
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.219578130434108,
            "upper_bound": 14.234993335571966
          },
          "point_estimate": 14.22521747672467,
          "standard_error": 0.0036471485010637046
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001401711112130749,
            "upper_bound": 0.019237470165000943
          },
          "point_estimate": 0.009942348319974148,
          "standard_error": 0.004683309575835489
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.219677186175554,
            "upper_bound": 14.231968556133763
          },
          "point_estimate": 14.22579478744732,
          "standard_error": 0.0031914507270791085
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005843468895397189,
            "upper_bound": 0.013880090308764711
          },
          "point_estimate": 0.010797885326995644,
          "standard_error": 0.0020766739699987083
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.071510440022283,
            "upper_bound": 14.098883118059453
          },
          "point_estimate": 14.08554116088501,
          "standard_error": 0.007006136330492612
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.071490698619794,
            "upper_bound": 14.103945149925472
          },
          "point_estimate": 14.086321288228024,
          "standard_error": 0.007615535017479387
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006626598236950367,
            "upper_bound": 0.04029925937088064
          },
          "point_estimate": 0.016054313162784124,
          "standard_error": 0.008801419352352046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.075550730613768,
            "upper_bound": 14.0914956062285
          },
          "point_estimate": 14.083560534705232,
          "standard_error": 0.004105801267692225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011066139779665616,
            "upper_bound": 0.03120366054000423
          },
          "point_estimate": 0.023319078805693397,
          "standard_error": 0.005190487864161803
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.02691705614355,
            "upper_bound": 17.052197444703427
          },
          "point_estimate": 17.038885582400333,
          "standard_error": 0.006471582505272483
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.01903948437919,
            "upper_bound": 17.05718231408599
          },
          "point_estimate": 17.03277982370577,
          "standard_error": 0.00842979567117697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014678669525248949,
            "upper_bound": 0.034695156404494124
          },
          "point_estimate": 0.020784086374837182,
          "standard_error": 0.00886613855382232
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.021893069664188,
            "upper_bound": 17.049639390253134
          },
          "point_estimate": 17.034546006925584,
          "standard_error": 0.007254786722622793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009497643745208633,
            "upper_bound": 0.02693382393276268
          },
          "point_estimate": 0.021584505723623817,
          "standard_error": 0.004177781864009642
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.596033728356698,
            "upper_bound": 17.623475283313372
          },
          "point_estimate": 17.608760744112857,
          "standard_error": 0.007079275251048305
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.591232773066842,
            "upper_bound": 17.628538578400597
          },
          "point_estimate": 17.60286258430924,
          "standard_error": 0.008514926486886539
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038452241935808392,
            "upper_bound": 0.03993975964697708
          },
          "point_estimate": 0.021413164254645095,
          "standard_error": 0.00916666347430998
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.595085596736592,
            "upper_bound": 17.61300839928959
          },
          "point_estimate": 17.603230259587402,
          "standard_error": 0.004469164654808412
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00991923539241861,
            "upper_bound": 0.031031381257108713
          },
          "point_estimate": 0.023588948135541556,
          "standard_error": 0.005451669102333792
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.27204875572907,
            "upper_bound": 11.288454310579333
          },
          "point_estimate": 11.280040661523314,
          "standard_error": 0.004171035035797166
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.270893082874435,
            "upper_bound": 11.292175217179294
          },
          "point_estimate": 11.275314134797656,
          "standard_error": 0.005323852553009292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006299354173838935,
            "upper_bound": 0.02406156823315621
          },
          "point_estimate": 0.012797577761671848,
          "standard_error": 0.006455530227542231
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.272839763359922,
            "upper_bound": 11.285348148145388
          },
          "point_estimate": 11.27804669345234,
          "standard_error": 0.0032391101079452043
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007260406757219683,
            "upper_bound": 0.01802494513391483
          },
          "point_estimate": 0.013863383043918933,
          "standard_error": 0.002797377566698495
        }
      }
    },
    "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.88481599692821,
            "upper_bound": 22.924412602847447
          },
          "point_estimate": 22.903154922711675,
          "standard_error": 0.010183146001736108
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.878883325132367,
            "upper_bound": 22.928676682299727
          },
          "point_estimate": 22.8912282745914,
          "standard_error": 0.0131314561092296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002166177479338857,
            "upper_bound": 0.0537138584249752
          },
          "point_estimate": 0.01846231809747294,
          "standard_error": 0.012879624526365409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.888775586714075,
            "upper_bound": 22.92447334414496
          },
          "point_estimate": 22.90646912689277,
          "standard_error": 0.009064159923779128
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011253925050048226,
            "upper_bound": 0.04155769164790236
          },
          "point_estimate": 0.0339095276037337,
          "standard_error": 0.007283303701692168
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52389.06994460501,
            "upper_bound": 52478.91901631743
          },
          "point_estimate": 52431.851989402705,
          "standard_error": 22.873944824667287
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52388.62100192678,
            "upper_bound": 52468.983236994216
          },
          "point_estimate": 52422.869761560694,
          "standard_error": 20.749206507027505
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.629421578837055,
            "upper_bound": 123.19345250940836
          },
          "point_estimate": 47.74662866822151,
          "standard_error": 26.68691075610265
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52409.059305107825,
            "upper_bound": 52528.04382321929
          },
          "point_estimate": 52463.49738007657,
          "standard_error": 31.860760983991632
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.992934374205987,
            "upper_bound": 105.61435000832664
          },
          "point_estimate": 76.34180608682746,
          "standard_error": 19.81691260948302
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48900.33685570083,
            "upper_bound": 48986.87212134686
          },
          "point_estimate": 48938.85353532439,
          "standard_error": 22.313672309896077
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48885.9572678331,
            "upper_bound": 48978.13728129206
          },
          "point_estimate": 48913.10983998803,
          "standard_error": 25.908026490293935
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.631365993781454,
            "upper_bound": 110.85345984219528
          },
          "point_estimate": 61.63009311850201,
          "standard_error": 24.18299742204419
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48899.44631433184,
            "upper_bound": 48945.65379097264
          },
          "point_estimate": 48919.21094195172,
          "standard_error": 11.638723055571724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.967861599959537,
            "upper_bound": 103.83574332541107
          },
          "point_estimate": 74.49447988273754,
          "standard_error": 21.47167949967213
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50569.24808931055,
            "upper_bound": 50649.20502284919
          },
          "point_estimate": 50609.03515630174,
          "standard_error": 20.518098348464555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50532.97426981919,
            "upper_bound": 50659.02489568845
          },
          "point_estimate": 50619.10741108682,
          "standard_error": 29.08966682534053
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.923527555149793,
            "upper_bound": 128.72663224871943
          },
          "point_estimate": 71.2896645062694,
          "standard_error": 30.71733612801734
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50572.83050996755,
            "upper_bound": 50639.69454018192
          },
          "point_estimate": 50612.8641547604,
          "standard_error": 17.11662336813769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.50804986957582,
            "upper_bound": 85.1694567599942
          },
          "point_estimate": 68.26590155859031,
          "standard_error": 11.65558384334376
        }
      }
    },
    "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/sliceslice_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14056.661043162818,
            "upper_bound": 14078.174974443433
          },
          "point_estimate": 14067.18614080396,
          "standard_error": 5.508668827766247
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14051.398897911831,
            "upper_bound": 14084.730239752513
          },
          "point_estimate": 14064.265910995102,
          "standard_error": 8.01387368554763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9291286989653744,
            "upper_bound": 30.86809537232284
          },
          "point_estimate": 19.905392884234857,
          "standard_error": 7.021251705497012
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14059.84293600631,
            "upper_bound": 14079.087100632649
          },
          "point_estimate": 14068.985459165737,
          "standard_error": 5.052584672610154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.511672645913311,
            "upper_bound": 22.561803593783157
          },
          "point_estimate": 18.29097114885597,
          "standard_error": 3.0705244119700676
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23125.35062450713,
            "upper_bound": 23164.69818682135
          },
          "point_estimate": 23144.55343165504,
          "standard_error": 10.01239520479201
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23122.327092811647,
            "upper_bound": 23168.48136942675
          },
          "point_estimate": 23141.31193205945,
          "standard_error": 11.255789515731143
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.483201848217693,
            "upper_bound": 56.51857605073306
          },
          "point_estimate": 32.84362643283131,
          "standard_error": 13.94805054451641
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23131.87966128309,
            "upper_bound": 23161.25625639579
          },
          "point_estimate": 23143.926243692615,
          "standard_error": 7.600163339482893
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.924166893284582,
            "upper_bound": 42.73476169825954
          },
          "point_estimate": 33.363666686010355,
          "standard_error": 6.528118358653716
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17832.500285971182,
            "upper_bound": 17851.420270952534
          },
          "point_estimate": 17841.84110492227,
          "standard_error": 4.8388059321951635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17826.570181461502,
            "upper_bound": 17858.10282818375
          },
          "point_estimate": 17841.64976704267,
          "standard_error": 8.077895639093656
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.591816774553358,
            "upper_bound": 26.861290418163733
          },
          "point_estimate": 22.84220111237141,
          "standard_error": 6.003875191323456
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17829.87239625906,
            "upper_bound": 17847.865087928258
          },
          "point_estimate": 17839.43345541168,
          "standard_error": 4.591506210315001
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.8805523583442,
            "upper_bound": 19.4078924250155
          },
          "point_estimate": 16.089697835125236,
          "standard_error": 2.413621929969892
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17673.45551843645,
            "upper_bound": 17708.591921330088
          },
          "point_estimate": 17688.37640678948,
          "standard_error": 9.189676474349938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17668.155184798983,
            "upper_bound": 17694.893223844283
          },
          "point_estimate": 17682.790762368208,
          "standard_error": 7.0188391053918195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.329688382264794,
            "upper_bound": 34.57161867090449
          },
          "point_estimate": 16.673778633421627,
          "standard_error": 7.491404505744716
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17671.162812887345,
            "upper_bound": 17689.634729346835
          },
          "point_estimate": 17681.34611432363,
          "standard_error": 4.636017652237568
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.613017655153248,
            "upper_bound": 45.030874373843645
          },
          "point_estimate": 30.63186639534568,
          "standard_error": 11.087654244339864
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17434.858465710167,
            "upper_bound": 17498.985276635547
          },
          "point_estimate": 17460.684633417997,
          "standard_error": 17.191296060695873
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17431.190653531958,
            "upper_bound": 17471.46864488227
          },
          "point_estimate": 17441.42945699183,
          "standard_error": 11.220094233746774
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2645915200764233,
            "upper_bound": 49.568318490482206
          },
          "point_estimate": 15.495485174201676,
          "standard_error": 12.95875896849077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17430.042426254946,
            "upper_bound": 17457.966721283494
          },
          "point_estimate": 17442.79647522108,
          "standard_error": 7.090585279023064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.07549539764761,
            "upper_bound": 85.73220730661569
          },
          "point_estimate": 57.193116234296554,
          "standard_error": 23.382122432599058
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30756.596757253727,
            "upper_bound": 30790.307382284147
          },
          "point_estimate": 30773.723303486924,
          "standard_error": 8.635212465649714
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30751.958546280523,
            "upper_bound": 30801.100511073255
          },
          "point_estimate": 30775.831627220738,
          "standard_error": 14.37028558562963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.530446033256585,
            "upper_bound": 46.89458384661249
          },
          "point_estimate": 35.036211558562194,
          "standard_error": 11.037224580690994
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30755.21144191161,
            "upper_bound": 30792.27492127407
          },
          "point_estimate": 30778.705431536095,
          "standard_error": 9.400814728050957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.649675560667056,
            "upper_bound": 35.28953473383257
          },
          "point_estimate": 28.675986864240873,
          "standard_error": 4.607219390268008
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32218.13765073165,
            "upper_bound": 32278.13354045759
          },
          "point_estimate": 32248.44945114087,
          "standard_error": 15.38881867030586
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32206.38158482143,
            "upper_bound": 32294.637648809527
          },
          "point_estimate": 32246.987698412697,
          "standard_error": 18.475252915015428
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.013814528651178,
            "upper_bound": 103.98489041952344
          },
          "point_estimate": 42.29257073353101,
          "standard_error": 24.197664827721297
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32225.846488534215,
            "upper_bound": 32266.83066667096
          },
          "point_estimate": 32250.597043135436,
          "standard_error": 10.454136414192226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.60071024469907,
            "upper_bound": 64.39242119229142
          },
          "point_estimate": 51.21975066664596,
          "standard_error": 9.187481724885616
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49971.07289715336,
            "upper_bound": 50039.94323825375
          },
          "point_estimate": 50005.60441995715,
          "standard_error": 17.607742868726053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49960.0305785124,
            "upper_bound": 50049.25792011019
          },
          "point_estimate": 50006.10640495868,
          "standard_error": 21.03384627613761
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.77128403850038,
            "upper_bound": 105.51927109497228
          },
          "point_estimate": 57.02041719016171,
          "standard_error": 24.436332605274018
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49945.883336080304,
            "upper_bound": 50020.688828197126
          },
          "point_estimate": 49985.55528245859,
          "standard_error": 19.658915468499632
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.900325546030025,
            "upper_bound": 74.57731107673027
          },
          "point_estimate": 58.68510793883077,
          "standard_error": 10.816740919080788
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17575.109678041306,
            "upper_bound": 17594.74054674895
          },
          "point_estimate": 17584.780486331998,
          "standard_error": 5.033395954773743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17570.99451435947,
            "upper_bound": 17597.8130041949
          },
          "point_estimate": 17583.011447241046,
          "standard_error": 7.920589155446136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.310479158727952,
            "upper_bound": 27.76304798241675
          },
          "point_estimate": 21.366495022409417,
          "standard_error": 6.600341597432118
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17571.549153216325,
            "upper_bound": 17593.915191487882
          },
          "point_estimate": 17583.101996454658,
          "standard_error": 5.797601163804693
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.92653586272984,
            "upper_bound": 21.123372483794277
          },
          "point_estimate": 16.76770561268312,
          "standard_error": 2.900090382813732
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18144.055661023656,
            "upper_bound": 18166.316331155853
          },
          "point_estimate": 18154.741707360605,
          "standard_error": 5.715111564863713
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18141.93607362985,
            "upper_bound": 18167.87848932677
          },
          "point_estimate": 18151.98065550558,
          "standard_error": 4.64653184835761
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1010521418706407,
            "upper_bound": 35.014847354372996
          },
          "point_estimate": 6.4528037210219,
          "standard_error": 9.815264047985712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18144.465928663576,
            "upper_bound": 18167.868633059355
          },
          "point_estimate": 18155.569156979953,
          "standard_error": 6.2072810355297
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.62225356097532,
            "upper_bound": 24.85567656535231
          },
          "point_estimate": 19.02052009294131,
          "standard_error": 4.240295180289138
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82734.66214068771,
            "upper_bound": 82891.97319973244
          },
          "point_estimate": 82815.78499638427,
          "standard_error": 40.14663503441165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82737.22987851177,
            "upper_bound": 82910.2960516325
          },
          "point_estimate": 82819.4194191344,
          "standard_error": 41.00199643216916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.811433654712076,
            "upper_bound": 227.37493491543748
          },
          "point_estimate": 91.73313541013404,
          "standard_error": 51.43323819304966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82788.88597149852,
            "upper_bound": 82944.76760607434
          },
          "point_estimate": 82858.8535455433,
          "standard_error": 41.621489262685905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.05596458230436,
            "upper_bound": 179.64270564403017
          },
          "point_estimate": 134.06933643368075,
          "standard_error": 30.296117643710925
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32522.44181165919,
            "upper_bound": 32561.500777434336
          },
          "point_estimate": 32543.28705637412,
          "standard_error": 10.02393531765252
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32516.235874439462,
            "upper_bound": 32569.766995515696
          },
          "point_estimate": 32552.22440743113,
          "standard_error": 14.341590453148603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.893966496257872,
            "upper_bound": 56.37868958203753
          },
          "point_estimate": 27.961151665029163,
          "standard_error": 13.342144506815854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32541.030849155613,
            "upper_bound": 32568.305675770123
          },
          "point_estimate": 32558.032587502184,
          "standard_error": 7.0182614759834046
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.611886198264854,
            "upper_bound": 43.113551553863275
          },
          "point_estimate": 33.480630488411315,
          "standard_error": 7.153827228925641
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72682.24004676018,
            "upper_bound": 72830.5874201677
          },
          "point_estimate": 72750.34826629449,
          "standard_error": 38.17591640221153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72644.9832999332,
            "upper_bound": 72818.35721442886
          },
          "point_estimate": 72732.9049766199,
          "standard_error": 49.98231550538519
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.260490867322112,
            "upper_bound": 211.11561131206565
          },
          "point_estimate": 128.5220805339123,
          "standard_error": 44.678056206099754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 72655.57722893333,
            "upper_bound": 72747.96578280527
          },
          "point_estimate": 72690.87681857221,
          "standard_error": 23.519213428160068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.00479160011523,
            "upper_bound": 172.13472303863662
          },
          "point_estimate": 126.86913129402048,
          "standard_error": 31.654403372373736
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18443.654326784505,
            "upper_bound": 18474.457010159214
          },
          "point_estimate": 18458.2919966297,
          "standard_error": 7.926713371424897
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18435.80306105192,
            "upper_bound": 18480.99858664927
          },
          "point_estimate": 18457.3058684255,
          "standard_error": 9.130779215974911
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.570327097948964,
            "upper_bound": 47.4537076684345
          },
          "point_estimate": 22.499397993502036,
          "standard_error": 11.653171189949482
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18446.163165905633,
            "upper_bound": 18481.930357939546
          },
          "point_estimate": 18463.07527196294,
          "standard_error": 9.563830090305164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.924211498664832,
            "upper_bound": 33.29810097832171
          },
          "point_estimate": 26.48022829305826,
          "standard_error": 5.476345623895997
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22282.493836326554,
            "upper_bound": 22317.318812589412
          },
          "point_estimate": 22299.6207652088,
          "standard_error": 8.923672753153467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22272.692060085836,
            "upper_bound": 22325.711526670755
          },
          "point_estimate": 22294.31102254922,
          "standard_error": 15.384742458039968
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.349086171129353,
            "upper_bound": 48.36266566499605
          },
          "point_estimate": 42.568390445978075,
          "standard_error": 11.048239283151634
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22272.983864537517,
            "upper_bound": 22306.10725001763
          },
          "point_estimate": 22286.432968380486,
          "standard_error": 8.454738413476823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.702505901457,
            "upper_bound": 35.77862986792318
          },
          "point_estimate": 29.718228559447017,
          "standard_error": 4.4145781049914
        }
      }
    },
    "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35240.06444281525,
            "upper_bound": 35312.892745397534
          },
          "point_estimate": 35273.785764907145,
          "standard_error": 18.678073008456153
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35231.936461388075,
            "upper_bound": 35308.97148908439
          },
          "point_estimate": 35256.94312072336,
          "standard_error": 21.434306372705116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.992537210322807,
            "upper_bound": 96.77569879508316
          },
          "point_estimate": 51.161051682130314,
          "standard_error": 21.81304140433264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35245.60047643268,
            "upper_bound": 35302.21236346636
          },
          "point_estimate": 35268.09074659456,
          "standard_error": 14.464831504110668
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.850753559792167,
            "upper_bound": 84.23539935722408
          },
          "point_estimate": 62.24613091433614,
          "standard_error": 15.60008087768943
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376614.6726689576,
            "upper_bound": 377007.7261474799
          },
          "point_estimate": 376823.59173007694,
          "standard_error": 101.1919772467797
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376583.0618556701,
            "upper_bound": 377040.2048969072
          },
          "point_estimate": 376941.8808705613,
          "standard_error": 126.96600247074534
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.422523210218493,
            "upper_bound": 519.742148848322
          },
          "point_estimate": 153.4468076279855,
          "standard_error": 141.3319251905167
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376794.0991809625,
            "upper_bound": 377019.96194274223
          },
          "point_estimate": 376936.7632882581,
          "standard_error": 57.84967724731716
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.06872256221152,
            "upper_bound": 425.62171666802544
          },
          "point_estimate": 336.8706155130723,
          "standard_error": 71.50077551041466
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2735816.628409227,
            "upper_bound": 2739518.007312925
          },
          "point_estimate": 2737696.1752295913,
          "standard_error": 950.4280778152348
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2734859.9119047616,
            "upper_bound": 2740861.125
          },
          "point_estimate": 2737857.1600765307,
          "standard_error": 1846.2145548937624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 256.55598294541807,
            "upper_bound": 4687.9555873076115
          },
          "point_estimate": 3950.233192369227,
          "standard_error": 1377.9731465397722
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2735420.9520819928,
            "upper_bound": 2739851.633184469
          },
          "point_estimate": 2737725.0955473096,
          "standard_error": 1160.761667910691
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2056.906240992299,
            "upper_bound": 3681.19083421414
          },
          "point_estimate": 3161.8031042059633,
          "standard_error": 419.93529042047703
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164632.96389845936,
            "upper_bound": 164968.91546577605
          },
          "point_estimate": 164786.75333907924,
          "standard_error": 86.32211760811707
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164600.94462400343,
            "upper_bound": 164975.17722473605
          },
          "point_estimate": 164724.70429864252,
          "standard_error": 83.46014626961893
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.322523230205572,
            "upper_bound": 414.46719580917
          },
          "point_estimate": 200.87402881686873,
          "standard_error": 107.04221574304177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164649.25435269703,
            "upper_bound": 164875.86991969345
          },
          "point_estimate": 164738.2126109185,
          "standard_error": 57.54319977678621
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.31961762274264,
            "upper_bound": 398.5167592644618
          },
          "point_estimate": 288.61032520536565,
          "standard_error": 79.9824457263003
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10659.56886305458,
            "upper_bound": 10683.9018278459
          },
          "point_estimate": 10672.480837446552,
          "standard_error": 6.24379493281188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10651.86545886546,
            "upper_bound": 10688.780331155333
          },
          "point_estimate": 10678.228106128106,
          "standard_error": 8.933237504136542
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.2490310234005135,
            "upper_bound": 31.45965299657373
          },
          "point_estimate": 17.419073353258312,
          "standard_error": 7.925771551583731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10658.97200146937,
            "upper_bound": 10682.914726489726
          },
          "point_estimate": 10672.269563640992,
          "standard_error": 6.097076149476945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.449756339509866,
            "upper_bound": 26.26092120051903
          },
          "point_estimate": 20.83134252042988,
          "standard_error": 4.230772610272443
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10501.855198210058,
            "upper_bound": 10518.882659323022
          },
          "point_estimate": 10510.806768889446,
          "standard_error": 4.385708023969113
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10499.213247738546,
            "upper_bound": 10519.35379826865
          },
          "point_estimate": 10517.919786151986,
          "standard_error": 5.756186242966045
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4469616494910141,
            "upper_bound": 23.132340236143833
          },
          "point_estimate": 6.566269588706922,
          "standard_error": 6.849751145871387
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10502.606553879454,
            "upper_bound": 10520.103527442736
          },
          "point_estimate": 10512.544280522512,
          "standard_error": 4.521513120795385
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.0397215641554824,
            "upper_bound": 18.12160970620131
          },
          "point_estimate": 14.61794014392956,
          "standard_error": 2.937813760657677
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14154.147565736568,
            "upper_bound": 14182.546992498485
          },
          "point_estimate": 14166.586045900707,
          "standard_error": 7.376902234916821
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14152.07052613361,
            "upper_bound": 14180.056026090344
          },
          "point_estimate": 14157.299581386293,
          "standard_error": 5.788855367624459
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0656764647234924,
            "upper_bound": 31.01487430334722
          },
          "point_estimate": 9.15306783789068,
          "standard_error": 6.967749912390325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14150.937597352024,
            "upper_bound": 14159.44078606381
          },
          "point_estimate": 14154.822051624387,
          "standard_error": 2.150941016712929
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.282693245871991,
            "upper_bound": 33.553226744123954
          },
          "point_estimate": 24.6015278914684,
          "standard_error": 7.623323150776801
        }
      }
    },
    "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/sliceslice_prebuilt_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.60672552538465,
            "upper_bound": 28.661612559052134
          },
          "point_estimate": 28.63280154821595,
          "standard_error": 0.014068433049208798
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.59030997536421,
            "upper_bound": 28.668902720984295
          },
          "point_estimate": 28.632504722780293,
          "standard_error": 0.01809042287402878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024783665763180264,
            "upper_bound": 0.08293717529527997
          },
          "point_estimate": 0.053099843317987706,
          "standard_error": 0.021333129201384547
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.593777301818953,
            "upper_bound": 28.629072398936813
          },
          "point_estimate": 28.608010184632224,
          "standard_error": 0.009027682246383315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022621128937326672,
            "upper_bound": 0.05857741532939911
          },
          "point_estimate": 0.046871780545522365,
          "standard_error": 0.009293277461386008
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-haystack/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174857.78115384615,
            "upper_bound": 175385.9847538919
          },
          "point_estimate": 175143.7942958257,
          "standard_error": 136.08257050877452
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174863.70192307694,
            "upper_bound": 175531.0989010989
          },
          "point_estimate": 175225.59748931625,
          "standard_error": 170.34329413463422
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.8595442741146,
            "upper_bound": 724.9293448342455
          },
          "point_estimate": 466.7563857358429,
          "standard_error": 155.82923037695866
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 175181.3387706044,
            "upper_bound": 175480.1016778812
          },
          "point_estimate": 175364.01870629372,
          "standard_error": 76.64101807376493
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.3365237337047,
            "upper_bound": 620.0110160265497
          },
          "point_estimate": 454.8447433537752,
          "standard_error": 117.72131807667206
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26800162.625732142,
            "upper_bound": 26840766.23670635
          },
          "point_estimate": 26819009.64670635,
          "standard_error": 10434.137371379888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26792092.2,
            "upper_bound": 26837864.166666668
          },
          "point_estimate": 26814339.496031743,
          "standard_error": 11216.058759088484
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9025.636214761083,
            "upper_bound": 53736.821925981545
          },
          "point_estimate": 33930.75828761009,
          "standard_error": 11448.797009856498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26810879.90898551,
            "upper_bound": 26836817.95081967
          },
          "point_estimate": 26822819.293506492,
          "standard_error": 6577.059832736939
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15779.003017518302,
            "upper_bound": 47391.76207355382
          },
          "point_estimate": 34684.756227840524,
          "standard_error": 8820.580608718212
        }
      }
    },
    "memmem/sliceslice/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/sliceslice_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87048307.11416668,
            "upper_bound": 87145277.165
          },
          "point_estimate": 87093943.63333333,
          "standard_error": 24907.781805683564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87033133.0,
            "upper_bound": 87143517.5
          },
          "point_estimate": 87076755.83333333,
          "standard_error": 26465.71694108832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7836.7763608620435,
            "upper_bound": 133827.87502406555
          },
          "point_estimate": 75886.87965273857,
          "standard_error": 32325.782324634773
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38886.39128767066,
            "upper_bound": 109067.13001463884
          },
          "point_estimate": 82961.23676745701,
          "standard_error": 18572.14086559302
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.99051903431862,
            "upper_bound": 6.000891649348533
          },
          "point_estimate": 5.995322478150221,
          "standard_error": 0.002662289882969417
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.989588043145323,
            "upper_bound": 5.999631957032894
          },
          "point_estimate": 5.994102621172665,
          "standard_error": 0.002383950289837007
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00107214505760117,
            "upper_bound": 0.01362043388512092
          },
          "point_estimate": 0.007104015641967496,
          "standard_error": 0.003342996504821231
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.989383489175927,
            "upper_bound": 5.994696388352202
          },
          "point_estimate": 5.992353140020794,
          "standard_error": 0.001340344407976777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003829038863030777,
            "upper_bound": 0.0121670612345206
          },
          "point_estimate": 0.008892618900397043,
          "standard_error": 0.0023192678864013853
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.357842903497288,
            "upper_bound": 6.366057342325945
          },
          "point_estimate": 6.3623041540991805,
          "standard_error": 0.0021084688559348513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.357908919740257,
            "upper_bound": 6.367835051009156
          },
          "point_estimate": 6.363942532789181,
          "standard_error": 0.0026822615470530626
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012551051528099052,
            "upper_bound": 0.011454980142738672
          },
          "point_estimate": 0.006589752200320699,
          "standard_error": 0.002447058998309071
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.361287986196241,
            "upper_bound": 6.367789147342854
          },
          "point_estimate": 6.364620186212665,
          "standard_error": 0.0017095828747810688
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032975617878516494,
            "upper_bound": 0.009602448009860796
          },
          "point_estimate": 0.0070463823097182245,
          "standard_error": 0.0018110993660738171
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.369039474787524,
            "upper_bound": 6.388931154205391
          },
          "point_estimate": 6.378398928303293,
          "standard_error": 0.005077574045718269
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.36398668224364,
            "upper_bound": 6.388145557542929
          },
          "point_estimate": 6.380303945166657,
          "standard_error": 0.0065920439139673555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0012556144445304792,
            "upper_bound": 0.029602749647491367
          },
          "point_estimate": 0.016422342641282114,
          "standard_error": 0.006727420128225742
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.3674169063002815,
            "upper_bound": 6.380213696639146
          },
          "point_estimate": 6.374770066971836,
          "standard_error": 0.003247237593662417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008829816572958615,
            "upper_bound": 0.02271347178128861
          },
          "point_estimate": 0.016935235064374023,
          "standard_error": 0.003914263501900678
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/never-two-space",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.355799626699133,
            "upper_bound": 6.363697990503712
          },
          "point_estimate": 6.359519155961853,
          "standard_error": 0.002029736218525401
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.3558465081866276,
            "upper_bound": 6.363805017825376
          },
          "point_estimate": 6.357397444997403,
          "standard_error": 0.002215972679243417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001379275394582631,
            "upper_bound": 0.010510406905571952
          },
          "point_estimate": 0.002351634300485076,
          "standard_error": 0.003069204214143624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.3558994606530295,
            "upper_bound": 6.36194508721161
          },
          "point_estimate": 6.358305376480388,
          "standard_error": 0.001561202732020355
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002467207072970477,
            "upper_bound": 0.008656391844465148
          },
          "point_estimate": 0.006780646232829287,
          "standard_error": 0.0014810470693447362
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.259497030118675,
            "upper_bound": 5.26474245766718
          },
          "point_estimate": 5.262106615233741,
          "standard_error": 0.0013479810115256763
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.258669079806794,
            "upper_bound": 5.26698649367623
          },
          "point_estimate": 5.260912273231975,
          "standard_error": 0.00223928501375832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00018467468592944715,
            "upper_bound": 0.00729759560553518
          },
          "point_estimate": 0.00512715450158755,
          "standard_error": 0.0019186985814194917
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.258522032772499,
            "upper_bound": 5.263028401174925
          },
          "point_estimate": 5.260558502618484,
          "standard_error": 0.0011222452200555252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002791167712084359,
            "upper_bound": 0.005458325493898342
          },
          "point_estimate": 0.00449293642521822,
          "standard_error": 0.000686310397209869
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.666994242169624,
            "upper_bound": 14.68699303888098
          },
          "point_estimate": 14.677239080533749,
          "standard_error": 0.005102365117589416
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.66348011519562,
            "upper_bound": 14.68951342119367
          },
          "point_estimate": 14.680251776668506,
          "standard_error": 0.006774619557741998
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037582124034964337,
            "upper_bound": 0.02884758261967001
          },
          "point_estimate": 0.015145927991323173,
          "standard_error": 0.006444598938878044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.665130647193184,
            "upper_bound": 14.683722791996372
          },
          "point_estimate": 14.67222207174166,
          "standard_error": 0.004744400654661479
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009102864465778404,
            "upper_bound": 0.02192491654950278
          },
          "point_estimate": 0.017009999195253596,
          "standard_error": 0.0033161846643523607
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.092056620458061,
            "upper_bound": 7.100318668881188
          },
          "point_estimate": 7.095898491466722,
          "standard_error": 0.002122218734489151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.0897020184612165,
            "upper_bound": 7.102588879785058
          },
          "point_estimate": 7.0934483675494695,
          "standard_error": 0.0034211274472248352
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002905510728654572,
            "upper_bound": 0.011857549817727586
          },
          "point_estimate": 0.005844888132500158,
          "standard_error": 0.003156287402239989
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.090615421447799,
            "upper_bound": 7.096050737877165
          },
          "point_estimate": 7.093172005566635,
          "standard_error": 0.001382805001771133
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0036434728435853674,
            "upper_bound": 0.00897745110812127
          },
          "point_estimate": 0.007095012707719911,
          "standard_error": 0.0014040668463649825
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.847915949709699,
            "upper_bound": 6.858209538882372
          },
          "point_estimate": 6.853188471232744,
          "standard_error": 0.0026300667312153937
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.844883922060488,
            "upper_bound": 6.862029994877903
          },
          "point_estimate": 6.853136728523033,
          "standard_error": 0.004579930050250008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000523118806968125,
            "upper_bound": 0.014516267598744612
          },
          "point_estimate": 0.01271038355389572,
          "standard_error": 0.00440893235766827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.84886079136074,
            "upper_bound": 6.857354001781279
          },
          "point_estimate": 6.853146725777532,
          "standard_error": 0.002105829687618642
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005141544785402244,
            "upper_bound": 0.010450553000155632
          },
          "point_estimate": 0.0087804646312651,
          "standard_error": 0.0013064191364089808
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.293942653341078,
            "upper_bound": 9.313412185563804
          },
          "point_estimate": 9.30272308687293,
          "standard_error": 0.005017268193250755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.291986184602637,
            "upper_bound": 9.312914557000884
          },
          "point_estimate": 9.298292104460687,
          "standard_error": 0.004485860668459393
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014173479944803646,
            "upper_bound": 0.023129484684247345
          },
          "point_estimate": 0.009720081291529403,
          "standard_error": 0.006713326548476271
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.292813307322447,
            "upper_bound": 9.304321947940824
          },
          "point_estimate": 9.29784090018703,
          "standard_error": 0.002881865242716817
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006288398510565015,
            "upper_bound": 0.023308885890083113
          },
          "point_estimate": 0.01672154876981932,
          "standard_error": 0.004859222908020162
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.354478144518549,
            "upper_bound": 6.367601605628792
          },
          "point_estimate": 6.3598673548645355,
          "standard_error": 0.0034852528968754583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.352840851368677,
            "upper_bound": 6.362179944159911
          },
          "point_estimate": 6.355893285721737,
          "standard_error": 0.0026437712155672997
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001053030971561442,
            "upper_bound": 0.011233494149996612
          },
          "point_estimate": 0.007096102680736492,
          "standard_error": 0.002568368954693163
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.35475405641435,
            "upper_bound": 6.362867144472823
          },
          "point_estimate": 6.358580688240989,
          "standard_error": 0.0021059355474374445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00314888974407845,
            "upper_bound": 0.017350924020377842
          },
          "point_estimate": 0.011618881970200967,
          "standard_error": 0.0045836904959577745
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.8913423506106835,
            "upper_bound": 4.897067692672383
          },
          "point_estimate": 4.894226564661984,
          "standard_error": 0.001466060908553237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.889947800961814,
            "upper_bound": 4.898035367717393
          },
          "point_estimate": 4.894819314058071,
          "standard_error": 0.001996328186289536
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0011569997541960118,
            "upper_bound": 0.008910002279559659
          },
          "point_estimate": 0.004915986806860493,
          "standard_error": 0.0019134002777609876
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.892591371697696,
            "upper_bound": 4.897803931630971
          },
          "point_estimate": 4.895707045842809,
          "standard_error": 0.0013031794296450004
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002869311117471161,
            "upper_bound": 0.006045206617365471
          },
          "point_estimate": 0.004891755979929857,
          "standard_error": 0.0008143176025865605
        }
      }
    },
    "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/sliceslice/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/sliceslice_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.563465680596751,
            "upper_bound": 15.79359843204888
          },
          "point_estimate": 15.668218388689292,
          "standard_error": 0.05921932933059532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.518715478694414,
            "upper_bound": 15.78774350404962
          },
          "point_estimate": 15.61789818387235,
          "standard_error": 0.0756942341465558
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019377106485928337,
            "upper_bound": 0.290828870231933
          },
          "point_estimate": 0.14197007591267097,
          "standard_error": 0.07903420695475397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.537818301281998,
            "upper_bound": 15.668704026989996
          },
          "point_estimate": 15.601128655894906,
          "standard_error": 0.03368979088422687
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08356145613264283,
            "upper_bound": 0.2668990697747817
          },
          "point_estimate": 0.19716826901218565,
          "standard_error": 0.050868151150297955
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217850.2257989417,
            "upper_bound": 1221810.110269444
          },
          "point_estimate": 1219594.819334656,
          "standard_error": 1025.494990754773
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217072.6904761903,
            "upper_bound": 1221268.8645833335
          },
          "point_estimate": 1218870.296111111,
          "standard_error": 919.2488686964202
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 392.42479803311977,
            "upper_bound": 4298.363109938524
          },
          "point_estimate": 2141.7789440867864,
          "standard_error": 1105.2208263377229
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217830.1826390624,
            "upper_bound": 1219522.3724632077
          },
          "point_estimate": 1218718.125108225,
          "standard_error": 436.5995196620556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1048.2530106026986,
            "upper_bound": 4772.08995838909
          },
          "point_estimate": 3424.2757229891495,
          "standard_error": 1048.287638648695
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366818.7510699222,
            "upper_bound": 1370364.6814286818
          },
          "point_estimate": 1368454.2865064668,
          "standard_error": 912.83521577788
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366354.2648148148,
            "upper_bound": 1370493.7851851853
          },
          "point_estimate": 1367678.9837962962,
          "standard_error": 917.9952382647105
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 509.7444113206577,
            "upper_bound": 4910.4122961562225
          },
          "point_estimate": 1830.6350878700864,
          "standard_error": 1080.835573928076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366060.6961222969,
            "upper_bound": 1368603.824468486
          },
          "point_estimate": 1367452.7194805194,
          "standard_error": 674.2673878510094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1113.9672163663586,
            "upper_bound": 3967.3183738954895
          },
          "point_estimate": 3029.276617758402,
          "standard_error": 744.9215843908843
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1268120.513297756,
            "upper_bound": 1272007.7699940477
          },
          "point_estimate": 1269814.260489874,
          "standard_error": 1012.2384258861654
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1267381.1494252873,
            "upper_bound": 1270889.5555555555
          },
          "point_estimate": 1269085.7471264368,
          "standard_error": 1190.5074504679667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.22949534750003,
            "upper_bound": 4680.987050803964
          },
          "point_estimate": 2600.781418194745,
          "standard_error": 1103.451131209031
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1268237.3786288674,
            "upper_bound": 1270665.375932442
          },
          "point_estimate": 1269865.4161218093,
          "standard_error": 622.6507213032063
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1347.3759396623263,
            "upper_bound": 4853.839573910646
          },
          "point_estimate": 3359.0240963620695,
          "standard_error": 1100.9373714829442
        }
      }
    },
    "memmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338541.559178158,
            "upper_bound": 339189.96443819813
          },
          "point_estimate": 338863.1057653586,
          "standard_error": 164.71883224502253
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338512.17700617283,
            "upper_bound": 339166.7708333333
          },
          "point_estimate": 338913.26785714284,
          "standard_error": 166.83002577158268
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.48164001799496,
            "upper_bound": 944.36097795646
          },
          "point_estimate": 440.05522609945336,
          "standard_error": 198.1846580512892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338515.7754323973,
            "upper_bound": 338977.7694287727
          },
          "point_estimate": 338722.5373256373,
          "standard_error": 120.81804775106492
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 243.3967502876298,
            "upper_bound": 742.1053950515463
          },
          "point_estimate": 548.2277436725008,
          "standard_error": 128.7860262928953
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266711.4511648201,
            "upper_bound": 267547.3728128259
          },
          "point_estimate": 267098.52142364735,
          "standard_error": 214.61176128705804
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266598.2032643958,
            "upper_bound": 267511.400243309
          },
          "point_estimate": 266998.50580031285,
          "standard_error": 183.6890196310861
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.20650471136247,
            "upper_bound": 1129.6370191785154
          },
          "point_estimate": 349.49859452506365,
          "standard_error": 291.8745756886874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266806.4844808312,
            "upper_bound": 267101.16799143807
          },
          "point_estimate": 266975.7040856953,
          "standard_error": 76.0668388896708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.98811614667335,
            "upper_bound": 970.2725961935035
          },
          "point_estimate": 717.1505967766013,
          "standard_error": 187.42745650138215
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452616.2229028635,
            "upper_bound": 453787.70194787375
          },
          "point_estimate": 453219.0292215363,
          "standard_error": 299.3359350635874
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452614.8691358025,
            "upper_bound": 453850.3526234568
          },
          "point_estimate": 453294.52345679014,
          "standard_error": 256.69549517128945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.32626032558558,
            "upper_bound": 1762.5952197621616
          },
          "point_estimate": 631.0059377685848,
          "standard_error": 475.60940630776815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 453179.77671934,
            "upper_bound": 453633.97430912766
          },
          "point_estimate": 453402.9641814975,
          "standard_error": 114.59500149642663
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473.5946795151564,
            "upper_bound": 1318.1586309051756
          },
          "point_estimate": 998.424525315166,
          "standard_error": 217.7558548037199
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237981.74360151467,
            "upper_bound": 238417.28063855172
          },
          "point_estimate": 238168.02186948856,
          "standard_error": 112.5961622919826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237861.9411764706,
            "upper_bound": 238212.90377632537
          },
          "point_estimate": 238155.25571895423,
          "standard_error": 81.14502255250603
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.143725292356958,
            "upper_bound": 454.2949569673221
          },
          "point_estimate": 93.65157126763371,
          "standard_error": 130.12849209222134
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237925.31945304057,
            "upper_bound": 238201.5410926275
          },
          "point_estimate": 238077.40205415495,
          "standard_error": 70.52424896185973
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.85846526191648,
            "upper_bound": 553.716476743034
          },
          "point_estimate": 376.1442913271167,
          "standard_error": 134.92653535661552
        }
      }
    },
    "memmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632214.8829446667,
            "upper_bound": 632955.8102515051
          },
          "point_estimate": 632588.1124336343,
          "standard_error": 189.4221359227253
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632191.3534482758,
            "upper_bound": 633156.9932950193
          },
          "point_estimate": 632470.977586207,
          "standard_error": 261.7826143328201
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.9614840240592,
            "upper_bound": 1153.771512504881
          },
          "point_estimate": 592.1154946603408,
          "standard_error": 258.71387562807803
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632062.0351262643,
            "upper_bound": 633039.2832940207
          },
          "point_estimate": 632531.757411554,
          "standard_error": 251.6229433385495
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356.8569287923409,
            "upper_bound": 803.7612477669634
          },
          "point_estimate": 630.585930046083,
          "standard_error": 115.50470972438184
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117780.03522287717,
            "upper_bound": 118029.73673141086
          },
          "point_estimate": 117898.45625417373,
          "standard_error": 64.00226226434847
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117771.1423370319,
            "upper_bound": 118018.74559510968
          },
          "point_estimate": 117863.98071736784,
          "standard_error": 58.58373445751939
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.19662687401484,
            "upper_bound": 360.4099360292839
          },
          "point_estimate": 142.0724215553849,
          "standard_error": 83.06800432724447
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117809.79319048738,
            "upper_bound": 117958.8729809421
          },
          "point_estimate": 117869.1592064893,
          "standard_error": 38.17617471152281
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.06000288648599,
            "upper_bound": 285.9888006812379
          },
          "point_estimate": 214.0885356621355,
          "standard_error": 51.39564513040611
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125552.98739792008,
            "upper_bound": 125831.60673234813
          },
          "point_estimate": 125685.8414668856,
          "standard_error": 70.76085916218703
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125572.75632183909,
            "upper_bound": 125830.1827586207
          },
          "point_estimate": 125631.95392720308,
          "standard_error": 67.1011862494146
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.60790777309143,
            "upper_bound": 402.9489694324288
          },
          "point_estimate": 139.13663949534654,
          "standard_error": 89.28524924525284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125580.06606046292,
            "upper_bound": 125744.26440080996
          },
          "point_estimate": 125641.0456515898,
          "standard_error": 41.9652863038431
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.68594868641182,
            "upper_bound": 325.70483519969514
          },
          "point_estimate": 235.9174137121899,
          "standard_error": 62.680115584424485
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205847.9570291902,
            "upper_bound": 206132.1412077449
          },
          "point_estimate": 205978.37623822977,
          "standard_error": 72.7777463261771
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205752.14053672316,
            "upper_bound": 206099.56261770247
          },
          "point_estimate": 205967.6702448211,
          "standard_error": 89.71708970237509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.560982917492254,
            "upper_bound": 397.5764590009309
          },
          "point_estimate": 216.5161359865761,
          "standard_error": 88.4847282731018
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205766.072478552,
            "upper_bound": 206078.23115539044
          },
          "point_estimate": 205872.7907109839,
          "standard_error": 79.6372470864604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.10693478828884,
            "upper_bound": 332.1795367683946
          },
          "point_estimate": 243.15348356937437,
          "standard_error": 62.84016342554592
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291167.59137369844,
            "upper_bound": 291766.6090517143
          },
          "point_estimate": 291474.87990920636,
          "standard_error": 152.6791844395372
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291137.08,
            "upper_bound": 291761.284
          },
          "point_estimate": 291570.0428,
          "standard_error": 185.800842394661
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.47121559381073,
            "upper_bound": 890.3605881929087
          },
          "point_estimate": 427.44242094470206,
          "standard_error": 202.1492470465676
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291355.22981847357,
            "upper_bound": 291709.39734780573
          },
          "point_estimate": 291527.6272207792,
          "standard_error": 89.71267887650342
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.14776354843443,
            "upper_bound": 678.9950863072897
          },
          "point_estimate": 506.6497246975541,
          "standard_error": 115.80336618679328
        }
      }
    },
    "memmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309247.47712099814,
            "upper_bound": 309809.4824618896
          },
          "point_estimate": 309535.8594212403,
          "standard_error": 143.62199320825331
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309235.9096045198,
            "upper_bound": 309929.4975786925
          },
          "point_estimate": 309500.06233521656,
          "standard_error": 155.25879904602098
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.06148158133885,
            "upper_bound": 882.1929316755893
          },
          "point_estimate": 419.81479548464125,
          "standard_error": 213.72527462410753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309416.44737510313,
            "upper_bound": 309935.22072476405
          },
          "point_estimate": 309697.71730134275,
          "standard_error": 138.63824423487185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247.7018724151463,
            "upper_bound": 634.3343462345249
          },
          "point_estimate": 478.8028010054161,
          "standard_error": 102.79871424671798
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328993.1065006256,
            "upper_bound": 329661.957715966
          },
          "point_estimate": 329287.1051476477,
          "standard_error": 173.38198634267656
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328924.47347347345,
            "upper_bound": 329514.34384384385
          },
          "point_estimate": 329114.7483108108,
          "standard_error": 173.26965973645397
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.29462803742337,
            "upper_bound": 745.6272049306679
          },
          "point_estimate": 438.9564462610178,
          "standard_error": 175.5687661561432
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328965.08892532287,
            "upper_bound": 329278.99828814255
          },
          "point_estimate": 329092.47743067745,
          "standard_error": 79.80720065839132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 217.9972978562782,
            "upper_bound": 823.2023771573786
          },
          "point_estimate": 576.0364727596527,
          "standard_error": 182.57004882298656
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405528.1773703704,
            "upper_bound": 406524.5263015873
          },
          "point_estimate": 405990.7075855379,
          "standard_error": 255.40097980463315
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405305.64222222223,
            "upper_bound": 406736.4365079365
          },
          "point_estimate": 405756.3425925926,
          "standard_error": 369.0628345970111
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.03520997363743,
            "upper_bound": 1346.7561954421494
          },
          "point_estimate": 687.6132114220275,
          "standard_error": 388.838533761454
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405349.9044454733,
            "upper_bound": 406141.9849981552
          },
          "point_estimate": 405661.4688888889,
          "standard_error": 203.30383805708155
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 359.7143684566873,
            "upper_bound": 1053.1168704006143
          },
          "point_estimate": 853.6183773211794,
          "standard_error": 180.86534440127497
        }
      }
    },
    "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285808.864671875,
            "upper_bound": 286497.90910308156
          },
          "point_estimate": 286107.61427951383,
          "standard_error": 177.92213975495423
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285720.7923177084,
            "upper_bound": 286325.8483072916
          },
          "point_estimate": 285950.818359375,
          "standard_error": 151.40030668611385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.87713653765957,
            "upper_bound": 754.9290316233472
          },
          "point_estimate": 342.4923054299551,
          "standard_error": 171.83188501008743
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285647.95111062715,
            "upper_bound": 286006.7137942395
          },
          "point_estimate": 285788.95012175327,
          "standard_error": 92.428629003847
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 185.50540548455825,
            "upper_bound": 844.8222712404576
          },
          "point_estimate": 592.8585371661956,
          "standard_error": 190.83059882067064
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123554.52201614206,
            "upper_bound": 123760.14826755448
          },
          "point_estimate": 123646.26623365616,
          "standard_error": 53.019879385475235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123504.35799031478,
            "upper_bound": 123742.38305084746
          },
          "point_estimate": 123616.78055084748,
          "standard_error": 65.35060271538238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.500243926601208,
            "upper_bound": 278.10447567282614
          },
          "point_estimate": 170.0005041429233,
          "standard_error": 64.6267026419198
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123578.06194038574,
            "upper_bound": 123734.30524745764
          },
          "point_estimate": 123648.20977327756,
          "standard_error": 40.136010608497834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.0708650633865,
            "upper_bound": 246.1325136089893
          },
          "point_estimate": 176.2806871925109,
          "standard_error": 50.23412560954612
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208189.1191040483,
            "upper_bound": 208675.38622497296
          },
          "point_estimate": 208416.8553952471,
          "standard_error": 124.73720365563906
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208098.4737215909,
            "upper_bound": 208638.5799512987
          },
          "point_estimate": 208360.76512784092,
          "standard_error": 114.65001318329676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.67638689860771,
            "upper_bound": 701.6730800143839
          },
          "point_estimate": 268.1416366221105,
          "standard_error": 177.3570387357235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208244.9198085466,
            "upper_bound": 208463.2386568756
          },
          "point_estimate": 208331.7935507674,
          "standard_error": 55.532397824245145
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 184.14219271744184,
            "upper_bound": 554.1009056975878
          },
          "point_estimate": 416.9628931510682,
          "standard_error": 99.53172897484038
        }
      }
    },
    "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154445.05230080208,
            "upper_bound": 154601.34877275414
          },
          "point_estimate": 154524.0262657886,
          "standard_error": 39.83451349532603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154415.64954407295,
            "upper_bound": 154632.6026004728
          },
          "point_estimate": 154545.5015957447,
          "standard_error": 51.93804133585847
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.810604453518506,
            "upper_bound": 235.4978622020924
          },
          "point_estimate": 136.35803177064648,
          "standard_error": 56.109146232223225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154417.3829444063,
            "upper_bound": 154565.79414893617
          },
          "point_estimate": 154489.3992815695,
          "standard_error": 37.599252894233594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.97098357277737,
            "upper_bound": 168.42036738996524
          },
          "point_estimate": 132.862558782373,
          "standard_error": 24.217201683520077
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44355.22436231636,
            "upper_bound": 44422.93863900154
          },
          "point_estimate": 44388.23459207899,
          "standard_error": 17.364679490546177
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44348.13823478109,
            "upper_bound": 44443.15408357075
          },
          "point_estimate": 44368.002258852255,
          "standard_error": 26.53651148795897
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.540604313870401,
            "upper_bound": 96.86896092980294
          },
          "point_estimate": 63.52050269808847,
          "standard_error": 25.17833887317178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44350.95855059973,
            "upper_bound": 44398.61065659571
          },
          "point_estimate": 44373.37940789369,
          "standard_error": 12.268219991717446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.38165186375561,
            "upper_bound": 70.34539081193232
          },
          "point_estimate": 57.84440984663532,
          "standard_error": 9.134719168682809
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1226033.6926296295,
            "upper_bound": 1228944.911138889
          },
          "point_estimate": 1227589.797429894,
          "standard_error": 747.807509223239
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225911.9166666667,
            "upper_bound": 1229364.8079365075
          },
          "point_estimate": 1228278.4412962962,
          "standard_error": 825.8064873599654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251.0222177654861,
            "upper_bound": 3985.251266336422
          },
          "point_estimate": 2008.8224770029217,
          "standard_error": 974.60216178007
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1227502.1114166668,
            "upper_bound": 1228805.407364209
          },
          "point_estimate": 1228225.7706493507,
          "standard_error": 326.5446142092927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1131.2979800685812,
            "upper_bound": 3333.664192982852
          },
          "point_estimate": 2486.946371519961,
          "standard_error": 596.453312377583
        }
      }
    },
    "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104463.75226719196,
            "upper_bound": 104617.69100286534
          },
          "point_estimate": 104536.43893505256,
          "standard_error": 39.42524720436535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104432.88849092645,
            "upper_bound": 104617.5152817574
          },
          "point_estimate": 104514.31053008596,
          "standard_error": 53.88817313310436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.48726711198782,
            "upper_bound": 228.46854974043447
          },
          "point_estimate": 133.35965522551692,
          "standard_error": 47.31874384740454
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104426.95374643656,
            "upper_bound": 104579.1999646446
          },
          "point_estimate": 104499.79244594948,
          "standard_error": 40.193352769708
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.27816801872302,
            "upper_bound": 173.80883207112802
          },
          "point_estimate": 131.25430735351796,
          "standard_error": 29.341163120234615
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29282.86305967477,
            "upper_bound": 29363.042020358866
          },
          "point_estimate": 29319.847058667532,
          "standard_error": 20.658860070514585
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29275.007380568975,
            "upper_bound": 29354.230193236715
          },
          "point_estimate": 29313.349414826316,
          "standard_error": 17.63124493391674
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2551711556148377,
            "upper_bound": 108.54840188930346
          },
          "point_estimate": 42.50086884086922,
          "standard_error": 25.99427263172365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29296.602371541503,
            "upper_bound": 29332.57785911246
          },
          "point_estimate": 29315.933220402785,
          "standard_error": 8.972161050921537
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.75054954911853,
            "upper_bound": 93.41519378975627
          },
          "point_estimate": 68.62298753295822,
          "standard_error": 18.18454130561905
        }
      }
    },
    "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28922.45558942078,
            "upper_bound": 28968.293696648943
          },
          "point_estimate": 28945.519239365334,
          "standard_error": 11.726054138375236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28916.505203594177,
            "upper_bound": 28978.056210191084
          },
          "point_estimate": 28945.629627786624,
          "standard_error": 16.969126460341645
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.340996777481193,
            "upper_bound": 65.29459995544053
          },
          "point_estimate": 40.10795906103233,
          "standard_error": 13.787612270450149
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28922.315765828724,
            "upper_bound": 28961.777610327572
          },
          "point_estimate": 28943.02109562412,
          "standard_error": 10.191387626470188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.786199843060725,
            "upper_bound": 49.21127590468264
          },
          "point_estimate": 39.12365180341194,
          "standard_error": 6.776552875526714
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550754.7589502165,
            "upper_bound": 551705.4880904581
          },
          "point_estimate": 551217.2523340548,
          "standard_error": 242.0939318020297
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550732.3825757576,
            "upper_bound": 551912.1464646464
          },
          "point_estimate": 551083.2515151515,
          "standard_error": 304.24298398326675
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.64336876374003,
            "upper_bound": 1424.9949743981883
          },
          "point_estimate": 552.774964958987,
          "standard_error": 333.4219984442498
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550840.9755659361,
            "upper_bound": 551641.9124885215
          },
          "point_estimate": 551202.4005116096,
          "standard_error": 207.93524856333505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.5544289365561,
            "upper_bound": 1057.844743909504
          },
          "point_estimate": 807.1009429882494,
          "standard_error": 171.0351564619168
        }
      }
    },
    "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1129.8226012821938,
            "upper_bound": 1131.8223498755483
          },
          "point_estimate": 1130.829189730097,
          "standard_error": 0.5094310859367616
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1130.1017893644616,
            "upper_bound": 1131.7584474708171
          },
          "point_estimate": 1130.742927552344,
          "standard_error": 0.33768452478610617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14981374675667972,
            "upper_bound": 2.793587558186058
          },
          "point_estimate": 0.6251174789149293,
          "standard_error": 0.7117086534049133
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1129.0763619561003,
            "upper_bound": 1131.450325912611
          },
          "point_estimate": 1130.129771974329,
          "standard_error": 0.5965303107804183
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5597857099333546,
            "upper_bound": 2.3511763373734342
          },
          "point_estimate": 1.696036927823873,
          "standard_error": 0.44091238569495467
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.481998464742595,
            "upper_bound": 34.51828831424556
          },
          "point_estimate": 34.49892832110463,
          "standard_error": 0.009312567004961306
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.47339980298259,
            "upper_bound": 34.516760214082964
          },
          "point_estimate": 34.49536170393485,
          "standard_error": 0.011859997021331876
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007355072726005814,
            "upper_bound": 0.05356290252430236
          },
          "point_estimate": 0.028847843542309532,
          "standard_error": 0.01094099768129272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.478258231690134,
            "upper_bound": 34.52106180614474
          },
          "point_estimate": 34.49788400652544,
          "standard_error": 0.01118235869027322
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015277356819520816,
            "upper_bound": 0.04151915902389773
          },
          "point_estimate": 0.030895923560327967,
          "standard_error": 0.007391066554518593
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.89325762786493,
            "upper_bound": 37.928232261623606
          },
          "point_estimate": 37.91146188396565,
          "standard_error": 0.009013278267258604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.886424847466515,
            "upper_bound": 37.93633664914915
          },
          "point_estimate": 37.91948018620472,
          "standard_error": 0.014348526567837666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005150390048702637,
            "upper_bound": 0.050310868244171275
          },
          "point_estimate": 0.02661024552356628,
          "standard_error": 0.012709617600431732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.89370408287163,
            "upper_bound": 37.934085587008475
          },
          "point_estimate": 37.91667427979928,
          "standard_error": 0.010367157951770666
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01599743519363953,
            "upper_bound": 0.03676420789229196
          },
          "point_estimate": 0.03019005170725539,
          "standard_error": 0.005229230116981894
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.58579947028803,
            "upper_bound": 26.6248715748046
          },
          "point_estimate": 26.602225777789688,
          "standard_error": 0.010206604162388706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.58203167530105,
            "upper_bound": 26.61276497852285
          },
          "point_estimate": 26.58488511878863,
          "standard_error": 0.009196471130918713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006248111330808884,
            "upper_bound": 0.03947999255372075
          },
          "point_estimate": 0.005003785373642153,
          "standard_error": 0.011263074188569065
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.58364797592435,
            "upper_bound": 26.59992910113331
          },
          "point_estimate": 26.58885604142016,
          "standard_error": 0.004248515440499751
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007776800390721092,
            "upper_bound": 0.049523013766056674
          },
          "point_estimate": 0.034043058133020505,
          "standard_error": 0.012279745223655807
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.3859062517818,
            "upper_bound": 33.41229232661588
          },
          "point_estimate": 33.39826169853824,
          "standard_error": 0.006767981237282505
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.37999861842408,
            "upper_bound": 33.41493850890678
          },
          "point_estimate": 33.391400697312065,
          "standard_error": 0.009569137990352142
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004341546884992446,
            "upper_bound": 0.037719731611634374
          },
          "point_estimate": 0.02111781383126734,
          "standard_error": 0.008334926337784127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.38499275215421,
            "upper_bound": 33.42627765834679
          },
          "point_estimate": 33.40714254564035,
          "standard_error": 0.011010510780097866
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01124117469953848,
            "upper_bound": 0.028887429922625107
          },
          "point_estimate": 0.022461460888692365,
          "standard_error": 0.004692800277498144
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.58161701764488,
            "upper_bound": 40.61643188844697
          },
          "point_estimate": 40.59845791899244,
          "standard_error": 0.008933068918669199
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.57999925403029,
            "upper_bound": 40.61943855140241
          },
          "point_estimate": 40.59707507987955,
          "standard_error": 0.008556775762392006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002286097150236354,
            "upper_bound": 0.05161368608004278
          },
          "point_estimate": 0.020101312968811742,
          "standard_error": 0.013110652702624409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.56996662331727,
            "upper_bound": 40.62287432350202
          },
          "point_estimate": 40.59621303413127,
          "standard_error": 0.013668812041121806
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013668191439500902,
            "upper_bound": 0.03879594475849741
          },
          "point_estimate": 0.029816049764431512,
          "standard_error": 0.006322271494058973
        }
      }
    },
    "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.3291234027938,
            "upper_bound": 73.37906256588765
          },
          "point_estimate": 73.351336178459,
          "standard_error": 0.01288127192850158
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.32463313089954,
            "upper_bound": 73.3728819639719
          },
          "point_estimate": 73.33953851569233,
          "standard_error": 0.01332515304525697
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003255722574190265,
            "upper_bound": 0.05883545083056322
          },
          "point_estimate": 0.024092580326628622,
          "standard_error": 0.015401695555228353
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.33171111966625,
            "upper_bound": 73.41528193961886
          },
          "point_estimate": 73.37008913185545,
          "standard_error": 0.023638189926297026
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015096210837512856,
            "upper_bound": 0.05958780153158368
          },
          "point_estimate": 0.04279394404306598,
          "standard_error": 0.012576703455015027
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.03390470052528,
            "upper_bound": 66.13356872845874
          },
          "point_estimate": 66.08463612055034,
          "standard_error": 0.025527975402915112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.03633138321467,
            "upper_bound": 66.14313610349566
          },
          "point_estimate": 66.08169242786742,
          "standard_error": 0.025867873248341985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016705794719174216,
            "upper_bound": 0.14811116417837625
          },
          "point_estimate": 0.06091172276471067,
          "standard_error": 0.033023272021912416
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.05152310688207,
            "upper_bound": 66.09944276972162
          },
          "point_estimate": 66.07468450916707,
          "standard_error": 0.012138449094093251
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.040181467730688585,
            "upper_bound": 0.11395395369658816
          },
          "point_estimate": 0.08504406311085215,
          "standard_error": 0.018771930384794706
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.71673582280336,
            "upper_bound": 56.8409681720682
          },
          "point_estimate": 56.77005406734503,
          "standard_error": 0.0324735403705188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.69956349522076,
            "upper_bound": 56.795519351763886
          },
          "point_estimate": 56.747035344371966,
          "standard_error": 0.022440761263428692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009144516655882362,
            "upper_bound": 0.12467414683628272
          },
          "point_estimate": 0.06545414248203844,
          "standard_error": 0.029759110752462566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.71369291753332,
            "upper_bound": 56.765488562589645
          },
          "point_estimate": 56.73704011686105,
          "standard_error": 0.013328216456485424
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03354468001802881,
            "upper_bound": 0.15728466213174977
          },
          "point_estimate": 0.1080323565202419,
          "standard_error": 0.03776256983538413
        }
      }
    },
    "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.7939875691096,
            "upper_bound": 108.8775478600918
          },
          "point_estimate": 108.83802055529796,
          "standard_error": 0.021391455243475382
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.7653723620859,
            "upper_bound": 108.88276303287262
          },
          "point_estimate": 108.86407001051973,
          "standard_error": 0.028925152347965746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007580836474930191,
            "upper_bound": 0.11081270538290912
          },
          "point_estimate": 0.03680140090692344,
          "standard_error": 0.03021609349186834
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.76277086557604,
            "upper_bound": 108.87532670828824
          },
          "point_estimate": 108.81901954071512,
          "standard_error": 0.03045301126616539
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029040698921230273,
            "upper_bound": 0.08612768594157094
          },
          "point_estimate": 0.07129804462874849,
          "standard_error": 0.013134939224653214
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.79886225600665,
            "upper_bound": 45.853641292381255
          },
          "point_estimate": 45.82511039899711,
          "standard_error": 0.014024798528373251
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.79930414409913,
            "upper_bound": 45.85473038185968
          },
          "point_estimate": 45.8132241708907,
          "standard_error": 0.014277924276261431
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004047415216580844,
            "upper_bound": 0.0786511447693604
          },
          "point_estimate": 0.029954913408780183,
          "standard_error": 0.01972784206644009
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.80059519658087,
            "upper_bound": 45.82948097354857
          },
          "point_estimate": 45.812556653851544,
          "standard_error": 0.007544221944780875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021391177503448145,
            "upper_bound": 0.06231167693058981
          },
          "point_estimate": 0.04675398432711247,
          "standard_error": 0.010744918764131056
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.12974399601037,
            "upper_bound": 50.190075905003496
          },
          "point_estimate": 50.158641049398895,
          "standard_error": 0.015427927707570094
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.118726531280736,
            "upper_bound": 50.21449891840316
          },
          "point_estimate": 50.14042069102583,
          "standard_error": 0.025551314304447095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006646437145247111,
            "upper_bound": 0.07761186959603841
          },
          "point_estimate": 0.03737606296801226,
          "standard_error": 0.020698310665978263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.13265081137116,
            "upper_bound": 50.205011402946205
          },
          "point_estimate": 50.16651207754961,
          "standard_error": 0.01834454192921798
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02417399783035049,
            "upper_bound": 0.06017829172690231
          },
          "point_estimate": 0.05135957156963845,
          "standard_error": 0.00831341230129203
        }
      }
    },
    "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.5222442175356,
            "upper_bound": 91.633228595518
          },
          "point_estimate": 91.57954355535628,
          "standard_error": 0.02843141102541139
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.50028982949723,
            "upper_bound": 91.654078894295
          },
          "point_estimate": 91.59457422276564,
          "standard_error": 0.034677422692466243
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013149139269594252,
            "upper_bound": 0.17177785329187736
          },
          "point_estimate": 0.07467054614662023,
          "standard_error": 0.03885190299492474
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.54812007822264,
            "upper_bound": 91.64431365072562
          },
          "point_estimate": 91.60192608007114,
          "standard_error": 0.02396629084553047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04925064177131402,
            "upper_bound": 0.12048981858019464
          },
          "point_estimate": 0.0947903011493346,
          "standard_error": 0.01831264816031305
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1114264.2877633476,
            "upper_bound": 1115538.2302204787
          },
          "point_estimate": 1114902.1961459836,
          "standard_error": 325.9910331192944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1113935.3636363638,
            "upper_bound": 1115760.196969697
          },
          "point_estimate": 1114966.2306397306,
          "standard_error": 541.6858802534763
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 94.7247403638185,
            "upper_bound": 1751.6496247860473
          },
          "point_estimate": 1199.8949367786083,
          "standard_error": 424.9262387219094
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1114529.4477763665,
            "upper_bound": 1116103.392273248
          },
          "point_estimate": 1115468.6465958285,
          "standard_error": 403.6830405395939
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 670.0029435642716,
            "upper_bound": 1332.7507868762043
          },
          "point_estimate": 1088.0778886757903,
          "standard_error": 169.21884171176902
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355194.8312825176,
            "upper_bound": 1357363.1216358026
          },
          "point_estimate": 1356226.6048809527,
          "standard_error": 556.388090277892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1354712.1705246912,
            "upper_bound": 1357955.2925925923
          },
          "point_estimate": 1355467.5582010583,
          "standard_error": 936.744257389844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 360.7287324846509,
            "upper_bound": 3056.516208328701
          },
          "point_estimate": 1680.1923738746189,
          "standard_error": 756.9724219046633
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1355042.0969718758,
            "upper_bound": 1357765.974519033
          },
          "point_estimate": 1356669.772101972,
          "standard_error": 695.6236354502607
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1025.454952222871,
            "upper_bound": 2244.8322687212963
          },
          "point_estimate": 1863.9780905852103,
          "standard_error": 306.9475293530698
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1453044.9223244842,
            "upper_bound": 1455253.792588889
          },
          "point_estimate": 1454295.1603349207,
          "standard_error": 573.0022456066944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1453827.8616666668,
            "upper_bound": 1455971.275
          },
          "point_estimate": 1454384.2122857142,
          "standard_error": 519.9214666743924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.1166005552754,
            "upper_bound": 2643.6038920663987
          },
          "point_estimate": 1091.1985226273007,
          "standard_error": 618.2284347763576
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1453821.7651430033,
            "upper_bound": 1455221.59740113
          },
          "point_estimate": 1454524.701818182,
          "standard_error": 351.9342652690263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 697.9641182952146,
            "upper_bound": 2772.216597746906
          },
          "point_estimate": 1910.6906462126196,
          "standard_error": 643.5593393335411
        }
      }
    },
    "memmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1486017.105466984,
            "upper_bound": 1489163.9243790477
          },
          "point_estimate": 1487514.1603126985,
          "standard_error": 811.1979257191695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1485442.3997777775,
            "upper_bound": 1490170.92
          },
          "point_estimate": 1486254.6667857142,
          "standard_error": 1218.6342977723596
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.9371870857224,
            "upper_bound": 4366.1753794848155
          },
          "point_estimate": 2306.626250349081,
          "standard_error": 1159.7686997801916
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1485497.3694206355,
            "upper_bound": 1488138.8180945716
          },
          "point_estimate": 1486450.0692987014,
          "standard_error": 673.740095224768
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1323.1233530331017,
            "upper_bound": 3276.2125685313213
          },
          "point_estimate": 2701.2848049510385,
          "standard_error": 487.96848244605394
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1342947.4238244048,
            "upper_bound": 1399082.9852380953
          },
          "point_estimate": 1371489.2510544215,
          "standard_error": 14443.59226926725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1318855.142857143,
            "upper_bound": 1412263.7933673467
          },
          "point_estimate": 1393548.4767857145,
          "standard_error": 31607.1783264625
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1673.5897377877968,
            "upper_bound": 71512.35458790197
          },
          "point_estimate": 42013.28870554358,
          "standard_error": 20555.34753225121
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1381320.6939601114,
            "upper_bound": 1412900.8976860298
          },
          "point_estimate": 1402485.0826530613,
          "standard_error": 8123.308497747266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31037.69895350885,
            "upper_bound": 53138.1859750067
          },
          "point_estimate": 48141.86534580241,
          "standard_error": 5617.507363366317
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397797.88803154114,
            "upper_bound": 398492.0428765528
          },
          "point_estimate": 398159.48828114214,
          "standard_error": 177.72333808833702
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397752.9326086957,
            "upper_bound": 398608.51708074537
          },
          "point_estimate": 398298.21474939614,
          "standard_error": 213.483341812916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.73813298689687,
            "upper_bound": 990.1216248132116
          },
          "point_estimate": 483.855828411669,
          "standard_error": 224.92424807045992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397935.52171459416,
            "upper_bound": 398557.72837609466
          },
          "point_estimate": 398221.8308865048,
          "standard_error": 160.59641197300874
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 297.3078601841584,
            "upper_bound": 769.8988146558938
          },
          "point_estimate": 592.4698697600454,
          "standard_error": 121.73770896227772
        }
      }
    },
    "memmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463644.2610367692,
            "upper_bound": 464658.9053911618
          },
          "point_estimate": 464087.0906886679,
          "standard_error": 263.7412966751391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463376.7151898734,
            "upper_bound": 464328.9870780591
          },
          "point_estimate": 463980.0056962025,
          "standard_error": 277.84471560044193
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 75.10080452114242,
            "upper_bound": 1184.4328094784416
          },
          "point_estimate": 536.1684397533082,
          "standard_error": 271.84513627643315
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463558.8289453098,
            "upper_bound": 464119.8040648957
          },
          "point_estimate": 463820.73184284073,
          "standard_error": 141.31488720199533
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338.47124912282396,
            "upper_bound": 1271.9408601021294
          },
          "point_estimate": 879.030778799095,
          "standard_error": 290.6231992811638
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 754020.050770975,
            "upper_bound": 754988.2311698454
          },
          "point_estimate": 754474.3231430191,
          "standard_error": 248.27174907487307
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753816.9642857143,
            "upper_bound": 755112.9931972789
          },
          "point_estimate": 754262.713010204,
          "standard_error": 283.3705504855548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.40570395679975,
            "upper_bound": 1390.8948977556197
          },
          "point_estimate": 729.0322284856811,
          "standard_error": 356.8214120578613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 754171.0616047662,
            "upper_bound": 755178.8808599821
          },
          "point_estimate": 754583.8629737609,
          "standard_error": 258.85588655919275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390.74545419175064,
            "upper_bound": 1085.813240066301
          },
          "point_estimate": 825.3266434328697,
          "standard_error": 184.67737308972016
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 834547.70598796,
            "upper_bound": 840857.5414866974
          },
          "point_estimate": 837611.650160534,
          "standard_error": 1627.393079252403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 833130.8011363636,
            "upper_bound": 842242.2027597402
          },
          "point_estimate": 837657.4473484849,
          "standard_error": 2278.112843574052
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 779.5555588874944,
            "upper_bound": 9286.911326260712
          },
          "point_estimate": 5983.303722979702,
          "standard_error": 2352.60440760438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 837359.1919165686,
            "upper_bound": 844207.1682707862
          },
          "point_estimate": 841741.5659976387,
          "standard_error": 1747.2899706894573
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2913.9225106158024,
            "upper_bound": 6789.007989321482
          },
          "point_estimate": 5437.667058465875,
          "standard_error": 975.3148236644688
        }
      }
    },
    "memmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 734261.6036930556,
            "upper_bound": 734936.8501624999
          },
          "point_estimate": 734584.8779222223,
          "standard_error": 172.9821255785592
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 734128.5033333334,
            "upper_bound": 734992.18
          },
          "point_estimate": 734446.2152777778,
          "standard_error": 229.79082771974907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.05305371685378,
            "upper_bound": 987.074290975873
          },
          "point_estimate": 704.7104902556254,
          "standard_error": 229.85012565402977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 734411.4612794613,
            "upper_bound": 734944.8044897959
          },
          "point_estimate": 734704.984987013,
          "standard_error": 136.20859249913312
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 303.4399643651131,
            "upper_bound": 753.8001703252596
          },
          "point_estimate": 578.8104805377787,
          "standard_error": 120.24432826987989
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316359.6399621118,
            "upper_bound": 316688.53560869565
          },
          "point_estimate": 316522.5676335404,
          "standard_error": 84.30845412151787
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316205.98260869563,
            "upper_bound": 316768.5406521739
          },
          "point_estimate": 316498.17360248446,
          "standard_error": 131.89704953685316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.1332925972296,
            "upper_bound": 482.5653416501507
          },
          "point_estimate": 377.9920863327769,
          "standard_error": 118.56837443605856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 316509.09058330016,
            "upper_bound": 316828.8497350162
          },
          "point_estimate": 316703.0407001694,
          "standard_error": 80.78822339614767
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.64931344787126,
            "upper_bound": 339.4322937313697
          },
          "point_estimate": 280.63986986042,
          "standard_error": 43.08101606240485
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 622993.4459887005,
            "upper_bound": 623655.0139386603
          },
          "point_estimate": 623334.9464245359,
          "standard_error": 168.5381707162144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 622960.2169491525,
            "upper_bound": 623695.1347457627
          },
          "point_estimate": 623403.4406779661,
          "standard_error": 182.9418938098638
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.56509625813321,
            "upper_bound": 952.1290536048112
          },
          "point_estimate": 476.33172984855815,
          "standard_error": 212.92461778998697
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 623161.8883693747,
            "upper_bound": 623851.4351821919
          },
          "point_estimate": 623544.0329738059,
          "standard_error": 176.01031406818052
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 268.3293394419239,
            "upper_bound": 750.6333446401601
          },
          "point_estimate": 560.4997767790375,
          "standard_error": 125.42503567223729
        }
      }
    },
    "memmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187056.73098107448,
            "upper_bound": 187351.6055441595
          },
          "point_estimate": 187188.97560520965,
          "standard_error": 75.70775867711637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186990.33333333337,
            "upper_bound": 187342.90982905985
          },
          "point_estimate": 187106.8676841677,
          "standard_error": 86.79506896462628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.5964544094729,
            "upper_bound": 369.6166085661759
          },
          "point_estimate": 204.0018281218431,
          "standard_error": 87.28500205695006
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 187086.9487661245,
            "upper_bound": 187280.72576509512
          },
          "point_estimate": 187173.2289044289,
          "standard_error": 49.6993000580793
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.50471723868438,
            "upper_bound": 346.4558582088703
          },
          "point_estimate": 252.56588394477393,
          "standard_error": 69.46581637592716
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158206.99523628366,
            "upper_bound": 158535.2922705314
          },
          "point_estimate": 158360.7381193927,
          "standard_error": 84.02365470928353
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158171.33167701864,
            "upper_bound": 158502.43671497586
          },
          "point_estimate": 158324.7927536232,
          "standard_error": 75.1545354651633
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.22226035076608,
            "upper_bound": 425.5516220101577
          },
          "point_estimate": 194.7009834998827,
          "standard_error": 97.45537270923862
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158248.45168756627,
            "upper_bound": 158455.63638497653
          },
          "point_estimate": 158353.68844720497,
          "standard_error": 52.62245255961455
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 114.18342406381272,
            "upper_bound": 386.55627616703674
          },
          "point_estimate": 279.4751267175309,
          "standard_error": 73.97672735170882
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428051.7031562208,
            "upper_bound": 429496.3817142857
          },
          "point_estimate": 428614.53329925303,
          "standard_error": 396.4238029954535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427934.2741013071,
            "upper_bound": 428665.62411764706
          },
          "point_estimate": 428205.5285714286,
          "standard_error": 274.5364243855248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.230109123640943,
            "upper_bound": 961.455423715056
          },
          "point_estimate": 502.3465389247052,
          "standard_error": 267.4482929544463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427992.30850206816,
            "upper_bound": 428583.100589122
          },
          "point_estimate": 428288.65097020625,
          "standard_error": 156.2484250810031
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282.8794358168748,
            "upper_bound": 2012.1052341835568
          },
          "point_estimate": 1319.360774713297,
          "standard_error": 590.2121441538565
        }
      }
    },
    "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 905.2324069920962,
            "upper_bound": 906.1783955363366
          },
          "point_estimate": 905.7077693342216,
          "standard_error": 0.2421940043577884
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 904.8518767497704,
            "upper_bound": 906.34942628082
          },
          "point_estimate": 905.9388314161936,
          "standard_error": 0.46240756302968405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07304227636490826,
            "upper_bound": 1.2827509924391634
          },
          "point_estimate": 0.9962080990352192,
          "standard_error": 0.3364637695021668
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 905.0411371606228,
            "upper_bound": 906.0686473026876
          },
          "point_estimate": 905.6061922740142,
          "standard_error": 0.2649558798282979
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5260428265571883,
            "upper_bound": 0.9426150296743588
          },
          "point_estimate": 0.8070717972875788,
          "standard_error": 0.10654932821514516
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-haystack/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/prebuilt/sliceslice-haystack/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-haystack/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-haystack/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-haystack_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1534417.4404861112,
            "upper_bound": 1538670.0080555554
          },
          "point_estimate": 1536741.6831200398,
          "standard_error": 1091.453388104547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1535655.5338541665,
            "upper_bound": 1539606.1944444445
          },
          "point_estimate": 1536675.0178571427,
          "standard_error": 1009.692844908516
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383.5753088152427,
            "upper_bound": 5209.837002819691
          },
          "point_estimate": 2190.3561361134052,
          "standard_error": 1214.662813824089
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1536050.803030303,
            "upper_bound": 1538038.57426156
          },
          "point_estimate": 1536900.7955627705,
          "standard_error": 512.0649699276435
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1456.3004166288852,
            "upper_bound": 5151.314331796262
          },
          "point_estimate": 3638.668474877116,
          "standard_error": 1077.047871830965
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-i386/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/prebuilt/sliceslice-i386/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-i386/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-i386/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-i386_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305328208.1,
            "upper_bound": 305683999.4
          },
          "point_estimate": 305509255.4,
          "standard_error": 90981.4576074144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 305282733.0,
            "upper_bound": 305690802.0
          },
          "point_estimate": 305594216.0,
          "standard_error": 109388.9003899305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9848.911625146866,
            "upper_bound": 540395.0883060694
          },
          "point_estimate": 220573.8110840321,
          "standard_error": 136295.70638347566
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153656.09361905133,
            "upper_bound": 389872.0816914184
          },
          "point_estimate": 302511.2470512276,
          "standard_error": 59437.98593916491
        }
      }
    },
    "memmem/stud/prebuilt/sliceslice-words/words": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/stud/prebuilt/sliceslice-words/words",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "stud/prebuilt/sliceslice-words/words",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memmem/stud/prebuilt/sliceslice-words/words",
        "directory_name": "memmem/stud_prebuilt_sliceslice-words_words"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274162546.2,
            "upper_bound": 274500899.3
          },
          "point_estimate": 274336067.9,
          "standard_error": 86534.1334167246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274146451.5,
            "upper_bound": 274551556.5
          },
          "point_estimate": 274388179.5,
          "standard_error": 108627.00074120233
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67113.59430849552,
            "upper_bound": 480630.7420670986
          },
          "point_estimate": 295757.19704926014,
          "standard_error": 106534.53463661352
        },
        "slope": null,
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151116.36847748235,
            "upper_bound": 374221.4060677431
          },
          "point_estimate": 287975.12656572345,
          "standard_error": 58184.55683316522
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159389.7316669451,
            "upper_bound": 159617.23623433587
          },
          "point_estimate": 159496.4700696185,
          "standard_error": 58.50183639086731
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159344.53781676415,
            "upper_bound": 159673.87280701756
          },
          "point_estimate": 159424.6528822055,
          "standard_error": 81.34555464679785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.978048169743655,
            "upper_bound": 321.31615416389934
          },
          "point_estimate": 123.56062909288651,
          "standard_error": 73.66943305511731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159353.68690302144,
            "upper_bound": 159548.9901065449
          },
          "point_estimate": 159414.53121439964,
          "standard_error": 50.61080221177281
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.54631919261834,
            "upper_bound": 235.4334905537541
          },
          "point_estimate": 195.16522574301865,
          "standard_error": 38.45762108172737
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224909.89826646092,
            "upper_bound": 225332.56626445844
          },
          "point_estimate": 225110.31130878895,
          "standard_error": 108.21778375470284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224834.46283436212,
            "upper_bound": 225283.74074074073
          },
          "point_estimate": 225090.97724867729,
          "standard_error": 102.61188775973606
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.407902836628686,
            "upper_bound": 601.8867794378407
          },
          "point_estimate": 245.79433143874968,
          "standard_error": 142.02508067957993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224818.8928209031,
            "upper_bound": 225112.26032745175
          },
          "point_estimate": 224974.0791085458,
          "standard_error": 74.47524507487182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.18467061688042,
            "upper_bound": 484.78782312583417
          },
          "point_estimate": 359.9918623443458,
          "standard_error": 85.74804562358815
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224933.47666504997,
            "upper_bound": 225324.1486884002
          },
          "point_estimate": 225089.70096928277,
          "standard_error": 105.52937837285008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224923.5705467372,
            "upper_bound": 225120.89043209876
          },
          "point_estimate": 224983.54197530865,
          "standard_error": 50.14698922935864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.386916180446917,
            "upper_bound": 280.0706853055164
          },
          "point_estimate": 102.53355484635574,
          "standard_error": 69.28992727104476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224897.24628190085,
            "upper_bound": 225066.24703305456
          },
          "point_estimate": 224974.76062209395,
          "standard_error": 42.872911402587505
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.995937442871394,
            "upper_bound": 530.4990233405396
          },
          "point_estimate": 352.2110101065365,
          "standard_error": 149.9776199968603
        }
      }
    },
    "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136265.9541398138,
            "upper_bound": 136563.77708020047
          },
          "point_estimate": 136375.1800044755,
          "standard_error": 85.87457691682381
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136257.1442669173,
            "upper_bound": 136338.93565162906
          },
          "point_estimate": 136284.23872180452,
          "standard_error": 28.80728753794067
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.200038687973076,
            "upper_bound": 103.20261369409268
          },
          "point_estimate": 42.746056496727334,
          "standard_error": 33.7398737729385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136260.3284612275,
            "upper_bound": 136339.494784039
          },
          "point_estimate": 136302.00888585098,
          "standard_error": 20.17308750628957
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.23892634043774,
            "upper_bound": 441.1831327180998
          },
          "point_estimate": 287.1808426579641,
          "standard_error": 145.30206858903205
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96600.71390814596,
            "upper_bound": 96786.67132768358
          },
          "point_estimate": 96686.68052871458,
          "standard_error": 47.794596149602086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96562.7192012968,
            "upper_bound": 96793.36180371352
          },
          "point_estimate": 96646.33315649867,
          "standard_error": 55.021491467693785
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.05933799952483,
            "upper_bound": 252.50239366042953
          },
          "point_estimate": 129.23954309015653,
          "standard_error": 58.29332185604082
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96580.88363030556,
            "upper_bound": 96693.11241783809
          },
          "point_estimate": 96633.65453856488,
          "standard_error": 28.59830507384437
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.06313768245687,
            "upper_bound": 201.57913978565455
          },
          "point_estimate": 159.04070032083644,
          "standard_error": 36.11261323190256
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84891.16634267913,
            "upper_bound": 84983.0158749351
          },
          "point_estimate": 84939.18759865005,
          "standard_error": 23.58338064140005
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84884.26028037383,
            "upper_bound": 85008.8113317757
          },
          "point_estimate": 84956.4355140187,
          "standard_error": 33.22619165117243
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.19869263205663,
            "upper_bound": 136.46639953985107
          },
          "point_estimate": 85.02193170707343,
          "standard_error": 28.61913956616732
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84908.69224354203,
            "upper_bound": 84998.53997653005
          },
          "point_estimate": 84945.65030343489,
          "standard_error": 23.324228366474955
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.73453053719235,
            "upper_bound": 101.8098638420304
          },
          "point_estimate": 78.71212595610763,
          "standard_error": 15.889450403306592
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66180.72713636364,
            "upper_bound": 66262.6098969697
          },
          "point_estimate": 66224.48812424243,
          "standard_error": 21.06234476342274
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66167.61477272728,
            "upper_bound": 66278.78545454546
          },
          "point_estimate": 66261.40636363637,
          "standard_error": 30.49727576402157
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.0252140012578,
            "upper_bound": 112.62289904600662
          },
          "point_estimate": 29.791453725631587,
          "standard_error": 29.29816998279874
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66163.16489272285,
            "upper_bound": 66270.94121644167
          },
          "point_estimate": 66227.45991027154,
          "standard_error": 29.533014178509344
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.581258453343175,
            "upper_bound": 84.1118300375126
          },
          "point_estimate": 70.07434798321674,
          "standard_error": 14.371815706434758
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128682.29975008444,
            "upper_bound": 129082.94983827254
          },
          "point_estimate": 128875.4240982776,
          "standard_error": 102.71666816287204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128628.37044917257,
            "upper_bound": 129159.38386524822
          },
          "point_estimate": 128836.70330969268,
          "standard_error": 106.58283197632748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.26435387641749,
            "upper_bound": 647.2860618062471
          },
          "point_estimate": 207.9639377972624,
          "standard_error": 150.0484744067154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 128742.74025698996,
            "upper_bound": 129298.39987594918
          },
          "point_estimate": 129083.86996407848,
          "standard_error": 142.47491529127927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.2374253466811,
            "upper_bound": 437.4195327132592
          },
          "point_estimate": 342.3006764023223,
          "standard_error": 69.75536468364314
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78164.72709361665,
            "upper_bound": 78281.26705426694
          },
          "point_estimate": 78219.30961332991,
          "standard_error": 30.00881507554946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78149.50651135006,
            "upper_bound": 78279.85739247312
          },
          "point_estimate": 78197.57222734255,
          "standard_error": 29.69132910201343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.33780670976972,
            "upper_bound": 163.2792930689584
          },
          "point_estimate": 75.27473591091865,
          "standard_error": 38.61678428279675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78162.93118483984,
            "upper_bound": 78220.97276542067
          },
          "point_estimate": 78192.9063091747,
          "standard_error": 14.472997828872892
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.88565893897434,
            "upper_bound": 131.66281096236327
          },
          "point_estimate": 100.15817018857594,
          "standard_error": 23.34647670076435
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78347.97243246288,
            "upper_bound": 78460.4952329749
          },
          "point_estimate": 78402.05297345963,
          "standard_error": 28.70609754921257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78323.8470046083,
            "upper_bound": 78441.08325268817
          },
          "point_estimate": 78409.14086021506,
          "standard_error": 27.69036198494298
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.956385249099463,
            "upper_bound": 158.57610967217497
          },
          "point_estimate": 65.18562094844872,
          "standard_error": 38.157625859938584
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78366.44658123037,
            "upper_bound": 78437.70905590848
          },
          "point_estimate": 78404.84497695853,
          "standard_error": 18.191270653050104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.084808332595,
            "upper_bound": 130.96983677576364
          },
          "point_estimate": 95.59902184983645,
          "standard_error": 23.586475720615937
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84425.72771922073,
            "upper_bound": 84582.04982033063
          },
          "point_estimate": 84499.7598496483,
          "standard_error": 40.06826149422699
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84386.82366589327,
            "upper_bound": 84604.9055974478
          },
          "point_estimate": 84464.88712296984,
          "standard_error": 53.29916579417384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.382711735794796,
            "upper_bound": 223.15731033629436
          },
          "point_estimate": 135.42939220508603,
          "standard_error": 49.81045551835722
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84392.18175442495,
            "upper_bound": 84574.08699650927
          },
          "point_estimate": 84471.92304818152,
          "standard_error": 46.558663382622896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.52047253955057,
            "upper_bound": 165.96829788185485
          },
          "point_estimate": 133.10869609171422,
          "standard_error": 25.15523632765351
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76120.28270892483,
            "upper_bound": 76330.29164458392
          },
          "point_estimate": 76238.65752133558,
          "standard_error": 54.36456719607288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76185.19018977883,
            "upper_bound": 76349.25732217573
          },
          "point_estimate": 76277.68018363552,
          "standard_error": 41.43341547875123
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.13101607915461,
            "upper_bound": 222.94896194144985
          },
          "point_estimate": 112.46997972396925,
          "standard_error": 50.45906052842344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76210.34689172213,
            "upper_bound": 76318.49979166305
          },
          "point_estimate": 76263.45438243765,
          "standard_error": 27.07363039332484
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.84507040959684,
            "upper_bound": 260.94605063305363
          },
          "point_estimate": 181.3764454671492,
          "standard_error": 60.06261121067146
        }
      }
    },
    "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233683.87965811967,
            "upper_bound": 233983.8698335878
          },
          "point_estimate": 233830.4251302401,
          "standard_error": 76.88098924584386
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233665.1356837607,
            "upper_bound": 234091.70014245017
          },
          "point_estimate": 233729.0574786325,
          "standard_error": 127.1423845359583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.48154308017855,
            "upper_bound": 408.1073658315687
          },
          "point_estimate": 247.15566099673703,
          "standard_error": 116.4068669776994
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 233665.43997004893,
            "upper_bound": 233958.67211762923
          },
          "point_estimate": 233802.98771228772,
          "standard_error": 77.89504397169058
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152.06079790573966,
            "upper_bound": 315.6994302638588
          },
          "point_estimate": 255.87353231353552,
          "standard_error": 42.21110110040051
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153075.13978641457,
            "upper_bound": 153246.49746832068
          },
          "point_estimate": 153163.2228599773,
          "standard_error": 43.50414435635737
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153079.95780812326,
            "upper_bound": 153244.26421818728
          },
          "point_estimate": 153174.1888655462,
          "standard_error": 41.695872375341416
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31.105701898761826,
            "upper_bound": 240.2174041911702
          },
          "point_estimate": 101.82478623146903,
          "standard_error": 54.25109332245734
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 153109.0773705932,
            "upper_bound": 153224.78439241872
          },
          "point_estimate": 153171.214231147,
          "standard_error": 30.076530320414275
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.62705318323363,
            "upper_bound": 195.44727652977193
          },
          "point_estimate": 144.82551079468388,
          "standard_error": 33.57890268678553
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73729.6339696827,
            "upper_bound": 73843.2385601758
          },
          "point_estimate": 73779.08092308187,
          "standard_error": 29.474611445466724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73728.26247464503,
            "upper_bound": 73803.39807302231
          },
          "point_estimate": 73759.84396551724,
          "standard_error": 18.582545976101297
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.817828590098047,
            "upper_bound": 115.02558974964352
          },
          "point_estimate": 55.69801808824091,
          "standard_error": 26.498723875897475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73737.2968362809,
            "upper_bound": 73803.43687846918
          },
          "point_estimate": 73763.2445562551,
          "standard_error": 16.922566062820188
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.90463704077983,
            "upper_bound": 143.25925495037413
          },
          "point_estimate": 98.10248630132226,
          "standard_error": 34.21820051962752
        }
      }
    },
    "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228013.75298699157,
            "upper_bound": 228380.63633333327
          },
          "point_estimate": 228201.35207787697,
          "standard_error": 93.4483891813036
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228046.98854166668,
            "upper_bound": 228425.85555555555
          },
          "point_estimate": 228180.7269345238,
          "standard_error": 93.99502296968744
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.77939419793165,
            "upper_bound": 521.1264777481487
          },
          "point_estimate": 214.85863087295385,
          "standard_error": 120.22012291754731
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 228040.3512668919,
            "upper_bound": 228552.606361622
          },
          "point_estimate": 228360.97050324676,
          "standard_error": 129.53025641787298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.36011773829924,
            "upper_bound": 428.9693353639108
          },
          "point_estimate": 312.2820738885068,
          "standard_error": 75.73581092110054
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59611.51613534279,
            "upper_bound": 59671.05605629595
          },
          "point_estimate": 59640.22565044294,
          "standard_error": 15.260580343905533
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59606.95881069286,
            "upper_bound": 59679.61670303692
          },
          "point_estimate": 59634.39695171849,
          "standard_error": 14.340140219939384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.850503622240401,
            "upper_bound": 96.0800409292642
          },
          "point_estimate": 30.033204802318508,
          "standard_error": 23.62509226925139
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59628.83496635752,
            "upper_bound": 59675.5234946639
          },
          "point_estimate": 59646.07658724255,
          "standard_error": 12.454195716963657
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.612135471722013,
            "upper_bound": 65.8207154429809
          },
          "point_estimate": 50.95739574024003,
          "standard_error": 10.773639543210392
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65852.66208937198,
            "upper_bound": 65956.56400597394
          },
          "point_estimate": 65895.11845381872,
          "standard_error": 27.629603760000084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65846.7720410628,
            "upper_bound": 65902.33255693581
          },
          "point_estimate": 65879.56526771336,
          "standard_error": 18.1254078950304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.4700407170284757,
            "upper_bound": 77.84880885342096
          },
          "point_estimate": 36.23258869248446,
          "standard_error": 19.925500636361956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 65846.32674062498,
            "upper_bound": 65881.53732036293
          },
          "point_estimate": 65862.71049313006,
          "standard_error": 8.791338972871445
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.645556885842577,
            "upper_bound": 137.72330897901386
          },
          "point_estimate": 91.55999565921806,
          "standard_error": 37.2780364033616
        }
      }
    },
    "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69857.69739694727,
            "upper_bound": 69928.91623237709
          },
          "point_estimate": 69892.57743586814,
          "standard_error": 18.320838655824065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69841.50033589252,
            "upper_bound": 69967.95777351248
          },
          "point_estimate": 69881.89158669225,
          "standard_error": 28.59757773978442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.547537444208944,
            "upper_bound": 106.75466801835488
          },
          "point_estimate": 66.27977681752907,
          "standard_error": 26.069560303837285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69861.53828448203,
            "upper_bound": 69923.78392933679
          },
          "point_estimate": 69893.1408629758,
          "standard_error": 15.741536783506367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.75493436428059,
            "upper_bound": 72.87482182660635
          },
          "point_estimate": 61.05214208290086,
          "standard_error": 9.373369715257086
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95021.83694706744,
            "upper_bound": 95105.66370522729
          },
          "point_estimate": 95068.8354011884,
          "standard_error": 21.799459183124576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95035.92557965594,
            "upper_bound": 95099.48691099476
          },
          "point_estimate": 95094.0171611402,
          "standard_error": 16.803887188653153
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.150804935643857,
            "upper_bound": 100.5093247426462
          },
          "point_estimate": 13.794680684421024,
          "standard_error": 28.815629062519367
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95068.3928814594,
            "upper_bound": 95109.0392466701
          },
          "point_estimate": 95091.42930577276,
          "standard_error": 10.20619029770289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.756715146197983,
            "upper_bound": 103.31857679082393
          },
          "point_estimate": 72.57042836517795,
          "standard_error": 23.376592507403803
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66425.61384836117,
            "upper_bound": 66516.52603267826
          },
          "point_estimate": 66469.16285133919,
          "standard_error": 23.35381620407209
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66402.65641594847,
            "upper_bound": 66546.18793418647
          },
          "point_estimate": 66458.32938756855,
          "standard_error": 32.37159065689292
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.366470984394158,
            "upper_bound": 125.64658329645005
          },
          "point_estimate": 90.3244445207258,
          "standard_error": 32.37307907787363
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66425.32461645707,
            "upper_bound": 66473.4832781313
          },
          "point_estimate": 66451.12939053634,
          "standard_error": 11.965435593781317
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.341480992153414,
            "upper_bound": 95.5404319316549
          },
          "point_estimate": 78.05085558691638,
          "standard_error": 13.614016199346713
        }
      }
    },
    "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92196.3365373518,
            "upper_bound": 92295.61612203134
          },
          "point_estimate": 92247.08258026924,
          "standard_error": 25.44196410392546
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92186.48073136428,
            "upper_bound": 92304.25001808318
          },
          "point_estimate": 92254.538185654,
          "standard_error": 25.17797910347088
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.526101496513876,
            "upper_bound": 144.92788595023637
          },
          "point_estimate": 53.728609805611654,
          "standard_error": 34.98806304183944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92148.18786763326,
            "upper_bound": 92282.47661732022
          },
          "point_estimate": 92214.47087292456,
          "standard_error": 36.6244453651552
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.855773147975704,
            "upper_bound": 111.85140865848268
          },
          "point_estimate": 84.82319229585279,
          "standard_error": 18.16307749360224
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20090.593660818715,
            "upper_bound": 20146.032911064944
          },
          "point_estimate": 20116.63550907972,
          "standard_error": 14.25719606940403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20081.259279778395,
            "upper_bound": 20155.58135734072
          },
          "point_estimate": 20112.58049861496,
          "standard_error": 14.686201451353377
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.1853580427532515,
            "upper_bound": 78.69254152998278
          },
          "point_estimate": 33.62425853324563,
          "standard_error": 19.740433045931688
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20102.754916897506,
            "upper_bound": 20130.95909142165
          },
          "point_estimate": 20115.79386264705,
          "standard_error": 7.0406529310989
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.4706818397863,
            "upper_bound": 60.3224743915901
          },
          "point_estimate": 47.45514587223053,
          "standard_error": 10.54342026464374
        }
      }
    },
    "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20038.582535720863,
            "upper_bound": 20067.109710151937
          },
          "point_estimate": 20053.287864465492,
          "standard_error": 7.332337504529781
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20033.451539068665,
            "upper_bound": 20073.45414364641
          },
          "point_estimate": 20056.20814917127,
          "standard_error": 8.281270155194564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6868742787824247,
            "upper_bound": 39.591153109269534
          },
          "point_estimate": 29.65393024701982,
          "standard_error": 11.939408638861494
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20036.52241208387,
            "upper_bound": 20060.38941261213
          },
          "point_estimate": 20049.14579608237,
          "standard_error": 6.037134352652894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.64584747054795,
            "upper_bound": 31.642607734269298
          },
          "point_estimate": 24.480649612882445,
          "standard_error": 4.927643162457124
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267686.04895938374,
            "upper_bound": 268009.7517156863
          },
          "point_estimate": 267860.8104858193,
          "standard_error": 82.82779612646607
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267730.3174019608,
            "upper_bound": 268112.1875
          },
          "point_estimate": 267881.04779411765,
          "standard_error": 109.33638707617992
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.104384432751938,
            "upper_bound": 467.5390697142103
          },
          "point_estimate": 224.528760719726,
          "standard_error": 105.86745124465796
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267737.70107557584,
            "upper_bound": 268066.39136745606
          },
          "point_estimate": 267915.24155844154,
          "standard_error": 86.47947687347937
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.38064822391627,
            "upper_bound": 377.44210609355855
          },
          "point_estimate": 275.7383491078984,
          "standard_error": 71.56456479711846
        }
      }
    },
    "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 568.8296610760879,
            "upper_bound": 569.6237545140247
          },
          "point_estimate": 569.2459315223807,
          "standard_error": 0.20399774752856756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 568.8710230174262,
            "upper_bound": 569.9094916286149
          },
          "point_estimate": 569.2882791911285,
          "standard_error": 0.2429812721634527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0959123226808098,
            "upper_bound": 1.1662378331763867
          },
          "point_estimate": 0.7590759197917626,
          "standard_error": 0.2851206547418124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 568.9568131624887,
            "upper_bound": 569.6715865819234
          },
          "point_estimate": 569.3371176049,
          "standard_error": 0.1805519923098233
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.354690011279384,
            "upper_bound": 0.8975085251048839
          },
          "point_estimate": 0.6796474343045928,
          "standard_error": 0.1488902425911835
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.12834517634146,
            "upper_bound": 44.17202622199619
          },
          "point_estimate": 44.150162790906045,
          "standard_error": 0.011172173726308151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.12258959137592,
            "upper_bound": 44.18080007041057
          },
          "point_estimate": 44.14730768608218,
          "standard_error": 0.013642240314011685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008615412792877258,
            "upper_bound": 0.06725448238090916
          },
          "point_estimate": 0.03780525842150082,
          "standard_error": 0.016029381582009894
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.13826601221012,
            "upper_bound": 44.18022939941218
          },
          "point_estimate": 44.15562872410567,
          "standard_error": 0.010713254796288992
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02110021333188606,
            "upper_bound": 0.046725025329246675
          },
          "point_estimate": 0.037140329667948245,
          "standard_error": 0.006618034446746134
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.34288358724122,
            "upper_bound": 47.40733240118507
          },
          "point_estimate": 47.375081386492695,
          "standard_error": 0.016506880041509044
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.32464225633203,
            "upper_bound": 47.42705503427234
          },
          "point_estimate": 47.374933947089175,
          "standard_error": 0.02748164410737496
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018056261984091524,
            "upper_bound": 0.08999107261727905
          },
          "point_estimate": 0.07591859093933176,
          "standard_error": 0.019155504326710263
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.35928933538339,
            "upper_bound": 47.40855122960561
          },
          "point_estimate": 47.381608725795225,
          "standard_error": 0.012288356987558076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03425293667966185,
            "upper_bound": 0.06635373506010699
          },
          "point_estimate": 0.05487456388684531,
          "standard_error": 0.008187239523561606
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.35529217920197,
            "upper_bound": 27.39161182010767
          },
          "point_estimate": 27.37263946792425,
          "standard_error": 0.009317515022369587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.346541563817897,
            "upper_bound": 27.404890641735065
          },
          "point_estimate": 27.364527223316703,
          "standard_error": 0.013468699207517155
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006332228453827241,
            "upper_bound": 0.0509136401443436
          },
          "point_estimate": 0.027375439632823264,
          "standard_error": 0.012174045604357588
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.354117219089417,
            "upper_bound": 27.372680197715795
          },
          "point_estimate": 27.362612839880356,
          "standard_error": 0.004623729560787679
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01511243726369602,
            "upper_bound": 0.03745905111800505
          },
          "point_estimate": 0.030999116998748175,
          "standard_error": 0.005307913831767323
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.36749120782997,
            "upper_bound": 27.40140654341858
          },
          "point_estimate": 27.384461955098836,
          "standard_error": 0.008590840742379974
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.369856969131085,
            "upper_bound": 27.395811857368216
          },
          "point_estimate": 27.389189479192943,
          "standard_error": 0.00764700170162086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002941956786258702,
            "upper_bound": 0.04689157039868817
          },
          "point_estimate": 0.01417891127294508,
          "standard_error": 0.010908579792699954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.3756307473361,
            "upper_bound": 27.420769548339248
          },
          "point_estimate": 27.399503236375153,
          "standard_error": 0.012126598936107355
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010596575917732788,
            "upper_bound": 0.03920818775534529
          },
          "point_estimate": 0.02858304689290272,
          "standard_error": 0.007519730982295055
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.82492211764251,
            "upper_bound": 36.86847715453053
          },
          "point_estimate": 36.84610973192952,
          "standard_error": 0.01116096261994826
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.81087812809208,
            "upper_bound": 36.892128377487275
          },
          "point_estimate": 36.84025785011535,
          "standard_error": 0.019587241510316728
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0039106112818932635,
            "upper_bound": 0.06374870894808896
          },
          "point_estimate": 0.043602996175834505,
          "standard_error": 0.015956788268834426
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.82373589060722,
            "upper_bound": 36.85271968015311
          },
          "point_estimate": 36.83622537601505,
          "standard_error": 0.007346088910407261
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021488352315030004,
            "upper_bound": 0.04354264592905029
          },
          "point_estimate": 0.03728415444165422,
          "standard_error": 0.005343660902667121
        }
      }
    },
    "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.263033876687985,
            "upper_bound": 52.32149580639782
          },
          "point_estimate": 52.292541969077,
          "standard_error": 0.015009669297465588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.23851243447125,
            "upper_bound": 52.342299153827184
          },
          "point_estimate": 52.290090107572325,
          "standard_error": 0.027089805922296116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008804127447050643,
            "upper_bound": 0.08214045751248698
          },
          "point_estimate": 0.07200691293016925,
          "standard_error": 0.02102228630375436
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.23988613327415,
            "upper_bound": 52.3053402607782
          },
          "point_estimate": 52.26683675236171,
          "standard_error": 0.01676640500477799
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03147905908557726,
            "upper_bound": 0.05836386355925784
          },
          "point_estimate": 0.05006378538526085,
          "standard_error": 0.006734359814039853
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.48528970778776,
            "upper_bound": 68.57606855624503
          },
          "point_estimate": 68.53219091895899,
          "standard_error": 0.02319862643915815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.47822204500713,
            "upper_bound": 68.58812155112625
          },
          "point_estimate": 68.54215729879685,
          "standard_error": 0.027201178791971464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020652982437961803,
            "upper_bound": 0.1295246222255366
          },
          "point_estimate": 0.06737772598344881,
          "standard_error": 0.028185546984165283
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.49178955696046,
            "upper_bound": 68.59416259437478
          },
          "point_estimate": 68.54171399944626,
          "standard_error": 0.026291308600370315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03972928830247296,
            "upper_bound": 0.1004791062566894
          },
          "point_estimate": 0.07734696704616573,
          "standard_error": 0.015834824837426018
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.11748697753214,
            "upper_bound": 45.164534473912305
          },
          "point_estimate": 45.14444279741327,
          "standard_error": 0.01232092267422976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.128972237126945,
            "upper_bound": 45.169857058173015
          },
          "point_estimate": 45.15638532855781,
          "standard_error": 0.008583384109181908
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0041867820408839785,
            "upper_bound": 0.04588012114306538
          },
          "point_estimate": 0.018052045237084237,
          "standard_error": 0.011511493964930158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.12877078280622,
            "upper_bound": 45.16681217875542
          },
          "point_estimate": 45.15189355392697,
          "standard_error": 0.009993013868123918
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0097768724455544,
            "upper_bound": 0.058380552341100025
          },
          "point_estimate": 0.04098815457548135,
          "standard_error": 0.01388028254028651
        }
      }
    },
    "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.49432149049537,
            "upper_bound": 63.59658739744399
          },
          "point_estimate": 63.54587892432487,
          "standard_error": 0.026266800064872635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.477670118646856,
            "upper_bound": 63.61680579853285
          },
          "point_estimate": 63.56109198563926,
          "standard_error": 0.03503734088602311
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011151093506329889,
            "upper_bound": 0.15490985430034418
          },
          "point_estimate": 0.09762396039914448,
          "standard_error": 0.03633987513386573
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.46942057121129,
            "upper_bound": 63.57635723901048
          },
          "point_estimate": 63.517600830945405,
          "standard_error": 0.027270344821509772
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05026483895988971,
            "upper_bound": 0.10943813980678656
          },
          "point_estimate": 0.08752196364303899,
          "standard_error": 0.015151169403326082
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.13752988530464,
            "upper_bound": 53.24524671918167
          },
          "point_estimate": 53.188847419843135,
          "standard_error": 0.027718700085298555
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.110948493548605,
            "upper_bound": 53.26327076336146
          },
          "point_estimate": 53.161604676949366,
          "standard_error": 0.04489022875697896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012845620654085793,
            "upper_bound": 0.15104690039246343
          },
          "point_estimate": 0.07794059315291785,
          "standard_error": 0.04076098705988581
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.12325558235448,
            "upper_bound": 53.23877107798467
          },
          "point_estimate": 53.17389000333905,
          "standard_error": 0.0292169839442696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04656968410248001,
            "upper_bound": 0.11104884223046774
          },
          "point_estimate": 0.0925373298571424,
          "standard_error": 0.01597340008049815
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.27880641964978,
            "upper_bound": 39.36370492060729
          },
          "point_estimate": 39.31996430626421,
          "standard_error": 0.02182533813096119
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.26248974070337,
            "upper_bound": 39.38918562901878
          },
          "point_estimate": 39.2950429755042,
          "standard_error": 0.03790509497637663
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008389407679232882,
            "upper_bound": 0.11728061407579315
          },
          "point_estimate": 0.07227607603760718,
          "standard_error": 0.03100085262764267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.26093346055154,
            "upper_bound": 39.32114052900464
          },
          "point_estimate": 39.282337258727736,
          "standard_error": 0.015296964773771245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04243602445731287,
            "upper_bound": 0.08666721512139453
          },
          "point_estimate": 0.07286260612086527,
          "standard_error": 0.01112090316548319
        }
      }
    },
    "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.12816282156219,
            "upper_bound": 64.28516829804428
          },
          "point_estimate": 64.2041219911838,
          "standard_error": 0.040274777218881216
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.10537778357107,
            "upper_bound": 64.30384535892404
          },
          "point_estimate": 64.1730547051545,
          "standard_error": 0.04977272997089929
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019242213674096963,
            "upper_bound": 0.22746556870532772
          },
          "point_estimate": 0.10959981183105907,
          "standard_error": 0.05473319224564904
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.10167477620693,
            "upper_bound": 64.19682266566733
          },
          "point_estimate": 64.14385132399562,
          "standard_error": 0.02478501470668996
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06746416524594638,
            "upper_bound": 0.16811372311600117
          },
          "point_estimate": 0.1340805655724,
          "standard_error": 0.025143120213599175
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250959.7459980843,
            "upper_bound": 251434.25989846743
          },
          "point_estimate": 251174.13920881227,
          "standard_error": 121.74554741143577
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250834.5413793103,
            "upper_bound": 251370.99574712644
          },
          "point_estimate": 251118.4348275862,
          "standard_error": 117.56918385770582
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.28123395864512,
            "upper_bound": 595.091430676407
          },
          "point_estimate": 317.00224118243733,
          "standard_error": 146.93862074098078
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251059.30276436397,
            "upper_bound": 251243.7827457058
          },
          "point_estimate": 251151.2286789073,
          "standard_error": 46.15381254234498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.41662071435923,
            "upper_bound": 557.0933229967384
          },
          "point_estimate": 404.2352977711636,
          "standard_error": 110.10203270249669
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170482.87151429683,
            "upper_bound": 170737.45941866928
          },
          "point_estimate": 170603.39502614597,
          "standard_error": 65.28069633295794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170421.49859813083,
            "upper_bound": 170741.97523364486
          },
          "point_estimate": 170576.13328882953,
          "standard_error": 88.28002212878509
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 43.6849987454617,
            "upper_bound": 382.457678256747
          },
          "point_estimate": 218.17142756361085,
          "standard_error": 79.9893126825077
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 170428.13421660254,
            "upper_bound": 170653.73166889185
          },
          "point_estimate": 170519.8155480034,
          "standard_error": 57.77183161400127
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.4386480091892,
            "upper_bound": 285.3942663435708
          },
          "point_estimate": 216.94149194257187,
          "standard_error": 46.59253562040028
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 319659.1203258146,
            "upper_bound": 320142.72834725707
          },
          "point_estimate": 319907.6891346422,
          "standard_error": 123.49729813212085
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 319660.8611111111,
            "upper_bound": 320167.9537037037
          },
          "point_estimate": 319965.72055137844,
          "standard_error": 119.48065780727866
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.85175081046098,
            "upper_bound": 692.9688172295371
          },
          "point_estimate": 290.06604011343154,
          "standard_error": 158.72691703067807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 319871.1912004394,
            "upper_bound": 320143.451203543
          },
          "point_estimate": 319995.2511050353,
          "standard_error": 69.9139395016637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.34772217801975,
            "upper_bound": 550.8427065424153
          },
          "point_estimate": 410.4097415781643,
          "standard_error": 93.68093652597548
        }
      }
    },
    "memmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356613.9676136982,
            "upper_bound": 357208.089104439
          },
          "point_estimate": 356906.91185729846,
          "standard_error": 152.11224916170065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356402.95098039217,
            "upper_bound": 357363.80081699346
          },
          "point_estimate": 356799.45196078427,
          "standard_error": 266.21449405458753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.42576850808357,
            "upper_bound": 801.1270948620348
          },
          "point_estimate": 743.6585805229087,
          "standard_error": 212.53371911749508
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 356462.5170745301,
            "upper_bound": 357220.70194225246
          },
          "point_estimate": 356881.05854341737,
          "standard_error": 198.11762890755745
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 324.5991980942449,
            "upper_bound": 604.1247744363258
          },
          "point_estimate": 507.08749079905834,
          "standard_error": 71.90008964343552
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499686.1365650685,
            "upper_bound": 500240.6618378996
          },
          "point_estimate": 499973.3815052186,
          "standard_error": 142.02630739735008
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499662.4031311155,
            "upper_bound": 500323.55793379
          },
          "point_estimate": 500031.6239726028,
          "standard_error": 155.3242722682799
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.19316588729188,
            "upper_bound": 795.0176641733001
          },
          "point_estimate": 358.80019465739605,
          "standard_error": 186.99129333644845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499830.27099925664,
            "upper_bound": 500242.0214478552
          },
          "point_estimate": 500015.3517167764,
          "standard_error": 105.46073727770596
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.84619191657063,
            "upper_bound": 615.0563278620587
          },
          "point_estimate": 473.2631035708251,
          "standard_error": 97.64819642473508
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106510.96796336908,
            "upper_bound": 106607.71162171054
          },
          "point_estimate": 106557.44905295646,
          "standard_error": 24.77710733707154
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106490.5321637427,
            "upper_bound": 106640.16764132552
          },
          "point_estimate": 106519.79926900585,
          "standard_error": 45.37181670690134
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.164718698000801,
            "upper_bound": 130.26504856451825
          },
          "point_estimate": 61.71847417718066,
          "standard_error": 35.0747325846138
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 106487.17829686576,
            "upper_bound": 106580.72367452848
          },
          "point_estimate": 106523.9109212425,
          "standard_error": 24.099215060549906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.16507921656955,
            "upper_bound": 95.25690214535896
          },
          "point_estimate": 82.50666994292013,
          "standard_error": 12.123252041269849
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161254.36477208877,
            "upper_bound": 161459.55159783672
          },
          "point_estimate": 161348.49312227842,
          "standard_error": 52.718178811398175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161200.04614412136,
            "upper_bound": 161425.37555309734
          },
          "point_estimate": 161349.14085545723,
          "standard_error": 66.40491920790316
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.638144881069348,
            "upper_bound": 284.6305852860647
          },
          "point_estimate": 160.55896230436167,
          "standard_error": 63.49670747297783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 161214.9191505432,
            "upper_bound": 161393.7395176567
          },
          "point_estimate": 161302.53851281462,
          "standard_error": 46.48903806352128
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.32196018293699,
            "upper_bound": 241.9571378716624
          },
          "point_estimate": 175.5816558693884,
          "standard_error": 46.64123067618523
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306718.6161804722,
            "upper_bound": 307089.0778821529
          },
          "point_estimate": 306911.02284713887,
          "standard_error": 94.92674487343942
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306617.256302521,
            "upper_bound": 307136.80112044816
          },
          "point_estimate": 306996.81332533015,
          "standard_error": 120.90527924284986
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.62540978886499,
            "upper_bound": 515.8640161357256
          },
          "point_estimate": 278.7466230344121,
          "standard_error": 137.48679957340423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 306844.6335674895,
            "upper_bound": 307164.6227917586
          },
          "point_estimate": 307047.8582778566,
          "standard_error": 82.16272444693789
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 151.3184787512661,
            "upper_bound": 390.6777922834354
          },
          "point_estimate": 316.6111938825713,
          "standard_error": 57.24959838324621
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271601.2673916134,
            "upper_bound": 272003.1195703403
          },
          "point_estimate": 271804.3308821962,
          "standard_error": 102.98206311495544
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271522.8091684435,
            "upper_bound": 272080.6473880597
          },
          "point_estimate": 271780.4471393035,
          "standard_error": 137.69312397673468
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.04186211768657,
            "upper_bound": 621.5869540764912
          },
          "point_estimate": 312.07991834007004,
          "standard_error": 146.4319067259512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 271411.47952296864,
            "upper_bound": 272019.62942250975
          },
          "point_estimate": 271664.5281256057,
          "standard_error": 157.2848861777156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 198.1104058371753,
            "upper_bound": 427.28881610911304
          },
          "point_estimate": 343.4464222720835,
          "standard_error": 59.21178870427073
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145456.2966777778,
            "upper_bound": 145598.28417626585
          },
          "point_estimate": 145526.23123063493,
          "standard_error": 36.299788512848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145441.29822222222,
            "upper_bound": 145638.8205
          },
          "point_estimate": 145517.56691666666,
          "standard_error": 47.885766699247654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.53368016877103,
            "upper_bound": 205.58884368340568
          },
          "point_estimate": 107.92705116390084,
          "standard_error": 50.102105975556405
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145389.46900702652,
            "upper_bound": 145587.708503937
          },
          "point_estimate": 145470.9192,
          "standard_error": 51.07134713488835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.61362497771424,
            "upper_bound": 154.14846637730088
          },
          "point_estimate": 121.33060312341254,
          "standard_error": 22.429744274647454
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155923.08025648887,
            "upper_bound": 156330.2410944206
          },
          "point_estimate": 156107.05763846316,
          "standard_error": 103.18655909777436
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155957.6487839771,
            "upper_bound": 156158.71330472102
          },
          "point_estimate": 156106.27968526466,
          "standard_error": 60.11413821551608
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.18514568125803,
            "upper_bound": 440.02978632952994
          },
          "point_estimate": 126.60693230163558,
          "standard_error": 97.82627348941176
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155953.64450426455,
            "upper_bound": 156133.5908202986
          },
          "point_estimate": 156054.81210634857,
          "standard_error": 45.52008004843861
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.29558552266909,
            "upper_bound": 505.038344895002
          },
          "point_estimate": 342.99586605128934,
          "standard_error": 117.72132737952714
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152058.71158049416,
            "upper_bound": 152339.86518131103
          },
          "point_estimate": 152201.16111559406,
          "standard_error": 71.94737598736764
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152071.39031679617,
            "upper_bound": 152402.1237796374
          },
          "point_estimate": 152138.6623953975,
          "standard_error": 96.1330146359962
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.09978659857177,
            "upper_bound": 460.66533399031863
          },
          "point_estimate": 187.70811591438368,
          "standard_error": 110.85268748053996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152093.67437889555,
            "upper_bound": 152312.1437914387
          },
          "point_estimate": 152219.60467315113,
          "standard_error": 55.83736777910625
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.5030463050226,
            "upper_bound": 313.2001028337955
          },
          "point_estimate": 239.9230194721908,
          "standard_error": 47.72452287094126
        }
      }
    },
    "memmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90728.7881296758,
            "upper_bound": 90892.61533799431
          },
          "point_estimate": 90807.84583719273,
          "standard_error": 41.88208065190189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90706.80914380716,
            "upper_bound": 90910.63056644105
          },
          "point_estimate": 90782.3380299252,
          "standard_error": 53.549757719647545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.98723320488386,
            "upper_bound": 228.04283759732405
          },
          "point_estimate": 134.37032791370797,
          "standard_error": 51.40158529969096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90754.00048261402,
            "upper_bound": 90872.09578125692
          },
          "point_estimate": 90807.00795414062,
          "standard_error": 29.903481674723164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.8958813758255,
            "upper_bound": 178.16443401670696
          },
          "point_estimate": 139.89948117371418,
          "standard_error": 26.67146219160916
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28386.61138547867,
            "upper_bound": 28423.938477678566
          },
          "point_estimate": 28403.69695771329,
          "standard_error": 9.609107042569883
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28382.489453125,
            "upper_bound": 28423.216841517857
          },
          "point_estimate": 28395.28955078125,
          "standard_error": 9.401494510492023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1880862875043925,
            "upper_bound": 50.91823589289489
          },
          "point_estimate": 22.95076342066696,
          "standard_error": 11.626251884619409
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28384.260142881947,
            "upper_bound": 28411.904917557407
          },
          "point_estimate": 28397.65442573052,
          "standard_error": 7.121514512261636
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.36440556840207,
            "upper_bound": 41.23365014825973
          },
          "point_estimate": 31.97138447693884,
          "standard_error": 7.902711345720384
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1988176.2796491229,
            "upper_bound": 1990263.0790956556
          },
          "point_estimate": 1989213.8005868837,
          "standard_error": 535.5280728676464
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1987831.429824561,
            "upper_bound": 1990933.3421052631
          },
          "point_estimate": 1988923.2578947367,
          "standard_error": 690.219832858257
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 278.61044715898925,
            "upper_bound": 3282.39886751523
          },
          "point_estimate": 1883.3623528792984,
          "standard_error": 872.2691624134571
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1988506.1871521703,
            "upper_bound": 1990987.5608640823
          },
          "point_estimate": 1989853.2144907725,
          "standard_error": 634.5230759916241
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015.7972458026596,
            "upper_bound": 2207.233603218441
          },
          "point_estimate": 1788.5032155955305,
          "standard_error": 302.27449786280647
        }
      }
    },
    "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3982.629061716792,
            "upper_bound": 3988.9086731255225
          },
          "point_estimate": 3985.5715120091895,
          "standard_error": 1.6124225675800727
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3981.222445175438,
            "upper_bound": 3989.9372368421054
          },
          "point_estimate": 3984.636564066416,
          "standard_error": 2.2333498670420733
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6354967540684087,
            "upper_bound": 9.181593702125758
          },
          "point_estimate": 5.298552355493455,
          "standard_error": 2.0330349750949215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3981.706175057013,
            "upper_bound": 3987.5367452874207
          },
          "point_estimate": 3984.0270129870128,
          "standard_error": 1.5082984473983156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.672936041417819,
            "upper_bound": 7.0773244930727985
          },
          "point_estimate": 5.397121644184846,
          "standard_error": 1.182230218842284
        }
      }
    },
    "memrchr1/krate/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/krate/empty/never",
        "directory_name": "memrchr1/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8557877824291316,
            "upper_bound": 0.8571075303443694
          },
          "point_estimate": 0.8564103137035062,
          "standard_error": 0.000338931434726996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.855673230762624,
            "upper_bound": 0.8573259913343614
          },
          "point_estimate": 0.8558234144736876,
          "standard_error": 0.0004919578999534115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00003841034508965333,
            "upper_bound": 0.0018244401429585888
          },
          "point_estimate": 0.0006754930740818789,
          "standard_error": 0.0005302385587667608
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8557614700345777,
            "upper_bound": 0.8565663636490799
          },
          "point_estimate": 0.8560936327029801,
          "standard_error": 0.00020450588991512825
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005209064285049254,
            "upper_bound": 0.0014034289214707905
          },
          "point_estimate": 0.001134957827814138,
          "standard_error": 0.00022263011083617517
        }
      }
    },
    "memrchr1/krate/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/common",
        "directory_name": "memrchr1/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224765.2757725052,
            "upper_bound": 225268.77741769553
          },
          "point_estimate": 225018.14714653147,
          "standard_error": 128.71262183703843
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224799.3353909465,
            "upper_bound": 225344.5456790123
          },
          "point_estimate": 224942.99265138153,
          "standard_error": 154.32663908354772
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.70623908364914,
            "upper_bound": 825.9267018183542
          },
          "point_estimate": 230.98260423255252,
          "standard_error": 201.1468068532997
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224871.6579333889,
            "upper_bound": 225227.85051052665
          },
          "point_estimate": 225053.7455667789,
          "standard_error": 94.1952870176039
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 221.95346210920368,
            "upper_bound": 564.215386288686
          },
          "point_estimate": 429.6036200452256,
          "standard_error": 88.53646311346755
        }
      }
    },
    "memrchr1/krate/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/never",
        "directory_name": "memrchr1/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9682.537054960689,
            "upper_bound": 9694.53429437268
          },
          "point_estimate": 9687.818596556965,
          "standard_error": 3.108952034330693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9681.15739752823,
            "upper_bound": 9691.741203580214
          },
          "point_estimate": 9685.869979613612,
          "standard_error": 2.7079481475250384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.7637660223001044,
            "upper_bound": 13.27536872048037
          },
          "point_estimate": 6.247866376141012,
          "standard_error": 2.952334916177041
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9684.41537017431,
            "upper_bound": 9691.109622041617
          },
          "point_estimate": 9688.141005913263,
          "standard_error": 1.7031628630359446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.7518126594194183,
            "upper_bound": 14.811243003797532
          },
          "point_estimate": 10.354528113616698,
          "standard_error": 3.2848951997719484
        }
      }
    },
    "memrchr1/krate/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/rare",
        "directory_name": "memrchr1/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9770.910558622858,
            "upper_bound": 9781.444810201394
          },
          "point_estimate": 9775.998689813568,
          "standard_error": 2.7026529655055853
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9769.257743064907,
            "upper_bound": 9783.120576353353
          },
          "point_estimate": 9774.534035820092,
          "standard_error": 3.0661546601469856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4548356908211968,
            "upper_bound": 15.247001428745495
          },
          "point_estimate": 9.039772008243384,
          "standard_error": 3.8179135125241936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9768.117323757346,
            "upper_bound": 9779.9139791445
          },
          "point_estimate": 9774.565271894817,
          "standard_error": 3.057058478071181
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.622760726413304,
            "upper_bound": 11.631325032618747
          },
          "point_estimate": 8.997814200686975,
          "standard_error": 1.7993897632489682
        }
      }
    },
    "memrchr1/krate/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/uncommon",
        "directory_name": "memrchr1/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79776.53494776002,
            "upper_bound": 79966.21333028535
          },
          "point_estimate": 79866.54722143902,
          "standard_error": 48.7482080507693
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79726.59978070176,
            "upper_bound": 79975.2462406015
          },
          "point_estimate": 79842.3375,
          "standard_error": 67.03343246296672
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.562672146681365,
            "upper_bound": 269.285464627128
          },
          "point_estimate": 182.1525253845656,
          "standard_error": 62.85089387296705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79749.74829346093,
            "upper_bound": 79861.38839632065
          },
          "point_estimate": 79797.88787878788,
          "standard_error": 28.265477536031995
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.37036943388135,
            "upper_bound": 204.05010088282597
          },
          "point_estimate": 162.09370693163729,
          "standard_error": 31.78711479458248
        }
      }
    },
    "memrchr1/krate/huge/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/krate/huge/verycommon",
        "directory_name": "memrchr1/krate_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463240.4913554852,
            "upper_bound": 463614.8507172996
          },
          "point_estimate": 463415.6456715894,
          "standard_error": 96.14096464577716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463180.64556962025,
            "upper_bound": 463683.4433895921
          },
          "point_estimate": 463337.4845991561,
          "standard_error": 117.8373171584208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.88591083943393,
            "upper_bound": 531.7560627112629
          },
          "point_estimate": 245.73101471963545,
          "standard_error": 120.083551637812
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 463198.39694291854,
            "upper_bound": 463684.4070691251
          },
          "point_estimate": 463440.0851882295,
          "standard_error": 125.42038283823008
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 130.69108590804248,
            "upper_bound": 401.3878620367858
          },
          "point_estimate": 320.176418904963,
          "standard_error": 68.71297334677877
        }
      }
    },
    "memrchr1/krate/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/common",
        "directory_name": "memrchr1/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.30036870160595,
            "upper_bound": 227.59832430155996
          },
          "point_estimate": 227.4470514416733,
          "standard_error": 0.0761571159166328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.2248337722289,
            "upper_bound": 227.59266255640833
          },
          "point_estimate": 227.47998386198873,
          "standard_error": 0.09712349829625076
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05200507254221541,
            "upper_bound": 0.47275730225109486
          },
          "point_estimate": 0.19675751459958077,
          "standard_error": 0.10213781808741192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.36375670487408,
            "upper_bound": 227.5610941050952
          },
          "point_estimate": 227.47745013897975,
          "standard_error": 0.050015052025909536
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1355261889143743,
            "upper_bound": 0.33225025648797357
          },
          "point_estimate": 0.25342531311390276,
          "standard_error": 0.05156008094019416
        }
      }
    },
    "memrchr1/krate/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/never",
        "directory_name": "memrchr1/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.716732743030919,
            "upper_bound": 8.74798341414055
          },
          "point_estimate": 8.732655298656642,
          "standard_error": 0.007993430232090258
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.706383900578539,
            "upper_bound": 8.75283843918875
          },
          "point_estimate": 8.742394187376462,
          "standard_error": 0.014364401117726676
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002425497832501041,
            "upper_bound": 0.041383962360752306
          },
          "point_estimate": 0.02826512496050185,
          "standard_error": 0.011121666607387098
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.717274561323869,
            "upper_bound": 8.74909962618706
          },
          "point_estimate": 8.737248436067361,
          "standard_error": 0.008187448133028186
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016407802629325192,
            "upper_bound": 0.031314795511919236
          },
          "point_estimate": 0.02651976176165522,
          "standard_error": 0.003799106314824726
        }
      }
    },
    "memrchr1/krate/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/rare",
        "directory_name": "memrchr1/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.693790834574749,
            "upper_bound": 13.734726466119426
          },
          "point_estimate": 13.711150643029592,
          "standard_error": 0.010658254433729102
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.688837151502163,
            "upper_bound": 13.718642983928802
          },
          "point_estimate": 13.705957095220777,
          "standard_error": 0.007962519441911097
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002159492627005942,
            "upper_bound": 0.03760454013519807
          },
          "point_estimate": 0.017373158996409673,
          "standard_error": 0.008601940362032202
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 13.69897317201728,
            "upper_bound": 13.715039262225586
          },
          "point_estimate": 13.70719963737024,
          "standard_error": 0.004191736182480383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010739158730712116,
            "upper_bound": 0.05259747098194283
          },
          "point_estimate": 0.035639969241285374,
          "standard_error": 0.013060134961569344
        }
      }
    },
    "memrchr1/krate/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/uncommon",
        "directory_name": "memrchr1/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.92080402151032,
            "upper_bound": 47.015991669203466
          },
          "point_estimate": 46.9649243744432,
          "standard_error": 0.0244703384111642
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.90518925361429,
            "upper_bound": 47.02669473051482
          },
          "point_estimate": 46.9398679274975,
          "standard_error": 0.027334750815931223
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012623755799705474,
            "upper_bound": 0.13268053095215893
          },
          "point_estimate": 0.06378785622370035,
          "standard_error": 0.02923347075714761
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 46.90295110320348,
            "upper_bound": 46.94918459793811
          },
          "point_estimate": 46.92138576026634,
          "standard_error": 0.01178724179556765
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031678137648770574,
            "upper_bound": 0.10387904288157088
          },
          "point_estimate": 0.08154947648848718,
          "standard_error": 0.018461170909204124
        }
      }
    },
    "memrchr1/krate/small/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/krate/small/verycommon",
        "directory_name": "memrchr1/krate_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.5819814021236,
            "upper_bound": 496.66054653037503
          },
          "point_estimate": 496.07130680435984,
          "standard_error": 0.2769664513348433
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.4662939732752,
            "upper_bound": 496.6540196345787
          },
          "point_estimate": 495.8706489886201,
          "standard_error": 0.2255378962673376
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.037059143283557375,
            "upper_bound": 1.3083244447300586
          },
          "point_estimate": 0.4382312020643687,
          "standard_error": 0.35638606892992397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 495.6815434202951,
            "upper_bound": 496.2235704821924
          },
          "point_estimate": 495.9292434454011,
          "standard_error": 0.1353326169762746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3008769336734092,
            "upper_bound": 1.2246309814896958
          },
          "point_estimate": 0.9277685543475944,
          "standard_error": 0.24313982610394672
        }
      }
    },
    "memrchr1/krate/tiny/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/common",
        "directory_name": "memrchr1/krate_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.46031447969087,
            "upper_bound": 51.58495813072163
          },
          "point_estimate": 51.51271001998483,
          "standard_error": 0.032688292182425784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.45303195743701,
            "upper_bound": 51.53196957382397
          },
          "point_estimate": 51.49152160929978,
          "standard_error": 0.019999881689954747
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010990277135059434,
            "upper_bound": 0.11161387918249269
          },
          "point_estimate": 0.05851645398877803,
          "standard_error": 0.02668862181347944
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.48076733178115,
            "upper_bound": 51.53586047373564
          },
          "point_estimate": 51.50235637656464,
          "standard_error": 0.013967109869006743
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030051215656738132,
            "upper_bound": 0.16047517317066762
          },
          "point_estimate": 0.10848007048011292,
          "standard_error": 0.0404988213119566
        }
      }
    },
    "memrchr1/krate/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/never",
        "directory_name": "memrchr1/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.327974908612144,
            "upper_bound": 4.348060406606637
          },
          "point_estimate": 4.33862615027522,
          "standard_error": 0.005169682063624911
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.321949564913361,
            "upper_bound": 4.352010933187386
          },
          "point_estimate": 4.346290811980368,
          "standard_error": 0.00708956492480737
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0018662812586710672,
            "upper_bound": 0.030436482299959412
          },
          "point_estimate": 0.01272970195476479,
          "standard_error": 0.007142621423482389
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.33525154201755,
            "upper_bound": 4.348277412706731
          },
          "point_estimate": 4.34328799780249,
          "standard_error": 0.00337393450417705
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007102519025747731,
            "upper_bound": 0.02126232220841913
          },
          "point_estimate": 0.017258290047364816,
          "standard_error": 0.003582976815838971
        }
      }
    },
    "memrchr1/krate/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/rare",
        "directory_name": "memrchr1/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.6919028433947645,
            "upper_bound": 7.702999933285189
          },
          "point_estimate": 7.697324900567104,
          "standard_error": 0.002839037082771605
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.688940213032298,
            "upper_bound": 7.706609940760488
          },
          "point_estimate": 7.694965309002645,
          "standard_error": 0.005386670639759017
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0008448932564486253,
            "upper_bound": 0.01535667285451927
          },
          "point_estimate": 0.01162967231010795,
          "standard_error": 0.00376433382613066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.69048083642686,
            "upper_bound": 7.700500749254088
          },
          "point_estimate": 7.694348305073299,
          "standard_error": 0.0025394478142699256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005989272746453418,
            "upper_bound": 0.01097948659613694
          },
          "point_estimate": 0.00945384583367532,
          "standard_error": 0.0012622858959688023
        }
      }
    },
    "memrchr1/krate/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/krate/tiny/uncommon",
        "directory_name": "memrchr1/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.57422824316343,
            "upper_bound": 28.634743136451124
          },
          "point_estimate": 28.6008824308589,
          "standard_error": 0.015648698713776603
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.56660890487188,
            "upper_bound": 28.629685071330133
          },
          "point_estimate": 28.587531277728132,
          "standard_error": 0.014098711735522856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00401703292398394,
            "upper_bound": 0.06820530142104538
          },
          "point_estimate": 0.03470397423151347,
          "standard_error": 0.017985972263972597
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.580447932518293,
            "upper_bound": 28.615225138459717
          },
          "point_estimate": 28.59668121541096,
          "standard_error": 0.00903026777624714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017527199434805282,
            "upper_bound": 0.07374233917179328
          },
          "point_estimate": 0.0521854033864523,
          "standard_error": 0.016144055516126042
        }
      }
    },
    "memrchr1/libc/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr1/libc/empty/never",
        "directory_name": "memrchr1/libc_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.49085496064116446,
            "upper_bound": 0.4997754735734456
          },
          "point_estimate": 0.4948181115569886,
          "standard_error": 0.002299349141061818
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4896376299358358,
            "upper_bound": 0.4999536220123958
          },
          "point_estimate": 0.49105219636672853,
          "standard_error": 0.0028022146150372767
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003589294941683829,
            "upper_bound": 0.011859941547275692
          },
          "point_estimate": 0.0027883916618377624,
          "standard_error": 0.0031400892606326058
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4896434566690523,
            "upper_bound": 0.4944461787271212
          },
          "point_estimate": 0.4913329204431836,
          "standard_error": 0.0012403938056808983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002083193017755425,
            "upper_bound": 0.010470414302890842
          },
          "point_estimate": 0.007665302266655265,
          "standard_error": 0.0021362475057861976
        }
      }
    },
    "memrchr1/libc/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/common",
        "directory_name": "memrchr1/libc_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263079.083938849,
            "upper_bound": 263387.1896522782
          },
          "point_estimate": 263236.1226998401,
          "standard_error": 78.73366312386742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263091.6570743405,
            "upper_bound": 263437.21942446043
          },
          "point_estimate": 263203.0749100719,
          "standard_error": 99.04857843351478
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.71394684889184,
            "upper_bound": 465.2756627229103
          },
          "point_estimate": 221.36663467909145,
          "standard_error": 104.88305296861056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 263135.0584425571,
            "upper_bound": 263365.599930041
          },
          "point_estimate": 263232.10387741757,
          "standard_error": 58.952673292016335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.47834447487594,
            "upper_bound": 350.72628297105314
          },
          "point_estimate": 262.85488178154066,
          "standard_error": 57.607846108439865
        }
      }
    },
    "memrchr1/libc/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/never",
        "directory_name": "memrchr1/libc_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9552.190602156192,
            "upper_bound": 9565.946422829566
          },
          "point_estimate": 9559.190723332456,
          "standard_error": 3.522919627653971
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9548.967076430888,
            "upper_bound": 9567.79559996494
          },
          "point_estimate": 9558.890560084144,
          "standard_error": 4.290806025610969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.6004301931160594,
            "upper_bound": 20.36129953859391
          },
          "point_estimate": 11.08390648741834,
          "standard_error": 5.081348290263101
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9553.183073122334,
            "upper_bound": 9562.938601475638
          },
          "point_estimate": 9558.654359681865,
          "standard_error": 2.4668110625157165
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.148541026425062,
            "upper_bound": 15.001624680365271
          },
          "point_estimate": 11.74975923800232,
          "standard_error": 2.222618581102726
        }
      }
    },
    "memrchr1/libc/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/rare",
        "directory_name": "memrchr1/libc_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9940.67192694159,
            "upper_bound": 9963.818174168044
          },
          "point_estimate": 9951.991843214362,
          "standard_error": 5.929204827933587
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9935.07006133052,
            "upper_bound": 9975.590101175829
          },
          "point_estimate": 9948.695811685351,
          "standard_error": 9.805115001749364
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.527756623653391,
            "upper_bound": 33.207888007652876
          },
          "point_estimate": 23.06928126667701,
          "standard_error": 7.996530809516947
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9942.160654101948,
            "upper_bound": 9968.589075343803
          },
          "point_estimate": 9956.071098657972,
          "standard_error": 7.108849546562194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.500674080409954,
            "upper_bound": 23.365883170586972
          },
          "point_estimate": 19.847177578588894,
          "standard_error": 2.921092395349309
        }
      }
    },
    "memrchr1/libc/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/uncommon",
        "directory_name": "memrchr1/libc_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78334.75747787763,
            "upper_bound": 78706.00570343973
          },
          "point_estimate": 78511.03123248287,
          "standard_error": 95.8389582310976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78254.06973969631,
            "upper_bound": 78828.81959508316
          },
          "point_estimate": 78369.87369073443,
          "standard_error": 158.68844831923025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.664032247981662,
            "upper_bound": 507.8071072536031
          },
          "point_estimate": 233.6084226349698,
          "standard_error": 143.12803507078368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78245.27866492514,
            "upper_bound": 78432.71527036522
          },
          "point_estimate": 78302.32658534525,
          "standard_error": 48.88034251642752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 158.91602474021494,
            "upper_bound": 381.3100270078987
          },
          "point_estimate": 319.24400102901046,
          "standard_error": 55.701409206542664
        }
      }
    },
    "memrchr1/libc/huge/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/huge/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/huge/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr1/libc/huge/verycommon",
        "directory_name": "memrchr1/libc_huge_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549832.0251424722,
            "upper_bound": 550422.3783836768
          },
          "point_estimate": 550116.7085684671,
          "standard_error": 150.1461801015849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549670.8525230987,
            "upper_bound": 550390.0298507463
          },
          "point_estimate": 550227.6012437812,
          "standard_error": 244.171939421174
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.05639521266922,
            "upper_bound": 1010.0539635978228
          },
          "point_estimate": 508.1344025459937,
          "standard_error": 254.35589846803467
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 549641.1600819433,
            "upper_bound": 550182.7954041673
          },
          "point_estimate": 549890.512890095,
          "standard_error": 139.54506505806813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.7827550009125,
            "upper_bound": 651.3083128556946
          },
          "point_estimate": 500.7218455749016,
          "standard_error": 99.08902456547445
        }
      }
    },
    "memrchr1/libc/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/common",
        "directory_name": "memrchr1/libc_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.3796774957337,
            "upper_bound": 238.7159373811678
          },
          "point_estimate": 238.53775608904772,
          "standard_error": 0.0860774404335665
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.3093858939272,
            "upper_bound": 238.7379053407288
          },
          "point_estimate": 238.45405865229537,
          "standard_error": 0.13916724244075548
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01669341705544552,
            "upper_bound": 0.5210940633986176
          },
          "point_estimate": 0.2616226939115832,
          "standard_error": 0.13086712733252737
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238.30404412792265,
            "upper_bound": 238.570914147483
          },
          "point_estimate": 238.3934428041308,
          "standard_error": 0.0684746742219786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15479850692142394,
            "upper_bound": 0.3710949799523708
          },
          "point_estimate": 0.28728247249687644,
          "standard_error": 0.05918160875525271
        }
      }
    },
    "memrchr1/libc/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/never",
        "directory_name": "memrchr1/libc_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.990462249938544,
            "upper_bound": 7.997908708335756
          },
          "point_estimate": 7.994193648320048,
          "standard_error": 0.0019091687104362963
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.98771949340136,
            "upper_bound": 8.000740412475139
          },
          "point_estimate": 7.994116004077338,
          "standard_error": 0.0031389431504960934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016685857072049714,
            "upper_bound": 0.010451385771130415
          },
          "point_estimate": 0.00965240713802781,
          "standard_error": 0.002339042662084992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.989574821414021,
            "upper_bound": 8.000294473991062
          },
          "point_estimate": 7.995066954521674,
          "standard_error": 0.0028204211956125353
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003948219328217323,
            "upper_bound": 0.007578638828950371
          },
          "point_estimate": 0.006358653592833416,
          "standard_error": 0.000915241871988932
        }
      }
    },
    "memrchr1/libc/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/rare",
        "directory_name": "memrchr1/libc_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.33692622745934,
            "upper_bound": 14.368821556036515
          },
          "point_estimate": 14.350707185825826,
          "standard_error": 0.008244168839764726
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.333548166416207,
            "upper_bound": 14.355959915008372
          },
          "point_estimate": 14.348133709922472,
          "standard_error": 0.006178388247428258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003763265507831709,
            "upper_bound": 0.031018754334044062
          },
          "point_estimate": 0.015093595987343448,
          "standard_error": 0.006922287066381928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.33693444395304,
            "upper_bound": 14.352685135936625
          },
          "point_estimate": 14.34463152326702,
          "standard_error": 0.004114508062256059
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008365718029602566,
            "upper_bound": 0.04024197302440105
          },
          "point_estimate": 0.027494459768809563,
          "standard_error": 0.009689992723723609
        }
      }
    },
    "memrchr1/libc/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/uncommon",
        "directory_name": "memrchr1/libc_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.99663992069716,
            "upper_bound": 48.075763353326515
          },
          "point_estimate": 48.03256538764784,
          "standard_error": 0.020327477409262815
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.9801372259343,
            "upper_bound": 48.067352466277725
          },
          "point_estimate": 48.020447307986075,
          "standard_error": 0.02523887462405275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010282416869954078,
            "upper_bound": 0.10614749150438955
          },
          "point_estimate": 0.06431028795161814,
          "standard_error": 0.023001579504203742
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.003910874530064,
            "upper_bound": 48.06034846260482
          },
          "point_estimate": 48.03447875284265,
          "standard_error": 0.014863196993000378
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030435873678646943,
            "upper_bound": 0.0936496008035942
          },
          "point_estimate": 0.06804779941777367,
          "standard_error": 0.01828089005755978
        }
      }
    },
    "memrchr1/libc/small/verycommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/small/verycommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/small/verycommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr1/libc/small/verycommon",
        "directory_name": "memrchr1/libc_small_verycommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578.9767401137627,
            "upper_bound": 579.8704869639082
          },
          "point_estimate": 579.4119707397072,
          "standard_error": 0.2287156510644975
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578.8940419661922,
            "upper_bound": 579.9823935682391
          },
          "point_estimate": 579.2340174426633,
          "standard_error": 0.27334513738715027
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11956855296177642,
            "upper_bound": 1.2463532738669325
          },
          "point_estimate": 0.5951430575156924,
          "standard_error": 0.2978457029589859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579.1091573657411,
            "upper_bound": 579.5754573678212
          },
          "point_estimate": 579.3270622299718,
          "standard_error": 0.11839783655422026
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3805568771967227,
            "upper_bound": 0.9840440233800752
          },
          "point_estimate": 0.7631044094550751,
          "standard_error": 0.15264622782659645
        }
      }
    },
    "memrchr1/libc/tiny/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/tiny/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/common",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/common",
        "directory_name": "memrchr1/libc_tiny_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.61449804521503,
            "upper_bound": 50.66685690189314
          },
          "point_estimate": 50.64090743281073,
          "standard_error": 0.013387917305765698
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.609553195296336,
            "upper_bound": 50.66620786097175
          },
          "point_estimate": 50.6451335993778,
          "standard_error": 0.013986666876937764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007559306595614654,
            "upper_bound": 0.07794415984307684
          },
          "point_estimate": 0.033371230414760956,
          "standard_error": 0.018755675584333623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.61074000841298,
            "upper_bound": 50.69254696426978
          },
          "point_estimate": 50.65502065484723,
          "standard_error": 0.02136908786028792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020868761988970135,
            "upper_bound": 0.05905332912312715
          },
          "point_estimate": 0.0447322488544613,
          "standard_error": 0.009562643482034791
        }
      }
    },
    "memrchr1/libc/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/never",
        "directory_name": "memrchr1/libc_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9321811117808343,
            "upper_bound": 2.9369834321464396
          },
          "point_estimate": 2.934418406026241,
          "standard_error": 0.0012348867770500583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.9316160976423955,
            "upper_bound": 2.9377166459846302
          },
          "point_estimate": 2.93312002859757,
          "standard_error": 0.0014363689472482013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0004028073295551725,
            "upper_bound": 0.006787234440753
          },
          "point_estimate": 0.002444217732120322,
          "standard_error": 0.001651199499441687
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.932167453113146,
            "upper_bound": 2.9353122591667917
          },
          "point_estimate": 2.9333780488401073,
          "standard_error": 0.0008072821310609407
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001572851046819941,
            "upper_bound": 0.005347269034427514
          },
          "point_estimate": 0.0041198617133852535,
          "standard_error": 0.0009501906535479232
        }
      }
    },
    "memrchr1/libc/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/rare",
        "directory_name": "memrchr1/libc_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.854288659711295,
            "upper_bound": 5.869118628961948
          },
          "point_estimate": 5.863319369095334,
          "standard_error": 0.00405034968844728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.863463872721471,
            "upper_bound": 5.870255974764019
          },
          "point_estimate": 5.865902863166811,
          "standard_error": 0.0022111696101478296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007757749322724857,
            "upper_bound": 0.009433911608363214
          },
          "point_estimate": 0.003921152022142267,
          "standard_error": 0.0023272027717155578
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.864060181432659,
            "upper_bound": 5.8687554588463025
          },
          "point_estimate": 5.865653519735295,
          "standard_error": 0.0011972114391881382
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002607090688698998,
            "upper_bound": 0.020600587258160073
          },
          "point_estimate": 0.013512733107570632,
          "standard_error": 0.006060294062904252
        }
      }
    },
    "memrchr1/libc/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr1/libc/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr1",
        "function_id": "libc/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr1/libc/tiny/uncommon",
        "directory_name": "memrchr1/libc_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.654121106088038,
            "upper_bound": 17.680943463770287
          },
          "point_estimate": 17.667083157460574,
          "standard_error": 0.0068844453112344136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.652801930018597,
            "upper_bound": 17.682386028026592
          },
          "point_estimate": 17.662400516091495,
          "standard_error": 0.006860382195194996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001023706703264544,
            "upper_bound": 0.03773042957845205
          },
          "point_estimate": 0.018105662518816346,
          "standard_error": 0.010556140875813674
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.655510576005966,
            "upper_bound": 17.68211848025355
          },
          "point_estimate": 17.66912696632601,
          "standard_error": 0.006771064131181301
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011169781722244755,
            "upper_bound": 0.030115301698090575
          },
          "point_estimate": 0.0229236064626326,
          "standard_error": 0.004864194177699158
        }
      }
    },
    "memrchr2/krate/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr2/krate/empty/never",
        "directory_name": "memrchr2/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9832486394695464,
            "upper_bound": 0.9847719766438966
          },
          "point_estimate": 0.9839545448510564,
          "standard_error": 0.00039018076230053287
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.98301389582779,
            "upper_bound": 0.9845875471116332
          },
          "point_estimate": 0.983791679175778,
          "standard_error": 0.0004110806607477556
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002538993775287535,
            "upper_bound": 0.001986207620967373
          },
          "point_estimate": 0.0009835987862061506,
          "standard_error": 0.00042816430242567215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9829408891332944,
            "upper_bound": 0.9847683508562404
          },
          "point_estimate": 0.9837489239896666,
          "standard_error": 0.0004595036377474498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0005733957048968943,
            "upper_bound": 0.0017735376304661918
          },
          "point_estimate": 0.0012948810761233102,
          "standard_error": 0.00033337267787410404
        }
      }
    },
    "memrchr2/krate/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/common",
        "directory_name": "memrchr2/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445290.2872764228,
            "upper_bound": 445739.4980352304
          },
          "point_estimate": 445526.9463685636,
          "standard_error": 114.60601080389335
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445264.5871951219,
            "upper_bound": 445790.256097561
          },
          "point_estimate": 445620.962398374,
          "standard_error": 145.54836167512164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.82113997529929,
            "upper_bound": 636.1113267556177
          },
          "point_estimate": 349.5717611109361,
          "standard_error": 140.23921599895775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 445047.02416951774,
            "upper_bound": 445738.05815147626
          },
          "point_estimate": 445336.82144440926,
          "standard_error": 174.800959841202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 186.91334206191976,
            "upper_bound": 502.9964636372378
          },
          "point_estimate": 381.65402746584584,
          "standard_error": 83.99149699100631
        }
      }
    },
    "memrchr2/krate/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/never",
        "directory_name": "memrchr2/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12491.336449348091,
            "upper_bound": 12511.564990407776
          },
          "point_estimate": 12502.031627858072,
          "standard_error": 5.170769841123347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12492.979869236064,
            "upper_bound": 12513.706002905868
          },
          "point_estimate": 12502.228504244093,
          "standard_error": 4.632419854661907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.5402679767851648,
            "upper_bound": 28.299683950437487
          },
          "point_estimate": 11.703917651815177,
          "standard_error": 7.62131134385533
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12501.567199285788,
            "upper_bound": 12516.115846954805
          },
          "point_estimate": 12509.749222834977,
          "standard_error": 3.839167448353061
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8798767484890355,
            "upper_bound": 23.245408401097155
          },
          "point_estimate": 17.297257231704705,
          "standard_error": 4.146322449898666
        }
      }
    },
    "memrchr2/krate/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/rare",
        "directory_name": "memrchr2/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16895.832925318868,
            "upper_bound": 16961.47324667337
          },
          "point_estimate": 16926.801065077143,
          "standard_error": 16.836993555728395
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16883.225886607557,
            "upper_bound": 16971.186604226386
          },
          "point_estimate": 16901.698086794215,
          "standard_error": 27.496530924221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.783880473100383,
            "upper_bound": 95.79066361280046
          },
          "point_estimate": 42.97611436768026,
          "standard_error": 24.189112589033385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16915.28130207986,
            "upper_bound": 16996.1592867133
          },
          "point_estimate": 16964.12743877681,
          "standard_error": 20.40697173521249
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.86050670479263,
            "upper_bound": 70.41159294421882
          },
          "point_estimate": 56.256941924000905,
          "standard_error": 10.62658848607932
        }
      }
    },
    "memrchr2/krate/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr2/krate/huge/uncommon",
        "directory_name": "memrchr2/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164913.96588541663,
            "upper_bound": 165613.48038419912
          },
          "point_estimate": 165255.67892352093,
          "standard_error": 179.11866896538965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164801.01515151514,
            "upper_bound": 165708.01295454544
          },
          "point_estimate": 165205.64160353533,
          "standard_error": 210.82099387715735
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.97429790415595,
            "upper_bound": 1037.2495175396955
          },
          "point_estimate": 591.0892473242606,
          "standard_error": 238.12325578544272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165202.63057871634,
            "upper_bound": 165897.57381360375
          },
          "point_estimate": 165569.87943329397,
          "standard_error": 180.53474924987503
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 326.8510224605457,
            "upper_bound": 756.4856905076371
          },
          "point_estimate": 596.9494433962591,
          "standard_error": 110.49467035469787
        }
      }
    },
    "memrchr2/krate/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/common",
        "directory_name": "memrchr2/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462.31939291019904,
            "upper_bound": 462.8666387944488
          },
          "point_estimate": 462.6239785539554,
          "standard_error": 0.14141656257479063
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462.48306444424634,
            "upper_bound": 462.91614296608503
          },
          "point_estimate": 462.6598924443652,
          "standard_error": 0.10717967993905504
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.051869194194533584,
            "upper_bound": 0.6030368838478305
          },
          "point_estimate": 0.28879326254384663,
          "standard_error": 0.13891520510417316
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462.49969805816096,
            "upper_bound": 462.8777642262895
          },
          "point_estimate": 462.72636815262206,
          "standard_error": 0.097136358008892
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15504725579489864,
            "upper_bound": 0.6761319187289281
          },
          "point_estimate": 0.47110510177735376,
          "standard_error": 0.15278001537317462
        }
      }
    },
    "memrchr2/krate/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/never",
        "directory_name": "memrchr2/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.9574303402363,
            "upper_bound": 12.972699579732932
          },
          "point_estimate": 12.965076102570412,
          "standard_error": 0.003912361467241134
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.950303480455716,
            "upper_bound": 12.977107897192298
          },
          "point_estimate": 12.965020753968451,
          "standard_error": 0.0056887850087873025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001467127106422959,
            "upper_bound": 0.023754968126547583
          },
          "point_estimate": 0.016408573603596308,
          "standard_error": 0.006156480992657152
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.953354176243495,
            "upper_bound": 12.965924183847909
          },
          "point_estimate": 12.959836786757052,
          "standard_error": 0.0032008825033865928
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007810537775659611,
            "upper_bound": 0.015828519899423205
          },
          "point_estimate": 0.013003885943872808,
          "standard_error": 0.0020614127409619783
        }
      }
    },
    "memrchr2/krate/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/rare",
        "directory_name": "memrchr2/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.539100165370705,
            "upper_bound": 23.56825466710894
          },
          "point_estimate": 23.555403268922515,
          "standard_error": 0.007552679358822078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.54034150570391,
            "upper_bound": 23.571188812548613
          },
          "point_estimate": 23.567147973073077,
          "standard_error": 0.007947204919653917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001219944304672169,
            "upper_bound": 0.03581459483074406
          },
          "point_estimate": 0.010797091006134742,
          "standard_error": 0.009186022686055338
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.54987159741866,
            "upper_bound": 23.572605889862523
          },
          "point_estimate": 23.56390755213156,
          "standard_error": 0.005926101033360758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006963061142324959,
            "upper_bound": 0.035087770557932246
          },
          "point_estimate": 0.02520147658740985,
          "standard_error": 0.007545961617060877
        }
      }
    },
    "memrchr2/krate/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr2/krate/small/uncommon",
        "directory_name": "memrchr2/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.40282940765698,
            "upper_bound": 96.53025045517288
          },
          "point_estimate": 96.46166416985383,
          "standard_error": 0.03261389090712352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.36854892027306,
            "upper_bound": 96.51348652555548
          },
          "point_estimate": 96.45940303698924,
          "standard_error": 0.03653386866092786
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013340222957180214,
            "upper_bound": 0.1729465975799733
          },
          "point_estimate": 0.10341374257821064,
          "standard_error": 0.04108771787929205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 96.41869745788324,
            "upper_bound": 96.49693342558676
          },
          "point_estimate": 96.45914557638696,
          "standard_error": 0.019541968823731836
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05129831908039808,
            "upper_bound": 0.14628958069174233
          },
          "point_estimate": 0.10857381237454856,
          "standard_error": 0.02651349728656974
        }
      }
    },
    "memrchr2/krate/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/never",
        "directory_name": "memrchr2/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.781196421099844,
            "upper_bound": 4.827206764393981
          },
          "point_estimate": 4.799241370283438,
          "standard_error": 0.012522568808518151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.780467709993256,
            "upper_bound": 4.798590326568167
          },
          "point_estimate": 4.787754989436284,
          "standard_error": 0.005487223963517423
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0006549682212224686,
            "upper_bound": 0.030348568774719427
          },
          "point_estimate": 0.01095389142191746,
          "standard_error": 0.0076988620926071085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.778267750292247,
            "upper_bound": 4.788635177043323
          },
          "point_estimate": 4.784086047117202,
          "standard_error": 0.002619957273021893
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006945179735394093,
            "upper_bound": 0.06349517463596094
          },
          "point_estimate": 0.04167651058582715,
          "standard_error": 0.018541193079530793
        }
      }
    },
    "memrchr2/krate/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/rare",
        "directory_name": "memrchr2/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.680410667859688,
            "upper_bound": 15.74647110905834
          },
          "point_estimate": 15.711541454035476,
          "standard_error": 0.016904375947160318
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.673542315708891,
            "upper_bound": 15.746699507973478
          },
          "point_estimate": 15.69712862296173,
          "standard_error": 0.02184705367098224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007633150728262659,
            "upper_bound": 0.09397984538936588
          },
          "point_estimate": 0.045250071355609586,
          "standard_error": 0.0217611418395624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.675291254972551,
            "upper_bound": 15.726828941424596
          },
          "point_estimate": 15.696698504322322,
          "standard_error": 0.013097012309401091
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02717348160719402,
            "upper_bound": 0.07532460424608298
          },
          "point_estimate": 0.056386248460053315,
          "standard_error": 0.013042763183284652
        }
      }
    },
    "memrchr2/krate/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr2/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr2",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr2/krate/tiny/uncommon",
        "directory_name": "memrchr2/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.557886469736275,
            "upper_bound": 52.710694316546494
          },
          "point_estimate": 52.62707735768565,
          "standard_error": 0.03948205751605581
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.54128837525349,
            "upper_bound": 52.713153571469995
          },
          "point_estimate": 52.59212822810284,
          "standard_error": 0.03390297339098073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01187095572235088,
            "upper_bound": 0.18405807560516185
          },
          "point_estimate": 0.06023752937753139,
          "standard_error": 0.04472051685719723
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.56285602335556,
            "upper_bound": 52.70016879526147
          },
          "point_estimate": 52.61962209314368,
          "standard_error": 0.036373562816936246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.041231141628833526,
            "upper_bound": 0.17469350697335287
          },
          "point_estimate": 0.13164178780680189,
          "standard_error": 0.035462261095919594
        }
      }
    },
    "memrchr3/krate/empty/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/empty/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/empty/never",
        "value_str": null,
        "throughput": {
          "Bytes": 0,
          "Elements": null
        },
        "full_id": "memrchr3/krate/empty/never",
        "directory_name": "memrchr3/krate_empty_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2222428850720728,
            "upper_bound": 1.224242372436367
          },
          "point_estimate": 1.2231348851292698,
          "standard_error": 0.0005166861801658609
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.222437711982198,
            "upper_bound": 1.2238122173430466
          },
          "point_estimate": 1.2226098059465664,
          "standard_error": 0.00031996891322340256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00005342515908487214,
            "upper_bound": 0.002655713252965498
          },
          "point_estimate": 0.0003450134846760054,
          "standard_error": 0.0006528748265780267
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.2218990746046716,
            "upper_bound": 1.2236903944499442
          },
          "point_estimate": 1.2227548628930287,
          "standard_error": 0.0004520826789051315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003179795722862025,
            "upper_bound": 0.0024089118115373457
          },
          "point_estimate": 0.0017216901383008997,
          "standard_error": 0.0005369464337710655
        }
      }
    },
    "memrchr3/krate/huge/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/huge/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/common",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/common",
        "directory_name": "memrchr3/krate_huge_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 685134.3909168165,
            "upper_bound": 686099.4305699684
          },
          "point_estimate": 685628.2109778377,
          "standard_error": 246.95020706963695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 684971.8396226416,
            "upper_bound": 686273.5691823899
          },
          "point_estimate": 685731.3962264152,
          "standard_error": 360.0422700487248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 236.35930476605415,
            "upper_bound": 1380.974039539352
          },
          "point_estimate": 922.8681311629254,
          "standard_error": 293.41906791877705
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 684832.2023137157,
            "upper_bound": 685794.9369450611
          },
          "point_estimate": 685231.6742465082,
          "standard_error": 245.4424542195653
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 468.630871093396,
            "upper_bound": 1045.749345859187
          },
          "point_estimate": 823.0714495462611,
          "standard_error": 149.74326813525198
        }
      }
    },
    "memrchr3/krate/huge/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/huge/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/never",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/never",
        "directory_name": "memrchr3/krate_huge_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16587.95182371868,
            "upper_bound": 16632.55536965325
          },
          "point_estimate": 16609.37230153307,
          "standard_error": 11.463889176677837
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16576.839692482914,
            "upper_bound": 16647.617248797775
          },
          "point_estimate": 16597.472750569475,
          "standard_error": 18.508851821652332
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.286083907094373,
            "upper_bound": 63.23836748018025
          },
          "point_estimate": 45.03000596593257,
          "standard_error": 15.008975410785125
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16577.878681185564,
            "upper_bound": 16638.948900146537
          },
          "point_estimate": 16613.00626689939,
          "standard_error": 15.677910731427431
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.227108965383593,
            "upper_bound": 46.09118195806187
          },
          "point_estimate": 38.35993852359584,
          "standard_error": 6.208283599680645
        }
      }
    },
    "memrchr3/krate/huge/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/huge/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/rare",
        "directory_name": "memrchr3/krate_huge_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21665.34340491686,
            "upper_bound": 21698.186327192157
          },
          "point_estimate": 21681.538832107948,
          "standard_error": 8.41375113921557
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21663.12989976062,
            "upper_bound": 21704.65511455929
          },
          "point_estimate": 21673.971932974266,
          "standard_error": 10.615379924813514
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6917358090821398,
            "upper_bound": 48.04349983526904
          },
          "point_estimate": 30.75003859231695,
          "standard_error": 12.455998995367493
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21667.243417467573,
            "upper_bound": 21710.01206314656
          },
          "point_estimate": 21687.56767003194,
          "standard_error": 11.05813819987959
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.46665786924534,
            "upper_bound": 35.94535421837957
          },
          "point_estimate": 28.07938451576047,
          "standard_error": 5.252800908598331
        }
      }
    },
    "memrchr3/krate/huge/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/huge/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/huge/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 594933,
          "Elements": null
        },
        "full_id": "memrchr3/krate/huge/uncommon",
        "directory_name": "memrchr3/krate_huge_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219854.06190499495,
            "upper_bound": 220052.9265791738
          },
          "point_estimate": 219955.35559882387,
          "standard_error": 50.94945374499366
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219821.49578313253,
            "upper_bound": 220103.86316695352
          },
          "point_estimate": 219950.54804216867,
          "standard_error": 69.64965363622183
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.17761917160167,
            "upper_bound": 296.8519423201822
          },
          "point_estimate": 184.82779302889676,
          "standard_error": 66.79392973561863
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219908.80359867416,
            "upper_bound": 220100.6937875451
          },
          "point_estimate": 220006.02838366452,
          "standard_error": 48.96449492298249
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.57902809082404,
            "upper_bound": 211.20945387743245
          },
          "point_estimate": 170.0419334689779,
          "standard_error": 29.18468711805879
        }
      }
    },
    "memrchr3/krate/small/common": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/small/common",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/common",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/common",
        "directory_name": "memrchr3/krate_small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 710.1895315237854,
            "upper_bound": 711.2791576054336
          },
          "point_estimate": 710.7086692480132,
          "standard_error": 0.27896262729028004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 709.9031683374697,
            "upper_bound": 711.5987786528052
          },
          "point_estimate": 710.6102487081637,
          "standard_error": 0.38795336167496824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07536111487582971,
            "upper_bound": 1.523477694407611
          },
          "point_estimate": 1.1046226073603336,
          "standard_error": 0.3847214904164507
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 709.9320454385025,
            "upper_bound": 711.1965494110342
          },
          "point_estimate": 710.589103831896,
          "standard_error": 0.3343552314999122
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.46124251173833314,
            "upper_bound": 1.161511863689478
          },
          "point_estimate": 0.9271272547371416,
          "standard_error": 0.1724957488442991
        }
      }
    },
    "memrchr3/krate/small/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/small/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/never",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/never",
        "directory_name": "memrchr3/krate_small_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.88315875037887,
            "upper_bound": 15.91255308827946
          },
          "point_estimate": 15.89721383229423,
          "standard_error": 0.007514954260875849
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.88102197808457,
            "upper_bound": 15.9115047020158
          },
          "point_estimate": 15.894339485409358,
          "standard_error": 0.007806738005811896
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006075279147582617,
            "upper_bound": 0.041725553432413846
          },
          "point_estimate": 0.01929614122805258,
          "standard_error": 0.009174171955095888
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.881150100298663,
            "upper_bound": 15.901944032754171
          },
          "point_estimate": 15.890656083263863,
          "standard_error": 0.005457676785181122
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011403324747089674,
            "upper_bound": 0.03350016636255064
          },
          "point_estimate": 0.025087260815277942,
          "standard_error": 0.005773744904323827
        }
      }
    },
    "memrchr3/krate/small/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/small/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/rare",
        "directory_name": "memrchr3/krate_small_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.99762469113708,
            "upper_bound": 35.043824588840515
          },
          "point_estimate": 35.020833137044036,
          "standard_error": 0.011774386705485106
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.99293578943352,
            "upper_bound": 35.04313844033533
          },
          "point_estimate": 35.028297342715916,
          "standard_error": 0.013195056111346208
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005515974847566211,
            "upper_bound": 0.07111624599299823
          },
          "point_estimate": 0.029019922269409908,
          "standard_error": 0.01655831884043203
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.0085530948133,
            "upper_bound": 35.04203302113027
          },
          "point_estimate": 35.02743804230239,
          "standard_error": 0.008396126771491994
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019797156829360937,
            "upper_bound": 0.05163104637288649
          },
          "point_estimate": 0.039193643489716774,
          "standard_error": 0.00815433551742003
        }
      }
    },
    "memrchr3/krate/small/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/small/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/small/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 664,
          "Elements": null
        },
        "full_id": "memrchr3/krate/small/uncommon",
        "directory_name": "memrchr3/krate_small_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.06490481826202,
            "upper_bound": 155.17830300284865
          },
          "point_estimate": 155.12285954821445,
          "standard_error": 0.029025887239251324
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.04933384281895,
            "upper_bound": 155.21737236601408
          },
          "point_estimate": 155.1196016031687,
          "standard_error": 0.05222310653201822
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013790665760274146,
            "upper_bound": 0.17751079109715717
          },
          "point_estimate": 0.1217863794159593,
          "standard_error": 0.04289486062836339
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 155.03737880987634,
            "upper_bound": 155.19909930797357
          },
          "point_estimate": 155.1229422341871,
          "standard_error": 0.04164300843973838
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.060030470974676245,
            "upper_bound": 0.1190988279026743
          },
          "point_estimate": 0.09654939410016178,
          "standard_error": 0.01555444810225347
        }
      }
    },
    "memrchr3/krate/tiny/never": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/tiny/never",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/never",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/never",
        "directory_name": "memrchr3/krate_tiny_never"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.621537312754791,
            "upper_bound": 5.631072923195441
          },
          "point_estimate": 5.626496625828115,
          "standard_error": 0.0024483852995364104
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.62012806163523,
            "upper_bound": 5.631941491762346
          },
          "point_estimate": 5.629278016392166,
          "standard_error": 0.002937160062187781
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0001291710156603273,
            "upper_bound": 0.013358667754203416
          },
          "point_estimate": 0.005322788580656602,
          "standard_error": 0.0036934785558951854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.624180790316964,
            "upper_bound": 5.632307433389118
          },
          "point_estimate": 5.628611536434935,
          "standard_error": 0.002016147172079889
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003776573570790658,
            "upper_bound": 0.010316975605075702
          },
          "point_estimate": 0.008153945374991641,
          "standard_error": 0.0016146717580531357
        }
      }
    },
    "memrchr3/krate/tiny/rare": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/tiny/rare",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/rare",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/rare",
        "directory_name": "memrchr3/krate_tiny_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.224335741048298,
            "upper_bound": 15.300820101404168
          },
          "point_estimate": 15.26055411101,
          "standard_error": 0.01966122031427034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.205538293354635,
            "upper_bound": 15.317942128246171
          },
          "point_estimate": 15.246305865644484,
          "standard_error": 0.026037899760822783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015163764189342252,
            "upper_bound": 0.11640213197595645
          },
          "point_estimate": 0.06304198789277146,
          "standard_error": 0.025079255430444532
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.249785792383625,
            "upper_bound": 15.3438977534233
          },
          "point_estimate": 15.303907312146723,
          "standard_error": 0.024649473290349264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030539026715584564,
            "upper_bound": 0.08111188968164276
          },
          "point_estimate": 0.0657371334587376,
          "standard_error": 0.012675071718326271
        }
      }
    },
    "memrchr3/krate/tiny/uncommon": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrchr3/krate/tiny/uncommon",
      "criterion_benchmark_v1": {
        "group_id": "memrchr3",
        "function_id": "krate/tiny/uncommon",
        "value_str": null,
        "throughput": {
          "Bytes": 69,
          "Elements": null
        },
        "full_id": "memrchr3/krate/tiny/uncommon",
        "directory_name": "memrchr3/krate_tiny_uncommon"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.57966310743795,
            "upper_bound": 86.70452649007136
          },
          "point_estimate": 86.63215248837847,
          "standard_error": 0.032747937052957886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.5605006137364,
            "upper_bound": 86.65926430582618
          },
          "point_estimate": 86.6067779011187,
          "standard_error": 0.03291519935831764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01179144464217162,
            "upper_bound": 0.12960210648935744
          },
          "point_estimate": 0.07225606159565068,
          "standard_error": 0.02997779331441004
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.57810426679616,
            "upper_bound": 86.6428075825222
          },
          "point_estimate": 86.61320517812416,
          "standard_error": 0.016636338270082898
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03687896997951916,
            "upper_bound": 0.16076963832911623
          },
          "point_estimate": 0.1088834829192802,
          "standard_error": 0.03998641767428342
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303617.1170735543,
            "upper_bound": 1306281.872789116
          },
          "point_estimate": 1304759.8561422904,
          "standard_error": 697.9359777546683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303353.056547619,
            "upper_bound": 1305246.015873016
          },
          "point_estimate": 1304155.6234693876,
          "standard_error": 463.91987387958096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.25996699518777,
            "upper_bound": 2628.277779588821
          },
          "point_estimate": 1064.4736873517156,
          "standard_error": 657.0637915339064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303275.865134865,
            "upper_bound": 1304510.1711913547
          },
          "point_estimate": 1303909.7729128017,
          "standard_error": 315.707856773863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 692.4827772196859,
            "upper_bound": 3400.341616889395
          },
          "point_estimate": 2328.1867227018006,
          "standard_error": 829.9372924657208
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1490747.3419142854,
            "upper_bound": 1492789.500820238
          },
          "point_estimate": 1491710.2427460316,
          "standard_error": 523.6241298149192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1490184.08,
            "upper_bound": 1492682.93
          },
          "point_estimate": 1491708.014,
          "standard_error": 748.6796484913566
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 274.27358213066367,
            "upper_bound": 3218.927659052765
          },
          "point_estimate": 1735.9403031808013,
          "standard_error": 686.2960126487336
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1490436.5972769952,
            "upper_bound": 1492331.3491830986
          },
          "point_estimate": 1491281.0702337662,
          "standard_error": 479.2718142731006
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 945.7762412830808,
            "upper_bound": 2307.70937290132
          },
          "point_estimate": 1750.5092627412712,
          "standard_error": 379.3249836334967
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1399359.9223605003,
            "upper_bound": 1401531.4682844933
          },
          "point_estimate": 1400384.2364896217,
          "standard_error": 557.8039824280288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1399035.6794871795,
            "upper_bound": 1401647.7747252746
          },
          "point_estimate": 1400075.4211538462,
          "standard_error": 722.4803660908958
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.12782263073628,
            "upper_bound": 3108.019943283266
          },
          "point_estimate": 1549.2824418536998,
          "standard_error": 703.1559444730655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1399368.6285606625,
            "upper_bound": 1401714.974306683
          },
          "point_estimate": 1400539.8829170829,
          "standard_error": 611.9724689954849
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 916.6370859640944,
            "upper_bound": 2453.924461693894
          },
          "point_estimate": 1862.259631073082,
          "standard_error": 410.43633657365234
        }
      }
    },
    "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752492.8123979592,
            "upper_bound": 754732.7260204081
          },
          "point_estimate": 753509.7900623583,
          "standard_error": 575.8349013426661
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 751852.0867346938,
            "upper_bound": 754479.106122449
          },
          "point_estimate": 753035.6165816328,
          "standard_error": 644.5883369779333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.9579769952431,
            "upper_bound": 2900.66600136007
          },
          "point_estimate": 1779.5210016216097,
          "standard_error": 644.7113061735002
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752827.4338598183,
            "upper_bound": 754192.7596802979
          },
          "point_estimate": 753528.7349059104,
          "standard_error": 343.65185493911656
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 806.7429029470393,
            "upper_bound": 2624.6634492742546
          },
          "point_estimate": 1916.7660032341428,
          "standard_error": 513.9052915185081
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265704.1954153632,
            "upper_bound": 266513.54059089324
          },
          "point_estimate": 266076.5077441779,
          "standard_error": 207.9454186058062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265530.8673965937,
            "upper_bound": 266488.4087591241
          },
          "point_estimate": 265875.07627737225,
          "standard_error": 246.14600015615832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.98721194350355,
            "upper_bound": 1090.1807025432818
          },
          "point_estimate": 549.4070001000894,
          "standard_error": 262.6355195804144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265609.1755917234,
            "upper_bound": 266113.88955246465
          },
          "point_estimate": 265788.52355673525,
          "standard_error": 130.3571515652185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 230.42153247233097,
            "upper_bound": 903.0711562013744
          },
          "point_estimate": 692.860049173562,
          "standard_error": 166.41603756254517
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475540.0936565914,
            "upper_bound": 476154.2478653885
          },
          "point_estimate": 475810.10933725006,
          "standard_error": 158.6609190977259
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475462.76406926406,
            "upper_bound": 475990.3766233766
          },
          "point_estimate": 475686.71869202226,
          "standard_error": 154.0167869071347
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.81801930383817,
            "upper_bound": 676.3179295080715
          },
          "point_estimate": 351.0392392223649,
          "standard_error": 146.73146948520153
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475578.84121274407,
            "upper_bound": 476052.79462067987
          },
          "point_estimate": 475783.4838927306,
          "standard_error": 122.41971976842824
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 195.3549772356569,
            "upper_bound": 754.5957678986659
          },
          "point_estimate": 527.8148496330713,
          "standard_error": 168.18772804470294
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240961.9854181383,
            "upper_bound": 241705.39034689372
          },
          "point_estimate": 241280.3300446757,
          "standard_error": 192.62851057904328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240854.5408388521,
            "upper_bound": 241469.5120887207
          },
          "point_estimate": 241108.2273178808,
          "standard_error": 132.14744791889473
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.5099207962094,
            "upper_bound": 727.1936083700716
          },
          "point_estimate": 352.8702487021991,
          "standard_error": 209.90058895423857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240967.3667450641,
            "upper_bound": 241384.8509405078
          },
          "point_estimate": 241150.94583297495,
          "standard_error": 106.43498481334971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.76284239115958,
            "upper_bound": 924.6696882303872
          },
          "point_estimate": 643.8388360672312,
          "standard_error": 218.59843242040017
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706464.3332371797,
            "upper_bound": 707447.099381582
          },
          "point_estimate": 706978.4366094323,
          "standard_error": 251.27686913293573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706752.9336080586,
            "upper_bound": 707449.3533653846
          },
          "point_estimate": 706905.7326923078,
          "standard_error": 189.6265523380844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.83322552649273,
            "upper_bound": 1277.874182169029
          },
          "point_estimate": 256.8438137094699,
          "standard_error": 344.12854481879896
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706788.328197405,
            "upper_bound": 707264.9655177614
          },
          "point_estimate": 706973.9554445555,
          "standard_error": 123.82349995217513
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 293.0302948950812,
            "upper_bound": 1179.134714081655
          },
          "point_estimate": 832.5469364442167,
          "standard_error": 232.6052380815356
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2311.397546562833,
            "upper_bound": 2314.047487921728
          },
          "point_estimate": 2312.761198808211,
          "standard_error": 0.6772931537761406
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2310.795538441955,
            "upper_bound": 2314.05015062797
          },
          "point_estimate": 2313.359726061164,
          "standard_error": 0.7869910095306069
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.29085414113677377,
            "upper_bound": 4.067374128400736
          },
          "point_estimate": 1.292508248863215,
          "standard_error": 1.041298145579128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2312.853007833221,
            "upper_bound": 2315.186572728082
          },
          "point_estimate": 2313.9901511294206,
          "standard_error": 0.6537735735896878
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0844550395906865,
            "upper_bound": 2.9056480578075696
          },
          "point_estimate": 2.2610310499841724,
          "standard_error": 0.455496922826073
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702.4370962886034,
            "upper_bound": 703.0312061048014
          },
          "point_estimate": 702.7497990270139,
          "standard_error": 0.15183714028746909
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702.3762591597224,
            "upper_bound": 703.2587014945574
          },
          "point_estimate": 702.8015078152837,
          "standard_error": 0.1807057262225086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01547314022921536,
            "upper_bound": 0.7950723398608138
          },
          "point_estimate": 0.6541544911996151,
          "standard_error": 0.24101923438344827
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 702.165824342589,
            "upper_bound": 703.1266391494418
          },
          "point_estimate": 702.7067144220603,
          "standard_error": 0.25543753017420595
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.24697564024452845,
            "upper_bound": 0.6606974911843311
          },
          "point_estimate": 0.5072046760124831,
          "standard_error": 0.10841447125261934
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.37712896093723,
            "upper_bound": 141.645431282223
          },
          "point_estimate": 141.51584492360672,
          "standard_error": 0.06869495420483501
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.33676315400072,
            "upper_bound": 141.66757147010765
          },
          "point_estimate": 141.5570651807085,
          "standard_error": 0.08679430070116072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.040824844332175904,
            "upper_bound": 0.36978611911839177
          },
          "point_estimate": 0.1879015541164804,
          "standard_error": 0.09033453968192
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.53024465639032,
            "upper_bound": 141.74590474468258
          },
          "point_estimate": 141.6482998403755,
          "standard_error": 0.054867421055834
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1122514283911564,
            "upper_bound": 0.29320870319835984
          },
          "point_estimate": 0.22926683714743668,
          "standard_error": 0.045597307549583375
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.435844063630555,
            "upper_bound": 53.52277591931662
          },
          "point_estimate": 53.47897302474219,
          "standard_error": 0.02221996552328844
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.420237033028116,
            "upper_bound": 53.54351975292648
          },
          "point_estimate": 53.478422754000526,
          "standard_error": 0.02706935450221184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005710523331319726,
            "upper_bound": 0.12881192364853877
          },
          "point_estimate": 0.08435546349961305,
          "standard_error": 0.03264369736208126
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.46332799790705,
            "upper_bound": 53.54288130122561
          },
          "point_estimate": 53.50620409400739,
          "standard_error": 0.020225585797610458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04130291448452305,
            "upper_bound": 0.09460773109361784
          },
          "point_estimate": 0.07429206947953627,
          "standard_error": 0.013893208274711518
        }
      }
    },
    "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.33246082477751,
            "upper_bound": 80.47012395777895
          },
          "point_estimate": 80.39566889524569,
          "standard_error": 0.03542403034722482
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.30817034770268,
            "upper_bound": 80.48640899414582
          },
          "point_estimate": 80.35312743924072,
          "standard_error": 0.04441051892529883
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0033506662967308833,
            "upper_bound": 0.1873201910929751
          },
          "point_estimate": 0.078643535225676,
          "standard_error": 0.047213679815656905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.31363169031485,
            "upper_bound": 80.50033321287322
          },
          "point_estimate": 80.3936575651597,
          "standard_error": 0.04750460491439941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.043596196759517813,
            "upper_bound": 0.15169299489012084
          },
          "point_estimate": 0.11796745454716716,
          "standard_error": 0.028198712239512883
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286247.60597144713,
            "upper_bound": 286799.0616067553
          },
          "point_estimate": 286514.1580223834,
          "standard_error": 141.5923380524878
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286122.2666015625,
            "upper_bound": 287022.21484375
          },
          "point_estimate": 286434.5169270834,
          "standard_error": 224.45339188136865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 87.26748178400186,
            "upper_bound": 782.9409692250595
          },
          "point_estimate": 494.5280904717214,
          "standard_error": 181.2939381331372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286048.8563368809,
            "upper_bound": 286539.4534073795
          },
          "point_estimate": 286217.9424918831,
          "standard_error": 125.67483027203302
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.6573785050345,
            "upper_bound": 572.5058008828075
          },
          "point_estimate": 471.1000318507035,
          "standard_error": 77.21953563890877
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.91779268499493,
            "upper_bound": 63.050034415457034
          },
          "point_estimate": 62.97684206488458,
          "standard_error": 0.034261469728714085
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.907958243578165,
            "upper_bound": 63.039953790563544
          },
          "point_estimate": 62.9429869517903,
          "standard_error": 0.030253753404414073
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01615074314666114,
            "upper_bound": 0.16733615652123543
          },
          "point_estimate": 0.05787620982041976,
          "standard_error": 0.03691871097773943
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.8888337483993,
            "upper_bound": 62.959097450300234
          },
          "point_estimate": 62.918766757181395,
          "standard_error": 0.017802182889219084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.034327479571358184,
            "upper_bound": 0.15749333038478952
          },
          "point_estimate": 0.1144018642336871,
          "standard_error": 0.03366956254255595
        }
      }
    },
    "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.70117631359682,
            "upper_bound": 105.86017921287804
          },
          "point_estimate": 105.7837191509348,
          "standard_error": 0.04077540772289404
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.67349160605644,
            "upper_bound": 105.9026969988065
          },
          "point_estimate": 105.82245247867726,
          "standard_error": 0.06914047822379249
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027337290702239937,
            "upper_bound": 0.22033859611342735
          },
          "point_estimate": 0.16443604508231097,
          "standard_error": 0.052747835030588434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.71875909205438,
            "upper_bound": 105.8985344242693
          },
          "point_estimate": 105.81310136433008,
          "standard_error": 0.04718253539436476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07900609516856248,
            "upper_bound": 0.16869543204943635
          },
          "point_estimate": 0.1361261262061424,
          "standard_error": 0.023318635132404647
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126649.26634012222,
            "upper_bound": 126866.2917048179
          },
          "point_estimate": 126756.02369600686,
          "standard_error": 55.94260168280511
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126586.59030197444,
            "upper_bound": 126915.83318815332
          },
          "point_estimate": 126734.49605939936,
          "standard_error": 105.2293469787572
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.815892316045463,
            "upper_bound": 299.5657573994164
          },
          "point_estimate": 222.13254282465545,
          "standard_error": 76.64313354340307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126606.12729565911,
            "upper_bound": 126944.11153188458
          },
          "point_estimate": 126789.44744106068,
          "standard_error": 89.1012003135928
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.11930587572508,
            "upper_bound": 219.5064890734133
          },
          "point_estimate": 186.91080044268045,
          "standard_error": 25.16549677890447
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.5974781636714,
            "upper_bound": 60.719473826077184
          },
          "point_estimate": 60.65184532691804,
          "standard_error": 0.03149812217830172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.58047623549914,
            "upper_bound": 60.72421250102045
          },
          "point_estimate": 60.61779091680978,
          "standard_error": 0.03300875315309794
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011054922157594117,
            "upper_bound": 0.15862903070964574
          },
          "point_estimate": 0.06494812983603246,
          "standard_error": 0.03820275409896287
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.59595005797256,
            "upper_bound": 60.64228193206839
          },
          "point_estimate": 60.62056338757731,
          "standard_error": 0.011666655898908808
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03694748288745862,
            "upper_bound": 0.1444152291337414
          },
          "point_estimate": 0.104551993620248,
          "standard_error": 0.02958919126431963
        }
      }
    },
    "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.78836737869096,
            "upper_bound": 102.86069145854816
          },
          "point_estimate": 102.82540356340624,
          "standard_error": 0.018575117812227728
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.78490534648,
            "upper_bound": 102.87101314524568
          },
          "point_estimate": 102.82915398837898,
          "standard_error": 0.02443292945288179
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013794618489569692,
            "upper_bound": 0.10385545819726053
          },
          "point_estimate": 0.062109067429571306,
          "standard_error": 0.022638698686020073
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.79976518740466,
            "upper_bound": 102.88836003349692
          },
          "point_estimate": 102.84570968070658,
          "standard_error": 0.023551495803761875
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03321242206905663,
            "upper_bound": 0.08063403811995468
          },
          "point_estimate": 0.06172067915282674,
          "standard_error": 0.012353024622218272
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.81361925480164,
            "upper_bound": 255.1978247963187
          },
          "point_estimate": 254.98052983835973,
          "standard_error": 0.09940240347657968
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.74230947270968,
            "upper_bound": 255.0904602593467
          },
          "point_estimate": 254.94537419868425,
          "standard_error": 0.08616848140922077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0362624503479753,
            "upper_bound": 0.4130260918025163
          },
          "point_estimate": 0.2315548603316808,
          "standard_error": 0.09676641630218306
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.76234902804197,
            "upper_bound": 255.03379517994685
          },
          "point_estimate": 254.8975973780039,
          "standard_error": 0.06923500144619002
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11984949220430656,
            "upper_bound": 0.4785514759978029
          },
          "point_estimate": 0.3317663309660625,
          "standard_error": 0.10937678268865032
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.2410726072267,
            "upper_bound": 526.0888471100062
          },
          "point_estimate": 525.6032670223624,
          "standard_error": 0.22122027162803007
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.1060265760083,
            "upper_bound": 525.8061160333589
          },
          "point_estimate": 525.5420818458188,
          "standard_error": 0.17157697889372406
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017738827499535245,
            "upper_bound": 0.8255690604584631
          },
          "point_estimate": 0.5143945174404619,
          "standard_error": 0.22697377478768044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.1085266640295,
            "upper_bound": 525.520430907661
          },
          "point_estimate": 525.2909481343522,
          "standard_error": 0.10488188769036616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2363646465856888,
            "upper_bound": 1.0774791968532254
          },
          "point_estimate": 0.7391823250320033,
          "standard_error": 0.2582514031259146
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.02123972208033,
            "upper_bound": 32.06941727124882
          },
          "point_estimate": 32.04564243481443,
          "standard_error": 0.012327441689023338
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.01394536991495,
            "upper_bound": 32.0767916625084
          },
          "point_estimate": 32.047100176068625,
          "standard_error": 0.01422869642734532
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006563615251058931,
            "upper_bound": 0.07468483497935945
          },
          "point_estimate": 0.03447282596555929,
          "standard_error": 0.01725353434665435
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.043261813933825,
            "upper_bound": 32.072482170611416
          },
          "point_estimate": 32.05444778158828,
          "standard_error": 0.00739797685327117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022134184172740858,
            "upper_bound": 0.052050092690361344
          },
          "point_estimate": 0.040979654235289646,
          "standard_error": 0.007678974579136418
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32217.707581553423,
            "upper_bound": 32317.523438965647
          },
          "point_estimate": 32257.94871945466,
          "standard_error": 26.944705724513224
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32208.206492162084,
            "upper_bound": 32272.90900128167
          },
          "point_estimate": 32218.503374952463,
          "standard_error": 21.54450548599992
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.513590172765695,
            "upper_bound": 84.06546853859554
          },
          "point_estimate": 22.560661844367026,
          "standard_error": 21.663016484640515
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32213.170061880992,
            "upper_bound": 32254.366192783054
          },
          "point_estimate": 32230.238454003847,
          "standard_error": 11.128878332242667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.831547080387327,
            "upper_bound": 135.66392108323333
          },
          "point_estimate": 89.85121704569525,
          "standard_error": 37.718349847112705
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.82277026179528,
            "upper_bound": 142.02091433670918
          },
          "point_estimate": 141.92083492079018,
          "standard_error": 0.050669572321647434
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.7823294202048,
            "upper_bound": 142.0520390194818
          },
          "point_estimate": 141.9404784886628,
          "standard_error": 0.06680394876333963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013128800262592303,
            "upper_bound": 0.2960133212952876
          },
          "point_estimate": 0.19627597602365335,
          "standard_error": 0.07341154095867064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.8435295667032,
            "upper_bound": 142.05501261980285
          },
          "point_estimate": 141.938756062173,
          "standard_error": 0.054698364583363135
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09486782103849546,
            "upper_bound": 0.212040290634072
          },
          "point_estimate": 0.16901516452704618,
          "standard_error": 0.0298773001976116
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 979100.8765643274,
            "upper_bound": 980141.0125113304
          },
          "point_estimate": 979583.4099269006,
          "standard_error": 267.3313518155589
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978875.0818713448,
            "upper_bound": 980264.144736842
          },
          "point_estimate": 979181.3447368422,
          "standard_error": 407.0915969387215
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 52.242141177807,
            "upper_bound": 1524.2415250445597
          },
          "point_estimate": 644.6032559246246,
          "standard_error": 381.6996183432991
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 979065.4615470564,
            "upper_bound": 980131.8146034548
          },
          "point_estimate": 979468.6470266576,
          "standard_error": 274.02551007199554
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 440.91564537329,
            "upper_bound": 1141.5799284893662
          },
          "point_estimate": 891.1252550409453,
          "standard_error": 187.66338547541685
        }
      }
    },
    "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.3538286814696,
            "upper_bound": 1988.2091121254084
          },
          "point_estimate": 1986.7307022382215,
          "standard_error": 0.7290016764974854
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1985.152575829355,
            "upper_bound": 1988.104862249918
          },
          "point_estimate": 1986.4460322666291,
          "standard_error": 0.7448864521001667
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.48767603266708354,
            "upper_bound": 4.039193455676065
          },
          "point_estimate": 2.114296556246676,
          "standard_error": 0.8800370845562718
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.935632823967,
            "upper_bound": 1987.993999988557
          },
          "point_estimate": 1986.552197796737,
          "standard_error": 0.8016435312167859
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1239826535034612,
            "upper_bound": 3.26422095954923
          },
          "point_estimate": 2.4301557758416275,
          "standard_error": 0.5597472950036606
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.22186188042197,
            "upper_bound": 45.268310725339155
          },
          "point_estimate": 45.24565519516193,
          "standard_error": 0.011914999461857067
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.214335245638125,
            "upper_bound": 45.270976104971936
          },
          "point_estimate": 45.25832570460512,
          "standard_error": 0.015579172257351464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004262061166895473,
            "upper_bound": 0.06888512560222797
          },
          "point_estimate": 0.03433117841113661,
          "standard_error": 0.017282411165360854
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.250006275320146,
            "upper_bound": 45.275340064477675
          },
          "point_estimate": 45.262392104345594,
          "standard_error": 0.006318527135387907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02120722367407697,
            "upper_bound": 0.05044953172497919
          },
          "point_estimate": 0.039788492690425864,
          "standard_error": 0.007381765388936095
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.99541529622565,
            "upper_bound": 49.09993155293233
          },
          "point_estimate": 49.042001224748915,
          "standard_error": 0.02667621621426226
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.982310407089045,
            "upper_bound": 49.11272444484779
          },
          "point_estimate": 49.007808586994,
          "standard_error": 0.02624169570595877
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0065989581648153075,
            "upper_bound": 0.12406293046669396
          },
          "point_estimate": 0.04321156070255553,
          "standard_error": 0.027626703333503044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.00191671940821,
            "upper_bound": 49.11002068492041
          },
          "point_estimate": 49.04314263809676,
          "standard_error": 0.029804592470478244
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022342343299712717,
            "upper_bound": 0.1117016860145024
          },
          "point_estimate": 0.08874223674655925,
          "standard_error": 0.02342445514244616
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.461623288121835,
            "upper_bound": 35.52878006699289
          },
          "point_estimate": 35.492547478469746,
          "standard_error": 0.01723878158371821
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.45044651947237,
            "upper_bound": 35.53773803888535
          },
          "point_estimate": 35.47962898127433,
          "standard_error": 0.01826098386459725
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008381204393132259,
            "upper_bound": 0.08778190109573288
          },
          "point_estimate": 0.036160001911440025,
          "standard_error": 0.01982682838834036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.460426346244,
            "upper_bound": 35.4936674868406
          },
          "point_estimate": 35.47276983637077,
          "standard_error": 0.008528112697599078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020903062963169582,
            "upper_bound": 0.07256062854371113
          },
          "point_estimate": 0.05752718185496659,
          "standard_error": 0.013238830924136866
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.08311910662225,
            "upper_bound": 40.13658584139835
          },
          "point_estimate": 40.10587391370965,
          "standard_error": 0.014017098243060614
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.076148704450645,
            "upper_bound": 40.1215090535043
          },
          "point_estimate": 40.0890316072481,
          "standard_error": 0.012328197192421616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005562462100141144,
            "upper_bound": 0.0549979108018614
          },
          "point_estimate": 0.025453861114727604,
          "standard_error": 0.012394738146389096
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.077622099158454,
            "upper_bound": 40.10915793367012
          },
          "point_estimate": 40.09242467290804,
          "standard_error": 0.008178013603724589
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01425001527442742,
            "upper_bound": 0.06798689464075153
          },
          "point_estimate": 0.04671598627417662,
          "standard_error": 0.016253163804365065
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.2246413719811,
            "upper_bound": 48.2757393461669
          },
          "point_estimate": 48.25140555055215,
          "standard_error": 0.013066808758920547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.22695250479401,
            "upper_bound": 48.2835274102046
          },
          "point_estimate": 48.25574019730021,
          "standard_error": 0.015369239404431865
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007663322402450216,
            "upper_bound": 0.07064856406732842
          },
          "point_estimate": 0.03750415226719128,
          "standard_error": 0.015815963423523455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.236312587509886,
            "upper_bound": 48.27370458241264
          },
          "point_estimate": 48.25102869852278,
          "standard_error": 0.009536708072186786
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020515147323658157,
            "upper_bound": 0.05881447600290156
          },
          "point_estimate": 0.043563539881649446,
          "standard_error": 0.010231668266566397
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.40633109073124,
            "upper_bound": 74.54415538986919
          },
          "point_estimate": 74.46957603009251,
          "standard_error": 0.03543849894285541
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.36409052188638,
            "upper_bound": 74.5431754503002
          },
          "point_estimate": 74.45486278031508,
          "standard_error": 0.040130619402987275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016691996070455177,
            "upper_bound": 0.1800061398226637
          },
          "point_estimate": 0.10053437526994718,
          "standard_error": 0.04604185338282981
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.39153359022285,
            "upper_bound": 74.45893200418706
          },
          "point_estimate": 74.42740104458584,
          "standard_error": 0.01701139749886299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05113816367256493,
            "upper_bound": 0.15816204822042423
          },
          "point_estimate": 0.11794579991462048,
          "standard_error": 0.02961258576784195
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.4266094419945,
            "upper_bound": 88.53984657034809
          },
          "point_estimate": 88.4773559566946,
          "standard_error": 0.02910563402680399
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.42972619111424,
            "upper_bound": 88.50447084752425
          },
          "point_estimate": 88.46389905328962,
          "standard_error": 0.02336304745181144
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01126874440983786,
            "upper_bound": 0.1259741992963785
          },
          "point_estimate": 0.055408212813043915,
          "standard_error": 0.02792703013491037
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.44360083978962,
            "upper_bound": 88.62334428555948
          },
          "point_estimate": 88.52624884920816,
          "standard_error": 0.051727529050792
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02972876096815746,
            "upper_bound": 0.13942157100002087
          },
          "point_estimate": 0.0966624114948435,
          "standard_error": 0.031062756243070345
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.87242597872637,
            "upper_bound": 63.006613218278545
          },
          "point_estimate": 62.94151158212287,
          "standard_error": 0.03434519975760992
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.85179547242282,
            "upper_bound": 63.03344786078124
          },
          "point_estimate": 62.93395217762426,
          "standard_error": 0.04051367028037307
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01329409139645722,
            "upper_bound": 0.22428483854387551
          },
          "point_estimate": 0.11026477748928692,
          "standard_error": 0.05695243223476211
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.90605548224421,
            "upper_bound": 63.01932306178111
          },
          "point_estimate": 62.96031439913272,
          "standard_error": 0.02995091712301894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06091485814568361,
            "upper_bound": 0.14404933060121078
          },
          "point_estimate": 0.11433477297635614,
          "standard_error": 0.02152991214183405
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.66029488614612,
            "upper_bound": 105.79135559944534
          },
          "point_estimate": 105.71984569643443,
          "standard_error": 0.033589699770185784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.63880675127568,
            "upper_bound": 105.76072331404444
          },
          "point_estimate": 105.71200372163344,
          "standard_error": 0.036111983197863455
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010797760068241293,
            "upper_bound": 0.1642287991109222
          },
          "point_estimate": 0.07629509815556901,
          "standard_error": 0.03618339432716634
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.67494587393428,
            "upper_bound": 105.75915689218684
          },
          "point_estimate": 105.71638332446916,
          "standard_error": 0.02178924779082952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04667495477545446,
            "upper_bound": 0.15776410656729475
          },
          "point_estimate": 0.11234371512449633,
          "standard_error": 0.03265526325397674
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.78785011006821,
            "upper_bound": 63.84949534340343
          },
          "point_estimate": 63.81960154042613,
          "standard_error": 0.015756386052059514
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.775502123793544,
            "upper_bound": 63.86962969361898
          },
          "point_estimate": 63.830672163907025,
          "standard_error": 0.024225252859547713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011978615149169224,
            "upper_bound": 0.08738788594978418
          },
          "point_estimate": 0.05908043998058776,
          "standard_error": 0.019269985465931658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.7823451257495,
            "upper_bound": 63.84748267287436
          },
          "point_estimate": 63.80788578267987,
          "standard_error": 0.016449923042137278
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02982323797002398,
            "upper_bound": 0.06443795490558082
          },
          "point_estimate": 0.05240369748526132,
          "standard_error": 0.008852759149094945
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.59414157178991,
            "upper_bound": 60.65348703650064
          },
          "point_estimate": 60.623752181146685,
          "standard_error": 0.015216996215327518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.57957260356761,
            "upper_bound": 60.67583150997445
          },
          "point_estimate": 60.61609144068404,
          "standard_error": 0.0228661489240238
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01078583476801663,
            "upper_bound": 0.09160360514144268
          },
          "point_estimate": 0.06493831640068652,
          "standard_error": 0.022270481572040104
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.59780806820719,
            "upper_bound": 60.6737994353984
          },
          "point_estimate": 60.640920659655066,
          "standard_error": 0.01966121840448092
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03083998005837193,
            "upper_bound": 0.061686544061216333
          },
          "point_estimate": 0.0508046877838854,
          "standard_error": 0.007935893752120024
        }
      }
    },
    "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.8675531407426,
            "upper_bound": 103.05196730891578
          },
          "point_estimate": 102.95399166983688,
          "standard_error": 0.04738609256162655
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.8383849519756,
            "upper_bound": 103.0943531500364
          },
          "point_estimate": 102.91450988369446,
          "standard_error": 0.05762160857166047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02846414959366756,
            "upper_bound": 0.2504288540072141
          },
          "point_estimate": 0.13461310908193666,
          "standard_error": 0.06418551646061825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.83723226477947,
            "upper_bound": 103.06047464367794
          },
          "point_estimate": 102.93217515027762,
          "standard_error": 0.056981164630156816
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07109353543841895,
            "upper_bound": 0.20477998104477568
          },
          "point_estimate": 0.15807079367839308,
          "standard_error": 0.034086060399060356
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1170688.798191592,
            "upper_bound": 1172235.122783668
          },
          "point_estimate": 1171477.3513876486,
          "standard_error": 397.55123001425034
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1170018.96875,
            "upper_bound": 1172705.871875
          },
          "point_estimate": 1171577.439453125,
          "standard_error": 692.2719010768024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.36484486231416,
            "upper_bound": 2220.250133629457
          },
          "point_estimate": 1712.5599167053242,
          "standard_error": 604.7473756939443
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1171064.5705382947,
            "upper_bound": 1172508.1432979826
          },
          "point_estimate": 1171877.9934253246,
          "standard_error": 372.13376721230856
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 777.8348448894634,
            "upper_bound": 1547.9640109278537
          },
          "point_estimate": 1324.3478546196525,
          "standard_error": 188.541282386222
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552988.3636574077,
            "upper_bound": 1558207.7408399472
          },
          "point_estimate": 1555425.0169708994,
          "standard_error": 1332.9164112169556
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1553211.1481481483,
            "upper_bound": 1557019.6666666667
          },
          "point_estimate": 1554985.394345238,
          "standard_error": 966.8994892103476
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 513.7632508787705,
            "upper_bound": 6595.196693328443
          },
          "point_estimate": 2721.9351495926385,
          "standard_error": 1506.8598935196264
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1552456.0757112326,
            "upper_bound": 1555315.6118887062
          },
          "point_estimate": 1554045.713744589,
          "standard_error": 738.1030715709053
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1514.528648458711,
            "upper_bound": 6322.072450548008
          },
          "point_estimate": 4448.40803766057,
          "standard_error": 1279.8082895884695
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443727.06875,
            "upper_bound": 1448102.4814495193
          },
          "point_estimate": 1445734.884647436,
          "standard_error": 1123.3358749702536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1442913.4038461538,
            "upper_bound": 1448081.3076923075
          },
          "point_estimate": 1444336.023076923,
          "standard_error": 1506.5304825501507
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395.3801886537949,
            "upper_bound": 6424.457803443143
          },
          "point_estimate": 3421.252451183732,
          "standard_error": 1445.146835669986
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1443849.5735782776,
            "upper_bound": 1447635.90438787
          },
          "point_estimate": 1446093.4636363636,
          "standard_error": 980.5278717597756
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1789.2881119224407,
            "upper_bound": 5003.43028863779
          },
          "point_estimate": 3737.929154785843,
          "standard_error": 902.2582458125588
        }
      }
    },
    "memrmem/bstr/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389793.06661790784,
            "upper_bound": 390445.2764420804
          },
          "point_estimate": 390138.6726789936,
          "standard_error": 167.21715357040506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 389704.1382978724,
            "upper_bound": 390534.1879432624
          },
          "point_estimate": 390310.8456264775,
          "standard_error": 204.11405663040568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.88038523005392,
            "upper_bound": 923.7570463659754
          },
          "point_estimate": 476.3430734580946,
          "standard_error": 210.2023932603472
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390107.6322859014,
            "upper_bound": 390665.28235984466
          },
          "point_estimate": 390424.59671179886,
          "standard_error": 144.47109086569094
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.09926084330417,
            "upper_bound": 713.445039486885
          },
          "point_estimate": 557.6495706139965,
          "standard_error": 118.33570667644976
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560751.9807509157,
            "upper_bound": 561329.6599099359
          },
          "point_estimate": 561052.5132655677,
          "standard_error": 148.34992197742017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560722.6884615384,
            "upper_bound": 561475.676923077
          },
          "point_estimate": 561098.5131868132,
          "standard_error": 225.75115948230655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 110.16002919818638,
            "upper_bound": 903.7611060703822
          },
          "point_estimate": 535.8586745251467,
          "standard_error": 187.53113166312076
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560863.6561433087,
            "upper_bound": 561325.4734000223
          },
          "point_estimate": 561131.1632767232,
          "standard_error": 118.10085673181948
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 277.0489441346953,
            "upper_bound": 639.4262439175644
          },
          "point_estimate": 496.3237039058246,
          "standard_error": 98.33653030395728
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413807.5455871211,
            "upper_bound": 414489.0682893669
          },
          "point_estimate": 414134.683641775,
          "standard_error": 175.13064121227742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413670.34204545454,
            "upper_bound": 414626.51515151514
          },
          "point_estimate": 413967.4791666667,
          "standard_error": 274.5153814693066
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.49333287505894,
            "upper_bound": 926.8114317275904
          },
          "point_estimate": 676.6114543513999,
          "standard_error": 218.44309486651116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 413613.13410859567,
            "upper_bound": 414424.566470666
          },
          "point_estimate": 413904.714905549,
          "standard_error": 208.13269962501417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 335.5296138678514,
            "upper_bound": 725.0712269361654
          },
          "point_estimate": 583.2162945575295,
          "standard_error": 101.36482598959944
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485566.3057294444,
            "upper_bound": 486438.2778407936
          },
          "point_estimate": 486017.29022539675,
          "standard_error": 223.58030393069217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485349.36,
            "upper_bound": 486618.73
          },
          "point_estimate": 486174.6964444445,
          "standard_error": 303.6984877522255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.63248569965432,
            "upper_bound": 1271.1112057666203
          },
          "point_estimate": 697.5446727493947,
          "standard_error": 290.8125098255531
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 485912.7235351359,
            "upper_bound": 486553.64454787655
          },
          "point_estimate": 486215.8316190476,
          "standard_error": 161.51733776004616
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.6583331679184,
            "upper_bound": 925.8328630011424
          },
          "point_estimate": 747.0445888694732,
          "standard_error": 131.02699519376225
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971119.4676008772,
            "upper_bound": 973065.1353347952
          },
          "point_estimate": 972144.0586695909,
          "standard_error": 498.4840716297173
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971188.6026315788,
            "upper_bound": 973330.7934210526
          },
          "point_estimate": 972279.245979532,
          "standard_error": 652.5785160844458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.97875445929935,
            "upper_bound": 2750.9330385296403
          },
          "point_estimate": 1355.379240410929,
          "standard_error": 622.7344518802366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 971640.5852495652,
            "upper_bound": 972849.316773596
          },
          "point_estimate": 972126.3335611756,
          "standard_error": 306.8597615765759
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 834.3541237774236,
            "upper_bound": 2207.7720070218625
          },
          "point_estimate": 1658.8630393234537,
          "standard_error": 375.2442001730045
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298339.1856898907,
            "upper_bound": 298843.8454697258
          },
          "point_estimate": 298595.667358509,
          "standard_error": 129.25571829533942
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298254.23053278687,
            "upper_bound": 298992.6319672131
          },
          "point_estimate": 298559.7650273224,
          "standard_error": 190.41370166661343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.55091396318629,
            "upper_bound": 816.8690391861884
          },
          "point_estimate": 543.4264118071629,
          "standard_error": 191.83381286139044
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 298341.01347569865,
            "upper_bound": 298920.24290071323
          },
          "point_estimate": 298676.0189482648,
          "standard_error": 149.40422721773072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.8727062784766,
            "upper_bound": 521.9559941980797
          },
          "point_estimate": 430.83490618316887,
          "standard_error": 67.71090970299478
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 801570.0843676674,
            "upper_bound": 803434.1542009575
          },
          "point_estimate": 802491.9931530367,
          "standard_error": 475.83140917929126
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800906.2413043479,
            "upper_bound": 803571.7676630435
          },
          "point_estimate": 802430.39689441,
          "standard_error": 588.2120878641654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 38.668693661279555,
            "upper_bound": 2781.56014192176
          },
          "point_estimate": 1958.2228797997464,
          "standard_error": 776.27980033529
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 801172.3305074482,
            "upper_bound": 803360.7064935457
          },
          "point_estimate": 801999.0948051948,
          "standard_error": 563.042055460151
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 912.2547405989455,
            "upper_bound": 1975.2257820741584
          },
          "point_estimate": 1578.8164899538335,
          "standard_error": 276.0349638322767
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398980.93799344375,
            "upper_bound": 399352.08102946
          },
          "point_estimate": 399161.1554912871,
          "standard_error": 94.72940170886012
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398823.24516908213,
            "upper_bound": 399347.2041925466
          },
          "point_estimate": 399226.4293478261,
          "standard_error": 135.55647735826028
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.868595133461305,
            "upper_bound": 630.9505900665192
          },
          "point_estimate": 312.30404412944097,
          "standard_error": 151.94215004321754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398899.0443437041,
            "upper_bound": 399221.0000961474
          },
          "point_estimate": 399042.3783455675,
          "standard_error": 81.39863777102131
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 181.03164357036948,
            "upper_bound": 407.25811531071366
          },
          "point_estimate": 315.75022458165154,
          "standard_error": 61.50645424256682
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165763.74732803326,
            "upper_bound": 166112.6760440449
          },
          "point_estimate": 165940.48307186345,
          "standard_error": 89.57351040710233
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165698.4402587519,
            "upper_bound": 166188.32933789954
          },
          "point_estimate": 165973.19814090015,
          "standard_error": 129.45093028601306
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.41188394532831,
            "upper_bound": 498.2283981409942
          },
          "point_estimate": 338.20792793399363,
          "standard_error": 105.81931446902148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165631.30694488692,
            "upper_bound": 166122.96235350665
          },
          "point_estimate": 165836.14707940462,
          "standard_error": 127.09731788818335
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173.77219317371996,
            "upper_bound": 370.20979017770577
          },
          "point_estimate": 298.9444306013208,
          "standard_error": 50.15017187019035
        }
      }
    },
    "memrmem/bstr/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232296.5898089172,
            "upper_bound": 232659.49157719137
          },
          "point_estimate": 232476.3744085532,
          "standard_error": 92.90241060365852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232251.76135881105,
            "upper_bound": 232856.97179253868
          },
          "point_estimate": 232426.44585987265,
          "standard_error": 146.31947311232952
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.845161466614964,
            "upper_bound": 536.2882029630883
          },
          "point_estimate": 301.4474362087681,
          "standard_error": 140.2647213244594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232323.58992412177,
            "upper_bound": 232648.73734772907
          },
          "point_estimate": 232464.06026966663,
          "standard_error": 82.51105561472265
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 183.1426481374151,
            "upper_bound": 377.52766540702106
          },
          "point_estimate": 310.4941855037294,
          "standard_error": 48.79433245629107
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172770.16248025277,
            "upper_bound": 173183.32898894153
          },
          "point_estimate": 172938.72582938388,
          "standard_error": 110.18464753358371
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172761.14454976303,
            "upper_bound": 172974.73696682465
          },
          "point_estimate": 172838.80766192733,
          "standard_error": 57.997453408590246
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.887308817653217,
            "upper_bound": 322.0872321964902
          },
          "point_estimate": 130.73697729982354,
          "standard_error": 80.64185372024721
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172813.8324303809,
            "upper_bound": 172930.9298451394
          },
          "point_estimate": 172863.87358897028,
          "standard_error": 30.022428313520074
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.19169116612977,
            "upper_bound": 550.9435657290014
          },
          "point_estimate": 367.4576261018019,
          "standard_error": 149.9991241784531
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649487.3949829933,
            "upper_bound": 650133.5212181122
          },
          "point_estimate": 649797.1209204933,
          "standard_error": 165.56909570245202
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649428.625,
            "upper_bound": 650180.1642857143
          },
          "point_estimate": 649677.9787946428,
          "standard_error": 214.65108795896973
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.77031111936262,
            "upper_bound": 930.9939257930102
          },
          "point_estimate": 519.115172033888,
          "standard_error": 219.3380245783617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649278.9807980118,
            "upper_bound": 649930.86375
          },
          "point_estimate": 649555.7592300556,
          "standard_error": 164.882342746399
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 276.91171544618874,
            "upper_bound": 731.0751923090544
          },
          "point_estimate": 552.181062981338,
          "standard_error": 120.99860081938976
        }
      }
    },
    "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1406.0202939741216,
            "upper_bound": 1407.7286298930023
          },
          "point_estimate": 1406.930959274517,
          "standard_error": 0.4381751032191197
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1406.0742016644087,
            "upper_bound": 1407.950435455777
          },
          "point_estimate": 1407.356226602892,
          "standard_error": 0.5823708489994343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1234133614629678,
            "upper_bound": 2.3325512680528298
          },
          "point_estimate": 0.9416991832040326,
          "standard_error": 0.5907243982625127
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1406.122140876788,
            "upper_bound": 1407.775837950663
          },
          "point_estimate": 1407.144764552266,
          "standard_error": 0.4289977140475224
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.671040875183835,
            "upper_bound": 1.9201421189229184
          },
          "point_estimate": 1.4575158382691198,
          "standard_error": 0.33382978113679035
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1304137.2141308817,
            "upper_bound": 1306705.1780952378
          },
          "point_estimate": 1305340.2543168934,
          "standard_error": 658.1663072608006
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303667.5612244897,
            "upper_bound": 1307176.638392857
          },
          "point_estimate": 1304685.45,
          "standard_error": 900.2012953311223
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296.9460321089612,
            "upper_bound": 3563.9369785130643
          },
          "point_estimate": 1760.822802072433,
          "standard_error": 860.6899008619828
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303899.060301751,
            "upper_bound": 1306501.4133037478
          },
          "point_estimate": 1304750.1226345084,
          "standard_error": 675.8642254618863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 990.8180686295948,
            "upper_bound": 2736.322444985306
          },
          "point_estimate": 2185.053630892944,
          "standard_error": 445.3901476074172
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1491862.8714866666,
            "upper_bound": 1496320.4626877776
          },
          "point_estimate": 1493773.8532444446,
          "standard_error": 1166.497661718888
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1491082.818888889,
            "upper_bound": 1494863.1333333333
          },
          "point_estimate": 1493276.14,
          "standard_error": 916.7322636841452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.56509089464384,
            "upper_bound": 4311.821781849839
          },
          "point_estimate": 2610.2879173915876,
          "standard_error": 1164.8674525948843
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1491127.0690633007,
            "upper_bound": 1493388.4390391305
          },
          "point_estimate": 1492101.8977662337,
          "standard_error": 586.3248622522251
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1185.8612301210203,
            "upper_bound": 5678.281584241517
          },
          "point_estimate": 3899.737976996024,
          "standard_error": 1371.792645543852
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1397552.1102303113,
            "upper_bound": 1401726.5231776556
          },
          "point_estimate": 1399533.5375335775,
          "standard_error": 1066.022917262171
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1397251.6675824174,
            "upper_bound": 1401672.3076923075
          },
          "point_estimate": 1398742.6230769232,
          "standard_error": 1106.260392045853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 368.63626422487386,
            "upper_bound": 5919.340133372135
          },
          "point_estimate": 3091.2317161451833,
          "standard_error": 1344.7073949208425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1398030.1967610486,
            "upper_bound": 1400701.1917148833
          },
          "point_estimate": 1399220.3545454545,
          "standard_error": 697.3841391766673
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1663.9948263607512,
            "upper_bound": 4748.264010294399
          },
          "point_estimate": 3560.9918229756167,
          "standard_error": 816.9136262418408
        }
      }
    },
    "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/bstr_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752520.3138059402,
            "upper_bound": 755019.3102380952
          },
          "point_estimate": 753641.2364747326,
          "standard_error": 640.4984751336377
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752158.1959183674,
            "upper_bound": 755292.8571428572
          },
          "point_estimate": 752814.6731049563,
          "standard_error": 655.4363462966237
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253.85742406455665,
            "upper_bound": 2878.5434917527837
          },
          "point_estimate": 995.909264870098,
          "standard_error": 626.5767373699734
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752279.4704392295,
            "upper_bound": 753425.8328288704
          },
          "point_estimate": 752640.6689636894,
          "standard_error": 297.72181571672905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 536.3805517212619,
            "upper_bound": 2704.341927188757
          },
          "point_estimate": 2142.144253873861,
          "standard_error": 570.7657377617768
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265602.75443898444,
            "upper_bound": 266271.12492670317
          },
          "point_estimate": 265902.1982858301,
          "standard_error": 171.71384572921613
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265466.56934306567,
            "upper_bound": 266164.5556569343
          },
          "point_estimate": 265744.05393349554,
          "standard_error": 194.24014239403303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.20195173188237,
            "upper_bound": 852.780833266484
          },
          "point_estimate": 562.4500020218366,
          "standard_error": 197.2926324156156
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265571.1107729618,
            "upper_bound": 265970.5476851376
          },
          "point_estimate": 265763.7748980946,
          "standard_error": 100.23872992160292
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 244.13133127406465,
            "upper_bound": 800.0028720990911
          },
          "point_estimate": 570.8664605485684,
          "standard_error": 164.3735921629149
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474950.7288064317,
            "upper_bound": 475812.9367604617
          },
          "point_estimate": 475384.405452484,
          "standard_error": 220.61025549915857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474563.21428571426,
            "upper_bound": 475933.4301948052
          },
          "point_estimate": 475582.84451659454,
          "standard_error": 391.09607190102736
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.87833675667991,
            "upper_bound": 1201.7315173013292
          },
          "point_estimate": 824.9706126265745,
          "standard_error": 317.0050566094485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475073.4956001675,
            "upper_bound": 475761.9987305927
          },
          "point_estimate": 475484.22583909595,
          "standard_error": 175.65251199469682
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 469.4392400366593,
            "upper_bound": 882.5437677745392
          },
          "point_estimate": 734.9075730119723,
          "standard_error": 106.73697000121528
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240729.69628973503,
            "upper_bound": 241121.43850721384
          },
          "point_estimate": 240925.2249203721,
          "standard_error": 100.50624182710922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240642.13352649007,
            "upper_bound": 241163.9668874172
          },
          "point_estimate": 240932.16799116996,
          "standard_error": 147.33102796470686
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.86657936606236,
            "upper_bound": 540.6691091097861
          },
          "point_estimate": 340.7317477918811,
          "standard_error": 134.97258542555903
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240672.73242753852,
            "upper_bound": 241184.5836776433
          },
          "point_estimate": 240883.77230583987,
          "standard_error": 130.92946917405823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 188.5319687194952,
            "upper_bound": 425.2711686124891
          },
          "point_estimate": 333.7148798829628,
          "standard_error": 60.45401160647099
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706476.9683424908,
            "upper_bound": 707759.9108701925
          },
          "point_estimate": 707155.4683745422,
          "standard_error": 327.81233265122523
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706489.367032967,
            "upper_bound": 708177.9935897436
          },
          "point_estimate": 707381.6073717949,
          "standard_error": 446.5313541783904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.90618761677946,
            "upper_bound": 1898.5975682162652
          },
          "point_estimate": 1277.38240905273,
          "standard_error": 460.08111386328306
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 706600.3423610709,
            "upper_bound": 707601.0713570914
          },
          "point_estimate": 706960.2517482517,
          "standard_error": 254.64233264865769
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579.6628417136833,
            "upper_bound": 1438.679666051946
          },
          "point_estimate": 1090.407493391959,
          "standard_error": 238.96248424085272
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.4672255314745,
            "upper_bound": 357.7325672105334
          },
          "point_estimate": 357.61764109824406,
          "standard_error": 0.06914714458795326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.5030012440135,
            "upper_bound": 357.7535468316173
          },
          "point_estimate": 357.7035863319139,
          "standard_error": 0.05632353500857758
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020380944301681903,
            "upper_bound": 0.2950096854063778
          },
          "point_estimate": 0.12129829372010228,
          "standard_error": 0.0736072407306714
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.36991276702264,
            "upper_bound": 357.7364449068442
          },
          "point_estimate": 357.56344639950015,
          "standard_error": 0.09283385899514283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06066553013034372,
            "upper_bound": 0.32362754093272217
          },
          "point_estimate": 0.2298050706214385,
          "standard_error": 0.0735549233896039
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.65760164301588,
            "upper_bound": 170.01650928040252
          },
          "point_estimate": 169.80716512450778,
          "standard_error": 0.0939150596270886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.65201984794666,
            "upper_bound": 169.8506891215388
          },
          "point_estimate": 169.73416327818285,
          "standard_error": 0.06827355812103235
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02309055682039533,
            "upper_bound": 0.29454705979845874
          },
          "point_estimate": 0.14149078198046763,
          "standard_error": 0.07562229419709154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.69177614580906,
            "upper_bound": 169.84002154206752
          },
          "point_estimate": 169.78432409990458,
          "standard_error": 0.037938682474338195
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08220791588632782,
            "upper_bound": 0.46630922137588343
          },
          "point_estimate": 0.3141040382998405,
          "standard_error": 0.1213878161431719
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.384258409331174,
            "upper_bound": 25.543852136685885
          },
          "point_estimate": 25.459472931796117,
          "standard_error": 0.041023188352859945
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.34993377723625,
            "upper_bound": 25.555051145083063
          },
          "point_estimate": 25.432973426503196,
          "standard_error": 0.04943412751567432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025366596187809804,
            "upper_bound": 0.22357210780813205
          },
          "point_estimate": 0.12351932135847749,
          "standard_error": 0.05103636143645123
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.39516584219102,
            "upper_bound": 25.50283081827483
          },
          "point_estimate": 25.44836272127458,
          "standard_error": 0.026760418901925945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06304371241750031,
            "upper_bound": 0.17863056857370876
          },
          "point_estimate": 0.13670650339802165,
          "standard_error": 0.03012934302719889
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.948085511555227,
            "upper_bound": 18.00276209456912
          },
          "point_estimate": 17.975047395749176,
          "standard_error": 0.01393021407740948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.941254610708043,
            "upper_bound": 18.00082027069313
          },
          "point_estimate": 17.97934974436447,
          "standard_error": 0.013978877959189765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007586718151422026,
            "upper_bound": 0.08193688671267566
          },
          "point_estimate": 0.03584374053502772,
          "standard_error": 0.01826112516300514
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.956316438344082,
            "upper_bound": 18.000465172436694
          },
          "point_estimate": 17.97842750770398,
          "standard_error": 0.011842483711619342
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02186642578937085,
            "upper_bound": 0.06180959762230331
          },
          "point_estimate": 0.04632346013595693,
          "standard_error": 0.010124175030018842
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.236178675909507,
            "upper_bound": 25.31301470652985
          },
          "point_estimate": 25.27646728412295,
          "standard_error": 0.019702905331382917
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.22154178172223,
            "upper_bound": 25.324251645160977
          },
          "point_estimate": 25.29565823611206,
          "standard_error": 0.02541135742961439
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012258588151660027,
            "upper_bound": 0.10739638931547124
          },
          "point_estimate": 0.04419132978918485,
          "standard_error": 0.027086875134144865
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.281022200154315,
            "upper_bound": 25.340727396142153
          },
          "point_estimate": 25.3125130061204,
          "standard_error": 0.01613246227532315
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.030448773383040653,
            "upper_bound": 0.08140772099915478
          },
          "point_estimate": 0.06542513556892142,
          "standard_error": 0.012378864825749607
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285680.7654328884,
            "upper_bound": 286462.0221605282
          },
          "point_estimate": 286012.5690442088,
          "standard_error": 203.8741007571886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285551.8895089286,
            "upper_bound": 286167.3341471354
          },
          "point_estimate": 285949.6921006944,
          "standard_error": 198.04961378385056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.98178881082552,
            "upper_bound": 791.2889433541741
          },
          "point_estimate": 511.3068397505997,
          "standard_error": 193.10855972732423
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285730.28060269926,
            "upper_bound": 286063.7327863975
          },
          "point_estimate": 285921.2875,
          "standard_error": 84.71004862923289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237.51173162621788,
            "upper_bound": 995.5701070588988
          },
          "point_estimate": 679.5875473728195,
          "standard_error": 240.2410419747619
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.07082731736972,
            "upper_bound": 16.129920008213954
          },
          "point_estimate": 16.099478307349255,
          "standard_error": 0.01509170759171344
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.05812301119295,
            "upper_bound": 16.13399641736576
          },
          "point_estimate": 16.097173851538436,
          "standard_error": 0.01914160464303588
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015049864631155553,
            "upper_bound": 0.0862481019720571
          },
          "point_estimate": 0.05624495499735692,
          "standard_error": 0.017702356114736535
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.083117314912535,
            "upper_bound": 16.135691667934832
          },
          "point_estimate": 16.115047528496785,
          "standard_error": 0.013373152761243503
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026807423148551012,
            "upper_bound": 0.0654088044133146
          },
          "point_estimate": 0.05041171194970885,
          "standard_error": 0.010180214568653142
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.575784926204356,
            "upper_bound": 22.621940959058353
          },
          "point_estimate": 22.59798655932486,
          "standard_error": 0.01174086395095239
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.573879020598525,
            "upper_bound": 22.616035320637387
          },
          "point_estimate": 22.5954558880684,
          "standard_error": 0.009664178549303765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005114480126845031,
            "upper_bound": 0.06344429238316045
          },
          "point_estimate": 0.022730576448373955,
          "standard_error": 0.014988622853062556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.569974075434175,
            "upper_bound": 22.64390159053744
          },
          "point_estimate": 22.612440056733007,
          "standard_error": 0.018763935965476264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015508524303991671,
            "upper_bound": 0.05383225913726012
          },
          "point_estimate": 0.03908502058747977,
          "standard_error": 0.009868560478098622
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126330.82299761284,
            "upper_bound": 126498.48616307387
          },
          "point_estimate": 126414.6405479773,
          "standard_error": 42.90894073546245
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126278.88151041669,
            "upper_bound": 126532.0429976852
          },
          "point_estimate": 126404.6925154321,
          "standard_error": 63.965664403999945
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.204079571079603,
            "upper_bound": 246.98210832352487
          },
          "point_estimate": 176.81890110562674,
          "standard_error": 55.88391453238841
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126340.42216614907,
            "upper_bound": 126508.85433253286
          },
          "point_estimate": 126430.91871392496,
          "standard_error": 43.54109795788498
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.07803740849087,
            "upper_bound": 179.40211288767955
          },
          "point_estimate": 143.14602779784977,
          "standard_error": 24.473446780828528
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.28385858251638,
            "upper_bound": 24.3384221565621
          },
          "point_estimate": 24.30926243677877,
          "standard_error": 0.014000675714488242
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.275934432836976,
            "upper_bound": 24.3291113508741
          },
          "point_estimate": 24.30718219271156,
          "standard_error": 0.013483848672118244
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007690189282912278,
            "upper_bound": 0.07060599283790459
          },
          "point_estimate": 0.035043237727648745,
          "standard_error": 0.015465928958133975
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.28312650829125,
            "upper_bound": 24.32296670476957
          },
          "point_estimate": 24.30073626137172,
          "standard_error": 0.010001696347023132
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02016643458735346,
            "upper_bound": 0.06442290222550988
          },
          "point_estimate": 0.046666730732389174,
          "standard_error": 0.012256105043274096
        }
      }
    },
    "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.99171133327572,
            "upper_bound": 21.095020520689086
          },
          "point_estimate": 21.044314884778263,
          "standard_error": 0.0263476055787479
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.998707842162645,
            "upper_bound": 21.10478613880661
          },
          "point_estimate": 21.04481047583777,
          "standard_error": 0.0251151215381724
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014789021046671857,
            "upper_bound": 0.1480464764935241
          },
          "point_estimate": 0.07863583990610311,
          "standard_error": 0.032974492023909036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.929661841982817,
            "upper_bound": 21.05351630266598
          },
          "point_estimate": 20.98312389257919,
          "standard_error": 0.03257892365724246
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03844559922405896,
            "upper_bound": 0.11843173374519383
          },
          "point_estimate": 0.0874225405077177,
          "standard_error": 0.020551490435762813
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.06730719423215,
            "upper_bound": 42.12031081239667
          },
          "point_estimate": 42.0938714125907,
          "standard_error": 0.013568508434553293
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.04490322500972,
            "upper_bound": 42.13423390920174
          },
          "point_estimate": 42.09207384985511,
          "standard_error": 0.020204488952247045
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007891823535797447,
            "upper_bound": 0.07971730946607049
          },
          "point_estimate": 0.06342890922761478,
          "standard_error": 0.019508660413089313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.05820289017929,
            "upper_bound": 42.116981512745255
          },
          "point_estimate": 42.08580638003277,
          "standard_error": 0.01488461004705314
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.027316006233991823,
            "upper_bound": 0.05443834194802791
          },
          "point_estimate": 0.04524762026659378,
          "standard_error": 0.006889024498098701
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.97266391738512,
            "upper_bound": 75.02111317883323
          },
          "point_estimate": 74.99739948053924,
          "standard_error": 0.012440430728873152
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.95837328763592,
            "upper_bound": 75.03507822794677
          },
          "point_estimate": 75.00790585380082,
          "standard_error": 0.025487177243370854
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004745982709105168,
            "upper_bound": 0.06637041538438343
          },
          "point_estimate": 0.05079541809081141,
          "standard_error": 0.017599035171001053
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.97468925264916,
            "upper_bound": 75.0274246782046
          },
          "point_estimate": 75.00078460542075,
          "standard_error": 0.013363222759719752
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026974113177466723,
            "upper_bound": 0.04824730529806506
          },
          "point_estimate": 0.04150494281155083,
          "standard_error": 0.005398526745226727
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/bstr_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.893043458040532,
            "upper_bound": 10.939924593333824
          },
          "point_estimate": 10.917439985850567,
          "standard_error": 0.01196531382552125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.886153981127544,
            "upper_bound": 10.942936004086617
          },
          "point_estimate": 10.929581718658644,
          "standard_error": 0.016012116268031666
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006059291238128976,
            "upper_bound": 0.06781103537384621
          },
          "point_estimate": 0.03717443870281098,
          "standard_error": 0.017491947252379406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.889515042340712,
            "upper_bound": 10.93534150474882
          },
          "point_estimate": 10.91454028864417,
          "standard_error": 0.011745626972576036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020876749802691857,
            "upper_bound": 0.05205412215332081
          },
          "point_estimate": 0.03986146936569283,
          "standard_error": 0.008208630111915775
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31939.230351650767,
            "upper_bound": 32022.943744110384
          },
          "point_estimate": 31975.040136622305,
          "standard_error": 21.753529369350403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31934.77987697715,
            "upper_bound": 31987.626098418277
          },
          "point_estimate": 31968.898396309312,
          "standard_error": 15.606643061859366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.082363797277022,
            "upper_bound": 75.79189814915424
          },
          "point_estimate": 34.16873985911304,
          "standard_error": 17.860722919611973
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 31923.43134432567,
            "upper_bound": 31968.23212657023
          },
          "point_estimate": 31942.40556912332,
          "standard_error": 11.18836999887813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.57736391278849,
            "upper_bound": 106.73468979950208
          },
          "point_estimate": 72.43831254345018,
          "standard_error": 26.48496936741048
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/bstr_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.77637899248986,
            "upper_bound": 30.123872702865004
          },
          "point_estimate": 29.95440888519609,
          "standard_error": 0.08864260440676575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.67257906530408,
            "upper_bound": 30.245405269615446
          },
          "point_estimate": 29.95316213566648,
          "standard_error": 0.1256727274751426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05878155491563735,
            "upper_bound": 0.5568448807145338
          },
          "point_estimate": 0.3237366437059246,
          "standard_error": 0.13708645081928386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.687430302070418,
            "upper_bound": 30.238837763502556
          },
          "point_estimate": 29.98996459320129,
          "standard_error": 0.15139459300601457
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.167458261469293,
            "upper_bound": 0.3645159353088609
          },
          "point_estimate": 0.2961826764826986,
          "standard_error": 0.05085069211443323
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978622.1800543026,
            "upper_bound": 979702.26103031
          },
          "point_estimate": 979132.9942303678,
          "standard_error": 277.0509737558327
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978356.8973684212,
            "upper_bound": 979789.7719298246
          },
          "point_estimate": 978984.961988304,
          "standard_error": 370.1033844239162
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 234.47197820589955,
            "upper_bound": 1471.8397442642731
          },
          "point_estimate": 1062.1898935107956,
          "standard_error": 346.7462189827165
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978480.6098254856,
            "upper_bound": 979320.9371502084
          },
          "point_estimate": 978852.3902939166,
          "standard_error": 214.61026771947223
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461.5013382172497,
            "upper_bound": 1183.275028561136
          },
          "point_estimate": 923.1950538405072,
          "standard_error": 187.96333087151217
        }
      }
    },
    "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/bstr_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.3693219782388,
            "upper_bound": 1961.0291612771016
          },
          "point_estimate": 1959.7069609344733,
          "standard_error": 0.6812619078990163
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.47989255505,
            "upper_bound": 1961.7422175529337
          },
          "point_estimate": 1959.753704003017,
          "standard_error": 1.1132299199078264
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6094741446671773,
            "upper_bound": 3.7927672673988617
          },
          "point_estimate": 2.9183006431954635,
          "standard_error": 0.8513361860328461
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.0670415761897,
            "upper_bound": 1960.558366657907
          },
          "point_estimate": 1959.3008082160825,
          "standard_error": 0.6365435728639991
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.404589176351614,
            "upper_bound": 2.715857539885321
          },
          "point_estimate": 2.267525923213691,
          "standard_error": 0.33188332141442
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.823710960173559,
            "upper_bound": 7.836135168332813
          },
          "point_estimate": 7.82939781725157,
          "standard_error": 0.00320429327568695
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.820496590439274,
            "upper_bound": 7.834857017313439
          },
          "point_estimate": 7.82765959520319,
          "standard_error": 0.003943311940356296
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002078001797630963,
            "upper_bound": 0.01698032258074923
          },
          "point_estimate": 0.010180459457926012,
          "standard_error": 0.003660289179326164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.825994874480949,
            "upper_bound": 7.834942576210606
          },
          "point_estimate": 7.831268023946688,
          "standard_error": 0.0022914704364327175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004842067903332543,
            "upper_bound": 0.01456739318852325
          },
          "point_estimate": 0.010666715352820311,
          "standard_error": 0.0027790394518295066
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.102550819414023,
            "upper_bound": 7.114400529902205
          },
          "point_estimate": 7.108498193286015,
          "standard_error": 0.0029986405368049125
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.103930447373872,
            "upper_bound": 7.112478372193131
          },
          "point_estimate": 7.109435679176877,
          "standard_error": 0.0020714017709784686
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0003958152338563292,
            "upper_bound": 0.016438722405333988
          },
          "point_estimate": 0.004887017967256981,
          "standard_error": 0.003805567983366628
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.107065461262121,
            "upper_bound": 7.113398467786628
          },
          "point_estimate": 7.110161688618763,
          "standard_error": 0.0015873583263792542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0032517205629447746,
            "upper_bound": 0.013964829838436076
          },
          "point_estimate": 0.010036382581850571,
          "standard_error": 0.0027775734062333883
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.309620576413845,
            "upper_bound": 8.32286651606246
          },
          "point_estimate": 8.31635687261661,
          "standard_error": 0.0033864101707791203
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.308933957976048,
            "upper_bound": 8.3254747438911
          },
          "point_estimate": 8.315823495073321,
          "standard_error": 0.004662773381861565
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002067284611160055,
            "upper_bound": 0.020380788817134104
          },
          "point_estimate": 0.01110434554713075,
          "standard_error": 0.004268578042930292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.311672886139885,
            "upper_bound": 8.320298614733435
          },
          "point_estimate": 8.315471355669374,
          "standard_error": 0.0021774662407967734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006184896734227857,
            "upper_bound": 0.014802940158941406
          },
          "point_estimate": 0.011323314977691248,
          "standard_error": 0.002250433922964652
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.22790522782974,
            "upper_bound": 22.27661050403569
          },
          "point_estimate": 22.250627194028223,
          "standard_error": 0.012537492574554724
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.22030310361689,
            "upper_bound": 22.282087445985553
          },
          "point_estimate": 22.24012736406865,
          "standard_error": 0.015153557189446526
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006787790332527025,
            "upper_bound": 0.06827206663405828
          },
          "point_estimate": 0.03237831262934619,
          "standard_error": 0.015413584929228449
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.227728620382532,
            "upper_bound": 22.260002542560716
          },
          "point_estimate": 22.244719235036047,
          "standard_error": 0.008180356388007088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016737713348307443,
            "upper_bound": 0.05227769305558016
          },
          "point_estimate": 0.04176518725888151,
          "standard_error": 0.008883693921745662
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.219509604139942,
            "upper_bound": 14.240775016251536
          },
          "point_estimate": 14.231565376282427,
          "standard_error": 0.0055193069279950106
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.226839981532382,
            "upper_bound": 14.24173971866668
          },
          "point_estimate": 14.233941240481553,
          "standard_error": 0.003295476668700583
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001129405353261296,
            "upper_bound": 0.022390803300553364
          },
          "point_estimate": 0.007495767906460234,
          "standard_error": 0.0055403365008470625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.23220695254837,
            "upper_bound": 14.240479701830292
          },
          "point_estimate": 14.236092707645716,
          "standard_error": 0.002089932326777956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005035408749292691,
            "upper_bound": 0.026849981034129968
          },
          "point_estimate": 0.018464950831371466,
          "standard_error": 0.0064207200780307255
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.635546861089384,
            "upper_bound": 18.66138534308868
          },
          "point_estimate": 18.64904662390584,
          "standard_error": 0.006619206886938188
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.63153565211007,
            "upper_bound": 18.663986519663467
          },
          "point_estimate": 18.653855928473483,
          "standard_error": 0.007725700996373463
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0047329233788567775,
            "upper_bound": 0.03673439457917173
          },
          "point_estimate": 0.017211926800752397,
          "standard_error": 0.008575008766830667
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.647509327181,
            "upper_bound": 18.66080237887272
          },
          "point_estimate": 18.653795244791848,
          "standard_error": 0.003350879107039016
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01004376133201317,
            "upper_bound": 0.028270495309198264
          },
          "point_estimate": 0.02212384780485151,
          "standard_error": 0.004531915473252565
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.76332431420991,
            "upper_bound": 10.77663521700764
          },
          "point_estimate": 10.770266041590396,
          "standard_error": 0.003407442386539738
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.764293268653775,
            "upper_bound": 10.77961420229662
          },
          "point_estimate": 10.769380386361911,
          "standard_error": 0.0047855565464678465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007326775162297186,
            "upper_bound": 0.02092762119360662
          },
          "point_estimate": 0.01014119534474067,
          "standard_error": 0.004782943123563969
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.766173111131316,
            "upper_bound": 10.77600070681157
          },
          "point_estimate": 10.770711540884228,
          "standard_error": 0.002534052598304261
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006061250903298086,
            "upper_bound": 0.015161403483151578
          },
          "point_estimate": 0.011350887298521515,
          "standard_error": 0.002533556821898153
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.07345592283363,
            "upper_bound": 16.16010149282202
          },
          "point_estimate": 16.11560937068517,
          "standard_error": 0.022075886923717473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.07280267348205,
            "upper_bound": 16.154551478494685
          },
          "point_estimate": 16.11786698783863,
          "standard_error": 0.022230766133432913
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008867479588106538,
            "upper_bound": 0.1251013036589876
          },
          "point_estimate": 0.06033852521557828,
          "standard_error": 0.026758589103918783
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.07824335798441,
            "upper_bound": 16.14133494873355
          },
          "point_estimate": 16.107591802824714,
          "standard_error": 0.016702890185670744
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031113804760159947,
            "upper_bound": 0.10052760092022812
          },
          "point_estimate": 0.07365565516422715,
          "standard_error": 0.01811711131621189
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.5553420748912,
            "upper_bound": 22.586593546818836
          },
          "point_estimate": 22.56985133268143,
          "standard_error": 0.008022751594715848
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.55564822851241,
            "upper_bound": 22.589554809636937
          },
          "point_estimate": 22.558889324411343,
          "standard_error": 0.008537975817804749
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0010156026361650137,
            "upper_bound": 0.04097565626334604
          },
          "point_estimate": 0.01010409318418737,
          "standard_error": 0.011603540083787929
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.55359814102157,
            "upper_bound": 22.592195005699132
          },
          "point_estimate": 22.570804442700737,
          "standard_error": 0.010276485815700767
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009482286291285277,
            "upper_bound": 0.03447105372540534
          },
          "point_estimate": 0.02678309522520732,
          "standard_error": 0.006249118447255778
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.998407960088944,
            "upper_bound": 11.01493379845434
          },
          "point_estimate": 11.00587401005268,
          "standard_error": 0.004262338468973454
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.99486659224425,
            "upper_bound": 11.014413252335272
          },
          "point_estimate": 11.00268708463013,
          "standard_error": 0.005170094031338637
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002021778816235808,
            "upper_bound": 0.02236994854829016
          },
          "point_estimate": 0.011888248255296982,
          "standard_error": 0.004746693496531659
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.997272067339676,
            "upper_bound": 11.005253138614348
          },
          "point_estimate": 11.00035267349511,
          "standard_error": 0.0020260184834854883
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006260647242652568,
            "upper_bound": 0.01957314904303207
          },
          "point_estimate": 0.014192998091473195,
          "standard_error": 0.0038578435464154287
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.2444363169646,
            "upper_bound": 24.30279040873025
          },
          "point_estimate": 24.27215248237814,
          "standard_error": 0.014993408023293634
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.23771742134581,
            "upper_bound": 24.311475045694557
          },
          "point_estimate": 24.265875587994536,
          "standard_error": 0.016261173472429998
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006623396152094617,
            "upper_bound": 0.08913259510432331
          },
          "point_estimate": 0.03100993552384845,
          "standard_error": 0.020250163499831917
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.2452825527144,
            "upper_bound": 24.292201862584825
          },
          "point_estimate": 24.269185125527983,
          "standard_error": 0.011737303437423095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021757150393800193,
            "upper_bound": 0.06409942243648777
          },
          "point_estimate": 0.05000433748107352,
          "standard_error": 0.010654636993533194
        }
      }
    },
    "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/bstr_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.891643472871344,
            "upper_bound": 21.02014399827323
          },
          "point_estimate": 20.95429387434148,
          "standard_error": 0.03277812849498687
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.859976414698767,
            "upper_bound": 21.01796938152107
          },
          "point_estimate": 20.958062097067568,
          "standard_error": 0.034384570351462654
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012727742375002862,
            "upper_bound": 0.1867171897740023
          },
          "point_estimate": 0.10830663669171228,
          "standard_error": 0.04824991345272887
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.843036362535393,
            "upper_bound": 20.98327573603293
          },
          "point_estimate": 20.92286042235315,
          "standard_error": 0.03742706078339872
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05479948102824298,
            "upper_bound": 0.14428014401503989
          },
          "point_estimate": 0.10957237984738848,
          "standard_error": 0.02295021810832346
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128879.571170815,
            "upper_bound": 1129877.1564139612
          },
          "point_estimate": 1129329.226914382,
          "standard_error": 258.2019835262645
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128821.4356060603,
            "upper_bound": 1129799.1227272728
          },
          "point_estimate": 1129055.1991341992,
          "standard_error": 201.52226190504624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.28134948015213,
            "upper_bound": 1282.6215728352638
          },
          "point_estimate": 443.7918773736902,
          "standard_error": 316.11705695193825
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128801.477031181,
            "upper_bound": 1129291.958985459
          },
          "point_estimate": 1129107.1551357734,
          "standard_error": 125.14988488421298
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267.9008590047387,
            "upper_bound": 1172.7007395202602
          },
          "point_estimate": 856.8606487112824,
          "standard_error": 245.8093991162159
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1502105.3031630556,
            "upper_bound": 1503723.7392770238
          },
          "point_estimate": 1502840.958426984,
          "standard_error": 417.6209498129385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1501795.036,
            "upper_bound": 1503793.9328571428
          },
          "point_estimate": 1502521.9775,
          "standard_error": 447.4955329040015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.35169965031525,
            "upper_bound": 2057.789459466879
          },
          "point_estimate": 852.6462100624375,
          "standard_error": 499.33421258901177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1501942.196653513,
            "upper_bound": 1503733.4252641632
          },
          "point_estimate": 1502595.625974026,
          "standard_error": 462.29170448580527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 497.4980777405603,
            "upper_bound": 1838.7639363929532
          },
          "point_estimate": 1390.4056079234294,
          "standard_error": 358.1773357988285
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367462.7428554527,
            "upper_bound": 1369705.6285008818
          },
          "point_estimate": 1368562.5906907702,
          "standard_error": 573.8818142577977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367507.12345679,
            "upper_bound": 1369991.5061728396
          },
          "point_estimate": 1368159.8055555555,
          "standard_error": 517.949534115901
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.885423574907215,
            "upper_bound": 3366.569046157596
          },
          "point_estimate": 979.928636306366,
          "standard_error": 840.6597434310484
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367581.1565553283,
            "upper_bound": 1368842.9846553097
          },
          "point_estimate": 1368310.8218374215,
          "standard_error": 323.2685181549802
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673.6534932091269,
            "upper_bound": 2563.7225129478625
          },
          "point_estimate": 1914.101282596081,
          "standard_error": 440.23537867264866
        }
      }
    },
    "memrmem/bstr/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/bstr_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396898.4946730072,
            "upper_bound": 397850.6917341486
          },
          "point_estimate": 397349.0767481883,
          "standard_error": 244.62068599998588
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396770.0129076087,
            "upper_bound": 397977.843115942
          },
          "point_estimate": 397213.1920289855,
          "standard_error": 274.4744534878023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.63186195039293,
            "upper_bound": 1408.1785581520155
          },
          "point_estimate": 597.0593932044278,
          "standard_error": 313.1121878924372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 396972.9726116893,
            "upper_bound": 398409.5633041797
          },
          "point_estimate": 397825.346499153,
          "standard_error": 377.28826791727266
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 366.5312468936246,
            "upper_bound": 1047.1299699451492
          },
          "point_estimate": 815.9189032906582,
          "standard_error": 172.5784792665936
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538645.2541439951,
            "upper_bound": 539196.9105455474
          },
          "point_estimate": 538917.0104493464,
          "standard_error": 141.76652206854197
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538409.4215686275,
            "upper_bound": 539394.7205882353
          },
          "point_estimate": 538880.5459558824,
          "standard_error": 240.9164816544783
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.16200110228961,
            "upper_bound": 773.8377872174152
          },
          "point_estimate": 699.2521070711367,
          "standard_error": 180.71700372788175
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 538728.9098855025,
            "upper_bound": 539368.1923076923
          },
          "point_estimate": 539070.2214667685,
          "standard_error": 163.26348730006774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 292.24265743817176,
            "upper_bound": 563.0880666468973
          },
          "point_estimate": 472.7046818505328,
          "standard_error": 68.2724459877994
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 433023.21746350615,
            "upper_bound": 433777.949409486
          },
          "point_estimate": 433416.9806396447,
          "standard_error": 194.22965142225755
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 432951.3743386243,
            "upper_bound": 433948.8302154195
          },
          "point_estimate": 433487.50982142857,
          "standard_error": 273.45972622615443
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.74408970893658,
            "upper_bound": 1073.7847976031649
          },
          "point_estimate": 695.2939389060556,
          "standard_error": 255.64189909090845
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 433075.6472182933,
            "upper_bound": 433693.63705629297
          },
          "point_estimate": 433371.780148423,
          "standard_error": 155.1950496461348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 323.7754766295755,
            "upper_bound": 803.2580273293164
          },
          "point_estimate": 648.0873894564897,
          "standard_error": 118.9193482805586
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473746.2874706246,
            "upper_bound": 474387.87979437225
          },
          "point_estimate": 474040.6535214389,
          "standard_error": 165.2969582117184
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473734.09523809527,
            "upper_bound": 474407.6453308596
          },
          "point_estimate": 473822.6087662338,
          "standard_error": 170.76239385001287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.68040838646474,
            "upper_bound": 803.1929198313464
          },
          "point_estimate": 156.13045313720946,
          "standard_error": 213.6661300746669
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 473809.1359286043,
            "upper_bound": 474656.0173161853
          },
          "point_estimate": 474194.2866587957,
          "standard_error": 217.52477812107884
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.09469900820423,
            "upper_bound": 695.4519457156919
          },
          "point_estimate": 550.1960906175415,
          "standard_error": 134.5435660740224
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 946689.570206578,
            "upper_bound": 949198.3202248932
          },
          "point_estimate": 947801.4788492064,
          "standard_error": 648.9603636533515
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 946350.2205128204,
            "upper_bound": 949088.6137820513
          },
          "point_estimate": 947168.5402930405,
          "standard_error": 581.376880265247
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.16600132601036,
            "upper_bound": 2797.5140887956513
          },
          "point_estimate": 1024.9467053933276,
          "standard_error": 635.2676926773485
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 946368.5788461538,
            "upper_bound": 947685.7790134898
          },
          "point_estimate": 947010.7332001332,
          "standard_error": 354.3124458234309
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 554.6362264052702,
            "upper_bound": 2886.285747933097
          },
          "point_estimate": 2156.591184565273,
          "standard_error": 630.2777704009734
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286677.62610444,
            "upper_bound": 287138.68645544304
          },
          "point_estimate": 286917.4822203474,
          "standard_error": 118.1156502157398
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286625.88976377953,
            "upper_bound": 287137.59995625546
          },
          "point_estimate": 287018.8573490814,
          "standard_error": 132.50994907351844
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.7244149392898,
            "upper_bound": 667.0749283145111
          },
          "point_estimate": 176.30636571504996,
          "standard_error": 184.40944601721787
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286931.18939230393,
            "upper_bound": 287246.13495998183
          },
          "point_estimate": 287099.3913692607,
          "standard_error": 79.11589395119245
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164.86889064907382,
            "upper_bound": 508.4882532739108
          },
          "point_estimate": 393.3526834940598,
          "standard_error": 82.86441072232539
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 769574.7889295634,
            "upper_bound": 770848.1751421958
          },
          "point_estimate": 770173.8121908068,
          "standard_error": 326.68153666131934
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 769242.1536458334,
            "upper_bound": 770771.6226851852
          },
          "point_estimate": 770111.1381944444,
          "standard_error": 384.7621523141126
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.74609309848748,
            "upper_bound": 1799.0763818098828
          },
          "point_estimate": 942.9205663848128,
          "standard_error": 382.6758060590778
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 769388.5858656256,
            "upper_bound": 770427.4943274853
          },
          "point_estimate": 769871.4127705628,
          "standard_error": 266.3983869946181
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 533.4933019309798,
            "upper_bound": 1450.1922833113172
          },
          "point_estimate": 1084.880072669642,
          "standard_error": 251.38257073135577
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384391.94335338345,
            "upper_bound": 385153.5181578948
          },
          "point_estimate": 384753.4461779448,
          "standard_error": 196.18834606990288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384216.8596491228,
            "upper_bound": 385233.8263157895
          },
          "point_estimate": 384581.8731578947,
          "standard_error": 311.83151734778346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.33136077303408,
            "upper_bound": 1189.8494243496527
          },
          "point_estimate": 771.2075397293701,
          "standard_error": 257.9338096191131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 384274.061307815,
            "upper_bound": 384780.4618789966
          },
          "point_estimate": 384514.0481749829,
          "standard_error": 129.2710188738149
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 365.71059127784537,
            "upper_bound": 838.8954643817257
          },
          "point_estimate": 654.4207249788221,
          "standard_error": 129.09106064006224
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165350.6998131313,
            "upper_bound": 165587.61629361476
          },
          "point_estimate": 165455.17878932177,
          "standard_error": 61.39306664163288
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165312.73939393938,
            "upper_bound": 165536.80344155844
          },
          "point_estimate": 165398.26325757577,
          "standard_error": 64.40785469071703
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.9091641984052,
            "upper_bound": 273.8206029114811
          },
          "point_estimate": 147.36639392918528,
          "standard_error": 58.78706384410437
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165308.75385837053,
            "upper_bound": 165476.54042745335
          },
          "point_estimate": 165374.74593860685,
          "standard_error": 42.813927617472885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.11683860791858,
            "upper_bound": 291.5252178502967
          },
          "point_estimate": 205.089298055129,
          "standard_error": 64.26538890529066
        }
      }
    },
    "memrmem/bstr/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/bstr_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226562.9855102287,
            "upper_bound": 226807.6027076432
          },
          "point_estimate": 226684.85652568276,
          "standard_error": 62.75059947450215
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226512.08876811597,
            "upper_bound": 226867.51242236025
          },
          "point_estimate": 226667.49844720497,
          "standard_error": 74.14006282032639
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.36360370223123,
            "upper_bound": 407.6436253715829
          },
          "point_estimate": 167.33752025896345,
          "standard_error": 97.80210185372644
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226512.4605083477,
            "upper_bound": 226842.78722223823
          },
          "point_estimate": 226678.7426474147,
          "standard_error": 84.79686340209877
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.3385183469494,
            "upper_bound": 263.8783933829186
          },
          "point_estimate": 209.08629670134633,
          "standard_error": 37.59654893804933
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165723.40751440532,
            "upper_bound": 165880.86491483654
          },
          "point_estimate": 165798.63493005728,
          "standard_error": 40.39080973713957
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165682.59259259258,
            "upper_bound": 165920.63660578386
          },
          "point_estimate": 165759.17488584475,
          "standard_error": 65.44422625133339
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.665852715298175,
            "upper_bound": 232.92706198343717
          },
          "point_estimate": 123.25663708116072,
          "standard_error": 58.48970618783859
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165716.7975537855,
            "upper_bound": 165960.75318956553
          },
          "point_estimate": 165857.62020992706,
          "standard_error": 62.655074568107885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.97671573288082,
            "upper_bound": 161.10316473268287
          },
          "point_estimate": 134.39137643943607,
          "standard_error": 22.926307500389683
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648714.6026089499,
            "upper_bound": 649657.7630527212
          },
          "point_estimate": 649160.7576743198,
          "standard_error": 242.51636150958365
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648392.7232142857,
            "upper_bound": 649681.1116071428
          },
          "point_estimate": 649110.7276785714,
          "standard_error": 360.3913601060895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.97213038039829,
            "upper_bound": 1412.3904811749867
          },
          "point_estimate": 955.082298668912,
          "standard_error": 347.7231217658373
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648778.6667377538,
            "upper_bound": 649895.3423408613
          },
          "point_estimate": 649288.7498608534,
          "standard_error": 278.470915997387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427.1724289547189,
            "upper_bound": 1048.960048909631
          },
          "point_estimate": 809.7013697031251,
          "standard_error": 168.4951287528214
        }
      }
    },
    "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/bstr/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/bstr_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303.7491345144538,
            "upper_bound": 1306.5626215976613
          },
          "point_estimate": 1305.0043252199769,
          "standard_error": 0.7240617042792027
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303.2000916474983,
            "upper_bound": 1306.3180311960898
          },
          "point_estimate": 1304.3720112589797,
          "standard_error": 0.9157430446551033
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3013814836517454,
            "upper_bound": 3.68825073275244
          },
          "point_estimate": 1.9520224710952023,
          "standard_error": 0.8245753160582754
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303.5825990142566,
            "upper_bound": 1305.839330446473
          },
          "point_estimate": 1304.7963595849235,
          "standard_error": 0.586178345853194
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.0500019602710144,
            "upper_bound": 3.369485626778186
          },
          "point_estimate": 2.4107340056730666,
          "standard_error": 0.6906130818347082
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313828.6680720665,
            "upper_bound": 1316895.4924914965
          },
          "point_estimate": 1315352.0716184808,
          "standard_error": 786.3044407511723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313227.9553571427,
            "upper_bound": 1317654.6214285714
          },
          "point_estimate": 1315048.0226757368,
          "standard_error": 1140.8660838878086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 705.2202356941308,
            "upper_bound": 4393.933802944773
          },
          "point_estimate": 3211.2157076920457,
          "standard_error": 995.4706342733284
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314289.552556818,
            "upper_bound": 1317148.2903346654
          },
          "point_estimate": 1315737.0030612245,
          "standard_error": 736.915269819796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1543.720172994782,
            "upper_bound": 3284.367026087041
          },
          "point_estimate": 2626.2871904613785,
          "standard_error": 446.7157503690498
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1509897.4438342063,
            "upper_bound": 1513015.3061349206
          },
          "point_estimate": 1511489.8285349202,
          "standard_error": 801.5759365286319
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1509140.128,
            "upper_bound": 1513755.775
          },
          "point_estimate": 1511853.0765,
          "standard_error": 1243.4656016772242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 792.8088045915628,
            "upper_bound": 4570.178911963148
          },
          "point_estimate": 3229.408570099716,
          "standard_error": 948.4287965579832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510333.1933022933,
            "upper_bound": 1512943.3871374452
          },
          "point_estimate": 1511581.7655064934,
          "standard_error": 660.4111488559706
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1632.2301252441234,
            "upper_bound": 3259.683840679891
          },
          "point_estimate": 2679.2736211663255,
          "standard_error": 417.3356931186258
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415524.893195971,
            "upper_bound": 1417854.9869536017
          },
          "point_estimate": 1416690.036184371,
          "standard_error": 597.8175730399928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1414873.2183760684,
            "upper_bound": 1418502.5824175824
          },
          "point_estimate": 1416903.5740384615,
          "standard_error": 843.7352958392482
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 401.23020185100455,
            "upper_bound": 3629.5933190231817
          },
          "point_estimate": 2246.689232805595,
          "standard_error": 855.3309940063352
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415200.473722842,
            "upper_bound": 1417132.7009832272
          },
          "point_estimate": 1416285.4248751248,
          "standard_error": 510.6525625217415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1177.1449660487365,
            "upper_bound": 2463.3939029995545
          },
          "point_estimate": 1996.138363873223,
          "standard_error": 332.16971541325415
        }
      }
    },
    "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753124.3078401361,
            "upper_bound": 753939.9208477284
          },
          "point_estimate": 753529.185706997,
          "standard_error": 207.9334063587521
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752999.1020408163,
            "upper_bound": 753863.6656948493
          },
          "point_estimate": 753646.2326530612,
          "standard_error": 202.96975296451112
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.85725794355861,
            "upper_bound": 1266.36478466045
          },
          "point_estimate": 418.9807354187719,
          "standard_error": 321.26360459480276
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753001.2519406894,
            "upper_bound": 753847.6637653646
          },
          "point_estimate": 753466.958812616,
          "standard_error": 216.33648798870232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309.5219717427899,
            "upper_bound": 923.6551707748996
          },
          "point_estimate": 691.998201915157,
          "standard_error": 154.2752974022638
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266166.7158138687,
            "upper_bound": 266809.2203315954
          },
          "point_estimate": 266488.9259100916,
          "standard_error": 164.6641087408989
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266105.0651459854,
            "upper_bound": 266892.8083941606
          },
          "point_estimate": 266430.7277256402,
          "standard_error": 179.01248466478512
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.76658187183166,
            "upper_bound": 995.6808649873606
          },
          "point_estimate": 430.1690775819305,
          "standard_error": 263.6987646961275
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266271.8105446817,
            "upper_bound": 266632.9133563837
          },
          "point_estimate": 266436.19668214995,
          "standard_error": 93.32508055681429
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294.51997579795295,
            "upper_bound": 704.186644612087
          },
          "point_estimate": 549.308988331744,
          "standard_error": 106.1860638184898
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477555.17763334885,
            "upper_bound": 478195.7253324057
          },
          "point_estimate": 477862.58823283867,
          "standard_error": 164.34905750689893
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477417.0876623377,
            "upper_bound": 478212.8279220779
          },
          "point_estimate": 477875.768707483,
          "standard_error": 198.3681918786922
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 92.57699900579836,
            "upper_bound": 967.6023230814076
          },
          "point_estimate": 556.8835066717382,
          "standard_error": 201.58409688159463
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477403.652012987,
            "upper_bound": 477921.92850307585
          },
          "point_estimate": 477637.7165795244,
          "standard_error": 130.7286052611237
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 284.702950739401,
            "upper_bound": 719.7780304197048
          },
          "point_estimate": 545.5377068921562,
          "standard_error": 116.8548571638555
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241020.74896247243,
            "upper_bound": 241388.7708262378
          },
          "point_estimate": 241203.29921475876,
          "standard_error": 94.38007223523694
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240950.0675496689,
            "upper_bound": 241484.2715231788
          },
          "point_estimate": 241173.3407284768,
          "standard_error": 153.2429298982698
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.554557890679774,
            "upper_bound": 518.4267548357872
          },
          "point_estimate": 332.0456920189232,
          "standard_error": 119.96330203473016
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241012.7510645016,
            "upper_bound": 241462.3541846455
          },
          "point_estimate": 241243.69010062783,
          "standard_error": 114.12325240496988
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192.44599157146064,
            "upper_bound": 385.9405529566431
          },
          "point_estimate": 314.4751834638488,
          "standard_error": 49.357780264806266
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696073.3952725367,
            "upper_bound": 697347.3004986523
          },
          "point_estimate": 696696.3772544174,
          "standard_error": 326.69611591156604
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695575.9764150943,
            "upper_bound": 697572.162735849
          },
          "point_estimate": 696575.1829140461,
          "standard_error": 436.4628164367632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.0519893233313,
            "upper_bound": 1854.0012576509737
          },
          "point_estimate": 1479.7728933042383,
          "standard_error": 497.8389305308476
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696486.7533839616,
            "upper_bound": 697427.3183539518
          },
          "point_estimate": 696900.5999019848,
          "standard_error": 239.95361134082833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 612.5385604538798,
            "upper_bound": 1356.725772877497
          },
          "point_estimate": 1089.9586417933729,
          "standard_error": 189.548433495534
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2287.453990596065,
            "upper_bound": 2289.3742038920886
          },
          "point_estimate": 2288.4538027477065,
          "standard_error": 0.4928273176722922
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2286.9509273246053,
            "upper_bound": 2289.65648602719
          },
          "point_estimate": 2289.31807723509,
          "standard_error": 0.8656168966388655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029938789750213923,
            "upper_bound": 2.52182072771959
          },
          "point_estimate": 1.0344700772516284,
          "standard_error": 0.7602588728951885
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2287.602963674566,
            "upper_bound": 2289.574563491979
          },
          "point_estimate": 2288.8177611789015,
          "standard_error": 0.49929388180121903
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9025406326005452,
            "upper_bound": 1.956807218230007
          },
          "point_estimate": 1.6380937931478226,
          "standard_error": 0.2680872234628052
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.5931806444476,
            "upper_bound": 705.1504708672824
          },
          "point_estimate": 704.3324380871451,
          "standard_error": 0.40085140231953453
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.3640185073893,
            "upper_bound": 705.2134617994301
          },
          "point_estimate": 704.0064953569974,
          "standard_error": 0.5840841899690115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1316590816631489,
            "upper_bound": 2.3010637200607404
          },
          "point_estimate": 1.017248993565474,
          "standard_error": 0.5547924151800365
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.6863516701009,
            "upper_bound": 705.0310674377836
          },
          "point_estimate": 704.4824600171555,
          "standard_error": 0.339755287678961
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6875508707358394,
            "upper_bound": 1.731159707815667
          },
          "point_estimate": 1.3330421318449217,
          "standard_error": 0.28096402990318264
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.6605222419215,
            "upper_bound": 140.83761364472872
          },
          "point_estimate": 140.7451114401859,
          "standard_error": 0.045389941299136385
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.60887442029855,
            "upper_bound": 140.85137725273404
          },
          "point_estimate": 140.72199772445794,
          "standard_error": 0.06395623306080808
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.034218496081826894,
            "upper_bound": 0.2599412929219083
          },
          "point_estimate": 0.16191163859589186,
          "standard_error": 0.056100660170371235
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.6158649223435,
            "upper_bound": 140.7471551530411
          },
          "point_estimate": 140.67070669724762,
          "standard_error": 0.033606652409659446
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08134755175152915,
            "upper_bound": 0.1940665965680005
          },
          "point_estimate": 0.15116508072206433,
          "standard_error": 0.030008641474209615
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.694818957856384,
            "upper_bound": 53.802941107050366
          },
          "point_estimate": 53.75151709485019,
          "standard_error": 0.02775796764394122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.674714259977975,
            "upper_bound": 53.82341242183469
          },
          "point_estimate": 53.769152196105104,
          "standard_error": 0.03246131978466683
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016734291758107263,
            "upper_bound": 0.16498102925664726
          },
          "point_estimate": 0.07854615672383668,
          "standard_error": 0.03743272226450392
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.72665195154521,
            "upper_bound": 53.79300068621069
          },
          "point_estimate": 53.75380472326218,
          "standard_error": 0.016905053432723496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04308657295978751,
            "upper_bound": 0.11549767664101077
          },
          "point_estimate": 0.09218151515440098,
          "standard_error": 0.018235862025584797
        }
      }
    },
    "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.63438297312901,
            "upper_bound": 80.7743801058065
          },
          "point_estimate": 80.70126287207886,
          "standard_error": 0.03568596032605289
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.62258134656707,
            "upper_bound": 80.75188168950709
          },
          "point_estimate": 80.70296913857612,
          "standard_error": 0.031583459927898294
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018687950129205055,
            "upper_bound": 0.19266988657693931
          },
          "point_estimate": 0.07849211262364876,
          "standard_error": 0.04442647542353935
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.65792374639555,
            "upper_bound": 80.73825816135599
          },
          "point_estimate": 80.70349482663578,
          "standard_error": 0.020345950638473796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0455760450677143,
            "upper_bound": 0.16474404993772773
          },
          "point_estimate": 0.11863148750141728,
          "standard_error": 0.030712838586071777
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287272.6966192351,
            "upper_bound": 287717.34890353546
          },
          "point_estimate": 287494.9451871641,
          "standard_error": 113.35673851532518
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287204.36286089243,
            "upper_bound": 287802.4798228346
          },
          "point_estimate": 287506.78735783027,
          "standard_error": 117.19785544592192
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.15012565436373,
            "upper_bound": 761.5638078706098
          },
          "point_estimate": 226.80944479223825,
          "standard_error": 199.5386231693384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287312.22960766265,
            "upper_bound": 287549.3695033945
          },
          "point_estimate": 287457.79210553225,
          "standard_error": 60.23741085766082
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.37341965969975,
            "upper_bound": 481.53055211519023
          },
          "point_estimate": 376.637957818,
          "standard_error": 72.53102790505919
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.79192129084489,
            "upper_bound": 62.909272050407594
          },
          "point_estimate": 62.85203036738947,
          "standard_error": 0.030116456192295943
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.7663271750049,
            "upper_bound": 62.93342732378525
          },
          "point_estimate": 62.865824313281905,
          "standard_error": 0.0430457589851173
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02629012498411186,
            "upper_bound": 0.17535192415047562
          },
          "point_estimate": 0.1093076057491782,
          "standard_error": 0.038064347448221
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.79818139920413,
            "upper_bound": 62.93407585984815
          },
          "point_estimate": 62.871818512351055,
          "standard_error": 0.03492634797248069
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05598912281649782,
            "upper_bound": 0.12264944110946605
          },
          "point_estimate": 0.1004096853207467,
          "standard_error": 0.01662817303966746
        }
      }
    },
    "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.2270128484496,
            "upper_bound": 105.4663600458302
          },
          "point_estimate": 105.34507830644516,
          "standard_error": 0.06093768326240609
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.21779773032516,
            "upper_bound": 105.4499651652907
          },
          "point_estimate": 105.33893224681294,
          "standard_error": 0.045728446799789206
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005277509626160971,
            "upper_bound": 0.3487308352024963
          },
          "point_estimate": 0.08279142231181862,
          "standard_error": 0.09233313021382658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.2572683024336,
            "upper_bound": 105.3893098232743
          },
          "point_estimate": 105.3354182283851,
          "standard_error": 0.033546207513074064
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08073650307496831,
            "upper_bound": 0.2780880548169208
          },
          "point_estimate": 0.2029596785094776,
          "standard_error": 0.04905595594354923
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126620.18094521608,
            "upper_bound": 126774.7638555445
          },
          "point_estimate": 126697.7310719797,
          "standard_error": 39.648994744500065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126566.84336419751,
            "upper_bound": 126807.57056051587
          },
          "point_estimate": 126711.3201388889,
          "standard_error": 67.31865096261153
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.52638136501996,
            "upper_bound": 221.73706863281416
          },
          "point_estimate": 165.18022676655005,
          "standard_error": 47.31740814841638
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126605.05229250334,
            "upper_bound": 126796.86531045752
          },
          "point_estimate": 126697.55961399712,
          "standard_error": 48.649221015283736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.66272792853576,
            "upper_bound": 158.83027034363533
          },
          "point_estimate": 132.48941157820664,
          "standard_error": 19.183017331748392
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.54220200869039,
            "upper_bound": 60.694909007649755
          },
          "point_estimate": 60.615451482896354,
          "standard_error": 0.03919188017928204
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.48955354659017,
            "upper_bound": 60.730423322729536
          },
          "point_estimate": 60.587409195573805,
          "standard_error": 0.05295923733106271
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02395985053888143,
            "upper_bound": 0.23319184560739512
          },
          "point_estimate": 0.14554263792290553,
          "standard_error": 0.05212554702732429
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.553532939804384,
            "upper_bound": 60.635008148272775
          },
          "point_estimate": 60.59551454810296,
          "standard_error": 0.020856616885698863
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06811979090067553,
            "upper_bound": 0.16186289212034446
          },
          "point_estimate": 0.13080698958975737,
          "standard_error": 0.023691023007947645
        }
      }
    },
    "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.35423913201812,
            "upper_bound": 102.47661815156276
          },
          "point_estimate": 102.41175387848476,
          "standard_error": 0.03152705949389886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.33540758246409,
            "upper_bound": 102.52298748344728
          },
          "point_estimate": 102.36596438282234,
          "standard_error": 0.04812413677623035
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016108812265020978,
            "upper_bound": 0.16297327902414804
          },
          "point_estimate": 0.06232380999983994,
          "standard_error": 0.04073456508174011
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.33736904036272,
            "upper_bound": 102.39929680159509
          },
          "point_estimate": 102.36589248270892,
          "standard_error": 0.016309181368722107
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03872021019027064,
            "upper_bound": 0.12343262521824688
          },
          "point_estimate": 0.10489726442401648,
          "standard_error": 0.018781247106776783
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-freq_rare-alphab"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.57391092874383,
            "upper_bound": 254.98374294626657
          },
          "point_estimate": 254.7742592616718,
          "standard_error": 0.1051177544989722
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.44375987108464,
            "upper_bound": 255.08193062882316
          },
          "point_estimate": 254.71504573769573,
          "standard_error": 0.17290305226833333
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10558154731463042,
            "upper_bound": 0.5863007433511053
          },
          "point_estimate": 0.4295919687748322,
          "standard_error": 0.12993232432593205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.54836356928988,
            "upper_bound": 254.81544947633049
          },
          "point_estimate": 254.66278749590944,
          "standard_error": 0.06761238211776362
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2107493785714013,
            "upper_bound": 0.4205782734980726
          },
          "point_estimate": 0.3511331229097944,
          "standard_error": 0.05314989615167912
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector-repeated_rare-al"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.8077274089108,
            "upper_bound": 526.711302988789
          },
          "point_estimate": 526.2253473792725,
          "standard_error": 0.23246928882240145
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.7247028124094,
            "upper_bound": 526.8090392545344
          },
          "point_estimate": 526.0703964917368,
          "standard_error": 0.23779407574011507
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07891259079970624,
            "upper_bound": 1.2671863075985887
          },
          "point_estimate": 0.4318769160786475,
          "standard_error": 0.28609000671532453
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 525.9520022284731,
            "upper_bound": 526.9366460069359
          },
          "point_estimate": 526.3659134776502,
          "standard_error": 0.26104052687656754
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2590903815039923,
            "upper_bound": 0.9993044568133488
          },
          "point_estimate": 0.7763832954467462,
          "standard_error": 0.18482573679422376
        }
      }
    },
    "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.07814052778457,
            "upper_bound": 32.10721249932677
          },
          "point_estimate": 32.091272614364165,
          "standard_error": 0.007479250257562051
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.07176739620505,
            "upper_bound": 32.10027105163392
          },
          "point_estimate": 32.08970180977136,
          "standard_error": 0.0070422010197573985
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004165501706838327,
            "upper_bound": 0.03519285491232468
          },
          "point_estimate": 0.017552533099878014,
          "standard_error": 0.007959841851381249
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.07476895190963,
            "upper_bound": 32.09112494403835
          },
          "point_estimate": 32.08362649560228,
          "standard_error": 0.004186054443666207
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010122565104018103,
            "upper_bound": 0.03521830742238248
          },
          "point_estimate": 0.025008370028844738,
          "standard_error": 0.007342142147164602
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32713.103985054695,
            "upper_bound": 32787.58807220972
          },
          "point_estimate": 32745.044769984273,
          "standard_error": 19.28376832966209
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32709.099812312314,
            "upper_bound": 32757.603243243244
          },
          "point_estimate": 32735.375725725724,
          "standard_error": 11.783919609414832
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.562708748354541,
            "upper_bound": 67.81036064297525
          },
          "point_estimate": 27.58998340207319,
          "standard_error": 16.388361239213907
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32722.80357492144,
            "upper_bound": 32746.340300901276
          },
          "point_estimate": 32733.632533052532,
          "standard_error": 6.0164743986072216
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.700790972662285,
            "upper_bound": 94.8456331552962
          },
          "point_estimate": 64.48366057714442,
          "standard_error": 23.544615542931357
        }
      }
    },
    "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.80810871981947,
            "upper_bound": 147.77337940507036
          },
          "point_estimate": 144.72274419197802,
          "standard_error": 1.4600894386892322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.54714035208085,
            "upper_bound": 151.35777128466728
          },
          "point_estimate": 141.84198920180864,
          "standard_error": 2.451680557865875
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06453749363125509,
            "upper_bound": 7.52467794348548
          },
          "point_estimate": 0.4541709692968367,
          "standard_error": 2.1449675775258195
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.6341466145584,
            "upper_bound": 143.08853016574275
          },
          "point_estimate": 142.00773548683583,
          "standard_error": 0.3977136755195788
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27842082496113524,
            "upper_bound": 5.427894795546955
          },
          "point_estimate": 4.876678441272539,
          "standard_error": 0.9800893093902112
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978112.3426033834,
            "upper_bound": 979546.5253874528
          },
          "point_estimate": 978829.67304198,
          "standard_error": 366.0540929273528
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977963.426065163,
            "upper_bound": 979728.4890350878
          },
          "point_estimate": 978765.8601973684,
          "standard_error": 442.9796864781211
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.8463187159128,
            "upper_bound": 2110.5593727932405
          },
          "point_estimate": 1096.5563007954083,
          "standard_error": 462.0031476750206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978151.518597866,
            "upper_bound": 979561.239967358
          },
          "point_estimate": 978778.2209159262,
          "standard_error": 368.6898322996104
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 667.006787562145,
            "upper_bound": 1569.9730989057837
          },
          "point_estimate": 1220.2573404155562,
          "standard_error": 231.819147677104
        }
      }
    },
    "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1986.6910848402315,
            "upper_bound": 1988.1812704317
          },
          "point_estimate": 1987.4068037723268,
          "standard_error": 0.3822026784334536
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1986.1686997429024,
            "upper_bound": 1988.5160002188063
          },
          "point_estimate": 1987.2645770003203,
          "standard_error": 0.48776022906068384
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19791586405366637,
            "upper_bound": 1.996879120502702
          },
          "point_estimate": 1.7400538118954465,
          "standard_error": 0.5844769041152081
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1986.5226720007204,
            "upper_bound": 1988.650951798155
          },
          "point_estimate": 1987.4274812327328,
          "standard_error": 0.5492720867856044
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6640311515564609,
            "upper_bound": 1.6401254762412505
          },
          "point_estimate": 1.2785067520533926,
          "standard_error": 0.2522024186002214
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.71945916932008,
            "upper_bound": 24.804913628449096
          },
          "point_estimate": 24.76267438505044,
          "standard_error": 0.02198225829312943
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.70049502761919,
            "upper_bound": 24.82217052209088
          },
          "point_estimate": 24.77491339924417,
          "standard_error": 0.034741972286659206
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020244336176571315,
            "upper_bound": 0.12030756546325012
          },
          "point_estimate": 0.09310276959860364,
          "standard_error": 0.025816352031778927
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.69186646172379,
            "upper_bound": 24.81256994657762
          },
          "point_estimate": 24.750843485468533,
          "standard_error": 0.032122375714288054
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04492350866106062,
            "upper_bound": 0.08877901665271173
          },
          "point_estimate": 0.07320751741771134,
          "standard_error": 0.01125133420293387
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.747780438861884,
            "upper_bound": 24.885429032895377
          },
          "point_estimate": 24.814183154863336,
          "standard_error": 0.03496845575400339
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.757697864694396,
            "upper_bound": 24.849783231440966
          },
          "point_estimate": 24.820280913939463,
          "standard_error": 0.022514347985544338
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0090201950952836,
            "upper_bound": 0.18119618729683784
          },
          "point_estimate": 0.04825023269883049,
          "standard_error": 0.04244789090910984
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.755622308746435,
            "upper_bound": 24.831077221133715
          },
          "point_estimate": 24.78768517770896,
          "standard_error": 0.019191614155343397
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0335636142622008,
            "upper_bound": 0.16553149903685355
          },
          "point_estimate": 0.11638931746224616,
          "standard_error": 0.03343539892626388
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.781368763579053,
            "upper_bound": 31.09640266592672
          },
          "point_estimate": 30.937126266249827,
          "standard_error": 0.08052311091724486
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.711983820208808,
            "upper_bound": 31.150238731524063
          },
          "point_estimate": 30.904134184118167,
          "standard_error": 0.11227534457723017
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06490676023031304,
            "upper_bound": 0.4694027564337517
          },
          "point_estimate": 0.2755583019131158,
          "standard_error": 0.10131458150562712
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.771526573953594,
            "upper_bound": 31.028494246413977
          },
          "point_estimate": 30.897998266228907,
          "standard_error": 0.06454758547019024
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15699507746781435,
            "upper_bound": 0.33511812256693335
          },
          "point_estimate": 0.269112340508168,
          "standard_error": 0.0455761663419235
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.81240909906781,
            "upper_bound": 29.43365113470867
          },
          "point_estimate": 29.092831878331744,
          "standard_error": 0.15880389011484222
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.72260983785196,
            "upper_bound": 29.50726129444414
          },
          "point_estimate": 28.921049214749768,
          "standard_error": 0.16550829127495023
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07061197165373846,
            "upper_bound": 0.7272918879384006
          },
          "point_estimate": 0.32203470881371826,
          "standard_error": 0.1544625478394384
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.761188992134414,
            "upper_bound": 29.18969516722137
          },
          "point_estimate": 28.932684058546972,
          "standard_error": 0.10837973339200684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14220935881294833,
            "upper_bound": 0.6629812446322526
          },
          "point_estimate": 0.5287052029481302,
          "standard_error": 0.13655645571494618
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.60493958249844,
            "upper_bound": 19.683288115438124
          },
          "point_estimate": 19.64211998394263,
          "standard_error": 0.02015662434825742
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.587536004007028,
            "upper_bound": 19.714899865675612
          },
          "point_estimate": 19.617269316497232,
          "standard_error": 0.03026028572300628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009900707060638936,
            "upper_bound": 0.10408533370731785
          },
          "point_estimate": 0.050697310636295095,
          "standard_error": 0.02585100219157617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.589183005485783,
            "upper_bound": 19.672677417147284
          },
          "point_estimate": 19.626104873722028,
          "standard_error": 0.02234745482180828
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.028215492591605513,
            "upper_bound": 0.07978427488518793
          },
          "point_estimate": 0.06703762559283874,
          "standard_error": 0.011643157233275235
        }
      }
    },
    "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.507302155749255,
            "upper_bound": 21.567336152772295
          },
          "point_estimate": 21.535316550021204,
          "standard_error": 0.015323334942830178
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.501566876943613,
            "upper_bound": 21.57030294294409
          },
          "point_estimate": 21.529132159884337,
          "standard_error": 0.016925495706034792
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010147138462067492,
            "upper_bound": 0.08024325084161751
          },
          "point_estimate": 0.04444676983024828,
          "standard_error": 0.018269053758716212
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.529386104564775,
            "upper_bound": 21.58130407202732
          },
          "point_estimate": 21.557091607508635,
          "standard_error": 0.012937857934007905
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02289556261778735,
            "upper_bound": 0.06946838705536271
          },
          "point_estimate": 0.05104114476698121,
          "standard_error": 0.012726407397496408
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.22895521258375,
            "upper_bound": 34.312742428212076
          },
          "point_estimate": 34.26832620503105,
          "standard_error": 0.02143079954229716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.21586997124474,
            "upper_bound": 34.30382737581135
          },
          "point_estimate": 34.263318283839695,
          "standard_error": 0.017175858762651916
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010336700167524249,
            "upper_bound": 0.11638736810415916
          },
          "point_estimate": 0.0326017826927966,
          "standard_error": 0.03199672915384481
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.244350514566456,
            "upper_bound": 34.287012291339266
          },
          "point_estimate": 34.267461871407924,
          "standard_error": 0.010657853873973746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02969858804595387,
            "upper_bound": 0.09734170061273072
          },
          "point_estimate": 0.07141348675339831,
          "standard_error": 0.018273625335005907
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.870228580239083,
            "upper_bound": 25.989826726481283
          },
          "point_estimate": 25.925508002408954,
          "standard_error": 0.030661100710314547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.862403363807346,
            "upper_bound": 25.97752808389316
          },
          "point_estimate": 25.90512592700085,
          "standard_error": 0.02854646965283322
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01714262064028142,
            "upper_bound": 0.15299153731673898
          },
          "point_estimate": 0.06922189841027153,
          "standard_error": 0.03346156403429056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.86825341015693,
            "upper_bound": 25.933408647585235
          },
          "point_estimate": 25.894444173455945,
          "standard_error": 0.016609532756454777
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04025178148762831,
            "upper_bound": 0.14202021472730683
          },
          "point_estimate": 0.10223190491405126,
          "standard_error": 0.02800946686643028
        }
      }
    },
    "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.906942449640145,
            "upper_bound": 28.06561495717539
          },
          "point_estimate": 27.984451671977183,
          "standard_error": 0.04085163247046447
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.866032965417396,
            "upper_bound": 28.10484122531288
          },
          "point_estimate": 27.97930231669737,
          "standard_error": 0.06641241997392518
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.024488351437147293,
            "upper_bound": 0.22982465252279743
          },
          "point_estimate": 0.1432452558926292,
          "standard_error": 0.054708349486309794
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.84583090216865,
            "upper_bound": 27.96178652117947
          },
          "point_estimate": 27.88413465668466,
          "standard_error": 0.02964920641864389
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08104503872849851,
            "upper_bound": 0.1653335955164753
          },
          "point_estimate": 0.13615714579494445,
          "standard_error": 0.021512720746469493
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.813370403009245,
            "upper_bound": 28.100176697840016
          },
          "point_estimate": 27.94432261799482,
          "standard_error": 0.07338281383695192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.75536073177413,
            "upper_bound": 28.03977443354469
          },
          "point_estimate": 27.941673966787626,
          "standard_error": 0.0814481026004385
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03162853173521686,
            "upper_bound": 0.375111473585677
          },
          "point_estimate": 0.17773428788838327,
          "standard_error": 0.08202621961831665
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.808802583867497,
            "upper_bound": 27.994451448295
          },
          "point_estimate": 27.90619325259225,
          "standard_error": 0.04834778943873919
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10812641843527514,
            "upper_bound": 0.34088660341790356
          },
          "point_estimate": 0.2446418193243368,
          "standard_error": 0.06795585374471792
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.188608361714863,
            "upper_bound": 25.2863195164112
          },
          "point_estimate": 25.2369805012442,
          "standard_error": 0.025100656114118
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.16185222100125,
            "upper_bound": 25.30259957824295
          },
          "point_estimate": 25.247854525989503,
          "standard_error": 0.03213931336707267
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002332910103065685,
            "upper_bound": 0.15356971477699546
          },
          "point_estimate": 0.09481201297699984,
          "standard_error": 0.0381518296740164
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.20203841621694,
            "upper_bound": 25.27186061162282
          },
          "point_estimate": 25.235411557081516,
          "standard_error": 0.017780494262546496
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04664969681169474,
            "upper_bound": 0.1066950069866484
          },
          "point_estimate": 0.08401666089064877,
          "standard_error": 0.01551796500195599
        }
      }
    },
    "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.24907054097976,
            "upper_bound": 26.337958893137788
          },
          "point_estimate": 26.29269094097291,
          "standard_error": 0.022830305666959035
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.221198847545825,
            "upper_bound": 26.3510721515944
          },
          "point_estimate": 26.294173234599093,
          "standard_error": 0.03777628129954356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004101020750186627,
            "upper_bound": 0.1279234551872975
          },
          "point_estimate": 0.10478938272259695,
          "standard_error": 0.034408806620752345
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.23518131868212,
            "upper_bound": 26.325215628867618
          },
          "point_estimate": 26.268021644326296,
          "standard_error": 0.022843032132913724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04531900341606654,
            "upper_bound": 0.09293818725065012
          },
          "point_estimate": 0.07600254230020986,
          "standard_error": 0.01220147907791278
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169194.8430431546,
            "upper_bound": 1170788.8682914805
          },
          "point_estimate": 1169920.0693080358,
          "standard_error": 409.57014708814586
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169063.796875,
            "upper_bound": 1170514.6354166667
          },
          "point_estimate": 1169629.171875,
          "standard_error": 409.899722315822
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 203.85749638080597,
            "upper_bound": 1920.6577979328315
          },
          "point_estimate": 1045.8275658077916,
          "standard_error": 441.7360358289801
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169082.5296750662,
            "upper_bound": 1170535.2266517428
          },
          "point_estimate": 1169737.2357142856,
          "standard_error": 362.8245802826306
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 541.9178791149827,
            "upper_bound": 1926.9410635631016
          },
          "point_estimate": 1371.654130924383,
          "standard_error": 397.86217761735344
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563927.7585271988,
            "upper_bound": 1567655.838374008
          },
          "point_estimate": 1565735.5804993385,
          "standard_error": 959.3405750404004
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563358.40625,
            "upper_bound": 1568448.25
          },
          "point_estimate": 1564561.244047619,
          "standard_error": 1452.6083721914465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 376.26489957012706,
            "upper_bound": 5086.9920857294765
          },
          "point_estimate": 3219.829628947657,
          "standard_error": 1322.423094789521
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1562408.5060151014,
            "upper_bound": 1566013.1189516129
          },
          "point_estimate": 1563718.3725108225,
          "standard_error": 923.7396495218458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1804.6100447915571,
            "upper_bound": 3995.316563249253
          },
          "point_estimate": 3199.832286083924,
          "standard_error": 561.5531388484144
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446903.6184249087,
            "upper_bound": 1449842.5622880035
          },
          "point_estimate": 1448453.1211568988,
          "standard_error": 747.1020192889257
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1447439.4166666667,
            "upper_bound": 1449736.5240384615
          },
          "point_estimate": 1448664.833760684,
          "standard_error": 655.437246535685
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 470.02695896291294,
            "upper_bound": 3824.6127817147617
          },
          "point_estimate": 1694.4254945332027,
          "standard_error": 815.266241651624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1447954.8594089523,
            "upper_bound": 1449605.78824465
          },
          "point_estimate": 1448898.308891109,
          "standard_error": 420.7212479207594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 950.677788575398,
            "upper_bound": 3483.1424136523165
          },
          "point_estimate": 2493.8890661979126,
          "standard_error": 669.23691778461
        }
      }
    },
    "memrmem/krate/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388672.3192008611,
            "upper_bound": 389240.4719402229
          },
          "point_estimate": 388978.46918313066,
          "standard_error": 146.43280686749978
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388607.3098404256,
            "upper_bound": 389306.19427558256
          },
          "point_estimate": 389146.079787234,
          "standard_error": 179.79886634643458
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 60.73139573033487,
            "upper_bound": 798.7803089570391
          },
          "point_estimate": 306.47647221854396,
          "standard_error": 192.9722453233072
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 388685.98486918246,
            "upper_bound": 389398.14131692745
          },
          "point_estimate": 389123.15473887813,
          "standard_error": 183.4436036314971
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215.68650229476145,
            "upper_bound": 651.0843037401613
          },
          "point_estimate": 487.40851169106537,
          "standard_error": 118.99528258050285
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 557829.2056772637,
            "upper_bound": 559144.4250284992
          },
          "point_estimate": 558429.6426352813,
          "standard_error": 338.3352221327469
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 557538.9956709957,
            "upper_bound": 559218.4121212121
          },
          "point_estimate": 558111.2929292929,
          "standard_error": 403.4015159948038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.5921225515176,
            "upper_bound": 1836.3854966931149
          },
          "point_estimate": 881.2804763238195,
          "standard_error": 414.7622051519473
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 557628.0660791589,
            "upper_bound": 558846.9212047565
          },
          "point_estimate": 558115.6412042503,
          "standard_error": 319.2429998833907
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 397.7280285549168,
            "upper_bound": 1496.7376528106606
          },
          "point_estimate": 1126.1086813271806,
          "standard_error": 279.51435655320387
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415109.8614703057,
            "upper_bound": 416404.4682907197
          },
          "point_estimate": 415664.9407061688,
          "standard_error": 336.2744132588292
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414924.211038961,
            "upper_bound": 416065.71893939393
          },
          "point_estimate": 415345.765625,
          "standard_error": 309.4563371573764
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 137.3249801756573,
            "upper_bound": 1296.2418502825312
          },
          "point_estimate": 678.6805959055804,
          "standard_error": 338.51640124116415
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415088.02468706295,
            "upper_bound": 415769.1552396707
          },
          "point_estimate": 415380.2239079103,
          "standard_error": 175.57999993567458
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332.77933209116077,
            "upper_bound": 1606.290462878753
          },
          "point_estimate": 1120.4195198370865,
          "standard_error": 374.3816489144174
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475050.0159245517,
            "upper_bound": 476149.80764594936
          },
          "point_estimate": 475552.9248639456,
          "standard_error": 282.1786349569969
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474992.8181818182,
            "upper_bound": 475962.0055658627
          },
          "point_estimate": 475389.79134199134,
          "standard_error": 233.2558472596352
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 90.64478248162308,
            "upper_bound": 1344.540873856893
          },
          "point_estimate": 718.458595037012,
          "standard_error": 328.2650059548945
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475013.46357558673,
            "upper_bound": 475701.7224954828
          },
          "point_estimate": 475332.0610895598,
          "standard_error": 173.9286783935289
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369.96051405713064,
            "upper_bound": 1320.5382740416317
          },
          "point_estimate": 942.9942991106774,
          "standard_error": 270.7449098866312
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977077.0834711778,
            "upper_bound": 979861.4146773182
          },
          "point_estimate": 978352.9723088972,
          "standard_error": 709.3212184116163
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977215.8305921052,
            "upper_bound": 978783.9812030076
          },
          "point_estimate": 978273.0864035088,
          "standard_error": 373.55936981921224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.2080311876724,
            "upper_bound": 3119.351328830938
          },
          "point_estimate": 836.7771956705639,
          "standard_error": 781.3039200569816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977199.8251605964,
            "upper_bound": 978514.7814109742
          },
          "point_estimate": 977894.1167464114,
          "standard_error": 337.5981834349487
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 476.8199788061672,
            "upper_bound": 3457.739516154962
          },
          "point_estimate": 2370.3026219165477,
          "standard_error": 771.1008434235728
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295340.948890169,
            "upper_bound": 295763.94184651814
          },
          "point_estimate": 295560.9652672171,
          "standard_error": 108.59899229682576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295390.08410138247,
            "upper_bound": 295844.24238351255
          },
          "point_estimate": 295493.05544354836,
          "standard_error": 132.51981422805648
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.32498920977638,
            "upper_bound": 653.7861025193841
          },
          "point_estimate": 268.38905515449534,
          "standard_error": 154.8715025909433
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295414.81136900076,
            "upper_bound": 295909.0145785184
          },
          "point_estimate": 295669.080163385,
          "standard_error": 127.90521929925158
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 182.0038871199979,
            "upper_bound": 487.0790164936403
          },
          "point_estimate": 361.4336772342843,
          "standard_error": 82.46529459681241
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 801933.2732699276,
            "upper_bound": 802905.7748469203
          },
          "point_estimate": 802362.4329891305,
          "standard_error": 251.81835822661472
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 801800.6152173913,
            "upper_bound": 802589.454710145
          },
          "point_estimate": 802271.5720108696,
          "standard_error": 200.3179551725166
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.75995133683118,
            "upper_bound": 1132.6969793472144
          },
          "point_estimate": 545.3825922196773,
          "standard_error": 263.99817417108187
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 801712.4419647245,
            "upper_bound": 802377.677018017
          },
          "point_estimate": 801989.8933370977,
          "standard_error": 167.30503158043527
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308.06526064686983,
            "upper_bound": 1200.7959406857017
          },
          "point_estimate": 835.9968916610326,
          "standard_error": 268.38256322544083
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395619.8408785585,
            "upper_bound": 396111.6056651139
          },
          "point_estimate": 395851.4841278468,
          "standard_error": 126.12455998558995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395592.8195652174,
            "upper_bound": 396175.1974637681
          },
          "point_estimate": 395711.77418478264,
          "standard_error": 136.30037929377417
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.54436606936041,
            "upper_bound": 699.6448364918884
          },
          "point_estimate": 287.6230519589056,
          "standard_error": 174.54477468828455
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 395598.18500571226,
            "upper_bound": 395878.8422663668
          },
          "point_estimate": 395724.33258046303,
          "standard_error": 73.80353510369451
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.02904837393876,
            "upper_bound": 549.2613944964866
          },
          "point_estimate": 419.896537515922,
          "standard_error": 94.32329736294297
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164858.51327962722,
            "upper_bound": 165372.97440647223
          },
          "point_estimate": 165097.7503183581,
          "standard_error": 132.66850171454865
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164736.2913838612,
            "upper_bound": 165419.76979638007
          },
          "point_estimate": 164952.67586726998,
          "standard_error": 185.2687559056242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.632992576765105,
            "upper_bound": 738.2277288621588
          },
          "point_estimate": 326.54468473435503,
          "standard_error": 188.43337176778311
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164763.11268521936,
            "upper_bound": 165032.47434585873
          },
          "point_estimate": 164852.07736968913,
          "standard_error": 70.1045081303414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.9568317504031,
            "upper_bound": 544.4333192028549
          },
          "point_estimate": 441.6240744129119,
          "standard_error": 91.08377441657004
        }
      }
    },
    "memrmem/krate/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231960.14289619352,
            "upper_bound": 232343.3003884845
          },
          "point_estimate": 232153.93174729552,
          "standard_error": 97.81406744803172
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231898.36464968155,
            "upper_bound": 232338.4650477707
          },
          "point_estimate": 232249.45106662624,
          "standard_error": 116.0068750350807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.201887871992184,
            "upper_bound": 597.6771277967698
          },
          "point_estimate": 236.63110065564663,
          "standard_error": 153.12482800395395
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232128.69657275127,
            "upper_bound": 232371.80950812908
          },
          "point_estimate": 232262.23915956655,
          "standard_error": 60.68491044082562
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.6231319770787,
            "upper_bound": 423.24493625073546
          },
          "point_estimate": 327.4550855766764,
          "standard_error": 64.67037094218307
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173160.2247302721,
            "upper_bound": 173476.59319841268
          },
          "point_estimate": 173327.81804875282,
          "standard_error": 80.75824325789185
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173231.0007936508,
            "upper_bound": 173562.89809523808
          },
          "point_estimate": 173307.1092517007,
          "standard_error": 79.38340630438769
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.22422258770727,
            "upper_bound": 448.2465697087087
          },
          "point_estimate": 132.2045658910122,
          "standard_error": 111.20525647165115
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173236.65887211254,
            "upper_bound": 173376.73072009292
          },
          "point_estimate": 173294.87126777985,
          "standard_error": 35.10517217841776
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.27429174909534,
            "upper_bound": 371.35017159182775
          },
          "point_estimate": 269.7000370726796,
          "standard_error": 69.65767781060565
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648783.2461309524,
            "upper_bound": 649621.5259960673
          },
          "point_estimate": 649229.0862507087,
          "standard_error": 215.4138534452443
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648884.2589285715,
            "upper_bound": 649763.7010204082
          },
          "point_estimate": 649264.1238839286,
          "standard_error": 212.94649196393308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.72554312768648,
            "upper_bound": 1167.2430167773186
          },
          "point_estimate": 614.0055415991832,
          "standard_error": 270.23579848495154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649180.1625992063,
            "upper_bound": 649694.337187744
          },
          "point_estimate": 649451.2900278294,
          "standard_error": 131.90561123288876
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 331.4062196977332,
            "upper_bound": 975.4274036114026
          },
          "point_estimate": 717.5329331412341,
          "standard_error": 178.13640090332075
        }
      }
    },
    "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1352.9546175094215,
            "upper_bound": 1355.1611277384354
          },
          "point_estimate": 1354.1485531802234,
          "standard_error": 0.5653924696121042
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1353.3388557584794,
            "upper_bound": 1356.01062243757
          },
          "point_estimate": 1354.0925782705926,
          "standard_error": 0.6811982443088154
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.14905204357053567,
            "upper_bound": 3.25504280840596
          },
          "point_estimate": 1.4115467192560698,
          "standard_error": 0.7491200006790774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1353.5528294571052,
            "upper_bound": 1354.683144725822
          },
          "point_estimate": 1353.975752380307,
          "standard_error": 0.28751964101102073
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8989808657832229,
            "upper_bound": 2.599276552706746
          },
          "point_estimate": 1.8878163648365651,
          "standard_error": 0.494343316765797
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314069.2594448698,
            "upper_bound": 1316276.9289172336
          },
          "point_estimate": 1315126.456952948,
          "standard_error": 563.7963213253347
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313723.6607142857,
            "upper_bound": 1316574.9151785714
          },
          "point_estimate": 1315103.2928571429,
          "standard_error": 677.0129665714545
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.47841213643551,
            "upper_bound": 3225.547528449377
          },
          "point_estimate": 1864.0197027403317,
          "standard_error": 818.3670227537095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314091.5156790772,
            "upper_bound": 1315286.1343671782
          },
          "point_estimate": 1314690.9202226344,
          "standard_error": 303.7935558522604
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 871.3807719200095,
            "upper_bound": 2374.027955529927
          },
          "point_estimate": 1873.0540707317043,
          "standard_error": 372.5983072557173
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1511107.162095238,
            "upper_bound": 1514044.0751209522
          },
          "point_estimate": 1512393.5251174602,
          "standard_error": 760.5805638840926
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510679.72,
            "upper_bound": 1513197.1
          },
          "point_estimate": 1511952.4664444444,
          "standard_error": 650.5016935467287
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 428.2951277296216,
            "upper_bound": 3218.567810858952
          },
          "point_estimate": 1866.1337608695928,
          "standard_error": 722.144659780071
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1511335.194543032,
            "upper_bound": 1512561.1911316398
          },
          "point_estimate": 1512007.1065974026,
          "standard_error": 307.14974047167095
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 928.185110724133,
            "upper_bound": 3604.9885450485663
          },
          "point_estimate": 2527.1449167206706,
          "standard_error": 798.25440985486
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415560.456401099,
            "upper_bound": 1417835.2792307693
          },
          "point_estimate": 1416687.056790293,
          "standard_error": 583.0958127323734
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415175.8365384615,
            "upper_bound": 1418245.641025641
          },
          "point_estimate": 1416820.8115384616,
          "standard_error": 704.0973146531329
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.54646294854288,
            "upper_bound": 3697.898953579984
          },
          "point_estimate": 1822.0248435180224,
          "standard_error": 860.2108497539025
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415051.5557358274,
            "upper_bound": 1418099.4255536308
          },
          "point_estimate": 1416621.353046953,
          "standard_error": 779.5790834657174
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1084.2482433792452,
            "upper_bound": 2450.748853328174
          },
          "point_estimate": 1949.0755930436749,
          "standard_error": 350.91743286084306
        }
      }
    },
    "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752965.9888143828,
            "upper_bound": 754514.4014888242
          },
          "point_estimate": 753765.308658892,
          "standard_error": 394.5457539075322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752993.0357142857,
            "upper_bound": 754494.7871720116
          },
          "point_estimate": 754048.2081632653,
          "standard_error": 408.0314584497398
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.41587516493,
            "upper_bound": 2179.8637155853385
          },
          "point_estimate": 1165.144197681904,
          "standard_error": 506.0533831144923
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753247.7775649239,
            "upper_bound": 754305.5925491792
          },
          "point_estimate": 753853.2011661808,
          "standard_error": 273.23357646547277
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 578.0943183805365,
            "upper_bound": 1796.765258764712
          },
          "point_estimate": 1316.0438143084002,
          "standard_error": 319.2098045388708
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265972.2391765149,
            "upper_bound": 266602.01165855635
          },
          "point_estimate": 266261.1989491369,
          "standard_error": 161.68783662078383
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265938.10701251304,
            "upper_bound": 266544.81569343066
          },
          "point_estimate": 266116.99051094893,
          "standard_error": 160.6985452067367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 84.0036275232606,
            "upper_bound": 786.0943461900538
          },
          "point_estimate": 376.4203838281604,
          "standard_error": 182.1213484764438
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265882.60445532843,
            "upper_bound": 266211.8770043497
          },
          "point_estimate": 266030.76162669447,
          "standard_error": 82.24692633664047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.98905281071688,
            "upper_bound": 747.110605155494
          },
          "point_estimate": 538.5613850814683,
          "standard_error": 149.27745887553218
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477444.5161264688,
            "upper_bound": 478197.74380519474
          },
          "point_estimate": 477795.8934348587,
          "standard_error": 192.31604130660443
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477388.6701298701,
            "upper_bound": 478091.86724386725
          },
          "point_estimate": 477806.98187229433,
          "standard_error": 163.75105961095946
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.863247703659283,
            "upper_bound": 1010.4382763468822
          },
          "point_estimate": 436.07591768013265,
          "standard_error": 256.01224001449003
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477674.5784645255,
            "upper_bound": 478023.2128942486
          },
          "point_estimate": 477833.6046888177,
          "standard_error": 87.01007888114414
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264.92583253457826,
            "upper_bound": 884.2425728051799
          },
          "point_estimate": 640.5040799385416,
          "standard_error": 169.17158623306705
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240913.14660956716,
            "upper_bound": 241319.71386313468
          },
          "point_estimate": 241108.6191508987,
          "standard_error": 103.9963387851739
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240889.91887417217,
            "upper_bound": 241361.2472406181
          },
          "point_estimate": 241064.27218543043,
          "standard_error": 100.28068778493086
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.88637622777794,
            "upper_bound": 594.5097026076065
          },
          "point_estimate": 241.64145852459257,
          "standard_error": 150.58614815653758
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240915.14505070364,
            "upper_bound": 241249.47276171675
          },
          "point_estimate": 241099.9505289413,
          "standard_error": 84.74867770375079
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.1978466794265,
            "upper_bound": 451.2616943790214
          },
          "point_estimate": 346.4809166687324,
          "standard_error": 73.50830216084796
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696516.6961226977,
            "upper_bound": 697303.234855739
          },
          "point_estimate": 696920.8227321055,
          "standard_error": 200.47303539433895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696562.8632075472,
            "upper_bound": 697483.4811320754
          },
          "point_estimate": 696873.6897274633,
          "standard_error": 230.56787110935804
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 127.04358768160488,
            "upper_bound": 1147.5123984955176
          },
          "point_estimate": 550.8776591192476,
          "standard_error": 253.07189209281884
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696341.6925019289,
            "upper_bound": 697314.2536874698
          },
          "point_estimate": 696800.3685371232,
          "standard_error": 244.56020492828853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330.9732416261008,
            "upper_bound": 895.9993574579254
          },
          "point_estimate": 667.9117897219282,
          "standard_error": 148.58078917523437
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.394518820435,
            "upper_bound": 357.72968034594635
          },
          "point_estimate": 357.532819443021,
          "standard_error": 0.08880312349823384
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.3631883089442,
            "upper_bound": 357.5847212647995
          },
          "point_estimate": 357.4378444615884,
          "standard_error": 0.05855718232222983
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016021624901092945,
            "upper_bound": 0.28350336931453907
          },
          "point_estimate": 0.1435017931488163,
          "standard_error": 0.07070429089608993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.3718450513947,
            "upper_bound": 357.5449779955913
          },
          "point_estimate": 357.43972446136115,
          "standard_error": 0.04391994354721689
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07183126161675173,
            "upper_bound": 0.43854426128804974
          },
          "point_estimate": 0.29538778928893844,
          "standard_error": 0.11371882095602444
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.59720132399536,
            "upper_bound": 169.84252411179773
          },
          "point_estimate": 169.7095481962172,
          "standard_error": 0.06312730020042114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.53913449942507,
            "upper_bound": 169.87094517801887
          },
          "point_estimate": 169.67408640047864,
          "standard_error": 0.07831782830171917
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018562934687774132,
            "upper_bound": 0.31247371868517615
          },
          "point_estimate": 0.1897548798194303,
          "standard_error": 0.08485324761089397
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.55094412875428,
            "upper_bound": 169.70890168792
          },
          "point_estimate": 169.6106303934737,
          "standard_error": 0.04056889501905685
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08196386425149044,
            "upper_bound": 0.26964416335593494
          },
          "point_estimate": 0.2107070427805421,
          "standard_error": 0.04942362096933979
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.68957892390807,
            "upper_bound": 26.02222045002906
          },
          "point_estimate": 25.8464000851934,
          "standard_error": 0.08498616489177513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.655093277862292,
            "upper_bound": 26.06900837684173
          },
          "point_estimate": 25.780850542099575,
          "standard_error": 0.09191663668644907
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00510964250067785,
            "upper_bound": 0.4756898392447429
          },
          "point_estimate": 0.19148408644401751,
          "standard_error": 0.12522460788717757
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.681612276153675,
            "upper_bound": 25.814633619404404
          },
          "point_estimate": 25.749169296865738,
          "standard_error": 0.03312115917437993
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1300846851547509,
            "upper_bound": 0.3790639729622412
          },
          "point_estimate": 0.2836129059867864,
          "standard_error": 0.06572038352469674
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.85844903554513,
            "upper_bound": 17.886875232698067
          },
          "point_estimate": 17.870478982931154,
          "standard_error": 0.007399354124088523
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.857207047920085,
            "upper_bound": 17.87551735015462
          },
          "point_estimate": 17.866130035005497,
          "standard_error": 0.0050171217232797315
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0037174147871485742,
            "upper_bound": 0.02699070344848067
          },
          "point_estimate": 0.012006462842770335,
          "standard_error": 0.006059116561472355
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.859563471156058,
            "upper_bound": 17.87613900605796
          },
          "point_estimate": 17.86890289075231,
          "standard_error": 0.004363894969925475
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0070877749773813545,
            "upper_bound": 0.035966787748403194
          },
          "point_estimate": 0.02455464549649795,
          "standard_error": 0.008848145717649454
        }
      }
    },
    "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.21761957361456,
            "upper_bound": 25.26175882722148
          },
          "point_estimate": 25.23685676431879,
          "standard_error": 0.011428465931255767
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.21064387269085,
            "upper_bound": 25.256180393563582
          },
          "point_estimate": 25.224917190398944,
          "standard_error": 0.010966507850991142
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005641443749591005,
            "upper_bound": 0.0498095534252012
          },
          "point_estimate": 0.023503866815377087,
          "standard_error": 0.012095265303217029
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.210346357095812,
            "upper_bound": 25.250013984822775
          },
          "point_estimate": 25.228461156940583,
          "standard_error": 0.010330882929074078
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012337529145072171,
            "upper_bound": 0.05336490953410129
          },
          "point_estimate": 0.03798132409370422,
          "standard_error": 0.011738085010784584
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286921.922328459,
            "upper_bound": 287376.31436195475
          },
          "point_estimate": 287147.1929708786,
          "standard_error": 116.1417340483541
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286863.80314960633,
            "upper_bound": 287420.2890888639
          },
          "point_estimate": 287187.0341207349,
          "standard_error": 129.1954314348313
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.856888570664623,
            "upper_bound": 687.9678953976661
          },
          "point_estimate": 318.52162269154564,
          "standard_error": 172.38197953888735
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 286936.1452455086,
            "upper_bound": 287546.9980988292
          },
          "point_estimate": 287251.67671541055,
          "standard_error": 156.13669978255942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 204.08575030551404,
            "upper_bound": 501.356294957074
          },
          "point_estimate": 387.1913313978893,
          "standard_error": 76.38281356949041
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.024575154049185,
            "upper_bound": 16.13009023396737
          },
          "point_estimate": 16.076221763987867,
          "standard_error": 0.027078087073473308
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 15.99843153249164,
            "upper_bound": 16.156565900814243
          },
          "point_estimate": 16.068155445384654,
          "standard_error": 0.04030141716774615
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02647499013281509,
            "upper_bound": 0.15666130424944838
          },
          "point_estimate": 0.10632342835265356,
          "standard_error": 0.03421008040055116
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.021421007843486,
            "upper_bound": 16.11815952972444
          },
          "point_estimate": 16.06910868490763,
          "standard_error": 0.02429807679471913
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.051252909292048315,
            "upper_bound": 0.1091228812882584
          },
          "point_estimate": 0.08993904099249252,
          "standard_error": 0.014601823471736825
        }
      }
    },
    "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.641943512337733,
            "upper_bound": 22.747255919149843
          },
          "point_estimate": 22.693352557464625,
          "standard_error": 0.02693473976299119
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.62037683187164,
            "upper_bound": 22.793927117212025
          },
          "point_estimate": 22.67145402409809,
          "standard_error": 0.048419829639920635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01560904113965298,
            "upper_bound": 0.1432523365943341
          },
          "point_estimate": 0.08820075355430118,
          "standard_error": 0.036944505293668606
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.630270437663174,
            "upper_bound": 22.72298080309154
          },
          "point_estimate": 22.670330947025302,
          "standard_error": 0.02357466407991201
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.054982791687583105,
            "upper_bound": 0.10480992413965382
          },
          "point_estimate": 0.08969853118323971,
          "standard_error": 0.01259714269060517
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126574.28742669756,
            "upper_bound": 126756.99390505126
          },
          "point_estimate": 126659.72056451168,
          "standard_error": 47.01529528528198
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126549.08912037036,
            "upper_bound": 126796.4366815476
          },
          "point_estimate": 126622.78036265432,
          "standard_error": 52.559115621884295
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.50276284755465,
            "upper_bound": 263.2006194939197
          },
          "point_estimate": 97.74896167606856,
          "standard_error": 58.015641074036154
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126575.792318656,
            "upper_bound": 126744.75952107432
          },
          "point_estimate": 126650.52426046177,
          "standard_error": 43.27624037312259
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.378780871610765,
            "upper_bound": 198.04046242022352
          },
          "point_estimate": 156.62030546767787,
          "standard_error": 34.262399872355495
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.21356886356511,
            "upper_bound": 24.24309638647315
          },
          "point_estimate": 24.22910025722289,
          "standard_error": 0.00756081672751478
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.214103477631976,
            "upper_bound": 24.2485384658495
          },
          "point_estimate": 24.23423879245109,
          "standard_error": 0.008969622919738632
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004047171288678486,
            "upper_bound": 0.042083246057388925
          },
          "point_estimate": 0.026617214939388,
          "standard_error": 0.010281209639641615
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.2269881990297,
            "upper_bound": 24.251089641015973
          },
          "point_estimate": 24.24025491920258,
          "standard_error": 0.006297159014972236
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012202136541893085,
            "upper_bound": 0.03352677018283186
          },
          "point_estimate": 0.025150295869633164,
          "standard_error": 0.005689440394838142
        }
      }
    },
    "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.060144457824062,
            "upper_bound": 21.277457639946743
          },
          "point_estimate": 21.164838470398216,
          "standard_error": 0.05588042267286711
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.99474253144654,
            "upper_bound": 21.323623510404907
          },
          "point_estimate": 21.119665853367323,
          "standard_error": 0.08366899822974704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.040776614022219014,
            "upper_bound": 0.3134253135606066
          },
          "point_estimate": 0.2011902152223529,
          "standard_error": 0.07112899266099136
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.06248125532602,
            "upper_bound": 21.290555921997147
          },
          "point_estimate": 21.17747541524755,
          "standard_error": 0.057305973618820755
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09903165164209989,
            "upper_bound": 0.22433084019131388
          },
          "point_estimate": 0.1856564909579406,
          "standard_error": 0.031203495646187025
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.99164656836494,
            "upper_bound": 42.08177194380333
          },
          "point_estimate": 42.03712283059379,
          "standard_error": 0.02302720628191374
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.97654164988822,
            "upper_bound": 42.10476311148757
          },
          "point_estimate": 42.02978350115636,
          "standard_error": 0.0304433544343008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003915381669058148,
            "upper_bound": 0.13872679813916525
          },
          "point_estimate": 0.08000625213490216,
          "standard_error": 0.033230176838514314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.02723864433504,
            "upper_bound": 42.10575988714007
          },
          "point_estimate": 42.0692759652047,
          "standard_error": 0.020261450397943036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04393515673718605,
            "upper_bound": 0.09628209948369014
          },
          "point_estimate": 0.07668175542476012,
          "standard_error": 0.013558566166544682
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.90062621976871,
            "upper_bound": 75.10000023970177
          },
          "point_estimate": 75.00126555655586,
          "standard_error": 0.05113758800026637
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.90934504709978,
            "upper_bound": 75.13278002095156
          },
          "point_estimate": 74.98557689663895,
          "standard_error": 0.05909178290614477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.031470283421601186,
            "upper_bound": 0.3033914901037755
          },
          "point_estimate": 0.14070023907400164,
          "standard_error": 0.06630669911220906
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.93083125553373,
            "upper_bound": 75.0854235361046
          },
          "point_estimate": 75.00128941353618,
          "standard_error": 0.0390151225448299
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08618703867749718,
            "upper_bound": 0.22611763480627212
          },
          "point_estimate": 0.1705522182208168,
          "standard_error": 0.03609775509849635
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_prebuilt_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.758018316879015,
            "upper_bound": 10.791837785733058
          },
          "point_estimate": 10.77372104535466,
          "standard_error": 0.00874535305160696
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.752550362299193,
            "upper_bound": 10.794364501464871
          },
          "point_estimate": 10.766481381778233,
          "standard_error": 0.010578226895377869
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002075592017785886,
            "upper_bound": 0.04554710761325966
          },
          "point_estimate": 0.022111086967292408,
          "standard_error": 0.010887797541782363
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.758459466778657,
            "upper_bound": 10.77801930217735
          },
          "point_estimate": 10.768586892547532,
          "standard_error": 0.005077541879914472
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012512102367969,
            "upper_bound": 0.0383791976209766
          },
          "point_estimate": 0.02916933687960113,
          "standard_error": 0.0068245208233954965
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32518.665433715032,
            "upper_bound": 32564.92282810164
          },
          "point_estimate": 32540.834598441164,
          "standard_error": 11.804012681873552
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32509.51330343797,
            "upper_bound": 32565.56053811659
          },
          "point_estimate": 32536.45430012812,
          "standard_error": 17.298631710682308
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.682071432937655,
            "upper_bound": 70.71711005617995
          },
          "point_estimate": 41.54781432964191,
          "standard_error": 15.653453022957589
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32511.516428437968,
            "upper_bound": 32548.38717295222
          },
          "point_estimate": 32525.16385067847,
          "standard_error": 9.346603251093493
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.4608861794874,
            "upper_bound": 51.61020253563231
          },
          "point_estimate": 39.297802071349345,
          "standard_error": 8.11771990428756
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.671159428591164,
            "upper_bound": 30.27874607733647
          },
          "point_estimate": 29.94971576887719,
          "standard_error": 0.15638577305448909
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.562813029961013,
            "upper_bound": 30.217996311450744
          },
          "point_estimate": 29.83511380996469,
          "standard_error": 0.1739146564442923
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12393637707513704,
            "upper_bound": 0.8040459595983783
          },
          "point_estimate": 0.4570191825098366,
          "standard_error": 0.1710982958621299
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.680236829529864,
            "upper_bound": 30.139209558442435
          },
          "point_estimate": 29.92025625212461,
          "standard_error": 0.11900702079839674
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22737890217244291,
            "upper_bound": 0.7075530575268156
          },
          "point_estimate": 0.5208345672274662,
          "standard_error": 0.13436489811456384
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978409.7683333332,
            "upper_bound": 980591.8632935464
          },
          "point_estimate": 979287.1408270678,
          "standard_error": 583.6759423611151
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978187.9407894736,
            "upper_bound": 979452.5368421052
          },
          "point_estimate": 978824.5409774436,
          "standard_error": 420.99462016003474
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117.67161896364266,
            "upper_bound": 1727.5313416987042
          },
          "point_estimate": 923.6318222864492,
          "standard_error": 394.50383780560105
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978170.6317641962,
            "upper_bound": 978980.5652173389
          },
          "point_estimate": 978519.3682843472,
          "standard_error": 204.82300098872832
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 496.2969031674827,
            "upper_bound": 2942.159579963654
          },
          "point_estimate": 1947.932694427999,
          "standard_error": 809.5911546320431
        }
      }
    },
    "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_prebuilt_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1959.6747385496108,
            "upper_bound": 1962.94497377656
          },
          "point_estimate": 1961.3815032784112,
          "standard_error": 0.8402854252595626
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1959.2555105137455,
            "upper_bound": 1963.2800014365496
          },
          "point_estimate": 1961.7636346495717,
          "standard_error": 0.9151861829110602
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6594546858089424,
            "upper_bound": 4.861505720942099
          },
          "point_estimate": 2.20841496914273,
          "standard_error": 1.1215626259895095
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.7731478518376,
            "upper_bound": 1963.2091979575312
          },
          "point_estimate": 1961.030490621268,
          "standard_error": 1.1307539780918394
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3552190334239491,
            "upper_bound": 3.6010345772542975
          },
          "point_estimate": 2.799508089832954,
          "standard_error": 0.5733905087349845
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.823833375233862,
            "upper_bound": 7.837004487979037
          },
          "point_estimate": 7.830519541900069,
          "standard_error": 0.00337218955854003
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.822703621497038,
            "upper_bound": 7.840233439031719
          },
          "point_estimate": 7.830598314319637,
          "standard_error": 0.004882965881695015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014916481867331135,
            "upper_bound": 0.01947546094748052
          },
          "point_estimate": 0.011928302475346427,
          "standard_error": 0.00439105496477719
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.825633207074513,
            "upper_bound": 7.838589564175596
          },
          "point_estimate": 7.832778826701069,
          "standard_error": 0.003354318967420178
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006446163789074433,
            "upper_bound": 0.014331683106034944
          },
          "point_estimate": 0.011216567677960403,
          "standard_error": 0.002060233264451932
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.1012377009133845,
            "upper_bound": 7.109809823499468
          },
          "point_estimate": 7.105600008135245,
          "standard_error": 0.002195853632036326
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.100047888885287,
            "upper_bound": 7.111757226341833
          },
          "point_estimate": 7.105591682715346,
          "standard_error": 0.002601034337610274
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014433509808946554,
            "upper_bound": 0.01327465131995456
          },
          "point_estimate": 0.00584874002656825,
          "standard_error": 0.0030338523787888467
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.1002781197245035,
            "upper_bound": 7.107479480515777
          },
          "point_estimate": 7.103745461032638,
          "standard_error": 0.0017961179895001529
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003937983589427457,
            "upper_bound": 0.009429203236587693
          },
          "point_estimate": 0.007365944100178948,
          "standard_error": 0.001424160254657845
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.311018011350296,
            "upper_bound": 8.317296757706785
          },
          "point_estimate": 8.31406064283569,
          "standard_error": 0.0016112630153264217
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.30970964688758,
            "upper_bound": 8.318582485200373
          },
          "point_estimate": 8.312859769646662,
          "standard_error": 0.0028036029597469096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00021977198893425424,
            "upper_bound": 0.009166658232202836
          },
          "point_estimate": 0.0051211131636964505,
          "standard_error": 0.002555743343873177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.310016698818327,
            "upper_bound": 8.31857980434251
          },
          "point_estimate": 8.31397145036752,
          "standard_error": 0.002221837740612239
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0031285385314042745,
            "upper_bound": 0.006497649861919246
          },
          "point_estimate": 0.005371039195671311,
          "standard_error": 0.0008594927892072098
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.24619289694886,
            "upper_bound": 22.28220587642431
          },
          "point_estimate": 22.26247145619732,
          "standard_error": 0.009250174868129088
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.237743955603552,
            "upper_bound": 22.27340728662202
          },
          "point_estimate": 22.2612128580689,
          "standard_error": 0.009126530971728044
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0045876856435925805,
            "upper_bound": 0.04463688824700835
          },
          "point_estimate": 0.019027875670093487,
          "standard_error": 0.0099743604652879
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.2508040368556,
            "upper_bound": 22.287308799425247
          },
          "point_estimate": 22.268237849713344,
          "standard_error": 0.009148292479896812
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012507581077480366,
            "upper_bound": 0.04338735459540071
          },
          "point_estimate": 0.030811432863870933,
          "standard_error": 0.00899133094760028
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.286538855720485,
            "upper_bound": 14.318816474744768
          },
          "point_estimate": 14.301181622180568,
          "standard_error": 0.008323869879711683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.285282886553286,
            "upper_bound": 14.313188561597114
          },
          "point_estimate": 14.295486429982477,
          "standard_error": 0.0060850917222256655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0014267969940877888,
            "upper_bound": 0.042697330721341464
          },
          "point_estimate": 0.014265753976964344,
          "standard_error": 0.010687658111005386
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.286361021968595,
            "upper_bound": 14.309164655117236
          },
          "point_estimate": 14.297147261707774,
          "standard_error": 0.005868031208215121
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009597353796198615,
            "upper_bound": 0.03829671957470302
          },
          "point_estimate": 0.02782423420228402,
          "standard_error": 0.007867406730202371
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.59463830643298,
            "upper_bound": 18.623220279866064
          },
          "point_estimate": 18.609667074652283,
          "standard_error": 0.007292329357455506
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.594292799020295,
            "upper_bound": 18.623163620337472
          },
          "point_estimate": 18.611562722683768,
          "standard_error": 0.005467346942208013
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001825501471704049,
            "upper_bound": 0.0427304687174386
          },
          "point_estimate": 0.009352449678430044,
          "standard_error": 0.01094255974042536
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.60714323210436,
            "upper_bound": 18.61955915730911
          },
          "point_estimate": 18.61377467177531,
          "standard_error": 0.0031651734741050615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008459812570722336,
            "upper_bound": 0.032663215090237785
          },
          "point_estimate": 0.02429582560616624,
          "standard_error": 0.005950336596044379
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.75292086956293,
            "upper_bound": 10.772515762695264
          },
          "point_estimate": 10.762339109065636,
          "standard_error": 0.0050169258005823055
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.74884426079314,
            "upper_bound": 10.773731284454971
          },
          "point_estimate": 10.761035255781213,
          "standard_error": 0.005048226687908221
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002182928560140438,
            "upper_bound": 0.029795976392939003
          },
          "point_estimate": 0.008419126879272482,
          "standard_error": 0.007422637648885795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.76037559605445,
            "upper_bound": 10.77453594717198
          },
          "point_estimate": 10.7668555936765,
          "standard_error": 0.0036027026884352387
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008019632489841251,
            "upper_bound": 0.02196188978524774
          },
          "point_estimate": 0.016755936848213644,
          "standard_error": 0.0036341326140019057
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.045758217562263,
            "upper_bound": 16.141816040549852
          },
          "point_estimate": 16.092453293123164,
          "standard_error": 0.024609944809202023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.02413814665239,
            "upper_bound": 16.162582895371205
          },
          "point_estimate": 16.073729173322825,
          "standard_error": 0.03334362778251607
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01833013845845032,
            "upper_bound": 0.14536449775356575
          },
          "point_estimate": 0.0911982029136733,
          "standard_error": 0.03222772698159462
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.054976048853845,
            "upper_bound": 16.11979883582963
          },
          "point_estimate": 16.086195419892558,
          "standard_error": 0.01705107527876126
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04455412190985951,
            "upper_bound": 0.10306232290131288
          },
          "point_estimate": 0.08193216642891007,
          "standard_error": 0.01489693479450044
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.638969297740516,
            "upper_bound": 22.743959745497623
          },
          "point_estimate": 22.68697317016673,
          "standard_error": 0.027010454343136852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.616791429243925,
            "upper_bound": 22.737153213743028
          },
          "point_estimate": 22.67167451845055,
          "standard_error": 0.03381966987041692
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012403798995315886,
            "upper_bound": 0.13786454178063562
          },
          "point_estimate": 0.07128384697798493,
          "standard_error": 0.03386632828033815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.63270913545153,
            "upper_bound": 22.764893250716927
          },
          "point_estimate": 22.68832577780225,
          "standard_error": 0.03361954298027833
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.039179721775009266,
            "upper_bound": 0.12158103628792122
          },
          "point_estimate": 0.08973513510768692,
          "standard_error": 0.023058019212633937
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.00418680118227,
            "upper_bound": 11.016272332722082
          },
          "point_estimate": 11.010039239770649,
          "standard_error": 0.0031004873391323995
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.002845945797894,
            "upper_bound": 11.018969062763054
          },
          "point_estimate": 11.00690622485487,
          "standard_error": 0.00488524463399564
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.000655936549385976,
            "upper_bound": 0.017278719322363655
          },
          "point_estimate": 0.007901442260827642,
          "standard_error": 0.004578219212595521
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.003524912005512,
            "upper_bound": 11.01293754855907
          },
          "point_estimate": 11.007894265925252,
          "standard_error": 0.002362999248341945
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005717073225069312,
            "upper_bound": 0.012807677991196748
          },
          "point_estimate": 0.010340460026257964,
          "standard_error": 0.001792114955883123
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.21462603961175,
            "upper_bound": 24.280011538787615
          },
          "point_estimate": 24.246316715540488,
          "standard_error": 0.016733063172624355
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.21456941489734,
            "upper_bound": 24.284927329680343
          },
          "point_estimate": 24.23696079967985,
          "standard_error": 0.014270526561931077
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0038289245804064833,
            "upper_bound": 0.10183634526886393
          },
          "point_estimate": 0.027986337751941964,
          "standard_error": 0.025402666301572577
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.22911643278037,
            "upper_bound": 24.280278909414378
          },
          "point_estimate": 24.248795351055612,
          "standard_error": 0.01388520060188384
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02246797422656862,
            "upper_bound": 0.0741914633932479
          },
          "point_estimate": 0.05584016284079989,
          "standard_error": 0.01284398217614448
        }
      }
    },
    "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.97354866542647,
            "upper_bound": 21.200462010657404
          },
          "point_estimate": 21.09379318491286,
          "standard_error": 0.058397824101495986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.985360429947477,
            "upper_bound": 21.26296583386209
          },
          "point_estimate": 21.125666118855943,
          "standard_error": 0.09301566834929452
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.026232824354748024,
            "upper_bound": 0.3679946043545856
          },
          "point_estimate": 0.2044466340173789,
          "standard_error": 0.08703122848721469
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.05255480096877,
            "upper_bound": 21.261363237518943
          },
          "point_estimate": 21.17406132325905,
          "standard_error": 0.05434740905888
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10542291261410348,
            "upper_bound": 0.25902784644543064
          },
          "point_estimate": 0.19509429862365252,
          "standard_error": 0.044197139765235906
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169035.060388021,
            "upper_bound": 1170159.932361111
          },
          "point_estimate": 1169581.749548611,
          "standard_error": 288.2619333857053
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168750.5520833335,
            "upper_bound": 1170230.1875
          },
          "point_estimate": 1169589.8515625,
          "standard_error": 403.1655232700304
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 293.5007416642627,
            "upper_bound": 1704.4710597394517
          },
          "point_estimate": 1035.480253491551,
          "standard_error": 344.2115208162138
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1169192.9351340693,
            "upper_bound": 1170324.419615385
          },
          "point_estimate": 1169872.6369318182,
          "standard_error": 288.5458756748203
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 537.1387699546151,
            "upper_bound": 1224.4354766149193
          },
          "point_estimate": 956.2656436765334,
          "standard_error": 180.90091401596368
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563290.5739335318,
            "upper_bound": 1565658.7892596314
          },
          "point_estimate": 1564466.5619031084,
          "standard_error": 605.2031123646824
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563149.28125,
            "upper_bound": 1565566.7142857143
          },
          "point_estimate": 1564556.888888889,
          "standard_error": 539.3867765261588
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.9428327251801,
            "upper_bound": 3490.1553081764123
          },
          "point_estimate": 1783.7472099986562,
          "standard_error": 838.6175294713372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1562248.4491445003,
            "upper_bound": 1565501.8502218935
          },
          "point_estimate": 1563585.3957792209,
          "standard_error": 826.3861474013651
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 878.0302565897287,
            "upper_bound": 2734.5932084374595
          },
          "point_estimate": 2018.8383545527647,
          "standard_error": 476.8622170210498
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446059.5727508017,
            "upper_bound": 1448977.526901137
          },
          "point_estimate": 1447268.2074191088,
          "standard_error": 769.5539484599718
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1445815.782051282,
            "upper_bound": 1447804.427960928
          },
          "point_estimate": 1446344.3326923077,
          "standard_error": 554.5160561558798
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 202.15314000090825,
            "upper_bound": 2586.2402548862815
          },
          "point_estimate": 827.6818685749305,
          "standard_error": 649.0127401050643
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446015.0850388145,
            "upper_bound": 1447651.714906679
          },
          "point_estimate": 1446772.3536463536,
          "standard_error": 413.67223979329646
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 542.8007415184503,
            "upper_bound": 3778.171027822563
          },
          "point_estimate": 2566.289561214447,
          "standard_error": 967.2053610364924
        }
      }
    },
    "memrmem/krate/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387015.8927452719,
            "upper_bound": 387733.537234338
          },
          "point_estimate": 387340.6708362884,
          "standard_error": 183.63824494008136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387085.03723404254,
            "upper_bound": 387525.0957446809
          },
          "point_estimate": 387214.50864361704,
          "standard_error": 132.48394128660993
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.313841642587725,
            "upper_bound": 833.5261171168568
          },
          "point_estimate": 245.33699848123828,
          "standard_error": 187.35780556123245
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 386971.35031424783,
            "upper_bound": 388233.1872534437
          },
          "point_estimate": 387597.4618679193,
          "standard_error": 346.7608282663989
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.29906903813216,
            "upper_bound": 887.4701114945162
          },
          "point_estimate": 612.2107937690888,
          "standard_error": 197.68929786701509
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559833.4800929488,
            "upper_bound": 560527.747996337
          },
          "point_estimate": 560160.9862185592,
          "standard_error": 177.47891498534784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559735.9820512821,
            "upper_bound": 560486.7535714286
          },
          "point_estimate": 560180.2207692307,
          "standard_error": 190.72436476016367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 35.38395906410463,
            "upper_bound": 980.7964983566897
          },
          "point_estimate": 592.47964271212,
          "standard_error": 238.76414867989695
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 559884.1014495014,
            "upper_bound": 560898.8056743421
          },
          "point_estimate": 560364.135024975,
          "standard_error": 267.01835273741665
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 251.42718898716592,
            "upper_bound": 802.3246885621395
          },
          "point_estimate": 590.258570453014,
          "standard_error": 146.45326107784192
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414574.91128578194,
            "upper_bound": 415100.802222628
          },
          "point_estimate": 414843.66621167026,
          "standard_error": 134.76291441658282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414468.1573863636,
            "upper_bound": 415154.38125
          },
          "point_estimate": 414957.10886769486,
          "standard_error": 187.8927353902709
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.26268830040463,
            "upper_bound": 742.913858685585
          },
          "point_estimate": 372.1334357796478,
          "standard_error": 182.70888343301607
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414481.099369336,
            "upper_bound": 415101.3380216225
          },
          "point_estimate": 414811.8985832349,
          "standard_error": 172.42028284342254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 239.0221776557595,
            "upper_bound": 561.4297465466827
          },
          "point_estimate": 448.1245732250431,
          "standard_error": 80.73856003204962
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475070.61952844774,
            "upper_bound": 475467.0089105339
          },
          "point_estimate": 475264.6306220368,
          "standard_error": 101.49980692797564
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474972.1424860853,
            "upper_bound": 475503.67873376625
          },
          "point_estimate": 475280.1461038961,
          "standard_error": 132.3021817268529
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.59552037311478,
            "upper_bound": 594.4320930831083
          },
          "point_estimate": 347.884932135456,
          "standard_error": 131.96905105438043
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475026.19163405034,
            "upper_bound": 475471.2163947014
          },
          "point_estimate": 475239.2383875864,
          "standard_error": 119.55591475074236
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 190.01870705204269,
            "upper_bound": 425.8819556713663
          },
          "point_estimate": 338.0208300933811,
          "standard_error": 61.30893723386178
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 976305.7290702536,
            "upper_bound": 977841.1283751564
          },
          "point_estimate": 977109.1743890978,
          "standard_error": 392.2612039083701
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 976409.3451754386,
            "upper_bound": 978010.8067042606
          },
          "point_estimate": 977230.6858552631,
          "standard_error": 374.4055608906568
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.5973293742267,
            "upper_bound": 2224.618501294603
          },
          "point_estimate": 703.9272504633394,
          "standard_error": 498.7825184505068
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 976558.9709457654,
            "upper_bound": 977749.3431649592
          },
          "point_estimate": 977077.3572795624,
          "standard_error": 303.1561001406367
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 556.4323573556184,
            "upper_bound": 1782.110454994949
          },
          "point_estimate": 1306.2589262690435,
          "standard_error": 321.062059982147
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296177.8490252936,
            "upper_bound": 296632.808049458
          },
          "point_estimate": 296418.29162827466,
          "standard_error": 115.65192183771056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296251.29522357724,
            "upper_bound": 296657.7111111111
          },
          "point_estimate": 296435.1227642277,
          "standard_error": 101.629080320251
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 69.23018657585023,
            "upper_bound": 621.6837506498614
          },
          "point_estimate": 274.305437270998,
          "standard_error": 138.3795243707816
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296275.29881388956,
            "upper_bound": 296578.3524375129
          },
          "point_estimate": 296417.25169464684,
          "standard_error": 76.97793599297164
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159.5447671072275,
            "upper_bound": 533.0867119992455
          },
          "point_estimate": 386.5363457271424,
          "standard_error": 99.17637777415744
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800679.5844893892,
            "upper_bound": 801686.7110677192
          },
          "point_estimate": 801201.4600948931,
          "standard_error": 259.02875178479303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800521.170289855,
            "upper_bound": 801940.5931677019
          },
          "point_estimate": 801326.2532608695,
          "standard_error": 346.4275670505784
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 145.0656032216605,
            "upper_bound": 1619.2915473387077
          },
          "point_estimate": 863.5675714802172,
          "standard_error": 359.68171641506143
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800855.2679984578,
            "upper_bound": 801917.0205167392
          },
          "point_estimate": 801506.017730096,
          "standard_error": 268.299679060238
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452.2299733526486,
            "upper_bound": 1062.628028337666
          },
          "point_estimate": 862.1152960398139,
          "standard_error": 151.2142497475597
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 394441.927958184,
            "upper_bound": 394984.44632019114
          },
          "point_estimate": 394700.59275388287,
          "standard_error": 138.7706996600287
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 394270.87096774194,
            "upper_bound": 395002.4752688171
          },
          "point_estimate": 394684.5497311828,
          "standard_error": 204.00422501095335
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 95.26900476023884,
            "upper_bound": 814.2684384112896
          },
          "point_estimate": 519.6652399676763,
          "standard_error": 189.62085748904047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 394419.4116860984,
            "upper_bound": 394863.1529721283
          },
          "point_estimate": 394656.4528417819,
          "standard_error": 112.14102399197408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 247.59666028458597,
            "upper_bound": 597.4232445037144
          },
          "point_estimate": 462.7020871091766,
          "standard_error": 94.17398536193625
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164900.44382245207,
            "upper_bound": 165213.03323126753
          },
          "point_estimate": 165035.65601001942,
          "standard_error": 80.95711587861756
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164873.5298427063,
            "upper_bound": 165124.33325791854
          },
          "point_estimate": 164959.7491515837,
          "standard_error": 52.546503393027464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 11.29907457315453,
            "upper_bound": 345.86172689139886
          },
          "point_estimate": 105.27969247480968,
          "standard_error": 94.04354628065867
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 164858.8021627969,
            "upper_bound": 164973.7941613352
          },
          "point_estimate": 164921.66219662692,
          "standard_error": 30.940048166464347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 78.57775328190132,
            "upper_bound": 385.167207124085
          },
          "point_estimate": 270.1876004925591,
          "standard_error": 88.9073172220041
        }
      }
    },
    "memrmem/krate/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231815.33407693863,
            "upper_bound": 232242.98734499168
          },
          "point_estimate": 232019.72027120617,
          "standard_error": 109.69666186404956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231699.9275477707,
            "upper_bound": 232331.6648089172
          },
          "point_estimate": 231882.31760691537,
          "standard_error": 159.4112678759248
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.53228409060178,
            "upper_bound": 593.4495931265819
          },
          "point_estimate": 356.6833350688687,
          "standard_error": 145.93657847181018
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231826.0137168667,
            "upper_bound": 232107.80721414217
          },
          "point_estimate": 231931.46008768305,
          "standard_error": 71.41542552784213
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192.87915368471292,
            "upper_bound": 450.04892381148255
          },
          "point_estimate": 365.306122755756,
          "standard_error": 65.1113242081112
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173112.85869897963,
            "upper_bound": 173262.840758664
          },
          "point_estimate": 173184.94332105064,
          "standard_error": 38.43110464904901
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173091.5373393802,
            "upper_bound": 173281.04126984128
          },
          "point_estimate": 173163.1693452381,
          "standard_error": 45.6524590242909
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.086984535385431,
            "upper_bound": 221.43171873544725
          },
          "point_estimate": 125.78291187404776,
          "standard_error": 57.14201368523956
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173093.40337451806,
            "upper_bound": 173203.91126760977
          },
          "point_estimate": 173145.90672850958,
          "standard_error": 27.673708972462386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.80352231272163,
            "upper_bound": 159.35637018247547
          },
          "point_estimate": 128.2259511317482,
          "standard_error": 23.88840577407857
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649227.4769409013,
            "upper_bound": 650038.0369754463
          },
          "point_estimate": 649617.8260324546,
          "standard_error": 208.38596179375023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649140.1670918367,
            "upper_bound": 650067.1165674604
          },
          "point_estimate": 649549.2883184524,
          "standard_error": 215.0250943303038
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.61362656346542,
            "upper_bound": 1168.8551551415758
          },
          "point_estimate": 568.5528211562354,
          "standard_error": 282.12538244457596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649158.5566706022,
            "upper_bound": 649717.9953135101
          },
          "point_estimate": 649402.3496753246,
          "standard_error": 142.50490346428543
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 318.22079158025167,
            "upper_bound": 894.8266499884011
          },
          "point_estimate": 693.1630593164592,
          "standard_error": 147.46441424614136
        }
      }
    },
    "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_prebuiltiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.5160495589937,
            "upper_bound": 1315.0307827174388
          },
          "point_estimate": 1313.8067141368188,
          "standard_error": 0.643956819965387
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.1914521881629,
            "upper_bound": 1315.5809626229034
          },
          "point_estimate": 1313.737983028485,
          "standard_error": 0.7110302212949609
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.18956689187770528,
            "upper_bound": 4.074518505597929
          },
          "point_estimate": 1.71692582667945,
          "standard_error": 1.1159877396545952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313.4521545613609,
            "upper_bound": 1315.352197441008
          },
          "point_estimate": 1314.140615399638,
          "standard_error": 0.4918439071120031
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.1325461009806377,
            "upper_bound": 2.7448930682800743
          },
          "point_estimate": 2.1443772099555227,
          "standard_error": 0.41988101801017136
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314819.429202381,
            "upper_bound": 1316720.2380357145
          },
          "point_estimate": 1315764.09265873,
          "standard_error": 481.77214746503745
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1314925.1235119049,
            "upper_bound": 1316285.161507937
          },
          "point_estimate": 1316015.3821428572,
          "standard_error": 342.7878857857707
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.34576048900163,
            "upper_bound": 2572.3241918323633
          },
          "point_estimate": 806.1884456872019,
          "standard_error": 618.3956748788222
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1315553.731922904,
            "upper_bound": 1316162.2194616976
          },
          "point_estimate": 1315902.3381261595,
          "standard_error": 156.1250832878917
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 466.2452516136483,
            "upper_bound": 2250.971812897621
          },
          "point_estimate": 1601.2889183428515,
          "standard_error": 448.9592319658037
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510905.8731222225,
            "upper_bound": 1513433.2308888887
          },
          "point_estimate": 1512126.4705888887,
          "standard_error": 645.9405639391107
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510549.605,
            "upper_bound": 1513818.616
          },
          "point_estimate": 1511923.9866666668,
          "standard_error": 778.7746134677328
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 614.1275030971356,
            "upper_bound": 3574.8467038670883
          },
          "point_estimate": 1764.823998168173,
          "standard_error": 808.6102378336201
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1509908.5827692307,
            "upper_bound": 1512311.2420986632
          },
          "point_estimate": 1510821.2514285715,
          "standard_error": 616.7607656733218
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1093.5494985656412,
            "upper_bound": 2775.421204427672
          },
          "point_estimate": 2156.124719773122,
          "standard_error": 433.0083376330184
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416889.573525641,
            "upper_bound": 1419029.6409989316
          },
          "point_estimate": 1417899.4995405986,
          "standard_error": 548.5107651074273
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416621.119230769,
            "upper_bound": 1419113.7692307692
          },
          "point_estimate": 1417381.016025641,
          "standard_error": 623.114426380189
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 241.31809331214023,
            "upper_bound": 3029.110143658494
          },
          "point_estimate": 1324.3469198535258,
          "standard_error": 785.4880817832942
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416559.5109332292,
            "upper_bound": 1419563.214019996
          },
          "point_estimate": 1417862.605894106,
          "standard_error": 813.8403382964034
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 766.6858235755341,
            "upper_bound": 2301.690384578339
          },
          "point_estimate": 1833.51564484773,
          "standard_error": 376.0074250304321
        }
      }
    },
    "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752621.5122179299,
            "upper_bound": 753748.5953676709
          },
          "point_estimate": 753212.472159054,
          "standard_error": 289.35858509112086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752422.5357142857,
            "upper_bound": 753928.7201166181
          },
          "point_estimate": 753419.4787414966,
          "standard_error": 347.15149661711285
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.55263976235355,
            "upper_bound": 1700.570325761221
          },
          "point_estimate": 829.4584934374504,
          "standard_error": 367.6979028989769
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752845.055285272,
            "upper_bound": 753969.6583850932
          },
          "point_estimate": 753529.4599522926,
          "standard_error": 285.612633290318
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 462.89448058456446,
            "upper_bound": 1222.3067354347602
          },
          "point_estimate": 964.4872751701246,
          "standard_error": 192.55985638333195
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265956.18135132076,
            "upper_bound": 266466.3980152937
          },
          "point_estimate": 266191.700086896,
          "standard_error": 131.24237965075628
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265920.7322384428,
            "upper_bound": 266461.2417883212
          },
          "point_estimate": 266041.9838807786,
          "standard_error": 142.3641630464396
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 58.78490859142306,
            "upper_bound": 703.5135276561391
          },
          "point_estimate": 199.74201889912715,
          "standard_error": 173.04952766065227
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265882.72157840396,
            "upper_bound": 266196.78872894833
          },
          "point_estimate": 266019.1540240781,
          "standard_error": 79.6173080375309
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 162.3513078414086,
            "upper_bound": 573.1132712376054
          },
          "point_estimate": 439.7519094342869,
          "standard_error": 103.26468392922202
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477654.08718939393,
            "upper_bound": 478589.7703715729
          },
          "point_estimate": 478080.0125175221,
          "standard_error": 240.3160885421939
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477598.6576994434,
            "upper_bound": 478471.4750180375
          },
          "point_estimate": 477930.8876623376,
          "standard_error": 185.20873933154664
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 86.69433815786407,
            "upper_bound": 1256.3184201200454
          },
          "point_estimate": 384.9111931664254,
          "standard_error": 297.6776407628307
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477771.885679086,
            "upper_bound": 478509.4928618875
          },
          "point_estimate": 478091.6101534829,
          "standard_error": 189.34956122656968
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282.5739905738406,
            "upper_bound": 1097.2971412563977
          },
          "point_estimate": 798.5203048093366,
          "standard_error": 221.7427881033989
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240920.274571468,
            "upper_bound": 241453.5953416312
          },
          "point_estimate": 241161.66633317564,
          "standard_error": 137.54360944342784
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240803.9375591296,
            "upper_bound": 241377.4129139073
          },
          "point_estimate": 241073.1247240618,
          "standard_error": 156.65172600992054
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.72046130589936,
            "upper_bound": 689.4891491498306
          },
          "point_estimate": 422.72964878643614,
          "standard_error": 161.22848498408663
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240824.0261729862,
            "upper_bound": 241218.97869378136
          },
          "point_estimate": 240990.97103294055,
          "standard_error": 105.1928932063276
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 179.95910193869736,
            "upper_bound": 621.0672211769511
          },
          "point_estimate": 457.2580310025611,
          "standard_error": 120.44446376805452
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696211.9458490566,
            "upper_bound": 697007.8986792453
          },
          "point_estimate": 696611.9035744235,
          "standard_error": 204.08783731758535
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 695931.3698113208,
            "upper_bound": 697203.0591194968
          },
          "point_estimate": 696670.5230607967,
          "standard_error": 332.43682287733753
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 193.6723142975928,
            "upper_bound": 1130.758796780301
          },
          "point_estimate": 920.266587812954,
          "standard_error": 256.50278064028936
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696140.0173664981,
            "upper_bound": 697157.6368759287
          },
          "point_estimate": 696704.7429061504,
          "standard_error": 263.92529061935545
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427.4844347244593,
            "upper_bound": 808.5986097996491
          },
          "point_estimate": 680.5944807710781,
          "standard_error": 96.91187834674442
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2286.577929136056,
            "upper_bound": 2290.963558749742
          },
          "point_estimate": 2288.6023647441607,
          "standard_error": 1.1278561357171175
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2285.617252050943,
            "upper_bound": 2290.795010595665
          },
          "point_estimate": 2287.8166009495635,
          "standard_error": 1.6379700186976969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4001112835743636,
            "upper_bound": 6.565788995916051
          },
          "point_estimate": 3.5447088629201247,
          "standard_error": 1.5104604542746285
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2285.8273896460055,
            "upper_bound": 2289.2589886172877
          },
          "point_estimate": 2287.3176860277003,
          "standard_error": 0.873787042603835
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.900457448507656,
            "upper_bound": 5.066515234507871
          },
          "point_estimate": 3.767865611658778,
          "standard_error": 0.901721538673882
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.404262841044,
            "upper_bound": 704.4807838839654
          },
          "point_estimate": 703.9488491664092,
          "standard_error": 0.2749265306167835
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.3841369882899,
            "upper_bound": 704.5617052162974
          },
          "point_estimate": 703.9206561194754,
          "standard_error": 0.2716445614000822
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09625581259158214,
            "upper_bound": 1.6396468570480027
          },
          "point_estimate": 0.757316170141563,
          "standard_error": 0.4137166352614444
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 703.7570410195433,
            "upper_bound": 704.3915874369285
          },
          "point_estimate": 704.09385192568,
          "standard_error": 0.16580801284411595
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.4728875057318668,
            "upper_bound": 1.202663373885255
          },
          "point_estimate": 0.9195405482748372,
          "standard_error": 0.18942123419583157
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.78901999173354,
            "upper_bound": 140.98441341806455
          },
          "point_estimate": 140.8836074701091,
          "standard_error": 0.05036904608572196
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.7097722667117,
            "upper_bound": 141.0077770173583
          },
          "point_estimate": 140.8681426149443,
          "standard_error": 0.07434771699964682
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03303058035678633,
            "upper_bound": 0.2809435515589485
          },
          "point_estimate": 0.2209109177323808,
          "standard_error": 0.0637930858451591
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 140.79059649000115,
            "upper_bound": 140.99243118666368
          },
          "point_estimate": 140.9142757657635,
          "standard_error": 0.05081928239431502
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09556271348088664,
            "upper_bound": 0.20997250125135164
          },
          "point_estimate": 0.16749377765925522,
          "standard_error": 0.029792068302487915
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.92111716226133,
            "upper_bound": 54.08581461690835
          },
          "point_estimate": 54.00519674393969,
          "standard_error": 0.04231715076317487
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.89755916885467,
            "upper_bound": 54.144236571581146
          },
          "point_estimate": 54.00580198428592,
          "standard_error": 0.05434533039735156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03355728674125212,
            "upper_bound": 0.2446440332273413
          },
          "point_estimate": 0.15712099033254265,
          "standard_error": 0.05983896604237529
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.96712462639247,
            "upper_bound": 54.113636199749735
          },
          "point_estimate": 54.03939845892763,
          "standard_error": 0.038538915381394485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.07905445411019592,
            "upper_bound": 0.17751505485135877
          },
          "point_estimate": 0.14120950753009356,
          "standard_error": 0.02561847153053299
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.76039426991441,
            "upper_bound": 80.89884556016919
          },
          "point_estimate": 80.83163260103501,
          "standard_error": 0.03541556330698683
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.72866661832138,
            "upper_bound": 80.93668760389589
          },
          "point_estimate": 80.85270752215516,
          "standard_error": 0.05777443260585289
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021838244008891436,
            "upper_bound": 0.19029144941248305
          },
          "point_estimate": 0.13388252852133578,
          "standard_error": 0.04552995141536298
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 80.7326554680359,
            "upper_bound": 80.90980582222126
          },
          "point_estimate": 80.81148066842844,
          "standard_error": 0.04602167798535129
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06896754971043018,
            "upper_bound": 0.1424791307387027
          },
          "point_estimate": 0.11822235991769275,
          "standard_error": 0.018548491123645428
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287286.10832083493,
            "upper_bound": 287813.38477648114
          },
          "point_estimate": 287542.8935833021,
          "standard_error": 134.21325326173593
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287304.38320209977,
            "upper_bound": 287942.894488189
          },
          "point_estimate": 287453.67049431324,
          "standard_error": 137.66678296767904
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 67.82457103998395,
            "upper_bound": 772.1635823543406
          },
          "point_estimate": 281.6557301701992,
          "standard_error": 192.30801179573965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287369.33548322663,
            "upper_bound": 287685.0959757234
          },
          "point_estimate": 287517.5735555783,
          "standard_error": 79.86634177118758
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.3223497950564,
            "upper_bound": 588.9917871112973
          },
          "point_estimate": 447.511070817186,
          "standard_error": 97.22025799041388
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.96751526802775,
            "upper_bound": 63.0877451817284
          },
          "point_estimate": 63.02265058032051,
          "standard_error": 0.0310015090879666
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.96477465487095,
            "upper_bound": 63.098065425705755
          },
          "point_estimate": 62.99504707557318,
          "standard_error": 0.02697573665123807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010328623762974574,
            "upper_bound": 0.16596607442157374
          },
          "point_estimate": 0.04260823571458138,
          "standard_error": 0.03692676726046145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.97788953591265,
            "upper_bound": 63.039115124928074
          },
          "point_estimate": 62.999214798441294,
          "standard_error": 0.015772602979632556
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02449771136374414,
            "upper_bound": 0.13444461445042172
          },
          "point_estimate": 0.10351745953168504,
          "standard_error": 0.026500642829809285
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.67459323388346,
            "upper_bound": 105.8376773727469
          },
          "point_estimate": 105.7496231930712,
          "standard_error": 0.042025140778469805
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.63978130231172,
            "upper_bound": 105.82892854117098
          },
          "point_estimate": 105.71150989339728,
          "standard_error": 0.050237896675905995
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016648197284121636,
            "upper_bound": 0.2235291161624064
          },
          "point_estimate": 0.11752160896610048,
          "standard_error": 0.0533257359367037
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.65897590876573,
            "upper_bound": 105.7763612812572
          },
          "point_estimate": 105.7003213299165,
          "standard_error": 0.0299121207507372
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.048935567690535614,
            "upper_bound": 0.1817367058530459
          },
          "point_estimate": 0.1402809623175423,
          "standard_error": 0.032986901448406006
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126583.69584978708,
            "upper_bound": 126842.24917777989
          },
          "point_estimate": 126710.03173068412,
          "standard_error": 65.92797934834503
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126541.14518002325,
            "upper_bound": 126866.95709930314
          },
          "point_estimate": 126672.98120955698,
          "standard_error": 95.14526588597722
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.491614024877265,
            "upper_bound": 368.9878800345177
          },
          "point_estimate": 228.40706748967656,
          "standard_error": 83.66234786717439
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126577.28047332708,
            "upper_bound": 126800.82811975738
          },
          "point_estimate": 126673.01777455994,
          "standard_error": 57.14635974985628
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 121.67168608348676,
            "upper_bound": 282.28697033047274
          },
          "point_estimate": 218.63727638186128,
          "standard_error": 42.08282521157377
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.04408808656806,
            "upper_bound": 61.09550707911252
          },
          "point_estimate": 61.070098733201895,
          "standard_error": 0.013151513362449024
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.03743171646887,
            "upper_bound": 61.097956723273235
          },
          "point_estimate": 61.06954625173787,
          "standard_error": 0.012286517489120696
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005542330056111691,
            "upper_bound": 0.07664376096810517
          },
          "point_estimate": 0.028652613540209423,
          "standard_error": 0.020590675263911434
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.03662442045264,
            "upper_bound": 61.098858935772526
          },
          "point_estimate": 61.06770025573832,
          "standard_error": 0.016764159144772348
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02023579231904461,
            "upper_bound": 0.05770804622041096
          },
          "point_estimate": 0.0439402851187512,
          "standard_error": 0.009419546145991364
        }
      }
    },
    "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.94989792376148,
            "upper_bound": 103.1032339382516
          },
          "point_estimate": 103.02793941102912,
          "standard_error": 0.039361057415121505
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.8854561982721,
            "upper_bound": 103.13732180641357
          },
          "point_estimate": 103.05656827374165,
          "standard_error": 0.06885463680611233
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03140859010342403,
            "upper_bound": 0.21279912461220463
          },
          "point_estimate": 0.13507426624952648,
          "standard_error": 0.05029314302552892
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.9173292036656,
            "upper_bound": 103.14853032776492
          },
          "point_estimate": 103.05625810799653,
          "standard_error": 0.05960268594723591
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08156330367573533,
            "upper_bound": 0.1541262919802129
          },
          "point_estimate": 0.1307914142677391,
          "standard_error": 0.018469546110156256
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-freq_rare-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.92844185439063,
            "upper_bound": 255.35320750732393
          },
          "point_estimate": 255.12836112653983,
          "standard_error": 0.10879561803003068
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.88792584482744,
            "upper_bound": 255.29741886367708
          },
          "point_estimate": 255.07798142065764,
          "standard_error": 0.10244653842029862
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04722261825934745,
            "upper_bound": 0.567184501362976
          },
          "point_estimate": 0.2629985776851344,
          "standard_error": 0.13173148245121932
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 254.85409847503936,
            "upper_bound": 255.19456135998203
          },
          "point_estimate": 255.00744111875997,
          "standard_error": 0.08641328655045598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15591709397509376,
            "upper_bound": 0.49806395290286026
          },
          "point_estimate": 0.36250327845961583,
          "standard_error": 0.09286871406469804
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector-repeated_r"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 521.6277798353741,
            "upper_bound": 522.71168104442
          },
          "point_estimate": 522.1102238168879,
          "standard_error": 0.28001992793761743
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 521.4381081308454,
            "upper_bound": 522.4231135432925
          },
          "point_estimate": 522.043414437965,
          "standard_error": 0.30094591807580934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05837676173256071,
            "upper_bound": 1.3336281911835517
          },
          "point_estimate": 0.6216998552405293,
          "standard_error": 0.30434521441005064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 521.6538919052845,
            "upper_bound": 522.2947112985235
          },
          "point_estimate": 522.0662133591898,
          "standard_error": 0.1661691018732248
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.38553208878456147,
            "upper_bound": 1.3218018437696148
          },
          "point_estimate": 0.9306441690064036,
          "standard_error": 0.2820246611748325
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-defeat-simple-vector_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.47188709143754,
            "upper_bound": 32.52338197046618
          },
          "point_estimate": 32.49345506157006,
          "standard_error": 0.01351981607267965
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.468290343770526,
            "upper_bound": 32.50095746660828
          },
          "point_estimate": 32.483219337483185,
          "standard_error": 0.009059680355484963
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0054645512058064775,
            "upper_bound": 0.04417054448624116
          },
          "point_estimate": 0.022337358837067132,
          "standard_error": 0.010159197047608795
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.46439081726594,
            "upper_bound": 32.49420314558491
          },
          "point_estimate": 32.477431391205684,
          "standard_error": 0.007638961839000523
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01218741382493153,
            "upper_bound": 0.06727033630063707
          },
          "point_estimate": 0.045232543058727474,
          "standard_error": 0.017285817849262083
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32751.227619077166,
            "upper_bound": 32802.50658865448
          },
          "point_estimate": 32777.22502331527,
          "standard_error": 13.125021043521796
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32746.48181783995,
            "upper_bound": 32811.12763237064
          },
          "point_estimate": 32777.56917870036,
          "standard_error": 15.986807646838242
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.52717441765683,
            "upper_bound": 76.78654665120892
          },
          "point_estimate": 46.138062921858555,
          "standard_error": 17.640676664775345
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32745.47525608229,
            "upper_bound": 32784.39442390537
          },
          "point_estimate": 32766.17212011815,
          "standard_error": 10.20871400989635
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.73303914803903,
            "upper_bound": 55.26096064273179
          },
          "point_estimate": 43.85656251626536,
          "standard_error": 7.890390095725134
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.10311433715316,
            "upper_bound": 142.28727907492484
          },
          "point_estimate": 142.194711062972,
          "standard_error": 0.04721011095037402
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.06891385279857,
            "upper_bound": 142.32544613497862
          },
          "point_estimate": 142.19197788359412,
          "standard_error": 0.0765072575276604
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03678383052498144,
            "upper_bound": 0.2601244472657008
          },
          "point_estimate": 0.16015399077895043,
          "standard_error": 0.05952477024807799
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.12530012332326,
            "upper_bound": 142.36060959416074
          },
          "point_estimate": 142.24661703412747,
          "standard_error": 0.06279179357863096
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09503744848037889,
            "upper_bound": 0.1963190091139928
          },
          "point_estimate": 0.15752571159651138,
          "standard_error": 0.02605286358547311
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978342.8304532164,
            "upper_bound": 979708.705978984
          },
          "point_estimate": 979042.0285453215,
          "standard_error": 350.90431370691596
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 977937.7796052631,
            "upper_bound": 980030.9195906434
          },
          "point_estimate": 979447.8442982456,
          "standard_error": 647.7782487008828
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 282.92884471385196,
            "upper_bound": 1873.8457517326683
          },
          "point_estimate": 1280.2255107802634,
          "standard_error": 441.4315565720376
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978519.0105041936,
            "upper_bound": 980123.5237282088
          },
          "point_estimate": 979514.2689678742,
          "standard_error": 415.32932229217977
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 721.8905208551199,
            "upper_bound": 1364.984681702552
          },
          "point_estimate": 1169.007605099122,
          "standard_error": 163.4074507123191
        }
      }
    },
    "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_oneshot_pathological-repeated-rare-small_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.8083862940027,
            "upper_bound": 1987.69072904112
          },
          "point_estimate": 1986.0865856984933,
          "standard_error": 0.741407821188921
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.298265274513,
            "upper_bound": 1986.8228709220891
          },
          "point_estimate": 1985.8270909637567,
          "standard_error": 0.68185403227491
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3922117519974469,
            "upper_bound": 3.2510478335523123
          },
          "point_estimate": 1.75214311827627,
          "standard_error": 0.6969275791674567
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1984.911574981218,
            "upper_bound": 1986.4933040885649
          },
          "point_estimate": 1985.7984743294753,
          "standard_error": 0.4026159361263637
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9262840504007552,
            "upper_bound": 3.5539185679773313
          },
          "point_estimate": 2.4835565515032436,
          "standard_error": 0.784263495197917
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.74022696248488,
            "upper_bound": 45.84502171067857
          },
          "point_estimate": 45.791599604708715,
          "standard_error": 0.026858156877078
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.70385239339555,
            "upper_bound": 45.86772095599554
          },
          "point_estimate": 45.78396925054297,
          "standard_error": 0.05315321991427008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01104973367121846,
            "upper_bound": 0.14618964057493486
          },
          "point_estimate": 0.11427332351543196,
          "standard_error": 0.03932038300894546
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.72895586896737,
            "upper_bound": 45.83224459643176
          },
          "point_estimate": 45.7840630475667,
          "standard_error": 0.02670541268439947
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05803899238504387,
            "upper_bound": 0.10587836130672136
          },
          "point_estimate": 0.08940122965081677,
          "standard_error": 0.01233536049901441
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.731224261478545,
            "upper_bound": 49.78909102406094
          },
          "point_estimate": 49.76162461544534,
          "standard_error": 0.014848987452454566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.72071611573671,
            "upper_bound": 49.805305839249414
          },
          "point_estimate": 49.76862529711395,
          "standard_error": 0.019979775176063405
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010782082776311212,
            "upper_bound": 0.07854331093657849
          },
          "point_estimate": 0.05657069412051056,
          "standard_error": 0.018912314160977613
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.718657715019674,
            "upper_bound": 49.79370819560555
          },
          "point_estimate": 49.75784980429121,
          "standard_error": 0.019625839959327592
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02531451935320418,
            "upper_bound": 0.0634713954054766
          },
          "point_estimate": 0.04965589473923856,
          "standard_error": 0.009804972582603507
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.19213938636227,
            "upper_bound": 36.253787126014075
          },
          "point_estimate": 36.22365455522404,
          "standard_error": 0.015749955212390938
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.186268350718855,
            "upper_bound": 36.25368302900323
          },
          "point_estimate": 36.24033337119364,
          "standard_error": 0.019314064663441988
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00631620916158519,
            "upper_bound": 0.09335530621376996
          },
          "point_estimate": 0.03321138276359032,
          "standard_error": 0.024289216212531713
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 36.20362463207335,
            "upper_bound": 36.257791756299
          },
          "point_estimate": 36.2294622042286,
          "standard_error": 0.013711954684310714
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02606866044310566,
            "upper_bound": 0.06881358479929563
          },
          "point_estimate": 0.05247862654769185,
          "standard_error": 0.01103942256800336
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.80793911667733,
            "upper_bound": 40.868275951682634
          },
          "point_estimate": 40.83844252497263,
          "standard_error": 0.015469476113760268
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.79846963514473,
            "upper_bound": 40.87949797102623
          },
          "point_estimate": 40.83308858520846,
          "standard_error": 0.017699568459747943
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0049571616212564515,
            "upper_bound": 0.10182636009093478
          },
          "point_estimate": 0.04618269347950624,
          "standard_error": 0.027508773286199835
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.82077942378988,
            "upper_bound": 40.879674892157446
          },
          "point_estimate": 40.84723177770958,
          "standard_error": 0.0150475811955952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02854809498584504,
            "upper_bound": 0.06507196007260631
          },
          "point_estimate": 0.051746417958060835,
          "standard_error": 0.009475663415359414
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.326177376657135,
            "upper_bound": 48.384125495579354
          },
          "point_estimate": 48.35552144957136,
          "standard_error": 0.014790565346819651
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.31556464787813,
            "upper_bound": 48.39786147345038
          },
          "point_estimate": 48.3614138468621,
          "standard_error": 0.023046035264869393
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00949666134535592,
            "upper_bound": 0.08269339084623929
          },
          "point_estimate": 0.05785101646352944,
          "standard_error": 0.018242186098433637
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 48.33065801595636,
            "upper_bound": 48.380720265291146
          },
          "point_estimate": 48.352634863887886,
          "standard_error": 0.01305473859322802
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02975797792698121,
            "upper_bound": 0.060767822060617885
          },
          "point_estimate": 0.04931742967710311,
          "standard_error": 0.007999974464427802
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.44525185687156,
            "upper_bound": 74.57132973836957
          },
          "point_estimate": 74.504349070961,
          "standard_error": 0.03235171434822189
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.39934685083647,
            "upper_bound": 74.56748075156439
          },
          "point_estimate": 74.49398079952732,
          "standard_error": 0.03765569367257949
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015869708921662656,
            "upper_bound": 0.17695443089880294
          },
          "point_estimate": 0.12463765839684889,
          "standard_error": 0.04485567177196811
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.45012226342874,
            "upper_bound": 74.58931291560906
          },
          "point_estimate": 74.52953602647275,
          "standard_error": 0.03562370761763417
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.053613070803099745,
            "upper_bound": 0.14239281724154912
          },
          "point_estimate": 0.1077722775842925,
          "standard_error": 0.024209569804691713
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.80637087099662,
            "upper_bound": 88.90106318588431
          },
          "point_estimate": 88.85186465066768,
          "standard_error": 0.024299462720818663
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.78253295334838,
            "upper_bound": 88.9031315355743
          },
          "point_estimate": 88.84485623642402,
          "standard_error": 0.036417723740193496
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01801707260644562,
            "upper_bound": 0.1461115013203243
          },
          "point_estimate": 0.08768184858472944,
          "standard_error": 0.031934983777367976
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.79242473464947,
            "upper_bound": 88.8855822809028
          },
          "point_estimate": 88.83951136156915,
          "standard_error": 0.02374635417124192
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045499407417164094,
            "upper_bound": 0.10540205539078178
          },
          "point_estimate": 0.08095602464254116,
          "standard_error": 0.01609973776933597
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.015806069250765,
            "upper_bound": 63.15056467613927
          },
          "point_estimate": 63.077866449218995,
          "standard_error": 0.03473179973882822
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.987585023623325,
            "upper_bound": 63.17520597681888
          },
          "point_estimate": 63.04753104198669,
          "standard_error": 0.04477435196420051
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012427617168700392,
            "upper_bound": 0.19525292743532863
          },
          "point_estimate": 0.0904184454517314,
          "standard_error": 0.044370616512274086
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.99187435090323,
            "upper_bound": 63.079990735234205
          },
          "point_estimate": 63.02988605306159,
          "standard_error": 0.022633595891404092
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04291074154604276,
            "upper_bound": 0.15267495497683636
          },
          "point_estimate": 0.11580789834495613,
          "standard_error": 0.02755924746721564
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.67215819624552,
            "upper_bound": 105.86203240379484
          },
          "point_estimate": 105.77502508486994,
          "standard_error": 0.048963922712558575
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.67675259873008,
            "upper_bound": 105.89395189865296
          },
          "point_estimate": 105.81251514852522,
          "standard_error": 0.05097013024230924
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03905884898184412,
            "upper_bound": 0.25108853389592295
          },
          "point_estimate": 0.13202624517356223,
          "standard_error": 0.056980361347667506
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 105.75630810314944,
            "upper_bound": 105.88548892796722
          },
          "point_estimate": 105.8304995379947,
          "standard_error": 0.033278942257825794
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06448981947777424,
            "upper_bound": 0.2160348166950895
          },
          "point_estimate": 0.16318799872517462,
          "standard_error": 0.0399309931810768
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.33491439308378,
            "upper_bound": 64.4481946065116
          },
          "point_estimate": 64.38980063930714,
          "standard_error": 0.02896861332579916
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.3132351898465,
            "upper_bound": 64.45527745279935
          },
          "point_estimate": 64.37538893271173,
          "standard_error": 0.03654914102812701
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.022047708887300455,
            "upper_bound": 0.1623800388325779
          },
          "point_estimate": 0.08914968725338455,
          "standard_error": 0.03505228800176366
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.35654521388508,
            "upper_bound": 64.46972454509803
          },
          "point_estimate": 64.41812966202089,
          "standard_error": 0.027978580744636137
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.051324379812483584,
            "upper_bound": 0.12461606807609972
          },
          "point_estimate": 0.09667827910675342,
          "standard_error": 0.01907420499806373
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.09177381228708,
            "upper_bound": 61.17145847219681
          },
          "point_estimate": 61.13138979139319,
          "standard_error": 0.020414961249534833
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.069906935590296,
            "upper_bound": 61.20280874581431
          },
          "point_estimate": 61.11723781877037,
          "standard_error": 0.03355819064951303
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008445975087166543,
            "upper_bound": 0.1124501234046696
          },
          "point_estimate": 0.09081643289001756,
          "standard_error": 0.028148440420506075
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.08131971385784,
            "upper_bound": 61.184430974372255
          },
          "point_estimate": 61.13482721264907,
          "standard_error": 0.02636006021076182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04306061260411746,
            "upper_bound": 0.08053674233485568
          },
          "point_estimate": 0.06815416595399956,
          "standard_error": 0.009578786932101988
        }
      }
    },
    "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.8362873138881,
            "upper_bound": 103.04896678288604
          },
          "point_estimate": 102.94383878116548,
          "standard_error": 0.05417058675567915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.80437020620926,
            "upper_bound": 103.05207758913964
          },
          "point_estimate": 102.96773385199477,
          "standard_error": 0.0743773688263404
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014317270019847924,
            "upper_bound": 0.2968468820114534
          },
          "point_estimate": 0.14316727778690996,
          "standard_error": 0.08074946685015512
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 102.87176505140334,
            "upper_bound": 103.10890246674094
          },
          "point_estimate": 102.98861487425344,
          "standard_error": 0.06078427786547972
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09144172038100512,
            "upper_bound": 0.23533052410318944
          },
          "point_estimate": 0.18091480079700276,
          "standard_error": 0.03591243773073452
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1183817.1147401433,
            "upper_bound": 1185414.77281234
          },
          "point_estimate": 1184636.1127432154,
          "standard_error": 409.58556024274026
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1183332.3393817204,
            "upper_bound": 1185708.6129032257
          },
          "point_estimate": 1184877.0348182283,
          "standard_error": 615.0874802642256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 225.3551959991455,
            "upper_bound": 2274.106428443587
          },
          "point_estimate": 1344.1831249263685,
          "standard_error": 535.4697910854368
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1184054.4959374436,
            "upper_bound": 1185881.9061046818
          },
          "point_estimate": 1185032.4458315878,
          "standard_error": 485.3878190458585
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752.7372717618554,
            "upper_bound": 1671.5852333250202
          },
          "point_estimate": 1357.7118549929016,
          "standard_error": 230.0084785147699
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524739.9988425928,
            "upper_bound": 1526696.154074074
          },
          "point_estimate": 1525717.273068783,
          "standard_error": 500.18224244823426
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524494.787037037,
            "upper_bound": 1527162.3958333333
          },
          "point_estimate": 1525630.837797619,
          "standard_error": 673.9624969066547
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 341.14899949903963,
            "upper_bound": 2921.2041363324993
          },
          "point_estimate": 1709.4323765958145,
          "standard_error": 638.2184698301189
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1524629.8538765088,
            "upper_bound": 1525845.0751785284
          },
          "point_estimate": 1525138.8373376622,
          "standard_error": 307.5739421267325
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 923.7602701911768,
            "upper_bound": 2150.214853653671
          },
          "point_estimate": 1670.956870257736,
          "standard_error": 316.1544538677039
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1463769.5246751588,
            "upper_bound": 1467056.287502381
          },
          "point_estimate": 1465349.6830015874,
          "standard_error": 840.0223087537879
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1463406.838,
            "upper_bound": 1466903.9222222222
          },
          "point_estimate": 1465379.778,
          "standard_error": 768.6179230535283
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 331.88569917419204,
            "upper_bound": 4673.217715700603
          },
          "point_estimate": 1694.152163922704,
          "standard_error": 1198.5126381822206
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1462874.4898345154,
            "upper_bound": 1465652.7949076055
          },
          "point_estimate": 1464415.523844156,
          "standard_error": 713.8965451138359
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1289.460143736599,
            "upper_bound": 3752.4079086054903
          },
          "point_estimate": 2799.1257786487085,
          "standard_error": 650.9369095687831
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673343.3470899123,
            "upper_bound": 674424.5595373503
          },
          "point_estimate": 673917.3478453077,
          "standard_error": 277.1355325017023
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673294.62251462,
            "upper_bound": 674687.2550438596
          },
          "point_estimate": 674094.6359649124,
          "standard_error": 384.3766699586498
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 135.46620930544088,
            "upper_bound": 1493.410056600645
          },
          "point_estimate": 885.1419106013789,
          "standard_error": 369.16820567660216
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 673789.9294165646,
            "upper_bound": 674690.6484585629
          },
          "point_estimate": 674399.0992481203,
          "standard_error": 237.08604010069368
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 427.2190196909631,
            "upper_bound": 1170.3976033735617
          },
          "point_estimate": 919.8820506662144,
          "standard_error": 192.26978441030352
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446742.0911538464,
            "upper_bound": 1449438.0361177886
          },
          "point_estimate": 1448115.1033653847,
          "standard_error": 690.8279118427939
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446532.0423076923,
            "upper_bound": 1450276.7307692308
          },
          "point_estimate": 1447892.5649038462,
          "standard_error": 1138.22993942658
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.98578928988113,
            "upper_bound": 4119.519498594527
          },
          "point_estimate": 2517.3549649233073,
          "standard_error": 982.2035402595905
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446601.8667648248,
            "upper_bound": 1449215.6053709604
          },
          "point_estimate": 1447792.1362637363,
          "standard_error": 679.7939520642894
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1390.323507837096,
            "upper_bound": 2910.83237023146
          },
          "point_estimate": 2302.0127181533085,
          "standard_error": 403.1556458320248
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461734.3274506103,
            "upper_bound": 462180.9860699216
          },
          "point_estimate": 461964.9104083786,
          "standard_error": 113.98688171133496
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461698.76740506326,
            "upper_bound": 462229.4008438819
          },
          "point_estimate": 462017.4936708861,
          "standard_error": 141.41159352471024
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.41817043551694,
            "upper_bound": 637.4957809607342
          },
          "point_estimate": 323.32221185486577,
          "standard_error": 141.77358271915602
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 461625.2767790661,
            "upper_bound": 462258.0080881812
          },
          "point_estimate": 462003.7324017754,
          "standard_error": 164.86459690053383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191.38897433763793,
            "upper_bound": 496.7660516962578
          },
          "point_estimate": 379.9812013177425,
          "standard_error": 78.50617905730518
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579595.0692857143,
            "upper_bound": 580785.7786237088
          },
          "point_estimate": 580215.5856342907,
          "standard_error": 305.1744777713256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579250.6754850089,
            "upper_bound": 580912.5750661376
          },
          "point_estimate": 580700.4657596372,
          "standard_error": 489.57150569008184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 93.52999583949668,
            "upper_bound": 1661.242157544257
          },
          "point_estimate": 870.8586048566397,
          "standard_error": 456.3820959397676
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 579431.9477317911,
            "upper_bound": 580671.0980694981
          },
          "point_estimate": 579921.6643578644,
          "standard_error": 316.4680422699774
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 563.9881150102459,
            "upper_bound": 1263.5578751197231
          },
          "point_estimate": 1016.87714586322,
          "standard_error": 181.8184967170654
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1023699.472599041,
            "upper_bound": 1025727.4556349206
          },
          "point_estimate": 1024591.9633046736,
          "standard_error": 526.1553111690946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1023321.4814814816,
            "upper_bound": 1025383.1875
          },
          "point_estimate": 1023937.636728395,
          "standard_error": 586.4334604017457
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 171.69364047034676,
            "upper_bound": 2401.355606811848
          },
          "point_estimate": 1310.778190941946,
          "standard_error": 545.544008572054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1023463.9858354096,
            "upper_bound": 1024292.6641217688
          },
          "point_estimate": 1023837.6868686868,
          "standard_error": 208.75708232430088
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 675.3890325443224,
            "upper_bound": 2497.4934890344234
          },
          "point_estimate": 1753.3812511311723,
          "standard_error": 543.799853540215
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 728041.0493345003,
            "upper_bound": 728928.7110161843
          },
          "point_estimate": 728497.3625412388,
          "standard_error": 227.9059621009507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 727723.7823529412,
            "upper_bound": 729073.6960784313
          },
          "point_estimate": 728851.251633987,
          "standard_error": 454.3483164424293
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.77852947977986,
            "upper_bound": 1160.0274834904692
          },
          "point_estimate": 609.6695192415337,
          "standard_error": 341.05235530074987
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 728088.837143591,
            "upper_bound": 729133.9214157076
          },
          "point_estimate": 728651.0540361599,
          "standard_error": 287.0945190853902
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471.4788418273253,
            "upper_bound": 874.4361326953912
          },
          "point_estimate": 759.0530679945164,
          "standard_error": 100.97865469172984
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800615.6111870256,
            "upper_bound": 801536.5014278167
          },
          "point_estimate": 801079.9003355763,
          "standard_error": 236.054085867524
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800195.8731884058,
            "upper_bound": 801644.5652173914
          },
          "point_estimate": 801295.4910929952,
          "standard_error": 378.3329104402465
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.66236474438413,
            "upper_bound": 1400.5815762216105
          },
          "point_estimate": 884.6425408162492,
          "standard_error": 341.43229513355635
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800515.6370635225,
            "upper_bound": 801615.0211544594
          },
          "point_estimate": 801033.2473178995,
          "standard_error": 287.32916456132443
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 484.56200692107967,
            "upper_bound": 957.6679776470016
          },
          "point_estimate": 786.6003226754591,
          "standard_error": 121.2351808755828
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443093.703414073,
            "upper_bound": 443497.4439625167
          },
          "point_estimate": 443296.44358051254,
          "standard_error": 103.75809434230705
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 442947.0310886403,
            "upper_bound": 443626.0642570281
          },
          "point_estimate": 443343.72255689424,
          "standard_error": 184.51501512652047
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 41.05704649999452,
            "upper_bound": 588.0514722407148
          },
          "point_estimate": 435.9765125008315,
          "standard_error": 126.4550172297334
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 443024.3462033803,
            "upper_bound": 443417.4889777732
          },
          "point_estimate": 443174.9970270694,
          "standard_error": 99.68401447405584
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 220.47366310596857,
            "upper_bound": 409.3123160049002
          },
          "point_estimate": 346.75341523771493,
          "standard_error": 48.04063220882908
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329578.98249433475,
            "upper_bound": 330025.0553777151
          },
          "point_estimate": 329801.9134214007,
          "standard_error": 114.30268974884116
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329488.73976608185,
            "upper_bound": 330120.50313283206
          },
          "point_estimate": 329816.6817738791,
          "standard_error": 128.31474079692512
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 51.59859741727128,
            "upper_bound": 746.5883034852415
          },
          "point_estimate": 241.7943360362372,
          "standard_error": 181.32078229885371
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 329626.37876269623,
            "upper_bound": 329922.4566621013
          },
          "point_estimate": 329790.7134882661,
          "standard_error": 75.49096888880906
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.203422039471,
            "upper_bound": 482.7855313468103
          },
          "point_estimate": 380.98866649157105,
          "standard_error": 71.15328574382767
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248805.27843225628,
            "upper_bound": 249198.34908163265
          },
          "point_estimate": 249001.311840514,
          "standard_error": 100.72114171655836
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248712.32076719575,
            "upper_bound": 249353.768707483
          },
          "point_estimate": 248979.5380952381,
          "standard_error": 148.3971894680116
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.87226529876604,
            "upper_bound": 585.4703808756703
          },
          "point_estimate": 430.09014950722406,
          "standard_error": 132.74430482422224
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 248716.33686990145,
            "upper_bound": 249059.6984024379
          },
          "point_estimate": 248866.9263715876,
          "standard_error": 86.93079454207606
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 201.43912355024383,
            "upper_bound": 409.3940585583723
          },
          "point_estimate": 336.0172256713868,
          "standard_error": 53.1458281111322
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174582.02623604468,
            "upper_bound": 174711.3746570973
          },
          "point_estimate": 174646.74064327482,
          "standard_error": 33.04487440569294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174558.672036151,
            "upper_bound": 174736.35645933013
          },
          "point_estimate": 174650.8553429027,
          "standard_error": 46.37242332040366
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.074640083556693,
            "upper_bound": 190.5200081008168
          },
          "point_estimate": 88.70594268353597,
          "standard_error": 44.525687552410815
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174565.8245997927,
            "upper_bound": 174741.74109934887
          },
          "point_estimate": 174641.93753806004,
          "standard_error": 45.47199809760144
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.40519214523982,
            "upper_bound": 137.87997767512377
          },
          "point_estimate": 109.93607852331218,
          "standard_error": 19.09040521549988
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-huge_common-m"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3173538.108982639,
            "upper_bound": 3175925.69610367
          },
          "point_estimate": 3174624.80421627,
          "standard_error": 614.4018410332635
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3172965.55,
            "upper_bound": 3175425.4930555555
          },
          "point_estimate": 3174430.7430555555,
          "standard_error": 614.0975105046791
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 308.37564660853286,
            "upper_bound": 3091.7872545522387
          },
          "point_estimate": 1705.910417214059,
          "standard_error": 690.463406185132
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3173234.9269356993,
            "upper_bound": 3176678.829903049
          },
          "point_estimate": 3174686.640909091,
          "standard_error": 883.2102564987805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 891.0349433281397,
            "upper_bound": 2840.7903860962665
          },
          "point_estimate": 2038.1507164000936,
          "standard_error": 568.2425722333711
        }
      }
    },
    "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_oneshotiter_pathological-repeated-rare-small_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6391.546767056401,
            "upper_bound": 6398.415653479468
          },
          "point_estimate": 6395.036930445383,
          "standard_error": 1.7552744637209996
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6390.965755824138,
            "upper_bound": 6398.703209844106
          },
          "point_estimate": 6395.4963215974785,
          "standard_error": 1.5396248698795147
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.7751160135639346,
            "upper_bound": 10.026756447492096
          },
          "point_estimate": 3.6431000367702993,
          "standard_error": 2.484618313280551
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6390.694254685584,
            "upper_bound": 6398.160545358199
          },
          "point_estimate": 6394.175270306852,
          "standard_error": 1.968633001849065
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2.2815469656320637,
            "upper_bound": 7.852755356612607
          },
          "point_estimate": 5.862761803906533,
          "standard_error": 1.3581818269998
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313459.9349875285,
            "upper_bound": 1317075.841002976
          },
          "point_estimate": 1314967.9220351472,
          "standard_error": 955.181133051519
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313153.062755102,
            "upper_bound": 1315832.3253968256
          },
          "point_estimate": 1314113.0077380952,
          "standard_error": 663.7259377931911
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 262.04651963358964,
            "upper_bound": 3342.359764947246
          },
          "point_estimate": 1207.4453035635115,
          "standard_error": 795.1266846155079
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1313385.356524428,
            "upper_bound": 1314656.27019939
          },
          "point_estimate": 1313898.186456401,
          "standard_error": 327.5123518620683
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753.0588284253917,
            "upper_bound": 4676.012119604916
          },
          "point_estimate": 3178.406378480252,
          "standard_error": 1184.806504865131
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510459.8169846032,
            "upper_bound": 1513505.615822222
          },
          "point_estimate": 1511872.5203698413,
          "standard_error": 780.0307751196062
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510024.464,
            "upper_bound": 1513075.7933333337
          },
          "point_estimate": 1511623.4660714287,
          "standard_error": 665.4939592245616
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 288.77658687327755,
            "upper_bound": 3890.791311624713
          },
          "point_estimate": 1834.234139835833,
          "standard_error": 954.596926809225
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1510287.8256130791,
            "upper_bound": 1512437.3320958083
          },
          "point_estimate": 1511451.7657142857,
          "standard_error": 560.360454664915
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1078.6104670325974,
            "upper_bound": 3607.043263378812
          },
          "point_estimate": 2606.408856917575,
          "standard_error": 698.1326503394796
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416639.4082051285,
            "upper_bound": 1420698.7631227106
          },
          "point_estimate": 1418622.0579395606,
          "standard_error": 1039.9356672322365
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1415486.564102564,
            "upper_bound": 1420544.6217948718
          },
          "point_estimate": 1418961.42239011,
          "standard_error": 1415.1174027010668
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 307.9174820333719,
            "upper_bound": 6697.4244166739245
          },
          "point_estimate": 2627.9950062283965,
          "standard_error": 1506.963810573808
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1416896.510976052,
            "upper_bound": 1419530.192873303
          },
          "point_estimate": 1418354.079120879,
          "standard_error": 666.2618331245343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1942.716679601954,
            "upper_bound": 4498.378019300171
          },
          "point_estimate": 3457.9327709129075,
          "standard_error": 686.073635906979
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/krate_nopre_prebuilt_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753126.2614318108,
            "upper_bound": 753806.4108018707
          },
          "point_estimate": 753465.5003093617,
          "standard_error": 174.60120986776374
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 753038.3968253968,
            "upper_bound": 753863.1697278912
          },
          "point_estimate": 753483.2077259475,
          "standard_error": 249.45247922565164
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 99.69641945454435,
            "upper_bound": 1010.0015649260196
          },
          "point_estimate": 577.5083259377119,
          "standard_error": 226.52817011259103
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 752971.4655085175,
            "upper_bound": 753853.756607561
          },
          "point_estimate": 753424.8305327325,
          "standard_error": 228.3731082918171
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 330.0163808036567,
            "upper_bound": 742.0502983760628
          },
          "point_estimate": 580.4746418661601,
          "standard_error": 106.26852222316712
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265988.90679757553,
            "upper_bound": 266400.011335274
          },
          "point_estimate": 266187.4371787742,
          "standard_error": 105.32712433874896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265902.5912408759,
            "upper_bound": 266520.0405515004
          },
          "point_estimate": 266035.830839416,
          "standard_error": 198.35938851202275
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 61.59944367525667,
            "upper_bound": 522.8518669948882
          },
          "point_estimate": 328.81592551265607,
          "standard_error": 131.32606619798955
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 265934.3187519541,
            "upper_bound": 266330.1117612757
          },
          "point_estimate": 266117.6983979524,
          "standard_error": 102.60370234224634
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.36520011733575,
            "upper_bound": 412.0358959159166
          },
          "point_estimate": 350.970217096485,
          "standard_error": 49.83864353902939
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477758.3367225877,
            "upper_bound": 478347.80318374065
          },
          "point_estimate": 478041.7453999583,
          "standard_error": 150.4709311636928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477648.4578947368,
            "upper_bound": 478385.52138157893
          },
          "point_estimate": 477951.8954678363,
          "standard_error": 209.4823626205601
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 116.72005174359349,
            "upper_bound": 863.9450351005094
          },
          "point_estimate": 463.90368935182914,
          "standard_error": 185.07785587446617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477642.44453837106,
            "upper_bound": 478004.13611264486
          },
          "point_estimate": 477801.0248120301,
          "standard_error": 92.30056065467843
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266.8541929311033,
            "upper_bound": 647.3268344390098
          },
          "point_estimate": 500.0589864875422,
          "standard_error": 99.77310244158262
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240932.7284590692,
            "upper_bound": 241589.95766004417
          },
          "point_estimate": 241240.25869389257,
          "standard_error": 168.83220488595248
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240761.18101545257,
            "upper_bound": 241568.52207505517
          },
          "point_estimate": 241185.03211920528,
          "standard_error": 193.997730546019
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.6419239604956,
            "upper_bound": 885.4510603838405
          },
          "point_estimate": 507.80971896471146,
          "standard_error": 211.04526273436056
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 240957.3237988321,
            "upper_bound": 241454.46807320564
          },
          "point_estimate": 241213.8392706631,
          "standard_error": 124.03011840892326
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 264.0966914789132,
            "upper_bound": 747.7244828141536
          },
          "point_estimate": 562.7702585313235,
          "standard_error": 132.2421770724503
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696377.3570320456,
            "upper_bound": 698222.5203721174
          },
          "point_estimate": 697268.4314293202,
          "standard_error": 471.80249620624056
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696297.5204851752,
            "upper_bound": 698470.7409591195
          },
          "point_estimate": 697057.392557652,
          "standard_error": 475.8583433548555
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312.2257637022461,
            "upper_bound": 2658.072302809738
          },
          "point_estimate": 1084.2747807502458,
          "standard_error": 654.4775090936247
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 696505.6608516063,
            "upper_bound": 697814.9240107235
          },
          "point_estimate": 697058.2034795394,
          "standard_error": 329.9915482297958
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 733.2915095460482,
            "upper_bound": 2041.3905765315685
          },
          "point_estimate": 1572.0352161485805,
          "standard_error": 330.96215586675123
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-huge-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.1196635864783,
            "upper_bound": 357.5039524203114
          },
          "point_estimate": 357.31507393872755,
          "standard_error": 0.09829569114247072
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.05321622185727,
            "upper_bound": 357.6420401459495
          },
          "point_estimate": 357.339461863877,
          "standard_error": 0.15322358747300535
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0800091311437228,
            "upper_bound": 0.5466481546161955
          },
          "point_estimate": 0.3997923076870548,
          "standard_error": 0.1243507604901122
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 357.18386226195355,
            "upper_bound": 357.5598873497738
          },
          "point_estimate": 357.3765708813949,
          "standard_error": 0.09540796874473724
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.19724407732595584,
            "upper_bound": 0.4038527074099164
          },
          "point_estimate": 0.32686137401576115,
          "standard_error": 0.05343974334039838
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-long-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.52133560673633,
            "upper_bound": 169.77366357846176
          },
          "point_estimate": 169.64114711045704,
          "standard_error": 0.06480970497751284
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.4695828254951,
            "upper_bound": 169.8432347610175
          },
          "point_estimate": 169.56677765169383,
          "standard_error": 0.08787100979452078
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021273679966792493,
            "upper_bound": 0.3706253768476728
          },
          "point_estimate": 0.19076766307904955,
          "standard_error": 0.08721397667431645
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 169.54827752001583,
            "upper_bound": 169.81656673968027
          },
          "point_estimate": 169.66528546800046,
          "standard_error": 0.06874926060205709
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.10283150968638496,
            "upper_bound": 0.265971028490859
          },
          "point_estimate": 0.21564203958829092,
          "standard_error": 0.040764723510123786
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-medium-needle",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.716490143519017,
            "upper_bound": 26.02480457783208
          },
          "point_estimate": 25.85189218473229,
          "standard_error": 0.08017198830940063
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.685084879512317,
            "upper_bound": 25.977957135862383
          },
          "point_estimate": 25.722134079958533,
          "standard_error": 0.07421395372804432
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00801988409368789,
            "upper_bound": 0.34462103919975534
          },
          "point_estimate": 0.10577754738502508,
          "standard_error": 0.0894117302755675
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.685711377852822,
            "upper_bound": 25.814165357623903
          },
          "point_estimate": 25.734451853562145,
          "standard_error": 0.03311540142386257
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06137769992265355,
            "upper_bound": 0.36921799801670985
          },
          "point_estimate": 0.2673367985871009,
          "standard_error": 0.08235869187418768
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.881331844847416,
            "upper_bound": 17.91191498529522
          },
          "point_estimate": 17.896585394154606,
          "standard_error": 0.007839343961228297
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.87109417650357,
            "upper_bound": 17.921055210624267
          },
          "point_estimate": 17.896152709468794,
          "standard_error": 0.015216355731483108
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005618520276373922,
            "upper_bound": 0.04316667889988003
          },
          "point_estimate": 0.03343969421396876,
          "standard_error": 0.009915525102922752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.873110401110836,
            "upper_bound": 17.91671117425875
          },
          "point_estimate": 17.89480530419268,
          "standard_error": 0.011465589961313848
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01738467627485994,
            "upper_bound": 0.03032261829181018
          },
          "point_estimate": 0.026207051940371685,
          "standard_error": 0.0033175884580432767
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.20491927935678,
            "upper_bound": 25.233242104854728
          },
          "point_estimate": 25.21950046739899,
          "standard_error": 0.00727839642591677
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.195212390880744,
            "upper_bound": 25.238997109968903
          },
          "point_estimate": 25.224583842805057,
          "standard_error": 0.01032779288019665
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006008518188882712,
            "upper_bound": 0.04117220209415861
          },
          "point_estimate": 0.02412362992966752,
          "standard_error": 0.00937678462195266
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.20835985605359,
            "upper_bound": 25.232486777885732
          },
          "point_estimate": 25.222011819368696,
          "standard_error": 0.006200202448467542
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0133133306260377,
            "upper_bound": 0.029981671584106456
          },
          "point_estimate": 0.024321779729846463,
          "standard_error": 0.004153832911857642
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287198.3203690164,
            "upper_bound": 287600.1314457958
          },
          "point_estimate": 287402.8090216848,
          "standard_error": 102.93088646280977
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287138.0787401575,
            "upper_bound": 287669.5399325084
          },
          "point_estimate": 287450.2944553805,
          "standard_error": 126.31485468026438
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.88018147045794,
            "upper_bound": 590.6760013244324
          },
          "point_estimate": 351.68621531151143,
          "standard_error": 147.1704226839414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287095.5364293052,
            "upper_bound": 287537.7228403222
          },
          "point_estimate": 287342.555537376,
          "standard_error": 116.85081248828736
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 180.48499687138397,
            "upper_bound": 431.72729388859585
          },
          "point_estimate": 343.4460147287332,
          "standard_error": 62.440061839071035
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.03948382653219,
            "upper_bound": 16.13386620356969
          },
          "point_estimate": 16.086496541141003,
          "standard_error": 0.023786986211662246
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.061845132882084,
            "upper_bound": 16.107276501240573
          },
          "point_estimate": 16.090576979928972,
          "standard_error": 0.012900065231687827
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.006074102512538565,
            "upper_bound": 0.12766302258210194
          },
          "point_estimate": 0.028423954572998317,
          "standard_error": 0.026835423138193135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.06023366990241,
            "upper_bound": 16.143953800930344
          },
          "point_estimate": 16.093066124523855,
          "standard_error": 0.02141058249965084
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.018498019663461256,
            "upper_bound": 0.11193895340580756
          },
          "point_estimate": 0.07935317736666454,
          "standard_error": 0.024591049711126616
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.578343451297503,
            "upper_bound": 22.729287335277892
          },
          "point_estimate": 22.64669468991901,
          "standard_error": 0.038905928780298915
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.55362205797936,
            "upper_bound": 22.702350555437487
          },
          "point_estimate": 22.634169366060533,
          "standard_error": 0.03127850794137895
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01290895584215804,
            "upper_bound": 0.1860441511463114
          },
          "point_estimate": 0.07111385106002759,
          "standard_error": 0.04850663123680281
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.59087920197677,
            "upper_bound": 22.69837407471346
          },
          "point_estimate": 22.64103836604507,
          "standard_error": 0.027024661067232763
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04974862518436532,
            "upper_bound": 0.1812804752512754
          },
          "point_estimate": 0.13007445220739397,
          "standard_error": 0.037568056151521464
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126587.8573271943,
            "upper_bound": 126734.690889276
          },
          "point_estimate": 126662.27281179136,
          "standard_error": 37.54686373464661
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126552.4857142857,
            "upper_bound": 126770.4512195122
          },
          "point_estimate": 126676.79887727449,
          "standard_error": 49.872248919010175
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.04628914601218,
            "upper_bound": 222.16140703145612
          },
          "point_estimate": 127.55422310131752,
          "standard_error": 52.48098900904671
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 126557.14847642518,
            "upper_bound": 126723.62644247674
          },
          "point_estimate": 126636.80476944658,
          "standard_error": 42.6375195855068
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 71.61336934452346,
            "upper_bound": 157.09866681171312
          },
          "point_estimate": 125.4204997089335,
          "standard_error": 22.046315506426133
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.221716090798655,
            "upper_bound": 24.247309097132128
          },
          "point_estimate": 24.23421748857332,
          "standard_error": 0.006590749258153357
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.214839285605684,
            "upper_bound": 24.255166444393716
          },
          "point_estimate": 24.229322608162825,
          "standard_error": 0.011573636785996436
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004132571349338989,
            "upper_bound": 0.03693818015024496
          },
          "point_estimate": 0.026724350366157217,
          "standard_error": 0.008282375990371144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.215170803739205,
            "upper_bound": 24.24508002962927
          },
          "point_estimate": 24.228825135794896,
          "standard_error": 0.007807290092071167
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013519572590739971,
            "upper_bound": 0.026207945207154748
          },
          "point_estimate": 0.022030338778258635,
          "standard_error": 0.0032123516521910685
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.191618405534378,
            "upper_bound": 21.430685227591233
          },
          "point_estimate": 21.303908770627476,
          "standard_error": 0.06151642250349342
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.14379437242291,
            "upper_bound": 21.471930611599344
          },
          "point_estimate": 21.244403011401083,
          "standard_error": 0.08383684121322765
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03835498602240315,
            "upper_bound": 0.31269913188532633
          },
          "point_estimate": 0.1646071776182384,
          "standard_error": 0.07776830179813585
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.153272650303855,
            "upper_bound": 21.273669744417223
          },
          "point_estimate": 21.21487344507271,
          "standard_error": 0.030937232095732615
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.08530676366341414,
            "upper_bound": 0.2512519919929989
          },
          "point_estimate": 0.20406852709377993,
          "standard_error": 0.0393525611707141
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-freq_rare"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.03822304088612,
            "upper_bound": 42.12522203828647
          },
          "point_estimate": 42.07905215856304,
          "standard_error": 0.02234467764746899
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.03655947221397,
            "upper_bound": 42.12287370536046
          },
          "point_estimate": 42.05914248442853,
          "standard_error": 0.019356026399649617
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0030858158407400514,
            "upper_bound": 0.12778246220035647
          },
          "point_estimate": 0.05445845019301044,
          "standard_error": 0.030247542174293204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 42.01306316112813,
            "upper_bound": 42.12147005879141
          },
          "point_estimate": 42.0528021446313,
          "standard_error": 0.027432294350356864
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02939805021277614,
            "upper_bound": 0.10206952490780304
          },
          "point_estimate": 0.07447004810746816,
          "standard_error": 0.01933670507708748
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector-repeated_"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.814040899716,
            "upper_bound": 75.01956181456215
          },
          "point_estimate": 74.90421989438539,
          "standard_error": 0.05319711558126907
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.81985653056302,
            "upper_bound": 74.95220126953105
          },
          "point_estimate": 74.86862219390963,
          "standard_error": 0.031567597121200934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010742301973345376,
            "upper_bound": 0.2145728445539657
          },
          "point_estimate": 0.07048657992087617,
          "standard_error": 0.04853315765215128
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 74.81983300489611,
            "upper_bound": 74.87708947309146
          },
          "point_estimate": 74.85259019370358,
          "standard_error": 0.014441441573481028
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.045354951624477696,
            "upper_bound": 0.25602080297094854
          },
          "point_estimate": 0.17665393346265623,
          "standard_error": 0.061097748322043295
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-defeat-simple-vector_rare-alph"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.76764457443915,
            "upper_bound": 10.78634224640689
          },
          "point_estimate": 10.777141747139526,
          "standard_error": 0.004777597888087479
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.767608327047077,
            "upper_bound": 10.786291180921172
          },
          "point_estimate": 10.77656142885832,
          "standard_error": 0.00473239531762748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002875669866075775,
            "upper_bound": 0.027784652528631017
          },
          "point_estimate": 0.013313477605958971,
          "standard_error": 0.005600287330096774
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.772196698420943,
            "upper_bound": 10.784411956183654
          },
          "point_estimate": 10.77848137367115,
          "standard_error": 0.0030717923345925896
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00672746947785049,
            "upper_bound": 0.021728935346359033
          },
          "point_estimate": 0.015987561292234886,
          "standard_error": 0.003917271986767745
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32530.640910597045,
            "upper_bound": 32588.16953573294
          },
          "point_estimate": 32561.36958189453,
          "standard_error": 14.816207322304948
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32533.5496866607,
            "upper_bound": 32602.274203862387
          },
          "point_estimate": 32564.397717099375,
          "standard_error": 21.788073545472574
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.777179403412184,
            "upper_bound": 88.97657165222455
          },
          "point_estimate": 47.8884745191045,
          "standard_error": 19.816075185645627
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32547.83236741145,
            "upper_bound": 32592.49580221568
          },
          "point_estimate": 32567.835854387333,
          "standard_error": 11.201672000884338
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 25.770709158537823,
            "upper_bound": 66.49080688195444
          },
          "point_estimate": 49.34538344601099,
          "standard_error": 11.715699785620089
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.292614453952673,
            "upper_bound": 30.26864653346444
          },
          "point_estimate": 29.696897812179543,
          "standard_error": 0.2565471310313723
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.34245115853878,
            "upper_bound": 29.74945811777567
          },
          "point_estimate": 29.51658725483284,
          "standard_error": 0.133611683933357
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04597035318304736,
            "upper_bound": 0.7615331807785548
          },
          "point_estimate": 0.3100025451484114,
          "standard_error": 0.17149424080993475
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29.382393685156572,
            "upper_bound": 29.657050431788527
          },
          "point_estimate": 29.546619034860814,
          "standard_error": 0.0692059809023678
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.1577583642887977,
            "upper_bound": 1.2786800435675951
          },
          "point_estimate": 0.8555929015143456,
          "standard_error": 0.34954055433922876
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-huge_never-trick"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978440.9014442356,
            "upper_bound": 980229.3676499844
          },
          "point_estimate": 979375.0366948622,
          "standard_error": 458.7881234820808
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978295.5888157894,
            "upper_bound": 980795.3596491228
          },
          "point_estimate": 979622.82481203,
          "standard_error": 720.1773133502867
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317.3830375231855,
            "upper_bound": 2736.152882344565
          },
          "point_estimate": 1744.2845588354328,
          "standard_error": 610.9633882697633
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 978737.7400759342,
            "upper_bound": 980511.6084330144
          },
          "point_estimate": 979850.398427888,
          "standard_error": 449.68821977400927
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 867.08214344279,
            "upper_bound": 1942.1018404430083
          },
          "point_estimate": 1533.0522749025374,
          "standard_error": 286.9452026301951
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/krate_nopre_prebuilt_pathological-repeated-rare-small_never-tric"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.258646119829,
            "upper_bound": 1960.0833678733857
          },
          "point_estimate": 1959.147932651606,
          "standard_error": 0.4682625580082343
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1957.7347434191995,
            "upper_bound": 1960.530766326916
          },
          "point_estimate": 1959.1638605763185,
          "standard_error": 0.7497520661033609
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2353944056611336,
            "upper_bound": 2.6335922773920855
          },
          "point_estimate": 2.045840847031403,
          "standard_error": 0.6305998719692844
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1958.1447829147949,
            "upper_bound": 1960.5054347443393
          },
          "point_estimate": 1959.432322765408,
          "standard_error": 0.5979315645624397
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9300187482460855,
            "upper_bound": 1.9193568439060704
          },
          "point_estimate": 1.560426705369892,
          "standard_error": 0.25426324534050754
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.816701605092208,
            "upper_bound": 7.8278548712190785
          },
          "point_estimate": 7.822481962701886,
          "standard_error": 0.002911734920387525
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.8150113384448785,
            "upper_bound": 7.826318012089602
          },
          "point_estimate": 7.824982780446668,
          "standard_error": 0.002317601587451258
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0002420385357882748,
            "upper_bound": 0.017314165998277134
          },
          "point_estimate": 0.002707948341445968,
          "standard_error": 0.004269786473047036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.821928908054753,
            "upper_bound": 7.826388010794949
          },
          "point_estimate": 7.824788139949084,
          "standard_error": 0.0011487486210019738
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001823559379152238,
            "upper_bound": 0.012990738378944168
          },
          "point_estimate": 0.009702682296719042,
          "standard_error": 0.0024097386268324434
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.0925836424304745,
            "upper_bound": 7.101471818495023
          },
          "point_estimate": 7.096932725085873,
          "standard_error": 0.0022750645723687425
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.08972442827972,
            "upper_bound": 7.1041527845691315
          },
          "point_estimate": 7.095967283242146,
          "standard_error": 0.003943749333357477
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0016326175654113794,
            "upper_bound": 0.012415619243260631
          },
          "point_estimate": 0.009376486667586198,
          "standard_error": 0.002846075634756124
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.0919947138516495,
            "upper_bound": 7.1023110019478395
          },
          "point_estimate": 7.096560253571334,
          "standard_error": 0.002604055211570517
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004692985297737288,
            "upper_bound": 0.00909374233357431
          },
          "point_estimate": 0.007588367048350103,
          "standard_error": 0.0011250480692644362
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.314269684517317,
            "upper_bound": 8.32800708950438
          },
          "point_estimate": 8.320641924908482,
          "standard_error": 0.0035423180831637294
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.31249156823937,
            "upper_bound": 8.328450181490783
          },
          "point_estimate": 8.317127728435878,
          "standard_error": 0.0033953293141551037
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0007726568779350343,
            "upper_bound": 0.020231769031538515
          },
          "point_estimate": 0.008502248265729026,
          "standard_error": 0.005084029085478183
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.313545619218361,
            "upper_bound": 8.32192694625195
          },
          "point_estimate": 8.31794245322327,
          "standard_error": 0.0021160031854878243
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004873935604226226,
            "upper_bound": 0.015477964694827956
          },
          "point_estimate": 0.011808367200052057,
          "standard_error": 0.002750752941282443
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/never-two-space",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.245800488891497,
            "upper_bound": 22.284928187261745
          },
          "point_estimate": 22.26360820001057,
          "standard_error": 0.010028988800870282
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.233928801307993,
            "upper_bound": 22.277347886233187
          },
          "point_estimate": 22.26296338327336,
          "standard_error": 0.011872355839894957
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004662715329563374,
            "upper_bound": 0.05192642155665024
          },
          "point_estimate": 0.031661499906848425,
          "standard_error": 0.011948716721931586
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.249321351457652,
            "upper_bound": 22.272953501806036
          },
          "point_estimate": 22.263982334751724,
          "standard_error": 0.006052647126595575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.015390346677522112,
            "upper_bound": 0.04627596263470033
          },
          "point_estimate": 0.03343754813263867,
          "standard_error": 0.009107408351651513
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.2599542647658,
            "upper_bound": 14.32107996317921
          },
          "point_estimate": 14.287018120493084,
          "standard_error": 0.015779348787030467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.250405814881002,
            "upper_bound": 14.31004348058738
          },
          "point_estimate": 14.273704496606936,
          "standard_error": 0.016359309409443072
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008741812148420493,
            "upper_bound": 0.07110924577308146
          },
          "point_estimate": 0.03772927173166463,
          "standard_error": 0.016141928858287996
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 14.258105564123037,
            "upper_bound": 14.29329282211206
          },
          "point_estimate": 14.276328558060335,
          "standard_error": 0.008950461684073667
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01969171687353627,
            "upper_bound": 0.07371901973583378
          },
          "point_estimate": 0.05260634424105447,
          "standard_error": 0.015615289592240426
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.596210422494543,
            "upper_bound": 18.623320317947258
          },
          "point_estimate": 18.61081712346437,
          "standard_error": 0.006951232132267511
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.60299849968463,
            "upper_bound": 18.627052258236652
          },
          "point_estimate": 18.609600089969433,
          "standard_error": 0.006233431213654464
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.001581106196611919,
            "upper_bound": 0.035286670987498846
          },
          "point_estimate": 0.015748083761884985,
          "standard_error": 0.00829632410301641
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 18.60652805139498,
            "upper_bound": 18.626520330764095
          },
          "point_estimate": 18.616060923966906,
          "standard_error": 0.005059207545972511
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00957554222633374,
            "upper_bound": 0.0325227729629874
          },
          "point_estimate": 0.023273527971313917,
          "standard_error": 0.0064737984183272194
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.763328507817686,
            "upper_bound": 10.802650267118882
          },
          "point_estimate": 10.78064463941693,
          "standard_error": 0.010164551546574084
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.755376387257556,
            "upper_bound": 10.79087497102546
          },
          "point_estimate": 10.780692449972602,
          "standard_error": 0.010511559703732878
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003077962417548813,
            "upper_bound": 0.045444346739667706
          },
          "point_estimate": 0.029156671056855796,
          "standard_error": 0.011025256551406131
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.764441935215718,
            "upper_bound": 10.784986246581964
          },
          "point_estimate": 10.77526466749018,
          "standard_error": 0.00549948473388721
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013658203281306866,
            "upper_bound": 0.04823540283081434
          },
          "point_estimate": 0.03376901809452054,
          "standard_error": 0.010591528535940668
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.101497976148735,
            "upper_bound": 16.151338095276405
          },
          "point_estimate": 16.125572109581736,
          "standard_error": 0.012784939090365516
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.085938100553452,
            "upper_bound": 16.163523705541476
          },
          "point_estimate": 16.123587298558007,
          "standard_error": 0.020305262286274807
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005212735912484165,
            "upper_bound": 0.07566330387544822
          },
          "point_estimate": 0.05321233201433117,
          "standard_error": 0.019988626027189427
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.08951063561642,
            "upper_bound": 16.15222894481853
          },
          "point_estimate": 16.11705497823778,
          "standard_error": 0.015984272219825117
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023915966391376213,
            "upper_bound": 0.05194669578585685
          },
          "point_estimate": 0.042540161824529465,
          "standard_error": 0.007189877110480587
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.618520337567254,
            "upper_bound": 22.739994610798277
          },
          "point_estimate": 22.67752822305588,
          "standard_error": 0.03119706664319414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.58863596273157,
            "upper_bound": 22.78250270336399
          },
          "point_estimate": 22.644863580771055,
          "standard_error": 0.054002906871890334
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02262429861579579,
            "upper_bound": 0.16932451631347897
          },
          "point_estimate": 0.1303740024809514,
          "standard_error": 0.03872658358978793
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.596901989797228,
            "upper_bound": 22.691380028672494
          },
          "point_estimate": 22.63072103929388,
          "standard_error": 0.024004804303133943
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06300761190372293,
            "upper_bound": 0.12222432208221168
          },
          "point_estimate": 0.10372324472639784,
          "standard_error": 0.014927888287275322
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/never-john-watson",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.995574972177902,
            "upper_bound": 11.010619708086708
          },
          "point_estimate": 11.003113033479265,
          "standard_error": 0.0038554605345774017
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.99446503696566,
            "upper_bound": 11.013442197194536
          },
          "point_estimate": 11.00150362645597,
          "standard_error": 0.004810671734568576
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003130535536682129,
            "upper_bound": 0.02364119518798187
          },
          "point_estimate": 0.010854558475500284,
          "standard_error": 0.005170160079489372
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 10.998603606283291,
            "upper_bound": 11.012736577902048
          },
          "point_estimate": 11.005680488560031,
          "standard_error": 0.003666837053341463
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007030622593239556,
            "upper_bound": 0.01664713020399552
          },
          "point_estimate": 0.012863241991646786,
          "standard_error": 0.0024966985453377948
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.2335141320273,
            "upper_bound": 24.27755995511644
          },
          "point_estimate": 24.2535947250482,
          "standard_error": 0.01136508406735161
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.229146890767215,
            "upper_bound": 24.27802537630076
          },
          "point_estimate": 24.2430997092234,
          "standard_error": 0.01002093813898178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005612457728635496,
            "upper_bound": 0.05635106392633023
          },
          "point_estimate": 0.0177792918967143,
          "standard_error": 0.013121086412969425
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 24.23373446916997,
            "upper_bound": 24.25302419921777
          },
          "point_estimate": 24.243926683333815,
          "standard_error": 0.004876740423014598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.011825356914941711,
            "upper_bound": 0.050001287657371454
          },
          "point_estimate": 0.03795015423993506,
          "standard_error": 0.009957345534864107
        }
      }
    },
    "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuilt/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/krate_nopre_prebuilt_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.220540973119757,
            "upper_bound": 21.33682912984833
          },
          "point_estimate": 21.28152729730918,
          "standard_error": 0.02986797322903461
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.188912677725742,
            "upper_bound": 21.35032498086933
          },
          "point_estimate": 21.311755980702937,
          "standard_error": 0.03578552520649115
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0069195497845208525,
            "upper_bound": 0.1678149976677921
          },
          "point_estimate": 0.07390060678689225,
          "standard_error": 0.04668024195234495
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.258615712936894,
            "upper_bound": 21.37881561360187
          },
          "point_estimate": 21.33083107864701,
          "standard_error": 0.03112439258858386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.04879430481680418,
            "upper_bound": 0.12964027931067543
          },
          "point_estimate": 0.09978247165944217,
          "standard_error": 0.020687177054677396
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168643.2673195684,
            "upper_bound": 1170578.3163281248
          },
          "point_estimate": 1169506.873485863,
          "standard_error": 499.0132948795547
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168335.28125,
            "upper_bound": 1170427.7233072915
          },
          "point_estimate": 1169010.5587053571,
          "standard_error": 465.30576248444856
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 122.52720790292592,
            "upper_bound": 2413.0843502840507
          },
          "point_estimate": 1121.1411072832566,
          "standard_error": 550.2702859706756
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1168880.664051587,
            "upper_bound": 1170178.5999895558
          },
          "point_estimate": 1169323.3630681818,
          "standard_error": 338.90242979747956
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479.3885352572701,
            "upper_bound": 2218.9991941644357
          },
          "point_estimate": 1667.683934557058,
          "standard_error": 456.6376477495921
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1562338.2531018518,
            "upper_bound": 1565374.8559722223
          },
          "point_estimate": 1563896.4757407408,
          "standard_error": 774.6847822010532
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1561571.0260416665,
            "upper_bound": 1566076.3666666667
          },
          "point_estimate": 1564270.8020833333,
          "standard_error": 1142.122249812885
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 759.8959774813271,
            "upper_bound": 4523.17314157274
          },
          "point_estimate": 2593.685790341778,
          "standard_error": 969.2780976608702
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1563127.5373273406,
            "upper_bound": 1565826.0528455283
          },
          "point_estimate": 1564366.2761904765,
          "standard_error": 686.1573328929952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1470.9857617443015,
            "upper_bound": 3197.683993731262
          },
          "point_estimate": 2588.2443454820714,
          "standard_error": 441.07935128064565
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-let",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446022.5063942303,
            "upper_bound": 1448703.360073794
          },
          "point_estimate": 1447347.550941697,
          "standard_error": 687.038870193999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1445347.1025641025,
            "upper_bound": 1449072.788888889
          },
          "point_estimate": 1447501.689903846,
          "standard_error": 1066.7202810462527
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 581.6759240322415,
            "upper_bound": 3839.867008911913
          },
          "point_estimate": 2640.705212252515,
          "standard_error": 840.7534881914066
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1446015.7525266705,
            "upper_bound": 1448769.880297457
          },
          "point_estimate": 1447752.2603396603,
          "standard_error": 700.0680873343522
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1374.5204783015784,
            "upper_bound": 2875.105977051992
          },
          "point_estimate": 2293.143490976486,
          "standard_error": 388.2226494191992
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/code-rust-library/common-paren",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387821.38924584177,
            "upper_bound": 388885.1791843971
          },
          "point_estimate": 388326.5443334178,
          "standard_error": 271.2979231051086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387580.79196217493,
            "upper_bound": 388893.4410460993
          },
          "point_estimate": 388283.764893617,
          "standard_error": 376.9817954815934
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 136.58057949014218,
            "upper_bound": 1565.1892041272404
          },
          "point_estimate": 856.9095533417811,
          "standard_error": 337.43377939125895
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387579.2771865825,
            "upper_bound": 388402.5396389769
          },
          "point_estimate": 387975.6180989224,
          "standard_error": 212.9936213244036
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475.59204674293136,
            "upper_bound": 1194.5260077176497
          },
          "point_estimate": 908.0918340761796,
          "standard_error": 196.8400022194888
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560785.2843687729,
            "upper_bound": 561500.8167820513
          },
          "point_estimate": 561113.9858901098,
          "standard_error": 184.57243476648685
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560612.2025274725,
            "upper_bound": 561500.75
          },
          "point_estimate": 560956.2916666667,
          "standard_error": 247.19996629299655
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 81.27118855721126,
            "upper_bound": 936.0413941511988
          },
          "point_estimate": 509.3757324952439,
          "standard_error": 261.3604889173807
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 560648.5949857549,
            "upper_bound": 561106.9898191246
          },
          "point_estimate": 560831.0548651349,
          "standard_error": 120.4448438924966
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 267.42468052081216,
            "upper_bound": 814.1716080511077
          },
          "point_estimate": 615.6800369059326,
          "standard_error": 148.62868651681688
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415017.584170928,
            "upper_bound": 416277.6096717171
          },
          "point_estimate": 415526.1850910895,
          "standard_error": 336.6398230488473
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 414907.1022727273,
            "upper_bound": 415705.82875631313
          },
          "point_estimate": 415204.6573863636,
          "standard_error": 205.364741649323
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 100.1348864609048,
            "upper_bound": 969.0126946336904
          },
          "point_estimate": 458.99273457852433,
          "standard_error": 235.39863651192928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 415006.0528730703,
            "upper_bound": 415700.7579381359
          },
          "point_estimate": 415343.2329102715,
          "standard_error": 180.75630361334885
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 226.43371454870416,
            "upper_bound": 1676.808047921372
          },
          "point_estimate": 1118.9432179066894,
          "standard_error": 456.4619258662533
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-en/common-you",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475062.8392170171,
            "upper_bound": 476439.12324056897
          },
          "point_estimate": 475636.7527932385,
          "standard_error": 361.80081831978146
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 474912.9829004329,
            "upper_bound": 475831.56565656565
          },
          "point_estimate": 475415.9930426716,
          "standard_error": 215.2819057736026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 131.9610249299418,
            "upper_bound": 1171.0767915468364
          },
          "point_estimate": 483.5206717188152,
          "standard_error": 280.3090912331725
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 475031.2473945204,
            "upper_bound": 475636.8330680978
          },
          "point_estimate": 475341.439028504,
          "standard_error": 153.76266735295232
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 287.69571923531134,
            "upper_bound": 1805.2179359061809
          },
          "point_estimate": 1209.848853646542,
          "standard_error": 473.28147604601605
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975780.0100250626,
            "upper_bound": 976963.8741100144
          },
          "point_estimate": 976371.5263868002,
          "standard_error": 303.71349180635946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975526.0953947369,
            "upper_bound": 977144.1157894736
          },
          "point_estimate": 976356.3771929824,
          "standard_error": 416.2124055783025
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 249.96657231657656,
            "upper_bound": 1718.8126134322215
          },
          "point_estimate": 1131.0511350513464,
          "standard_error": 388.5566149934532
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 975644.1495057332,
            "upper_bound": 976992.2431548524
          },
          "point_estimate": 976429.1319890636,
          "standard_error": 346.3904253105787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 577.2538085214607,
            "upper_bound": 1279.999017618765
          },
          "point_estimate": 1014.6456904284368,
          "standard_error": 180.27217779883236
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296322.1572635501,
            "upper_bound": 296700.90953155246
          },
          "point_estimate": 296514.0902993934,
          "standard_error": 96.69926971850994
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296245.76422764227,
            "upper_bound": 296794.01345980127
          },
          "point_estimate": 296559.8403794038,
          "standard_error": 121.58824811760016
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.081485703676215,
            "upper_bound": 537.3733828174704
          },
          "point_estimate": 355.99199872051634,
          "standard_error": 145.09301830118966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 296506.78061196813,
            "upper_bound": 296796.7918677176
          },
          "point_estimate": 296640.75907507126,
          "standard_error": 73.5831790919263
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 174.07070980752164,
            "upper_bound": 413.03034738408707
          },
          "point_estimate": 323.00347731493565,
          "standard_error": 61.62856477856336
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-ru/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800670.9198903985,
            "upper_bound": 802390.774587086
          },
          "point_estimate": 801448.1757091098,
          "standard_error": 442.34544514888705
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800393.6385869565,
            "upper_bound": 802311.1195652174
          },
          "point_estimate": 801019.7503623188,
          "standard_error": 480.0537623987286
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 250.0993061033535,
            "upper_bound": 2236.967674959927
          },
          "point_estimate": 1144.3125592495776,
          "standard_error": 503.7644930198954
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 800730.299494748,
            "upper_bound": 801433.0427286357
          },
          "point_estimate": 801042.2889328063,
          "standard_error": 176.79232732634156
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 580.6360979176244,
            "upper_bound": 2016.0301945155616
          },
          "point_estimate": 1473.6505844143226,
          "standard_error": 397.2411321880196
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-do-not",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 394917.7573822464,
            "upper_bound": 395519.78040243266
          },
          "point_estimate": 395188.9311788302,
          "standard_error": 155.42891960079328
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 394879.79755434784,
            "upper_bound": 395377.125
          },
          "point_estimate": 395110.0471014492,
          "standard_error": 108.46481508410795
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.889836567781888,
            "upper_bound": 683.6921144924809
          },
          "point_estimate": 368.66882891676704,
          "standard_error": 168.27158014518864
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 394995.5037426852,
            "upper_bound": 395707.36767406674
          },
          "point_estimate": 395303.0782326369,
          "standard_error": 182.2787553807853
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 166.9182410144756,
            "upper_bound": 740.066425519775
          },
          "point_estimate": 517.9175071105902,
          "standard_error": 163.3584014169763
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-one-space",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165190.65278604196,
            "upper_bound": 165592.05126492586
          },
          "point_estimate": 165376.68077802917,
          "standard_error": 102.71759647704566
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165141.5406737054,
            "upper_bound": 165506.76187782804
          },
          "point_estimate": 165372.43137254904,
          "standard_error": 101.3697425301312
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.671983117144975,
            "upper_bound": 505.7319579445213
          },
          "point_estimate": 216.60010399773435,
          "standard_error": 121.42316495547897
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165045.07502941237,
            "upper_bound": 165447.38266532877
          },
          "point_estimate": 165236.33397191044,
          "standard_error": 106.83062076653354
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 133.02234428344102,
            "upper_bound": 479.5227904642422
          },
          "point_estimate": 342.1666341225191,
          "standard_error": 95.09254962843757
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/huge-zh/common-that",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232023.5151220807,
            "upper_bound": 232423.7620806794
          },
          "point_estimate": 232224.24327039733,
          "standard_error": 102.49340510384889
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231831.99761146496,
            "upper_bound": 232520.70806794055
          },
          "point_estimate": 232255.69681528665,
          "standard_error": 153.16259281790485
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.58164197242717,
            "upper_bound": 622.5125660182581
          },
          "point_estimate": 495.02014286764626,
          "standard_error": 163.77950963056753
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231855.87148204085,
            "upper_bound": 232286.68980471184
          },
          "point_estimate": 231996.1963437836,
          "standard_error": 110.8141453449413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.2157933819861,
            "upper_bound": 410.4948190247212
          },
          "point_estimate": 341.6848060873652,
          "standard_error": 51.577898142582235
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173115.90879682542,
            "upper_bound": 173261.25735079363
          },
          "point_estimate": 173186.20019160997,
          "standard_error": 37.175634863053595
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173079.49047619046,
            "upper_bound": 173283.06904761904
          },
          "point_estimate": 173162.83873015875,
          "standard_error": 50.773623000174894
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.38556274062611,
            "upper_bound": 198.1165398160751
          },
          "point_estimate": 130.8223860108178,
          "standard_error": 44.766847080135655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 173098.49159722222,
            "upper_bound": 173252.3655393586
          },
          "point_estimate": 173154.78688930118,
          "standard_error": 39.49818700102862
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.52404428381597,
            "upper_bound": 154.2281746339472
          },
          "point_estimate": 123.64211144767695,
          "standard_error": 22.20548313753232
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-huge_common-"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 649008.5662511338,
            "upper_bound": 649829.4334502551
          },
          "point_estimate": 649411.7405817745,
          "standard_error": 210.14817516217576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648895.9196428572,
            "upper_bound": 649972.2678571428
          },
          "point_estimate": 649257.3582589286,
          "standard_error": 296.5891284200929
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.01762764216232,
            "upper_bound": 1136.1455558711057
          },
          "point_estimate": 653.6743571448864,
          "standard_error": 261.0548284449966
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 648774.5472642898,
            "upper_bound": 649870.044165758
          },
          "point_estimate": 649221.3837198516,
          "standard_error": 277.77260084378236
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 393.0743542735756,
            "upper_bound": 873.3555137748675
          },
          "point_estimate": 699.5735912563218,
          "standard_error": 121.28663140099424
        }
      }
    },
    "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/krate_nopre/prebuiltiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/krate_nopre_prebuiltiter_pathological-repeated-rare-small_common"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.1238862471366,
            "upper_bound": 1314.6560283851693
          },
          "point_estimate": 1313.4772130339545,
          "standard_error": 0.6486036280873256
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.3408019719734,
            "upper_bound": 1314.7043033082923
          },
          "point_estimate": 1314.019617403448,
          "standard_error": 0.6445654140236255
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2795515038132924,
            "upper_bound": 3.5207037268623687
          },
          "point_estimate": 1.2494077672941777,
          "standard_error": 0.8314113542026556
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1312.1585985199306,
            "upper_bound": 1314.7251235831066
          },
          "point_estimate": 1313.2432509559208,
          "standard_error": 0.661312916763575
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8849279720267121,
            "upper_bound": 2.899008583624098
          },
          "point_estimate": 2.1572794526008767,
          "standard_error": 0.5337821238296121
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218180.1214153438,
            "upper_bound": 1220425.29326002
          },
          "point_estimate": 1219326.8745542327,
          "standard_error": 573.7726545598877
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1217711.8244444444,
            "upper_bound": 1220676.0377777778
          },
          "point_estimate": 1219535.3547619048,
          "standard_error": 634.1777891822015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 355.9393070140055,
            "upper_bound": 3714.9705220461537
          },
          "point_estimate": 1524.360559326096,
          "standard_error": 850.990130980857
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1218509.0235507246,
            "upper_bound": 1221207.6949875988
          },
          "point_estimate": 1220027.4787878788,
          "standard_error": 695.1836434324978
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 997.6159426220472,
            "upper_bound": 2424.6709516841456
          },
          "point_estimate": 1913.38014690088,
          "standard_error": 366.1714750425213
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1367023.2099029983,
            "upper_bound": 1370163.3046208113
          },
          "point_estimate": 1368513.7628659613,
          "standard_error": 808.8277225543352
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366094.0077160494,
            "upper_bound": 1370661.5833333333
          },
          "point_estimate": 1368362.0380952382,
          "standard_error": 1107.2439751204415
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 358.7045390021515,
            "upper_bound": 4634.9964714160305
          },
          "point_estimate": 3067.363636654555,
          "standard_error": 1226.1032506232054
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1366674.3273681714,
            "upper_bound": 1368893.1060395285
          },
          "point_estimate": 1367948.2626262626,
          "standard_error": 565.5926378440284
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1303.885465250782,
            "upper_bound": 3380.513649552491
          },
          "point_estimate": 2706.887073735467,
          "standard_error": 534.727004356498
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1268146.4567320405,
            "upper_bound": 1269748.8425205254
          },
          "point_estimate": 1268971.1219882322,
          "standard_error": 410.4104935538018
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1267708.8103448276,
            "upper_bound": 1270173.4137931033
          },
          "point_estimate": 1269132.451724138,
          "standard_error": 566.2803055659713
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 312.4707254869708,
            "upper_bound": 2287.3109318059683
          },
          "point_estimate": 1565.0212118705488,
          "standard_error": 523.0843996007745
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1268153.7540155004,
            "upper_bound": 1269930.9311399346
          },
          "point_estimate": 1269017.112494402,
          "standard_error": 453.0521797835408
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 765.2214692417347,
            "upper_bound": 1727.772087048182
          },
          "point_estimate": 1369.783948832054,
          "standard_error": 248.14577220914845
        }
      }
    },
    "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/stud_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338380.94253086415,
            "upper_bound": 338866.9690140542
          },
          "point_estimate": 338639.19384038803,
          "standard_error": 124.82478487570414
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338423.28626543214,
            "upper_bound": 338936.14814814815
          },
          "point_estimate": 338688.9354166667,
          "standard_error": 169.61490293830718
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 64.43818774490475,
            "upper_bound": 696.7968200367761
          },
          "point_estimate": 379.7515099247105,
          "standard_error": 162.00196258142526
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 338563.6589506173,
            "upper_bound": 338896.90058036795
          },
          "point_estimate": 338740.8593073593,
          "standard_error": 85.57005241240182
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208.36234905647095,
            "upper_bound": 565.8053022713906
          },
          "point_estimate": 418.0451786931633,
          "standard_error": 100.10674210912372
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266960.91359966976,
            "upper_bound": 267484.07653933496
          },
          "point_estimate": 267209.55630112387,
          "standard_error": 134.01883635805976
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266918.1613949716,
            "upper_bound": 267497.9403892944
          },
          "point_estimate": 267150.849270073,
          "standard_error": 131.5322313787523
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.64143654228651,
            "upper_bound": 752.8392216709343
          },
          "point_estimate": 294.4284483008391,
          "standard_error": 172.90219226149057
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266812.22275780275,
            "upper_bound": 267273.9084377726
          },
          "point_estimate": 267027.98270926153,
          "standard_error": 115.17625573332026
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.43193196499382,
            "upper_bound": 592.1066870184557
          },
          "point_estimate": 444.6608093674502,
          "standard_error": 105.87144325070255
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452641.9268877498,
            "upper_bound": 453157.47975651576
          },
          "point_estimate": 452898.1602669998,
          "standard_error": 131.50957355207507
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452572.7086419753,
            "upper_bound": 453259.5683641975
          },
          "point_estimate": 452776.6036155203,
          "standard_error": 204.41608552283623
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.38712196054376,
            "upper_bound": 738.2037846720535
          },
          "point_estimate": 534.4486909746047,
          "standard_error": 190.99033899341765
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 452667.19424038567,
            "upper_bound": 453116.3277305174
          },
          "point_estimate": 452844.51989738655,
          "standard_error": 115.2269119148994
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258.020900439343,
            "upper_bound": 553.8968996227313
          },
          "point_estimate": 438.0768599249601,
          "standard_error": 76.93682524437125
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237869.94159158104,
            "upper_bound": 238367.3109710551
          },
          "point_estimate": 238137.23265380223,
          "standard_error": 127.71493802675236
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 237839.15219421103,
            "upper_bound": 238437.2320261438
          },
          "point_estimate": 238275.68818082788,
          "standard_error": 141.99369234509714
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 57.43056107187334,
            "upper_bound": 685.5549815108897
          },
          "point_estimate": 319.1271448790377,
          "standard_error": 154.49867228502293
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 238118.5234606123,
            "upper_bound": 238413.0501374343
          },
          "point_estimate": 238308.2520329344,
          "standard_error": 75.24666186122563
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 172.30224977192157,
            "upper_bound": 550.0462785218349
          },
          "point_estimate": 423.4889976181492,
          "standard_error": 98.0679254516587
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632162.0350544951,
            "upper_bound": 633072.3438464697
          },
          "point_estimate": 632598.8491242474,
          "standard_error": 233.8552528426407
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 632055.8724137931,
            "upper_bound": 633495.2155172414
          },
          "point_estimate": 632365.1875,
          "standard_error": 326.2906587649367
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 103.89191505203723,
            "upper_bound": 1342.9543933993573
          },
          "point_estimate": 510.8584362465329,
          "standard_error": 341.1687548620117
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 631984.97349099,
            "upper_bound": 632817.8081152269
          },
          "point_estimate": 632313.655127631,
          "standard_error": 209.66187995451
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 390.5264678958311,
            "upper_bound": 956.6820984837508
          },
          "point_estimate": 778.4666571538436,
          "standard_error": 138.07345021628592
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117750.99965803666,
            "upper_bound": 118031.25689095006
          },
          "point_estimate": 117892.19220218832,
          "standard_error": 71.42830521930725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117748.14379719524,
            "upper_bound": 118066.56171983355
          },
          "point_estimate": 117871.55914239484,
          "standard_error": 89.50872510869434
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.32501871348648,
            "upper_bound": 413.20445111075486
          },
          "point_estimate": 219.3253563489073,
          "standard_error": 89.70729935638967
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117759.64425235346,
            "upper_bound": 117991.15985542643
          },
          "point_estimate": 117852.897877527,
          "standard_error": 59.42464703496952
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 120.07253333921572,
            "upper_bound": 316.5623683542153
          },
          "point_estimate": 239.2250091935898,
          "standard_error": 51.08144433068393
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125564.9553390805,
            "upper_bound": 125711.27633034006
          },
          "point_estimate": 125637.0559415709,
          "standard_error": 37.44436759000946
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125550.15672413792,
            "upper_bound": 125745.8301724138
          },
          "point_estimate": 125628.28227969348,
          "standard_error": 47.74630689521635
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 32.966121656119086,
            "upper_bound": 221.50171417101495
          },
          "point_estimate": 140.0539343238999,
          "standard_error": 47.266536989874304
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 125500.6308927728,
            "upper_bound": 125660.12815555422
          },
          "point_estimate": 125579.72458575908,
          "standard_error": 43.02649184840413
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 68.49568648343347,
            "upper_bound": 160.6395005504086
          },
          "point_estimate": 125.07712720740972,
          "standard_error": 23.663471774911965
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205980.4854990584,
            "upper_bound": 206310.18442709176
          },
          "point_estimate": 206137.34198547213,
          "standard_error": 84.67011108277272
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205905.57853107344,
            "upper_bound": 206320.6395480226
          },
          "point_estimate": 206082.85216572505,
          "standard_error": 148.89622150595628
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.041162779005493,
            "upper_bound": 554.7009593046578
          },
          "point_estimate": 291.3504394602645,
          "standard_error": 138.80184529841853
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 205935.4357844362,
            "upper_bound": 206212.42771409143
          },
          "point_estimate": 206056.7948051948,
          "standard_error": 72.44629686504402
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 165.07559570351194,
            "upper_bound": 364.4451347599211
          },
          "point_estimate": 282.7605305993942,
          "standard_error": 55.98693631852174
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291424.1511965079,
            "upper_bound": 291832.0747921429
          },
          "point_estimate": 291628.13226920634,
          "standard_error": 104.5207874249598
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291343.017,
            "upper_bound": 292010.776
          },
          "point_estimate": 291562.6358888889,
          "standard_error": 176.77023754752403
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.61962800822064,
            "upper_bound": 586.3802445230187
          },
          "point_estimate": 434.6864514827877,
          "standard_error": 142.19185069680623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 291307.2278961008,
            "upper_bound": 291838.0954679335
          },
          "point_estimate": 291571.67985454545,
          "standard_error": 142.37188523568742
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 219.54059037462596,
            "upper_bound": 417.8007162234955
          },
          "point_estimate": 348.1021727498249,
          "standard_error": 50.7903257640785
        }
      }
    },
    "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309817.9619210806,
            "upper_bound": 310140.5599717514
          },
          "point_estimate": 309979.95096751413,
          "standard_error": 82.79881809349887
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309706.6406779661,
            "upper_bound": 310192.6991525424
          },
          "point_estimate": 310034.49717514124,
          "standard_error": 147.5633327688837
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.75350706101339,
            "upper_bound": 435.5448915259787
          },
          "point_estimate": 353.05118621087206,
          "standard_error": 103.71141727371764
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 309760.4329828748,
            "upper_bound": 310042.17571274616
          },
          "point_estimate": 309882.820735197,
          "standard_error": 72.03550533627202
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 176.97886949989032,
            "upper_bound": 330.3810984059692
          },
          "point_estimate": 276.1682533266549,
          "standard_error": 39.51452128831237
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328754.87554329325,
            "upper_bound": 329238.84161311307
          },
          "point_estimate": 329002.6217092092,
          "standard_error": 124.07227759564572
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328647.4752252252,
            "upper_bound": 329338.6756756757
          },
          "point_estimate": 329088.3303303303,
          "standard_error": 170.36285464714442
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 85.1352240140964,
            "upper_bound": 699.817518544268
          },
          "point_estimate": 421.7400323325031,
          "standard_error": 162.18320226494836
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 328937.77104318806,
            "upper_bound": 329390.5286188444
          },
          "point_estimate": 329184.86998947,
          "standard_error": 120.53329044946946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 224.33535133493365,
            "upper_bound": 507.3700861691695
          },
          "point_estimate": 414.1021421627417,
          "standard_error": 69.96864798424664
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405650.53717989416,
            "upper_bound": 406287.92489470897
          },
          "point_estimate": 405946.3759607584,
          "standard_error": 163.72619901157086
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405583.91777777777,
            "upper_bound": 406212.6976080247
          },
          "point_estimate": 405850.6759259259,
          "standard_error": 192.88191186867348
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 55.594792679676395,
            "upper_bound": 877.8334420819793
          },
          "point_estimate": 401.1556253225758,
          "standard_error": 206.90868989990145
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 405675.1086275599,
            "upper_bound": 406156.13463388913
          },
          "point_estimate": 405895.9523520924,
          "standard_error": 123.02303446460198
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 244.242901572043,
            "upper_bound": 738.1017261754364
          },
          "point_estimate": 544.5084127604623,
          "standard_error": 135.42254484144277
        }
      }
    },
    "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285678.0123709537,
            "upper_bound": 286282.4065179352
          },
          "point_estimate": 285974.01273090864,
          "standard_error": 154.5856994854467
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285460.16535433073,
            "upper_bound": 286368.89173228346
          },
          "point_estimate": 285992.2507499063,
          "standard_error": 267.7934609150228
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.45065326060211,
            "upper_bound": 913.9294916485266
          },
          "point_estimate": 673.7010161496586,
          "standard_error": 217.5022669561148
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 285655.45126757486,
            "upper_bound": 286194.11359449633
          },
          "point_estimate": 285935.279680949,
          "standard_error": 141.1503649096162
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317.4523127493845,
            "upper_bound": 634.5149225494379
          },
          "point_estimate": 513.8657524021722,
          "standard_error": 83.40980527198201
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123410.8233808444,
            "upper_bound": 123681.1765127956
          },
          "point_estimate": 123535.79615092864,
          "standard_error": 69.31337851915512
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123329.05479969764,
            "upper_bound": 123683.1662414966
          },
          "point_estimate": 123463.06079931972,
          "standard_error": 106.6663551144186
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4.8999480875985775,
            "upper_bound": 428.1965566837059
          },
          "point_estimate": 203.5772396000684,
          "standard_error": 101.33208585637313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 123345.65462087274,
            "upper_bound": 123591.94280712285
          },
          "point_estimate": 123441.273937627,
          "standard_error": 63.976878683042365
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.53585075365784,
            "upper_bound": 308.8819309226664
          },
          "point_estimate": 231.32604065695205,
          "standard_error": 53.97420132840936
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208167.28785477544,
            "upper_bound": 208500.54573268964
          },
          "point_estimate": 208334.60547776875,
          "standard_error": 85.32197926321999
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208110.1200284091,
            "upper_bound": 208501.7026515151
          },
          "point_estimate": 208377.89595170453,
          "standard_error": 98.70698258055748
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.582932251836134,
            "upper_bound": 499.9481080749159
          },
          "point_estimate": 203.3472760063582,
          "standard_error": 123.46964090784886
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 208139.70480092635,
            "upper_bound": 208422.78572474036
          },
          "point_estimate": 208297.84034828807,
          "standard_error": 72.3833945373087
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 141.70706566060468,
            "upper_bound": 372.2331955144997
          },
          "point_estimate": 284.3944496590666,
          "standard_error": 57.72297562898914
        }
      }
    },
    "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154360.57585357816,
            "upper_bound": 154693.8020009416
          },
          "point_estimate": 154517.08688441617,
          "standard_error": 85.31344878535168
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154299.60395480227,
            "upper_bound": 154704.99915254238
          },
          "point_estimate": 154477.26415371938,
          "standard_error": 88.17130920321232
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.85026798897137,
            "upper_bound": 458.75950857500726
          },
          "point_estimate": 254.917045855687,
          "standard_error": 113.37021482408625
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154382.5974860653,
            "upper_bound": 154653.98860406273
          },
          "point_estimate": 154530.09998899404,
          "standard_error": 69.29077582331259
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 132.479038384362,
            "upper_bound": 382.90544894165635
          },
          "point_estimate": 284.55788855747244,
          "standard_error": 68.4756212806328
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-freq_rare-alphabe"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44371.34615164054,
            "upper_bound": 44453.14026287264
          },
          "point_estimate": 44412.77090456833,
          "standard_error": 20.874462520802147
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44346.38807588076,
            "upper_bound": 44471.86473577235
          },
          "point_estimate": 44425.32987804878,
          "standard_error": 30.230375621590586
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 7.580093706887254,
            "upper_bound": 122.25971825627956
          },
          "point_estimate": 74.87452732924407,
          "standard_error": 28.701114158780797
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44349.52996645208,
            "upper_bound": 44429.85180717568
          },
          "point_estimate": 44380.530566993984,
          "standard_error": 20.702942338205283
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.68014986476592,
            "upper_bound": 84.95985336420733
          },
          "point_estimate": 69.63045306167439,
          "standard_error": 11.191128534636473
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector-repeated_rare-alp"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223779.1289920637,
            "upper_bound": 1226527.0258440478
          },
          "point_estimate": 1225423.3924589946,
          "standard_error": 737.7023731252403
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225196.8977777776,
            "upper_bound": 1226962.2
          },
          "point_estimate": 1225933.3599537038,
          "standard_error": 445.6992530165567
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 209.65713919453523,
            "upper_bound": 1945.5955008753488
          },
          "point_estimate": 1189.4319590220944,
          "standard_error": 513.7128607272685
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1225142.090322166,
            "upper_bound": 1226558.4083576289
          },
          "point_estimate": 1225781.34995671,
          "standard_error": 367.90265043902787
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 561.4447087587037,
            "upper_bound": 3720.4085755283136
          },
          "point_estimate": 2465.714253935372,
          "standard_error": 1032.1319017685614
        }
      }
    },
    "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/stud_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104416.84697600288,
            "upper_bound": 104556.25511358984
          },
          "point_estimate": 104486.11148098877,
          "standard_error": 35.50710852327697
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104419.24755537364,
            "upper_bound": 104572.83037249284
          },
          "point_estimate": 104468.98352435532,
          "standard_error": 41.23456157586256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28.10520542271652,
            "upper_bound": 209.12099769614605
          },
          "point_estimate": 91.84160647836906,
          "standard_error": 45.355007486141744
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 104449.15016959466,
            "upper_bound": 104562.99050069413
          },
          "point_estimate": 104503.84279388232,
          "standard_error": 28.855145953730847
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 59.19447509019415,
            "upper_bound": 155.89627643812807
          },
          "point_estimate": 118.0592269810853,
          "standard_error": 24.984058944521415
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29257.39948700253,
            "upper_bound": 29298.07809514224
          },
          "point_estimate": 29278.25221867572,
          "standard_error": 10.452297104860502
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29239.18276972625,
            "upper_bound": 29308.05743424584
          },
          "point_estimate": 29292.5277945518,
          "standard_error": 21.09228456005084
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 5.248308409239318,
            "upper_bound": 54.533468355510315
          },
          "point_estimate": 31.74806696655034,
          "standard_error": 14.270639023337928
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 29250.96443733336,
            "upper_bound": 29298.015304885328
          },
          "point_estimate": 29273.419865319865,
          "standard_error": 12.276788796106056
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.3987381696572,
            "upper_bound": 39.31097161499989
          },
          "point_estimate": 34.83442031275554,
          "standard_error": 4.436340164579228
        }
      }
    },
    "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/stud_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28898.07587805748,
            "upper_bound": 28962.149897463096
          },
          "point_estimate": 28927.39350999482,
          "standard_error": 16.477985389809895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28886.90011491205,
            "upper_bound": 28956.702704852825
          },
          "point_estimate": 28914.117598590747,
          "standard_error": 18.980096375956805
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 9.514535858660972,
            "upper_bound": 86.89421635177824
          },
          "point_estimate": 41.10914269365845,
          "standard_error": 19.093624500481113
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28882.788747720548,
            "upper_bound": 28926.04817751773
          },
          "point_estimate": 28899.773300684996,
          "standard_error": 10.907606091136383
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.614244948791587,
            "upper_bound": 73.5697399031047
          },
          "point_estimate": 54.79689019623861,
          "standard_error": 13.587649660688331
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550737.8991277958,
            "upper_bound": 551994.7343236832
          },
          "point_estimate": 551337.3891552428,
          "standard_error": 323.9305462869391
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550561.3785984849,
            "upper_bound": 552298.8143939395
          },
          "point_estimate": 551020.1002164502,
          "standard_error": 449.253196463382
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 119.91383044685357,
            "upper_bound": 1733.1734090028217
          },
          "point_estimate": 786.6617830414305,
          "standard_error": 427.740010620505
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 550687.5020980682,
            "upper_bound": 551383.6994114701
          },
          "point_estimate": 551045.5444313262,
          "standard_error": 177.49916499995464
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 512.6897717662862,
            "upper_bound": 1317.265066476968
          },
          "point_estimate": 1078.4124960383383,
          "standard_error": 193.44175144271895
        }
      }
    },
    "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/stud_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.5935786704777,
            "upper_bound": 1130.2271653876826
          },
          "point_estimate": 1129.42902968072,
          "standard_error": 0.4187411980463322
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.1413717753544,
            "upper_bound": 1130.513940410725
          },
          "point_estimate": 1129.8152942181628,
          "standard_error": 0.6548330601444184
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2710215660089587,
            "upper_bound": 2.3503480304759212
          },
          "point_estimate": 1.414278402517575,
          "standard_error": 0.5641703378047215
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1128.8874126617052,
            "upper_bound": 1130.2088732129866
          },
          "point_estimate": 1129.5442128567677,
          "standard_error": 0.3432830776349015
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8205856203114357,
            "upper_bound": 1.7395956704158893
          },
          "point_estimate": 1.3938742110563307,
          "standard_error": 0.23923372675576896
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.49079336510836,
            "upper_bound": 34.53999714512599
          },
          "point_estimate": 34.51127217058797,
          "standard_error": 0.01285833528785114
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.491432242782224,
            "upper_bound": 34.515371237845414
          },
          "point_estimate": 34.50228104537592,
          "standard_error": 0.006044175549534815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0015672168176760644,
            "upper_bound": 0.042815903162928194
          },
          "point_estimate": 0.013603810147034106,
          "standard_error": 0.010816647130712311
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.49739419028756,
            "upper_bound": 34.518858111325045
          },
          "point_estimate": 34.50600685616791,
          "standard_error": 0.005446792307764077
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009342353846979914,
            "upper_bound": 0.06381003083481783
          },
          "point_estimate": 0.04290340693061994,
          "standard_error": 0.01663078621286139
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.916716860055494,
            "upper_bound": 38.00179780753871
          },
          "point_estimate": 37.95452228909021,
          "standard_error": 0.022019920380463725
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.90094831587663,
            "upper_bound": 37.99218523674848
          },
          "point_estimate": 37.93913696585962,
          "standard_error": 0.0214644808439015
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008442912349353715,
            "upper_bound": 0.10500293672856756
          },
          "point_estimate": 0.04594110113451743,
          "standard_error": 0.024418346132456553
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.91151246467775,
            "upper_bound": 37.95678732605302
          },
          "point_estimate": 37.92958533277074,
          "standard_error": 0.011551314121751949
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02499795221140356,
            "upper_bound": 0.10088392737407698
          },
          "point_estimate": 0.07311391271532289,
          "standard_error": 0.021177689120743245
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.5966138542146,
            "upper_bound": 26.61856167925005
          },
          "point_estimate": 26.60712219389846,
          "standard_error": 0.005640924553366065
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.592590991096277,
            "upper_bound": 26.627114685261336
          },
          "point_estimate": 26.601055500368854,
          "standard_error": 0.008666898019608441
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0041031424524824115,
            "upper_bound": 0.03027740051596695
          },
          "point_estimate": 0.014728240120820486,
          "standard_error": 0.007147997215015947
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 26.600977528227425,
            "upper_bound": 26.625611855603186
          },
          "point_estimate": 26.61296852502179,
          "standard_error": 0.006286160800686225
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009169228728592348,
            "upper_bound": 0.022612604008041496
          },
          "point_estimate": 0.01880902201785328,
          "standard_error": 0.0032007194689337736
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/stud_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.391911671383966,
            "upper_bound": 33.45014990449472
          },
          "point_estimate": 33.419253670850324,
          "standard_error": 0.014983204087145263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.38127499294487,
            "upper_bound": 33.45968287979335
          },
          "point_estimate": 33.40383958931527,
          "standard_error": 0.020993147215243448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00174901695311308,
            "upper_bound": 0.08598109467873179
          },
          "point_estimate": 0.037760306261703944,
          "standard_error": 0.02057534733828347
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.397399383278696,
            "upper_bound": 33.456965632447876
          },
          "point_estimate": 33.423328958346055,
          "standard_error": 0.015514038160455526
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02291985994294233,
            "upper_bound": 0.061785541259747494
          },
          "point_estimate": 0.04994558846002848,
          "standard_error": 0.009788369958173252
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.59186778515248,
            "upper_bound": 40.660206817170966
          },
          "point_estimate": 40.6243609493926,
          "standard_error": 0.017545870099800902
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.58158296405995,
            "upper_bound": 40.670493465758184
          },
          "point_estimate": 40.6099048483852,
          "standard_error": 0.023891298771235508
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012019702428589258,
            "upper_bound": 0.0960895694204884
          },
          "point_estimate": 0.05780616088564647,
          "standard_error": 0.02163797126928655
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 40.59327484051533,
            "upper_bound": 40.63474970569248
          },
          "point_estimate": 40.60910894432762,
          "standard_error": 0.01061431167426594
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02992909488391645,
            "upper_bound": 0.07488784497802133
          },
          "point_estimate": 0.0584433172496364,
          "standard_error": 0.01169605175006617
        }
      }
    },
    "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.35506978300185,
            "upper_bound": 73.49919975987991
          },
          "point_estimate": 73.41837487559069,
          "standard_error": 0.037361079118126576
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.33474128258243,
            "upper_bound": 73.48166694818909
          },
          "point_estimate": 73.37926801473762,
          "standard_error": 0.030691912577644864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012851713080194251,
            "upper_bound": 0.17408184093860932
          },
          "point_estimate": 0.06300266103838945,
          "standard_error": 0.0401313435711992
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 73.32856662024834,
            "upper_bound": 73.46403918364234
          },
          "point_estimate": 73.3887161032908,
          "standard_error": 0.03532846097402111
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03718824864891565,
            "upper_bound": 0.17352889576931085
          },
          "point_estimate": 0.12479606505431658,
          "standard_error": 0.03798126467631406
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.0002326892411,
            "upper_bound": 66.06072087087462
          },
          "point_estimate": 66.03243812696263,
          "standard_error": 0.015454134204159716
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.01007420994975,
            "upper_bound": 66.07430943013505
          },
          "point_estimate": 66.03026166377325,
          "standard_error": 0.01927095763824288
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005530276706027716,
            "upper_bound": 0.08847752014353405
          },
          "point_estimate": 0.03583812615848907,
          "standard_error": 0.020614702689789717
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.02366378632055,
            "upper_bound": 66.07109961463165
          },
          "point_estimate": 66.04737767609791,
          "standard_error": 0.012272890446757282
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.025251312403827757,
            "upper_bound": 0.06998425349812855
          },
          "point_estimate": 0.05132671174785721,
          "standard_error": 0.01285862314093703
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.75598393341228,
            "upper_bound": 56.8594355831414
          },
          "point_estimate": 56.808884289512605,
          "standard_error": 0.026380935500410263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.75838069771376,
            "upper_bound": 56.88003767029707
          },
          "point_estimate": 56.80290848525068,
          "standard_error": 0.02754086978152346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010821006248920071,
            "upper_bound": 0.15210756914636497
          },
          "point_estimate": 0.0670881423594507,
          "standard_error": 0.03607557714621188
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.75611745920213,
            "upper_bound": 56.82881568246624
          },
          "point_estimate": 56.796351485341766,
          "standard_error": 0.01863976817832047
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.044180232793367025,
            "upper_bound": 0.1160155768136946
          },
          "point_estimate": 0.08830321285483207,
          "standard_error": 0.01846718990920869
        }
      }
    },
    "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.78805533020606,
            "upper_bound": 108.95038803490692
          },
          "point_estimate": 108.86301357841856,
          "standard_error": 0.041783502359167814
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.75452363774976,
            "upper_bound": 108.98267731680085
          },
          "point_estimate": 108.81078135243632,
          "standard_error": 0.053726783328638096
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.020138654450579104,
            "upper_bound": 0.22173386109521412
          },
          "point_estimate": 0.10727171444554402,
          "standard_error": 0.052957505411114617
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 108.77040380699697,
            "upper_bound": 108.96411771950076
          },
          "point_estimate": 108.84475825986024,
          "standard_error": 0.05032877347277698
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05279146552161874,
            "upper_bound": 0.17357433126105773
          },
          "point_estimate": 0.13890863592989094,
          "standard_error": 0.031225250117407604
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.82113563201433,
            "upper_bound": 45.86496728757793
          },
          "point_estimate": 45.84377688743384,
          "standard_error": 0.011125745024514263
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.831714140354464,
            "upper_bound": 45.862747168053446
          },
          "point_estimate": 45.84196180331061,
          "standard_error": 0.008065443402541356
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.003326428109299444,
            "upper_bound": 0.057232985480328354
          },
          "point_estimate": 0.020185701357150577,
          "standard_error": 0.01380455111421977
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.83576027936573,
            "upper_bound": 45.85343513262583
          },
          "point_estimate": 45.84359966331479,
          "standard_error": 0.0045333638073197105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013002828946774157,
            "upper_bound": 0.05264392841428972
          },
          "point_estimate": 0.0372270986443335,
          "standard_error": 0.01023261904985877
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.1763982653851,
            "upper_bound": 50.22401926467583
          },
          "point_estimate": 50.20033498557946,
          "standard_error": 0.012156579513147223
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.174374751762485,
            "upper_bound": 50.22568973252868
          },
          "point_estimate": 50.2005612466304,
          "standard_error": 0.013286553132231422
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.007463960975107171,
            "upper_bound": 0.06758032712650575
          },
          "point_estimate": 0.03261326892281598,
          "standard_error": 0.015215551006802957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 50.17271686170008,
            "upper_bound": 50.218169971205
          },
          "point_estimate": 50.19454366064434,
          "standard_error": 0.012059076880821425
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01854799774356595,
            "upper_bound": 0.05422937719630513
          },
          "point_estimate": 0.0405223243974245,
          "standard_error": 0.009010378513339308
        }
      }
    },
    "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/stud_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.52190128037164,
            "upper_bound": 91.6442407639253
          },
          "point_estimate": 91.5828785204062,
          "standard_error": 0.031267700431695675
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.4723388764747,
            "upper_bound": 91.64381632092662
          },
          "point_estimate": 91.5990084773422,
          "standard_error": 0.044253865387767585
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0070605334471384945,
            "upper_bound": 0.20437957778711985
          },
          "point_estimate": 0.06798982811733362,
          "standard_error": 0.04865279108844594
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 91.5735397285672,
            "upper_bound": 91.63241837649204
          },
          "point_estimate": 91.60098369637429,
          "standard_error": 0.014977369703048514
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05794755087352536,
            "upper_bound": 0.13284719542284842
          },
          "point_estimate": 0.10397630788032686,
          "standard_error": 0.019289117656930907
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 968814.5137500002,
            "upper_bound": 969898.6191885964
          },
          "point_estimate": 969324.4214108188,
          "standard_error": 277.52027085249455
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 968502.0555555556,
            "upper_bound": 969858.9631578948
          },
          "point_estimate": 969264.5745614036,
          "standard_error": 342.01380495383006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192.73718374927756,
            "upper_bound": 1507.0108521927075
          },
          "point_estimate": 1005.8755877562502,
          "standard_error": 353.5126820587272
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 968697.1359649122,
            "upper_bound": 970488.9912280702
          },
          "point_estimate": 969549.412508544,
          "standard_error": 491.81461769441154
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 469.1163010598138,
            "upper_bound": 1208.9015114582392
          },
          "point_estimate": 925.5688765492682,
          "standard_error": 198.8599511300631
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1363514.840676073,
            "upper_bound": 1368464.3989094652
          },
          "point_estimate": 1365545.516654909,
          "standard_error": 1316.8550750461463
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1363047.8537037035,
            "upper_bound": 1365785.355452675
          },
          "point_estimate": 1365067.0030864198,
          "standard_error": 836.7425962985426
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 279.3534089293681,
            "upper_bound": 3763.746531513562
          },
          "point_estimate": 2038.9248696043576,
          "standard_error": 953.9702601715894
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1363549.209013209,
            "upper_bound": 1371138.8063435776
          },
          "point_estimate": 1366499.988071188,
          "standard_error": 2078.1490826156805
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1063.0353552705706,
            "upper_bound": 6612.379484277564
          },
          "point_estimate": 4392.808642342001,
          "standard_error": 1790.9360785417523
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1254574.4301532567,
            "upper_bound": 1258288.0128325126
          },
          "point_estimate": 1256221.2484168035,
          "standard_error": 957.619995446513
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1253870.4731800766,
            "upper_bound": 1257426.9827586208
          },
          "point_estimate": 1255620.2620073892,
          "standard_error": 884.3331287116239
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 612.4960101605411,
            "upper_bound": 4224.447655116195
          },
          "point_estimate": 2179.813060955851,
          "standard_error": 932.561916208875
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1255008.9885057472,
            "upper_bound": 1256533.0515832002
          },
          "point_estimate": 1255792.818898343,
          "standard_error": 392.7360100751607
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1222.1048557731192,
            "upper_bound": 4529.710302368129
          },
          "point_estimate": 3193.049681706034,
          "standard_error": 984.350981880938
        }
      }
    },
    "memrmem/stud/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/stud_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1385546.763915344,
            "upper_bound": 1386747.7125617284
          },
          "point_estimate": 1386141.4009523808,
          "standard_error": 307.56937319649296
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1385252.2103174604,
            "upper_bound": 1386915.1543209876
          },
          "point_estimate": 1386261.7160493827,
          "standard_error": 470.94026390805726
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 227.33840226023457,
            "upper_bound": 1763.0859242544975
          },
          "point_estimate": 1280.106103199476,
          "standard_error": 380.45682280877776
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1385526.260149038,
            "upper_bound": 1386842.0134420134
          },
          "point_estimate": 1386287.0784992783,
          "standard_error": 335.19746397011386
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 610.2450622352187,
            "upper_bound": 1266.8220880061178
          },
          "point_estimate": 1019.0276864727668,
          "standard_error": 168.97461890677192
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1248525.1477326388,
            "upper_bound": 1254119.9684457672
          },
          "point_estimate": 1251190.7679457671,
          "standard_error": 1435.563316041001
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1247082.7571428572,
            "upper_bound": 1255028.7333333334
          },
          "point_estimate": 1250028.2935185186,
          "standard_error": 2093.2687590108003
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1376.8754950000437,
            "upper_bound": 7862.956605404834
          },
          "point_estimate": 4682.119826431416,
          "standard_error": 1715.2309777228177
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1247214.6441066214,
            "upper_bound": 1253349.3836823734
          },
          "point_estimate": 1249894.8322077922,
          "standard_error": 1584.6741755706485
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2564.8021863250187,
            "upper_bound": 5972.661384760253
          },
          "point_estimate": 4776.931626155939,
          "standard_error": 871.5573008213609
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 354166.9316574202,
            "upper_bound": 354568.90398260514
          },
          "point_estimate": 354364.10122861766,
          "standard_error": 103.07193228827212
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 354068.11468446604,
            "upper_bound": 354700.5428802589
          },
          "point_estimate": 354260.8446601942,
          "standard_error": 177.8787583636224
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 76.51943165124868,
            "upper_bound": 551.3868470070306
          },
          "point_estimate": 423.5869691788612,
          "standard_error": 132.67624670892008
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 354170.2056352246,
            "upper_bound": 354718.5475468091
          },
          "point_estimate": 354452.95354936324,
          "standard_error": 143.1928168520043
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 212.06100819906223,
            "upper_bound": 403.41748202963026
          },
          "point_estimate": 343.07832684624265,
          "standard_error": 48.851490976117475
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/stud_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398531.3341836827,
            "upper_bound": 398998.8835038388
          },
          "point_estimate": 398736.3217175639,
          "standard_error": 120.952197846774
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398459.1747282608,
            "upper_bound": 398923.7732919255
          },
          "point_estimate": 398672.31068840576,
          "standard_error": 94.9980160520819
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.426750866350812,
            "upper_bound": 498.21805202446234
          },
          "point_estimate": 231.5496168673804,
          "standard_error": 138.25655742227775
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 398460.4297375835,
            "upper_bound": 398676.87134061335
          },
          "point_estimate": 398549.57106154715,
          "standard_error": 54.80444810614895
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 118.85676115862314,
            "upper_bound": 560.6893116913152
          },
          "point_estimate": 402.4981192074874,
          "standard_error": 123.7383755761675
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 917616.9263589288,
            "upper_bound": 918985.4869018848
          },
          "point_estimate": 918258.835123016,
          "standard_error": 351.4677790025709
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 917408.1875,
            "upper_bound": 919179.565
          },
          "point_estimate": 918046.5633928572,
          "standard_error": 366.97190730898006
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 207.7585875615132,
            "upper_bound": 1877.760521663008
          },
          "point_estimate": 829.3756915256381,
          "standard_error": 469.2008711427952
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 917544.5696808512,
            "upper_bound": 919335.915421456
          },
          "point_estimate": 918375.0971428572,
          "standard_error": 479.38210265910715
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 481.2165773486477,
            "upper_bound": 1479.8717989151946
          },
          "point_estimate": 1170.6733154610713,
          "standard_error": 256.5644399717451
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 799448.9965683229,
            "upper_bound": 801337.825478261
          },
          "point_estimate": 800323.5195031054,
          "standard_error": 484.9215625263944
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 798793.2956521739,
            "upper_bound": 800965.9217391304
          },
          "point_estimate": 800478.0679347826,
          "standard_error": 598.6778855500185
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 200.03818992684307,
            "upper_bound": 2731.0113102104588
          },
          "point_estimate": 1314.4707193808606,
          "standard_error": 652.7108675447039
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 799316.2977033078,
            "upper_bound": 800848.759029442
          },
          "point_estimate": 800220.0477131564,
          "standard_error": 393.5189426435027
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 785.0461052328087,
            "upper_bound": 2215.343547039776
          },
          "point_estimate": 1613.481806290935,
          "standard_error": 420.5535893669448
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 756635.1407523148,
            "upper_bound": 758150.3253360615
          },
          "point_estimate": 757292.9927447091,
          "standard_error": 392.8624162988736
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 756239.9699074074,
            "upper_bound": 757790.0704365079
          },
          "point_estimate": 757115.4244791667,
          "standard_error": 356.0841716047681
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 44.45483358576894,
            "upper_bound": 1573.80579549687
          },
          "point_estimate": 1050.838976795388,
          "standard_error": 459.24636251721296
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 756514.1798769924,
            "upper_bound": 757559.1268065841
          },
          "point_estimate": 756922.0317099567,
          "standard_error": 268.34664306655264
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 434.9174345602232,
            "upper_bound": 1874.9926619211672
          },
          "point_estimate": 1310.9615640037284,
          "standard_error": 424.549985763661
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321664.96681310044,
            "upper_bound": 322066.7569156746
          },
          "point_estimate": 321838.8170875105,
          "standard_error": 103.81746127057303
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321597.1649122807,
            "upper_bound": 321920.6871345029
          },
          "point_estimate": 321799.2461622807,
          "standard_error": 103.5695467246156
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.49047126946602,
            "upper_bound": 448.9771155816994
          },
          "point_estimate": 181.73473131308344,
          "standard_error": 109.62708036165238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 321623.5110057075,
            "upper_bound": 321851.9907250204
          },
          "point_estimate": 321729.92854864436,
          "standard_error": 59.52865620003941
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 124.4427050794139,
            "upper_bound": 502.3303943932826
          },
          "point_estimate": 346.7078161308266,
          "standard_error": 116.98268541380698
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573414.7695399306,
            "upper_bound": 574215.0302281746
          },
          "point_estimate": 573830.6220324901,
          "standard_error": 204.47667408025265
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573417.3736979167,
            "upper_bound": 574421.8625
          },
          "point_estimate": 573842.916015625,
          "standard_error": 252.66985863605996
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 163.11495413538069,
            "upper_bound": 1201.9151725679878
          },
          "point_estimate": 656.9047207594849,
          "standard_error": 265.0622377999846
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 573650.7480429696,
            "upper_bound": 574316.7758091739
          },
          "point_estimate": 574044.9572646103,
          "standard_error": 167.68932800020983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 363.7547500196261,
            "upper_bound": 897.4167438967376
          },
          "point_estimate": 680.2202718667816,
          "standard_error": 144.3094182270422
        }
      }
    },
    "memrmem/stud/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/stud_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189078.8466295748,
            "upper_bound": 189276.5478265174
          },
          "point_estimate": 189173.5346093428,
          "standard_error": 50.61732111755057
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189062.35126655153,
            "upper_bound": 189288.5051813471
          },
          "point_estimate": 189130.8875647669,
          "standard_error": 45.2221275728056
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.8188868248610397,
            "upper_bound": 289.4728436950179
          },
          "point_estimate": 123.02948742715584,
          "standard_error": 81.04276795780255
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 189030.32146860615,
            "upper_bound": 189235.1149160794
          },
          "point_estimate": 189121.9422649889,
          "standard_error": 51.85439432153032
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.0696764667069,
            "upper_bound": 224.4397487347997
          },
          "point_estimate": 168.68074076475403,
          "standard_error": 38.22246024578906
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/stud_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146275.42560623446,
            "upper_bound": 146758.30146697903
          },
          "point_estimate": 146476.84179113276,
          "standard_error": 127.1910483403476
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146217.94047619047,
            "upper_bound": 146543.35215863452
          },
          "point_estimate": 146375.34046184737,
          "standard_error": 90.97119018205817
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 21.95394147771401,
            "upper_bound": 453.31982749413936
          },
          "point_estimate": 228.6860592834105,
          "standard_error": 105.5401800119292
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 146264.02241995506,
            "upper_bound": 146505.83130708776
          },
          "point_estimate": 146398.76903979556,
          "standard_error": 62.653223737734486
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 113.3857509835142,
            "upper_bound": 634.047787732262
          },
          "point_estimate": 425.2395142167194,
          "standard_error": 164.41939023669002
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501765.16910184274,
            "upper_bound": 502698.21113068063
          },
          "point_estimate": 502241.4549130246,
          "standard_error": 239.8625280612281
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 501534.2111872146,
            "upper_bound": 502953.61339421617
          },
          "point_estimate": 502257.45181017614,
          "standard_error": 380.0756609194591
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 154.47896266844535,
            "upper_bound": 1383.347376468068
          },
          "point_estimate": 829.7753249945991,
          "standard_error": 316.79083102883385
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 502135.5283426773,
            "upper_bound": 502820.341978691
          },
          "point_estimate": 502533.3513965487,
          "standard_error": 176.16483321947086
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479.3281933928813,
            "upper_bound": 971.9704036063772
          },
          "point_estimate": 801.673995319949,
          "standard_error": 125.69470344480288
        }
      }
    },
    "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/stud/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/stud_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1049.8297299206554,
            "upper_bound": 1051.3938581426562
          },
          "point_estimate": 1050.5473871600832,
          "standard_error": 0.40120884979344235
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1049.5183262100036,
            "upper_bound": 1051.4692210783182
          },
          "point_estimate": 1050.414049163811,
          "standard_error": 0.3993146641929779
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.11396299179676976,
            "upper_bound": 2.1631859086618355
          },
          "point_estimate": 0.9642327827351592,
          "standard_error": 0.5497215418070855
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1049.6403103136347,
            "upper_bound": 1050.497076707353
          },
          "point_estimate": 1050.059083708252,
          "standard_error": 0.21634306928996175
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.5190682477242868,
            "upper_bound": 1.7491264641861093
          },
          "point_estimate": 1.336616563365261,
          "standard_error": 0.32394488374792657
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-quux": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-quux",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-quux",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-quux"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1119342.1246333874,
            "upper_bound": 1121155.1071297198
          },
          "point_estimate": 1120251.2182190958,
          "standard_error": 461.57898301099885
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1119300.2181818185,
            "upper_bound": 1121119.1079545454
          },
          "point_estimate": 1120297.1006493506,
          "standard_error": 489.3496425193882
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 269.7096452118117,
            "upper_bound": 2719.3674624033592
          },
          "point_estimate": 1328.407579648284,
          "standard_error": 553.6064028283205
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1119284.330359296,
            "upper_bound": 1121002.781172549
          },
          "point_estimate": 1120171.4913026367,
          "standard_error": 440.9247497393696
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 679.9194595337636,
            "upper_bound": 2068.526937698197
          },
          "point_estimate": 1534.1830923267144,
          "standard_error": 360.60760315994384
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223618.3453645832,
            "upper_bound": 1226403.3526111112
          },
          "point_estimate": 1224927.8588055556,
          "standard_error": 713.5984206789886
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223102.0333333334,
            "upper_bound": 1227780.2933333332
          },
          "point_estimate": 1224130.738888889,
          "standard_error": 1032.9810731239188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.83745801451124,
            "upper_bound": 3638.944648645973
          },
          "point_estimate": 1768.3101672731357,
          "standard_error": 931.533686546957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1223222.9988922458,
            "upper_bound": 1224979.0254574604
          },
          "point_estimate": 1223903.3453679653,
          "standard_error": 451.1623621780278
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 829.3221272119367,
            "upper_bound": 2822.699023928062
          },
          "point_estimate": 2380.2110505958917,
          "standard_error": 424.8115559248186
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/never-fn-strength-paren",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_never-fn-strength-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1140734.36812128,
            "upper_bound": 1143224.1685379464
          },
          "point_estimate": 1141912.3520796131,
          "standard_error": 636.3487322181777
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1140562.6149553573,
            "upper_bound": 1142538.9078125
          },
          "point_estimate": 1141990.4459635415,
          "standard_error": 467.52604502452624
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215.16907782055273,
            "upper_bound": 3121.9941608234894
          },
          "point_estimate": 1050.194699714437,
          "standard_error": 754.2584815874989
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1141483.7605308846,
            "upper_bound": 1142243.646721486
          },
          "point_estimate": 1141948.9433441558,
          "standard_error": 194.42516805549684
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 625.6192375870469,
            "upper_bound": 3008.395639468247
          },
          "point_estimate": 2121.6827610832447,
          "standard_error": 612.097213411361
        }
      }
    },
    "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/code-rust-library/rare-fn-from-str",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/code-rust-library/rare-fn-from-str",
        "directory_name": "memrmem/twoway_oneshot_code-rust-library_rare-fn-from-str"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 684398.8272016462,
            "upper_bound": 685286.7208759553
          },
          "point_estimate": 684845.3952527924,
          "standard_error": 227.80462659735895
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 684187.3425925926,
            "upper_bound": 685504.0222222222
          },
          "point_estimate": 684954.3345679012,
          "standard_error": 381.8861478162739
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 168.37119145528382,
            "upper_bound": 1259.8823849165929
          },
          "point_estimate": 1050.213419132744,
          "standard_error": 282.7137975528238
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 684526.9758041339,
            "upper_bound": 685580.9617249562
          },
          "point_estimate": 685184.777104377,
          "standard_error": 272.7253936783254
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 477.56504641620813,
            "upper_bound": 916.1926850419164
          },
          "point_estimate": 760.219601241555,
          "standard_error": 112.40651839159254
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253339.5921643519,
            "upper_bound": 254000.2614715608
          },
          "point_estimate": 253660.8584545855,
          "standard_error": 169.00902796483672
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253304.68352623456,
            "upper_bound": 253989.38033234127
          },
          "point_estimate": 253605.7734375,
          "standard_error": 195.87275533499644
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 82.1006895146564,
            "upper_bound": 955.8485905650144
          },
          "point_estimate": 446.8067268592554,
          "standard_error": 207.64740666184431
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 253326.98386566772,
            "upper_bound": 253880.023163402
          },
          "point_estimate": 253562.69374098125,
          "standard_error": 142.14423613163882
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 258.97997905468634,
            "upper_bound": 760.9997583270464
          },
          "point_estimate": 560.4401132670533,
          "standard_error": 131.67704612294455
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 404930.72246911377,
            "upper_bound": 405423.15457010583
          },
          "point_estimate": 405170.9090330688,
          "standard_error": 126.13294907103511
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 404872.7823412699,
            "upper_bound": 405659.42083333334
          },
          "point_estimate": 404988.570925926,
          "standard_error": 220.22212726265343
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 53.051133558198984,
            "upper_bound": 663.5540915528866
          },
          "point_estimate": 434.2743592901593,
          "standard_error": 182.08441211322085
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 404954.14629898407,
            "upper_bound": 405585.2621793603
          },
          "point_estimate": 405280.82496392494,
          "standard_error": 161.74693271456476
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 255.31202170692637,
            "upper_bound": 497.18648727502296
          },
          "point_estimate": 421.4664748747544,
          "standard_error": 61.469119344157335
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232777.170875594,
            "upper_bound": 233377.9926822364
          },
          "point_estimate": 233111.29670407443,
          "standard_error": 154.7417912657829
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232892.31278434943,
            "upper_bound": 233500.36433121015
          },
          "point_estimate": 233212.2266454353,
          "standard_error": 138.6626332635351
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 62.4155702294838,
            "upper_bound": 696.1549600399234
          },
          "point_estimate": 365.88773121119647,
          "standard_error": 157.10933750082856
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 232952.28285396015,
            "upper_bound": 233465.0344557727
          },
          "point_estimate": 233181.20785838368,
          "standard_error": 135.39393221115822
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 199.8190569947125,
            "upper_bound": 726.6901455773946
          },
          "point_estimate": 515.1773035211932,
          "standard_error": 155.56649188923907
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_huge-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 545058.4005828004,
            "upper_bound": 545974.4436648898
          },
          "point_estimate": 545519.4489410094,
          "standard_error": 235.2313875197237
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 544690.8258706467,
            "upper_bound": 546285.8731343284
          },
          "point_estimate": 545548.2825159915,
          "standard_error": 415.27696057430853
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 134.64552522160227,
            "upper_bound": 1307.6716170826976
          },
          "point_estimate": 1182.408515575264,
          "standard_error": 314.13143692315026
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 544839.5035321612,
            "upper_bound": 546043.7235727692
          },
          "point_estimate": 545501.1948051949,
          "standard_error": 308.3384110690725
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 499.8627766435095,
            "upper_bound": 919.0448116168722
          },
          "point_estimate": 785.8669328692276,
          "standard_error": 105.65868235314304
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-huge-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/rare-huge-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-huge-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-huge-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-huge-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1939.5768320153545,
            "upper_bound": 1942.622317568072
          },
          "point_estimate": 1941.025157707763,
          "standard_error": 0.7823935860688876
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1938.8404826812057,
            "upper_bound": 1942.676715843489
          },
          "point_estimate": 1940.8167667002656,
          "standard_error": 1.0295722316383402
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.9493673937007022,
            "upper_bound": 4.536749799632443
          },
          "point_estimate": 2.6752479765229857,
          "standard_error": 0.9273240797055924
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1939.6212204900337,
            "upper_bound": 1941.7361389377245
          },
          "point_estimate": 1940.4783677515557,
          "standard_error": 0.5404418905081185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.3951822770998192,
            "upper_bound": 3.381671351736592
          },
          "point_estimate": 2.6045858976072,
          "standard_error": 0.5298319940321525
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-long-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/rare-long-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-long-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-long-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-long-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 561.1371108733529,
            "upper_bound": 562.0271019351262
          },
          "point_estimate": 561.5993940948163,
          "standard_error": 0.22695462848248643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 561.2121733173028,
            "upper_bound": 562.2536993067636
          },
          "point_estimate": 561.520627634899,
          "standard_error": 0.2584492051174424
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.029244154941936874,
            "upper_bound": 1.3010762259804565
          },
          "point_estimate": 0.7461238138807331,
          "standard_error": 0.31006320438060314
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 561.361056386958,
            "upper_bound": 562.208009328084
          },
          "point_estimate": 561.7451737880813,
          "standard_error": 0.2234790429471037
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.3938203170055541,
            "upper_bound": 0.9982097487989856
          },
          "point_estimate": 0.7567335862126564,
          "standard_error": 0.1608485582853441
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-medium-needle": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/rare-medium-needle",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-medium-needle",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-medium-needle",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-medium-needle"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.91018862864571,
            "upper_bound": 113.09708563489414
          },
          "point_estimate": 112.99695673183462,
          "standard_error": 0.04826177990612896
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.88536904045428,
            "upper_bound": 113.14798477000652
          },
          "point_estimate": 112.93978362106,
          "standard_error": 0.0616372239710188
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.023467460674230545,
            "upper_bound": 0.24989016983223328
          },
          "point_estimate": 0.08762956466177634,
          "standard_error": 0.06004426313766099
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 112.90062007963492,
            "upper_bound": 113.0825033798907
          },
          "point_estimate": 112.98208968026076,
          "standard_error": 0.050421645421438746
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05308658097064792,
            "upper_bound": 0.2068329209722206
          },
          "point_estimate": 0.16135798623120215,
          "standard_error": 0.0362546793842248
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.91868906394386,
            "upper_bound": 37.95560430514585
          },
          "point_estimate": 37.93625897362965,
          "standard_error": 0.009470727853475558
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.9112429092002,
            "upper_bound": 37.958664263412516
          },
          "point_estimate": 37.927861056598985,
          "standard_error": 0.012324895593688974
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004269186824956731,
            "upper_bound": 0.0533951518981664
          },
          "point_estimate": 0.030678828878560188,
          "standard_error": 0.012035229241442036
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 37.908023685597655,
            "upper_bound": 37.94349800469953
          },
          "point_estimate": 37.92717033645694,
          "standard_error": 0.009183786187944822
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.016004781236358845,
            "upper_bound": 0.041444178402700246
          },
          "point_estimate": 0.03156499220689288,
          "standard_error": 0.006758597070391723
        }
      }
    },
    "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.20715537329384,
            "upper_bound": 63.356318622167
          },
          "point_estimate": 63.28333481420985,
          "standard_error": 0.03820935902769229
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.18492097969539,
            "upper_bound": 63.37902783449323
          },
          "point_estimate": 63.306172957495754,
          "standard_error": 0.050899949270701864
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02385246222398009,
            "upper_bound": 0.21244584051619023
          },
          "point_estimate": 0.1449064631794689,
          "standard_error": 0.04980110387499181
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 63.22050229212098,
            "upper_bound": 63.33086302933649
          },
          "point_estimate": 63.26964377534071,
          "standard_error": 0.028270223046554983
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06965566743832474,
            "upper_bound": 0.1635525269217484
          },
          "point_estimate": 0.12742899336752975,
          "standard_error": 0.024110804626164276
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266659.92797010776,
            "upper_bound": 267197.0074314172
          },
          "point_estimate": 266897.31954553357,
          "standard_error": 137.61720615868956
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266530.01824817515,
            "upper_bound": 267034.9912408759
          },
          "point_estimate": 266909.92639902676,
          "standard_error": 168.81166698166095
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 22.763726365950426,
            "upper_bound": 692.9861660182181
          },
          "point_estimate": 377.026255496253,
          "standard_error": 175.0726918267721
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 266559.72818706365,
            "upper_bound": 266877.4466494323
          },
          "point_estimate": 266692.6157550479,
          "standard_error": 81.57958759785734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 197.784996970698,
            "upper_bound": 655.0503777559558
          },
          "point_estimate": 458.62993572907413,
          "standard_error": 141.7949374773493
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.67522209079537,
            "upper_bound": 47.71296839508464
          },
          "point_estimate": 47.69326357086234,
          "standard_error": 0.00967714449658928
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.66146964006896,
            "upper_bound": 47.719218898183136
          },
          "point_estimate": 47.6932344210536,
          "standard_error": 0.014294746074020969
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005031353905317356,
            "upper_bound": 0.05540944949828252
          },
          "point_estimate": 0.04104026206095686,
          "standard_error": 0.013038212513982662
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.67270161671021,
            "upper_bound": 47.715894940468
          },
          "point_estimate": 47.69080669173093,
          "standard_error": 0.010891040070417557
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017930006878712577,
            "upper_bound": 0.04109201874955994
          },
          "point_estimate": 0.03227286893011506,
          "standard_error": 0.00612904970680294
        }
      }
    },
    "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.90679143924108,
            "upper_bound": 84.00205324561736
          },
          "point_estimate": 83.9554333169868,
          "standard_error": 0.02441269058447573
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.87605119304342,
            "upper_bound": 84.0318903208801
          },
          "point_estimate": 83.9655692049443,
          "standard_error": 0.04259568361359757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0155318675016898,
            "upper_bound": 0.14002750157182656
          },
          "point_estimate": 0.0987160763102297,
          "standard_error": 0.031782952468056624
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.92925340564955,
            "upper_bound": 84.01534865821337
          },
          "point_estimate": 83.97909601323582,
          "standard_error": 0.022401982529869226
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05018283703930708,
            "upper_bound": 0.0958397451595276
          },
          "point_estimate": 0.08099863770544623,
          "standard_error": 0.011628266713980854
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117665.31850286738,
            "upper_bound": 117830.72166593705
          },
          "point_estimate": 117747.64870058882,
          "standard_error": 42.35767477601986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117613.3754032258,
            "upper_bound": 117878.90967741936
          },
          "point_estimate": 117733.0440092166,
          "standard_error": 92.67591742094004
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 17.22496207322035,
            "upper_bound": 214.24484288188157
          },
          "point_estimate": 181.7452351604788,
          "standard_error": 59.751070886284325
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 117620.60022983124,
            "upper_bound": 117841.74025091255
          },
          "point_estimate": 117722.37694176791,
          "standard_error": 59.55056428108859
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 97.0224986739912,
            "upper_bound": 158.3223871723669
          },
          "point_estimate": 141.56640378160296,
          "standard_error": 15.695394477293096
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.19565253058518,
            "upper_bound": 47.23879322144557
          },
          "point_estimate": 47.2171880174368,
          "standard_error": 0.01105446649866861
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.18966832840123,
            "upper_bound": 47.25725610213125
          },
          "point_estimate": 47.21019289309777,
          "standard_error": 0.016687975178925857
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008395653100506266,
            "upper_bound": 0.06452760716873548
          },
          "point_estimate": 0.039353163126494936,
          "standard_error": 0.015166935595118596
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.19512126857462,
            "upper_bound": 47.25243339987491
          },
          "point_estimate": 47.22099619159249,
          "standard_error": 0.014531223310537916
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.021846214645718995,
            "upper_bound": 0.04438080165588646
          },
          "point_estimate": 0.03662553637258264,
          "standard_error": 0.0057063303009255145
        }
      }
    },
    "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/huge-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_huge-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.92589643319995,
            "upper_bound": 80.06003190200268
          },
          "point_estimate": 79.99193887016126,
          "standard_error": 0.0343281586835268
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.9053195049243,
            "upper_bound": 80.05745763008423
          },
          "point_estimate": 80.00892258506485,
          "standard_error": 0.04250101149485195
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.013985595766494592,
            "upper_bound": 0.1992818502620287
          },
          "point_estimate": 0.1141142468198194,
          "standard_error": 0.044969802703467406
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.94304929329239,
            "upper_bound": 80.02208946216602
          },
          "point_estimate": 79.99337128709622,
          "standard_error": 0.020346433671974796
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05748265488537806,
            "upper_bound": 0.1520530739592794
          },
          "point_estimate": 0.11431072879247874,
          "standard_error": 0.024492739024540407
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-freq/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-freq_rare-alpha"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.49516706416367,
            "upper_bound": 214.9993899185472
          },
          "point_estimate": 214.73399638156823,
          "standard_error": 0.12946413735236706
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.41265718440545,
            "upper_bound": 215.02959702675264
          },
          "point_estimate": 214.6139003589692,
          "standard_error": 0.18005895299698824
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.09628882653075609,
            "upper_bound": 0.7380448489902779
          },
          "point_estimate": 0.453882992396826,
          "standard_error": 0.16944144960371135
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 214.32997026756365,
            "upper_bound": 214.9333479700908
          },
          "point_estimate": 214.6187245720767,
          "standard_error": 0.15898587625383415
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.22625651377925576,
            "upper_bound": 0.5622882000167347
          },
          "point_estimate": 0.4311557791686117,
          "standard_error": 0.09059916537592748
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 720057,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector-repeated/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector-repeated_rare-a"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.9668742348284,
            "upper_bound": 487.63976641759893
          },
          "point_estimate": 487.30776216051055,
          "standard_error": 0.172033128875415
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.9502407591099,
            "upper_bound": 487.7253424841157
          },
          "point_estimate": 487.2769362751669,
          "standard_error": 0.17950975375310127
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.12176420168002192,
            "upper_bound": 0.993673074232972
          },
          "point_estimate": 0.4159366437778025,
          "standard_error": 0.22682718143362623
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 486.9303867502886,
            "upper_bound": 487.6679240922417
          },
          "point_estimate": 487.22707371446813,
          "standard_error": 0.18850807793725297
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.2850089604968631,
            "upper_bound": 0.758570587477921
          },
          "point_estimate": 0.5749030158149017,
          "standard_error": 0.12033852828761148
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "value_str": null,
        "throughput": {
          "Bytes": 550004,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-defeat-simple-vector/rare-alphabet",
        "directory_name": "memrmem/twoway_oneshot_pathological-defeat-simple-vector_rare-alphabet"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.061913029152265,
            "upper_bound": 19.10962504412889
          },
          "point_estimate": 19.086332703204004,
          "standard_error": 0.012247226119051857
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.05517159966034,
            "upper_bound": 19.12919835459045
          },
          "point_estimate": 19.085198506489483,
          "standard_error": 0.01919272807577305
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.008260994596023547,
            "upper_bound": 0.06976448854342525
          },
          "point_estimate": 0.04753011684467581,
          "standard_error": 0.01582531837166739
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 19.071446281425786,
            "upper_bound": 19.108811690907025
          },
          "point_estimate": 19.088029140560412,
          "standard_error": 0.009565374412922613
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02474250259594573,
            "upper_bound": 0.05088144091163011
          },
          "point_estimate": 0.04081793668646649,
          "standard_error": 0.006826968185321136
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/never-no-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/never-no-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_never-no-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28422.536052984586,
            "upper_bound": 28461.27577362788
          },
          "point_estimate": 28443.182134100894,
          "standard_error": 9.950332733740362
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28420.382455846186,
            "upper_bound": 28471.169405320812
          },
          "point_estimate": 28450.50770735524,
          "standard_error": 13.598190980395092
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 3.824249462621402,
            "upper_bound": 56.58476790387407
          },
          "point_estimate": 33.11473468205039,
          "standard_error": 12.332428938186832
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 28425.676129368807,
            "upper_bound": 28463.840760331408
          },
          "point_estimate": 28445.49625226104,
          "standard_error": 9.84044973783434
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 16.288604833757773,
            "upper_bound": 43.703885993950266
          },
          "point_estimate": 33.22971489830029,
          "standard_error": 7.493741289597972
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-md5-huge/rare-last-hash",
        "directory_name": "memrmem/twoway_oneshot_pathological-md5-huge_rare-last-hash"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.62685916695747,
            "upper_bound": 116.19281922137392
          },
          "point_estimate": 115.88457051653064,
          "standard_error": 0.14518391175442324
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.55499211628324,
            "upper_bound": 116.29065971069632
          },
          "point_estimate": 115.75347656660055,
          "standard_error": 0.15131430942516136
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.06269442839737964,
            "upper_bound": 0.6911107775383202
          },
          "point_estimate": 0.28739914005697564,
          "standard_error": 0.1535881416358926
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.67746307336115,
            "upper_bound": 116.208561614184
          },
          "point_estimate": 115.88697052379482,
          "standard_error": 0.13479341707122125
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.15378132664844246,
            "upper_bound": 0.609939687518422
          },
          "point_estimate": 0.48332978852715414,
          "standard_error": 0.1170163294682928
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-huge/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-huge_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963506.2946052632,
            "upper_bound": 965471.1724269006
          },
          "point_estimate": 964443.8314327484,
          "standard_error": 503.2204812564856
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963068.9263157896,
            "upper_bound": 965743.66374269
          },
          "point_estimate": 964343.7763157894,
          "standard_error": 609.6411672790276
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 332.6837293568056,
            "upper_bound": 3050.322644530249
          },
          "point_estimate": 1418.4111979760685,
          "standard_error": 713.0212852684951
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 963408.618225778,
            "upper_bound": 965566.2147781828
          },
          "point_estimate": 964619.3583732058,
          "standard_error": 546.269930298381
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 850.4756525739443,
            "upper_bound": 2121.8707889876905
          },
          "point_estimate": 1667.9053621430942,
          "standard_error": 331.3413017192398
        }
      }
    },
    "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/pathological-repeated-rare-small/never-tricky",
        "directory_name": "memrmem/twoway_oneshot_pathological-repeated-rare-small_never-tricky"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1974.4561850647392,
            "upper_bound": 1977.685397515303
          },
          "point_estimate": 1975.9017099669015,
          "standard_error": 0.8325263973874112
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1974.1406183973825,
            "upper_bound": 1977.314211398457
          },
          "point_estimate": 1975.2570303162013,
          "standard_error": 0.8206287113929301
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.27465330790241266,
            "upper_bound": 4.01370888995094
          },
          "point_estimate": 1.6209756132226942,
          "standard_error": 0.9812344581806496
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1974.013708314851,
            "upper_bound": 1975.728228131819
          },
          "point_estimate": 1974.9544383466148,
          "standard_error": 0.4400359716760076
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.965306844595236,
            "upper_bound": 3.846490597181893
          },
          "point_estimate": 2.7748913091091856,
          "standard_error": 0.7979303850766568
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-all-common-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-all-common-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-all-common-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.61044726730148,
            "upper_bound": 30.626604248061152
          },
          "point_estimate": 30.61853151096576,
          "standard_error": 0.004135479818499958
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.603655365505222,
            "upper_bound": 30.63052737441351
          },
          "point_estimate": 30.61964512847677,
          "standard_error": 0.006809270400811069
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.002797211366824804,
            "upper_bound": 0.02422973027639178
          },
          "point_estimate": 0.01962772774798622,
          "standard_error": 0.005394627089521566
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 30.607218652749843,
            "upper_bound": 30.626946863196093
          },
          "point_estimate": 30.616191172287408,
          "standard_error": 0.00499050344187404
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.00856554733817456,
            "upper_bound": 0.01635544150410196
          },
          "point_estimate": 0.013782428936881651,
          "standard_error": 0.0019673172305888493
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-en/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.749297887210126,
            "upper_bound": 33.79431305184535
          },
          "point_estimate": 33.76983933501869,
          "standard_error": 0.01152883431156504
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.7513752808592,
            "upper_bound": 33.776918477754414
          },
          "point_estimate": 33.771071130554695,
          "standard_error": 0.006770479177459448
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0013601943634043835,
            "upper_bound": 0.05212636428028259
          },
          "point_estimate": 0.009037935698641896,
          "standard_error": 0.013166181122587047
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 33.74530667446287,
            "upper_bound": 33.77323508858822
          },
          "point_estimate": 33.76158712839956,
          "standard_error": 0.007070870224295807
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.009403081269494249,
            "upper_bound": 0.05555438384664417
          },
          "point_estimate": 0.03824026473552904,
          "standard_error": 0.012342079722358552
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-some-rare-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-some-rare-bytes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-some-rare-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.70542516113205,
            "upper_bound": 23.736137373222753
          },
          "point_estimate": 23.72143391707646,
          "standard_error": 0.00785606462081583
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.703439651464755,
            "upper_bound": 23.741141193311503
          },
          "point_estimate": 23.726509922742057,
          "standard_error": 0.010867588810796537
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004701986885990474,
            "upper_bound": 0.046382811020362354
          },
          "point_estimate": 0.03151781781130868,
          "standard_error": 0.010140397177214565
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 23.708039609016957,
            "upper_bound": 23.730854235400365
          },
          "point_estimate": 23.719565060235116,
          "standard_error": 0.005757736226556347
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.014011521543652056,
            "upper_bound": 0.0339892169981867
          },
          "point_estimate": 0.02604855251188763,
          "standard_error": 0.005321456817251002
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/never-two-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-en/never-two-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/never-two-space",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/never-two-space",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_never-two-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.395324217818047,
            "upper_bound": 27.437627823746432
          },
          "point_estimate": 27.415512247060292,
          "standard_error": 0.010848586746829947
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.382222404923674,
            "upper_bound": 27.435982523193665
          },
          "point_estimate": 27.41916642073276,
          "standard_error": 0.013717714346170178
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0024111621192330675,
            "upper_bound": 0.06408663607818602
          },
          "point_estimate": 0.042801626893435904,
          "standard_error": 0.015156497340002772
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 27.388269812819797,
            "upper_bound": 27.426248275917267
          },
          "point_estimate": 27.406117910080532,
          "standard_error": 0.009850128797602364
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.019220580554434766,
            "upper_bound": 0.04741758183103567
          },
          "point_estimate": 0.03611365782374424,
          "standard_error": 0.007708334114353833
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-en/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.70970649247663,
            "upper_bound": 34.7471582954956
          },
          "point_estimate": 34.728996617661835,
          "standard_error": 0.009595730051566413
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.703594870732836,
            "upper_bound": 34.75427967145026
          },
          "point_estimate": 34.72798585853573,
          "standard_error": 0.012168065954063324
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.004377016047244411,
            "upper_bound": 0.05338386398285862
          },
          "point_estimate": 0.0366226413999091,
          "standard_error": 0.011927220653718614
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 34.71638727497848,
            "upper_bound": 34.75303166950631
          },
          "point_estimate": 34.73224193790457,
          "standard_error": 0.009291987203108908
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.017109765165821075,
            "upper_bound": 0.04159040205800577
          },
          "point_estimate": 0.03197349785073921,
          "standard_error": 0.006432914082899613
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 28,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-en/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-en_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.6921188181474,
            "upper_bound": 56.74824746759111
          },
          "point_estimate": 56.719640667775046,
          "standard_error": 0.014351542339948757
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.67910432074541,
            "upper_bound": 56.74908653703336
          },
          "point_estimate": 56.720755969942175,
          "standard_error": 0.018247610663162057
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005774046187949817,
            "upper_bound": 0.08412111912535782
          },
          "point_estimate": 0.04311540027098158,
          "standard_error": 0.018262537565641414
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 56.693937932705275,
            "upper_bound": 56.73848102491629
          },
          "point_estimate": 56.71954663233293,
          "standard_error": 0.011610553871325185
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0244173065517912,
            "upper_bound": 0.06399195738323854
          },
          "point_estimate": 0.04799667303776099,
          "standard_error": 0.010451399596891236
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-ru/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.41171354657008,
            "upper_bound": 66.48119152539408
          },
          "point_estimate": 66.4472917823322,
          "standard_error": 0.017824738519292643
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.39857392443722,
            "upper_bound": 66.49437516335331
          },
          "point_estimate": 66.46017532535973,
          "standard_error": 0.028848619822979007
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.010837100517314296,
            "upper_bound": 0.0983869082394546
          },
          "point_estimate": 0.07266576296575851,
          "standard_error": 0.021876202202460823
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 66.38588958601719,
            "upper_bound": 66.46738364547701
          },
          "point_estimate": 66.41739971623673,
          "standard_error": 0.020499669173915783
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03539441582504036,
            "upper_bound": 0.07365398956779531
          },
          "point_estimate": 0.05952373613985464,
          "standard_error": 0.009928681744860396
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.67211714307868,
            "upper_bound": 47.74325395002144
          },
          "point_estimate": 47.7072879502411,
          "standard_error": 0.01822132222827457
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.66086409722131,
            "upper_bound": 47.7554725366892
          },
          "point_estimate": 47.70408470607765,
          "standard_error": 0.021291571166373927
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.01228670540520568,
            "upper_bound": 0.10358433629513554
          },
          "point_estimate": 0.06193117434967619,
          "standard_error": 0.02560507514167658
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.66606352229025,
            "upper_bound": 47.72194017816528
          },
          "point_estimate": 47.69482687414935,
          "standard_error": 0.013992929715229923
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.03276880392237318,
            "upper_bound": 0.07800679019538616
          },
          "point_estimate": 0.06060893229509017,
          "standard_error": 0.01163314161663296
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 42,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-ru/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-ru_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.88062888781609,
            "upper_bound": 84.0063170122879
          },
          "point_estimate": 83.94379192087715,
          "standard_error": 0.03204084342353243
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.85324180050495,
            "upper_bound": 84.03264812680314
          },
          "point_estimate": 83.95461538689989,
          "standard_error": 0.041228716708513746
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012940982069647682,
            "upper_bound": 0.20033864890571343
          },
          "point_estimate": 0.10429326078403736,
          "standard_error": 0.050777062888221064
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 83.90279839542207,
            "upper_bound": 84.01182328375585
          },
          "point_estimate": 83.95599985972731,
          "standard_error": 0.02784647015308942
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.061524091184749816,
            "upper_bound": 0.13278436866469576
          },
          "point_estimate": 0.10706550677366852,
          "standard_error": 0.018270959401374187
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/never-john-watson": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-zh/never-john-watson",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/never-john-watson",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/never-john-watson",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_never-john-watson"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.23281646475176,
            "upper_bound": 45.286891020176384
          },
          "point_estimate": 45.25722507074825,
          "standard_error": 0.01392501841209165
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.224959102342034,
            "upper_bound": 45.28964960019014
          },
          "point_estimate": 45.243897579986864,
          "standard_error": 0.01319637467236346
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.005797125257319891,
            "upper_bound": 0.06327526600762123
          },
          "point_estimate": 0.025739377890338925,
          "standard_error": 0.014149868873658204
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 45.232640001103846,
            "upper_bound": 45.24857362850437
          },
          "point_estimate": 45.24065116772451,
          "standard_error": 0.003973984708132072
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.012749551024168105,
            "upper_bound": 0.05948544648981835
          },
          "point_estimate": 0.04640816981179746,
          "standard_error": 0.012374581936568778
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.17536998952794,
            "upper_bound": 47.216147545408354
          },
          "point_estimate": 47.19498102964992,
          "standard_error": 0.010484221419315794
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.163072215390486,
            "upper_bound": 47.22302250069394
          },
          "point_estimate": 47.19106006716885,
          "standard_error": 0.019284467888747815
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.0026755564069247764,
            "upper_bound": 0.06700578628058856
          },
          "point_estimate": 0.03973630425012889,
          "standard_error": 0.018347625944683185
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 47.1702000885065,
            "upper_bound": 47.22333049358098
          },
          "point_estimate": 47.19261261787847,
          "standard_error": 0.013588521715457252
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02076847654892833,
            "upper_bound": 0.04268401840996554
          },
          "point_estimate": 0.03492149188025756,
          "standard_error": 0.0057557834457238
        }
      }
    },
    "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "value_str": null,
        "throughput": {
          "Bytes": 31,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshot/teeny-zh/rare-sherlock-holmes",
        "directory_name": "memrmem/twoway_oneshot_teeny-zh_rare-sherlock-holmes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.92561646918821,
            "upper_bound": 80.03601045859199
          },
          "point_estimate": 79.98235461456065,
          "standard_error": 0.028338851986500373
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.89562504245245,
            "upper_bound": 80.06054047717646
          },
          "point_estimate": 80.00597581348629,
          "standard_error": 0.045181490671606414
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.02742316014443607,
            "upper_bound": 0.1547818625158464
          },
          "point_estimate": 0.11120428336752286,
          "standard_error": 0.034689914552125806
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 79.9633231348506,
            "upper_bound": 80.04913768202302
          },
          "point_estimate": 80.01449107671245,
          "standard_error": 0.021804059058192105
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.05552381028709018,
            "upper_bound": 0.11412746271130685
          },
          "point_estimate": 0.09444932045258532,
          "standard_error": 0.014820614970057783
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/code-rust-library/common-fn",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 962391.5362426902,
            "upper_bound": 963337.7588523392
          },
          "point_estimate": 962860.0887896826,
          "standard_error": 243.05764261150892
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 962176.1398026316,
            "upper_bound": 963668.4157894736
          },
          "point_estimate": 962656.4788011695,
          "standard_error": 459.84995733613584
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 142.47834516794484,
            "upper_bound": 1250.6699902302537
          },
          "point_estimate": 1127.2647611275042,
          "standard_error": 307.25225649350233
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 962572.4868888828,
            "upper_bound": 963605.1492702344
          },
          "point_estimate": 963099.9912508545,
          "standard_error": 266.16309033976734
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 526.4935441910698,
            "upper_bound": 939.5087611005636
          },
          "point_estimate": 807.8313165540129,
          "standard_error": 106.2491939767516
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-fn-is-empty",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-fn-is-empty"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1231421.3284208332,
            "upper_bound": 1235039.0276554232
          },
          "point_estimate": 1233243.490239418,
          "standard_error": 928.6604452746144
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230480.5577777778,
            "upper_bound": 1235583.1333333333
          },
          "point_estimate": 1233323.1324074073,
          "standard_error": 1517.2566118853708
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 283.3425049697866,
            "upper_bound": 5014.873819301718
          },
          "point_estimate": 3155.129436763244,
          "standard_error": 1335.3828475510993
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1230745.339195544,
            "upper_bound": 1235024.87504363
          },
          "point_estimate": 1232701.307965368,
          "standard_error": 1121.54270704823
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1849.1607558575563,
            "upper_bound": 3846.262984628965
          },
          "point_estimate": 3102.528355614468,
          "standard_error": 510.8426130741228
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-let": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/code-rust-library/common-let",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-let",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-let",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-let"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1293593.4130387933,
            "upper_bound": 1296150.6537869456
          },
          "point_estimate": 1294840.547865353,
          "standard_error": 649.36220916101
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1293808.638505747,
            "upper_bound": 1296179.9310344828
          },
          "point_estimate": 1294585.552545156,
          "standard_error": 515.5753474657189
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 215.22409617895096,
            "upper_bound": 3594.346358601336
          },
          "point_estimate": 1177.4156549585175,
          "standard_error": 854.9439276020317
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1293808.570446079,
            "upper_bound": 1295281.173504157
          },
          "point_estimate": 1294542.033049709,
          "standard_error": 371.63000978562343
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 787.135522100236,
            "upper_bound": 2992.0744515068077
          },
          "point_estimate": 2161.5523247850415,
          "standard_error": 577.0761135565654
        }
      }
    },
    "memrmem/twoway/oneshotiter/code-rust-library/common-paren": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/code-rust-library/common-paren",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/code-rust-library/common-paren",
        "value_str": null,
        "throughput": {
          "Bytes": 1648109,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/code-rust-library/common-paren",
        "directory_name": "memrmem/twoway_oneshotiter_code-rust-library_common-paren"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 372400.028712828,
            "upper_bound": 373373.21800546657
          },
          "point_estimate": 372896.1123906706,
          "standard_error": 249.6684947866122
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371984.9030612245,
            "upper_bound": 373598.6173469387
          },
          "point_estimate": 372973.5772594753,
          "standard_error": 372.8082566886684
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 147.0988795313018,
            "upper_bound": 1448.1072636786405
          },
          "point_estimate": 1131.4761028714138,
          "standard_error": 361.582057426313
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 371947.1404132151,
            "upper_bound": 373116.3612721622
          },
          "point_estimate": 372339.32011661807,
          "standard_error": 297.0851503942399
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 471.2695767790348,
            "upper_bound": 994.97751076642
          },
          "point_estimate": 831.9001749188001,
          "standard_error": 127.74806253256077
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-en/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 551485.3068572631,
            "upper_bound": 552198.9047811448
          },
          "point_estimate": 551832.9825354738,
          "standard_error": 182.88751721016192
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 551206.2545454545,
            "upper_bound": 552100.5521885522
          },
          "point_estimate": 551963.5928030303,
          "standard_error": 233.2099792029469
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 49.19676674018255,
            "upper_bound": 1245.2101655068038
          },
          "point_estimate": 362.29801356797054,
          "standard_error": 301.0542158615178
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 551508.6442847005,
            "upper_bound": 552073.2173998045
          },
          "point_estimate": 551804.4946084219,
          "standard_error": 141.75355669922268
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 317.5084817751401,
            "upper_bound": 811.5514722761071
          },
          "point_estimate": 609.6113412882049,
          "standard_error": 134.436553120932
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-en/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383862.82732274424,
            "upper_bound": 384712.4744210526
          },
          "point_estimate": 384236.3782293233,
          "standard_error": 219.6796232004498
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383739.6438157895,
            "upper_bound": 384571.2442982456
          },
          "point_estimate": 384039.7031578948,
          "standard_error": 194.3535521225639
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 12.38517199062067,
            "upper_bound": 1016.8240250003688
          },
          "point_estimate": 460.3303849853915,
          "standard_error": 234.8962539754158
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 383762.14240569714,
            "upper_bound": 384197.2601503759
          },
          "point_estimate": 383925.0667395762,
          "standard_error": 112.42137850321753
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191.221981057106,
            "upper_bound": 1000.6311428243998
          },
          "point_estimate": 731.1425760248004,
          "standard_error": 218.09664483304084
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-en/common-you": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-en/common-you",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-en/common-you",
        "value_str": null,
        "throughput": {
          "Bytes": 613345,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-en/common-you",
        "directory_name": "memrmem/twoway_oneshotiter_huge-en_common-you"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 480018.25170578784,
            "upper_bound": 480757.0951649958
          },
          "point_estimate": 480356.42835578526,
          "standard_error": 190.01207594260484
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479845.8894736842,
            "upper_bound": 480744.57236842107
          },
          "point_estimate": 480211.68452380947,
          "standard_error": 246.91320220942129
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 115.04282181719634,
            "upper_bound": 1030.881131917494
          },
          "point_estimate": 560.1016181153586,
          "standard_error": 233.43723874030957
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 479854.614264067,
            "upper_bound": 480575.8007909455
          },
          "point_estimate": 480201.61514012306,
          "standard_error": 191.46670067310345
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 289.3052187955559,
            "upper_bound": 855.3587751690712
          },
          "point_estimate": 634.7561964271291,
          "standard_error": 158.34847550423538
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-ru/common-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1016521.3073070988,
            "upper_bound": 1018206.5800864196
          },
          "point_estimate": 1017354.056705247,
          "standard_error": 432.0696190110421
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1016123.1416666666,
            "upper_bound": 1018767.2191358024
          },
          "point_estimate": 1017248.0300925926,
          "standard_error": 780.5577903334757
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 387.9016914466691,
            "upper_bound": 2347.6105733215577
          },
          "point_estimate": 1844.058533650276,
          "standard_error": 522.522530368752
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1015953.3108154366,
            "upper_bound": 1018008.1073804572
          },
          "point_estimate": 1016810.7645743146,
          "standard_error": 543.1362255171257
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 929.7041556016252,
            "upper_bound": 1695.9655685550065
          },
          "point_estimate": 1439.8996156179817,
          "standard_error": 195.24026852827464
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-ru/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 295068.4239925755,
            "upper_bound": 295880.947437276
          },
          "point_estimate": 295426.06893209164,
          "standard_error": 209.36983347305852
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294958.72904665896,
            "upper_bound": 295680.42439516133
          },
          "point_estimate": 295210.7851702509,
          "standard_error": 169.35519875728357
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 39.70662782331313,
            "upper_bound": 952.3228137380664
          },
          "point_estimate": 402.11830596580296,
          "standard_error": 268.7028990322547
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 294863.20510105236,
            "upper_bound": 295343.1105917174
          },
          "point_estimate": 295066.8393590281,
          "standard_error": 125.21111857115984
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 231.3734120454937,
            "upper_bound": 969.1492191298552
          },
          "point_estimate": 693.9553801703521,
          "standard_error": 206.83040224811927
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-ru/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-ru/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-ru/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613402,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-ru/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-ru_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 751028.5508010203,
            "upper_bound": 752924.5267397959
          },
          "point_estimate": 751983.6895918369,
          "standard_error": 483.0927486098568
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 750771.4353741497,
            "upper_bound": 752633.9
          },
          "point_estimate": 752430.0612244897,
          "standard_error": 502.72310637065704
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 70.43635803535246,
            "upper_bound": 2761.290005262987
          },
          "point_estimate": 663.0222382289817,
          "standard_error": 770.1470239501974
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 751894.3945532449,
            "upper_bound": 753425.6200816326
          },
          "point_estimate": 752649.7139146568,
          "standard_error": 382.0540121878946
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 644.5396066596528,
            "upper_bound": 2204.3201388750444
          },
          "point_estimate": 1616.9718995465498,
          "standard_error": 398.6788338187882
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-do-not": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-zh/common-do-not",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-do-not",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-do-not",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-do-not"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369605.0668197279,
            "upper_bound": 370492.6670428207
          },
          "point_estimate": 370042.8888398931,
          "standard_error": 226.59033262376136
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369544.2474489796,
            "upper_bound": 370480.6081632653
          },
          "point_estimate": 370047.7261904762,
          "standard_error": 242.03520856958008
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160.28144817486222,
            "upper_bound": 1269.8287231702425
          },
          "point_estimate": 606.84724203245,
          "standard_error": 269.9949421326144
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 369731.2473275024,
            "upper_bound": 370350.9876418173
          },
          "point_estimate": 370080.08014842303,
          "standard_error": 159.85300096370003
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 351.0236621890322,
            "upper_bound": 1019.34023855205
          },
          "point_estimate": 754.7009096212504,
          "standard_error": 172.09739167887665
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-one-space": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-zh/common-one-space",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-one-space",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-one-space",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-one-space"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160025.95735867447,
            "upper_bound": 160265.6972015455
          },
          "point_estimate": 160135.08659130466,
          "standard_error": 61.887545225275986
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 159994.9721247563,
            "upper_bound": 160234.8392857143
          },
          "point_estimate": 160110.50438596492,
          "standard_error": 58.03137730754843
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 20.911162260345534,
            "upper_bound": 313.4258250203713
          },
          "point_estimate": 150.37696516653406,
          "standard_error": 72.30547500253653
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 160054.50047031543,
            "upper_bound": 160370.28560808802
          },
          "point_estimate": 160190.39681020734,
          "standard_error": 82.47203316894598
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 77.49930239307328,
            "upper_bound": 282.65671525530297
          },
          "point_estimate": 206.03950954945353,
          "standard_error": 56.82898419416834
        }
      }
    },
    "memrmem/twoway/oneshotiter/huge-zh/common-that": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/huge-zh/common-that",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/huge-zh/common-that",
        "value_str": null,
        "throughput": {
          "Bytes": 613427,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/huge-zh/common-that",
        "directory_name": "memrmem/twoway_oneshotiter_huge-zh_common-that"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191903.30777083337,
            "upper_bound": 192193.6427180451
          },
          "point_estimate": 192051.7235551378,
          "standard_error": 74.2563926982016
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 191798.8368421053,
            "upper_bound": 192285.65037593985
          },
          "point_estimate": 192097.03004385965,
          "standard_error": 103.77671537554544
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 8.179985249505958,
            "upper_bound": 416.9790317024201
          },
          "point_estimate": 316.80100555987394,
          "standard_error": 115.50229224269344
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 192009.5998297642,
            "upper_bound": 192190.82777553264
          },
          "point_estimate": 192095.2387286398,
          "standard_error": 45.71929007333813
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 139.53881562747867,
            "upper_bound": 301.1616935570563
          },
          "point_estimate": 248.00590507718127,
          "standard_error": 39.694199093866544
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "value_str": null,
        "throughput": {
          "Bytes": 151305,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-md5-huge/common-two-bytes",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-md5-huge_common-two-bytes"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152697.7671647659,
            "upper_bound": 152963.81848739492
          },
          "point_estimate": 152818.06537214882,
          "standard_error": 68.46735608198809
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152673.69663865547,
            "upper_bound": 152956.69747899158
          },
          "point_estimate": 152746.25900360144,
          "standard_error": 75.00576359086256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 6.811238702573263,
            "upper_bound": 343.83393360161216
          },
          "point_estimate": 109.5924373820669,
          "standard_error": 85.75024875389022
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 152696.4083048034,
            "upper_bound": 152895.60861437087
          },
          "point_estimate": 152762.62645421806,
          "standard_error": 51.26713964567089
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 88.12961449051052,
            "upper_bound": 311.84612680389716
          },
          "point_estimate": 228.42318744780263,
          "standard_error": 61.84650857290792
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 500100,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-huge/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-huge_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2128420.137751543,
            "upper_bound": 2130869.47368287
          },
          "point_estimate": 2129651.867330247,
          "standard_error": 626.5624579681828
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2128127.451388889,
            "upper_bound": 2131255.685185185
          },
          "point_estimate": 2129787.938271605,
          "standard_error": 837.2327135797256
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 111.53681968648462,
            "upper_bound": 3432.164027955388
          },
          "point_estimate": 2318.959672024591,
          "standard_error": 897.4123545931046
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 2127971.4391977284,
            "upper_bound": 2130465.3335148874
          },
          "point_estimate": 2129316.3366522365,
          "standard_error": 623.3424567232221
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1164.605463487006,
            "upper_bound": 2657.674640253776
          },
          "point_estimate": 2091.98088512886,
          "standard_error": 381.12573008542006
        }
      }
    },
    "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match": {
      "baseline": "2021-05-06",
      "fullname": "2021-05-06/memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
      "criterion_benchmark_v1": {
        "group_id": "memrmem",
        "function_id": "twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "value_str": null,
        "throughput": {
          "Bytes": 1001,
          "Elements": null
        },
        "full_id": "memrmem/twoway/oneshotiter/pathological-repeated-rare-small/common-match",
        "directory_name": "memrmem/twoway_oneshotiter_pathological-repeated-rare-small_common-match"
      },
      "criterion_estimates_v1": {
        "mean": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4298.5723534371355,
            "upper_bound": 4304.206816936659
          },
          "point_estimate": 4301.100791046583,
          "standard_error": 1.454979782800602
        },
        "median": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4298.188691813488,
            "upper_bound": 4303.321470801871
          },
          "point_estimate": 4299.7261661936855,
          "standard_error": 1.225838137559026
        },
        "median_abs_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 0.6475402332712892,
            "upper_bound": 6.851086238780268
          },
          "point_estimate": 2.9498931588738944,
          "standard_error": 1.6397307865253965
        },
        "slope": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 4297.9329242797685,
            "upper_bound": 4300.059127054025
          },
          "point_estimate": 4298.963162768413,
          "standard_error": 0.5321941829728256
        },
        "std_dev": {
          "confidence_interval": {
            "confidence_level": 0.95,
            "lower_bound": 1.6576821167209403,
            "upper_bound": 6.689606041587362
          },
          "point_estimate": 4.848276217642698,
          "standard_error": 1.3962268600234562
        }
      }
    }
  }
}
